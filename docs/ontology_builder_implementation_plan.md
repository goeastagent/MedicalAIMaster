# Ontology Builder Node êµ¬í˜„ ê³„íš ë° ì§„í–‰ ìƒí™©

**ì‘ì„±ì¼:** 2025-12-16 (ìµœì¢… ì—…ë°ì´íŠ¸: 2025-12-17)  
**ê¸°ë°˜ ë¬¸ì„œ:** `ontology_and_multilevel_anchor_analysis.md`  
**ëª©ì :** "í…Œì´ë¸” ê°„ ì¡±ë³´ ê·¸ë¦¬ê¸°" ëŠ¥ë ¥ì„ ê°€ì§„ ë²”ìš© ì¸ë±ì‹± ì‹œìŠ¤í…œ êµ¬í˜„

**ğŸ“Š í˜„ì¬ ìƒíƒœ: Phase 0-2 ì™„ë£Œ, Phase 3 ê³„íš ìˆ˜ë¦½ ì™„ë£Œ (85% êµ¬í˜„)**

| Phase | ìƒíƒœ | ì™„ë£Œì¼ | ë‹¬ì„±ë¥  | ë¹„ê³  |
|-------|------|--------|--------|------|
| Phase 0: ê¸°ë°˜ êµ¬ì¡° | âœ… ì™„ë£Œ | 2025-12-17 | 100% | State, Cache, Manager |
| Phase 1: ë©”íƒ€ë°ì´í„° íŒŒì‹± | âœ… ì™„ë£Œ | 2025-12-17 | 100% | 310ê°œ ìš©ì–´ ì¶”ì¶œ |
| Phase 2: ê´€ê³„ ì¶”ë¡  | âœ… ì™„ë£Œ | 2025-12-17 | 100% | FK ë°œê²¬, ê³„ì¸µ ìƒì„± |
| Phase 3: DB + VectorDB | ğŸ”œ ê³„íš ì™„ë£Œ | - | 0% | **ì „ë¬¸ê°€ ê²€í†  ì™„ë£Œ** |
| Phase 4: ê³ ê¸‰ ê¸°ëŠ¥ | ğŸ”œ í–¥í›„ | - | 0% | Re-ranking, ìµœì í™” |

**ì „ë¬¸ê°€ ê²€í† :** 2ì°¨ ì™„ë£Œ (2025-12-17)
- âœ… 1ì°¨ ê²€í†  (2025-12-16): Rule/LLM ì—­í•  ë¶„ë‹´, Negative Evidence, Context Window
- âœ… **2ì°¨ ê²€í†  (2025-12-17)**: Phase 3 ë³‘ëª© í•´ê²°, VectorDB í™•ì¥ì„±

**í•œ ì¤„ ìš”ì•½:**  
_Ruleì´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ë©´(unique values, í†µê³„, ê³µí†µ ì»¬ëŸ¼), LLMì´ ì˜ë¯¸ë¥¼ íŒë‹¨í•œë‹¤(PKì¸ê°€? FKì¸ê°€? ë©”íƒ€ë°ì´í„°ì¸ê°€?)_

**ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (VitalDB 5ê°œ íŒŒì¼):**
- âœ… ë©”íƒ€ë°ì´í„° ê°ì§€: 100% ì •í™• (3/3 íŒŒì¼ ìë™ ìŠ¤í‚µ)
- âœ… ìš©ì–´ ì¶”ì¶œ: 310ê°œ
- âœ… ê´€ê³„ ë°œê²¬: 1ê°œ (lab_data â†’ clinical_data)
- âœ… ê³„ì¸µ ìƒì„±: 3ë ˆë²¨ (Patient > Case > Lab)
- âœ… LLM ìºì‹œ: 83% Hit Rate ($0.30 ì ˆì•½)

**ì˜ˆì‹œ:**
```python
# caseid ì»¬ëŸ¼ ë¶„ì„
unique_vals = [1, 2, 3, 4, 5, ...]  # â† Ruleë¡œ ì¶”ì¶œ
ratio = 0.45                         # â† Ruleë¡œ ê³„ì‚°

prompt = f"unique_vals={unique_vals}, ratio={ratio}, ì´ê²Œ PKì•¼ FKì•¼?"
llm_result = {"role": "FK", "confidence": 0.92}  # â† LLM íŒë‹¨
```

**í•µì‹¬ ë°©ë²•ë¡ :** 
- âŒ **Rule-based íŒ¨í„´ ë§¤ì¹­ ì œê±°** (í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸, íœ´ë¦¬ìŠ¤í‹± ë“±)
- âœ… **LLM ê¸°ë°˜ ì¢…í•© íŒë‹¨** (íŒŒì¼ëª… + êµ¬ì¡° + ë‚´ìš©)
- âœ… **Confidence-driven Decision** (í™•ì‹ ë„ ê¸°ë°˜ Human-in-the-Loop)
- âœ… **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¨í†¨ë¡œì§€** (Git ë²„ì „ ê´€ë¦¬)

**ì„¤ê³„ ì² í•™: "Rule Prepares, LLM Decides"**
```
ì›ì¹™ 1: Ruleì€ ë°ì´í„° ì „ì²˜ë¦¬ (í†µê³„, unique values ì¶”ì¶œ, ì»¬ëŸ¼ íŒŒì‹±)
ì›ì¹™ 2: LLMì€ ìµœì¢… íŒë‹¨ (Ruleì´ ì •ë¦¬í•œ ì •ë³´ë¥¼ ë³´ê³  ì¶”ë¡ )
ì›ì¹™ 3: í™•ì‹ ë„ë¡œ ë¶ˆí™•ì‹¤ì„± í‘œí˜„ (ì´ì§„ íŒë‹¨ ê¸ˆì§€)
ì›ì¹™ 4: Humanì€ ìµœì¢… ê²€ì¦ì (LLMì´ ëª¨ë¥´ë©´ ë¬¼ì–´ë´„)

ì—­í•  ë¶„ë‹´:
- Rule: "ë¬´ì—‡ì´ ìˆëŠ”ê°€?" (What) - ë°ì´í„° ìˆ˜ì§‘/ì •ë¦¬
- LLM: "ê·¸ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?" (Meaning) - í•´ì„/íŒë‹¨

êµ¬ì²´ì  ì˜ˆì‹œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ íŒŒì¼: lab_data.csv                               â”‚
â”‚ ì»¬ëŸ¼: [caseid, dt, name, result]                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ruleì˜ ì‘ì—…:                                     â”‚
â”‚ â€¢ caseid unique values: [1,2,3,4,5,...]         â”‚
â”‚ â€¢ caseid uniqueness_ratio: 0.45 (ë°˜ë³µë¨)        â”‚
â”‚ â€¢ ê³µí†µ ì»¬ëŸ¼ ë°œê²¬: caseid âˆˆ clinical_data        â”‚
â”‚ â€¢ íŒŒì¼ëª… íŒŒì‹±: parts=['lab','data']             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLMì˜ íŒë‹¨:                                      â”‚
â”‚ "caseidê°€ ë°˜ë³µë˜ê³ (0.45), clinical_dataì—ë„     â”‚
â”‚  ìˆìœ¼ë©°, ê°’ì´ [1,2,3,...]ìœ¼ë¡œ ID íŒ¨í„´           â”‚
â”‚  â†’ FKë‹¤! (confidence: 0.92)"                    â”‚
â”‚                                                  â”‚
â”‚ "íŒŒì¼ëª…ì´ 'lab_data'ì´ê³  'lab_parameters'ì™€     â”‚
â”‚  base_name ê°™ìŒ â†’ ê´€ë ¨ í…Œì´ë¸”! (confidence:0.9)"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 0. ì „ë¬¸ê°€ ê²€í†  ì˜ê²¬ ë°˜ì˜ (Expert Review)

### âœ… ê²€í†  ê²°ê³¼: í”„ë ˆì„ì›Œí¬ ë…¼ë¦¬ì  ê²°í•¨ ì—†ìŒ

**ì¸ìƒì ì¸ ë¶€ë¶„:**
1. âœ… **ì—­í•  ë¶„ë‹´ì˜ ëª…í™•ì„±**: Rule(Fact Collection) â†” LLM(Judgment)
2. âœ… **íŒŒì¼ëª… ìš°ì„  ì „ëµ**: ë°ì´í„° ì‘ì„±ìì˜ ì˜ë„ë¥¼ ìµœëŒ€í•œ í™œìš©
3. âœ… **ì ì§„ì  ì§€ì‹ êµ¬ì¶•**: íŒŒì¼ í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ë©° ì˜¨í†¨ë¡œì§€ ì„±ì¥

---

### ğŸ’¡ ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜ ì‚¬í•­

#### 1. **Negative Evidence (ë¶€ì • ì¦ê±°) í™œìš©**

**ë¬¸ì œ ì¸ì‹:**
- ê¸°ì¡´: ê¸ì •ì  íŒíŠ¸ë§Œ ì œê³µ (uniqueness_ratio=0.99 â†’ PK!)
- ê°œì„ : ë¶€ì •ì  íŒíŠ¸ë„ ì œê³µ (BUT 1% ì¤‘ë³µ ìˆìŒ â†’ ë°ì´í„° ì˜¤ë¥˜?)

**êµ¬í˜„:**
```python
# Ruleë¡œ ìˆ˜ì§‘
negative = {
    "issues": ["99% unique BUT 1% duplicates"],
    "null_ratio": 0.05  # 5% ê²°ì¸¡
}

# LLM í”„ë¡¬í”„íŠ¸
"Positive: looks like PK
 Negative: has duplicates + 5% nulls
 â†’ ì§„ì§œ PKì¸ê°€? ë°ì´í„° í’ˆì§ˆ ë¬¸ì œì¸ê°€?"
```

---

#### 2. **Context Window ê´€ë¦¬**

**ë¬¸ì œ ì¸ì‹:**
- ê¸°ì¡´: unique_values 20ê°œ ë¬´ì¡°ê±´ ì œê³µ
- ê°œì„ : ê°’ì´ ê¸´ í…ìŠ¤íŠ¸ë©´ ìš”ì•½ (í† í° ì ˆì•½ + í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)

**êµ¬í˜„:**
```python
# Ruleë¡œ ìš”ì•½
values = ["This is a very long clinical note about...", "Another long text..."]
summarized = ["[Text: 150 chars]", "[Text: 200 chars]"]  # ë©”íƒ€ ì •ë³´ë¡œ ëŒ€ì²´

# LLMì—ê²Œ
"unique_values: ['[Text: 150 chars]', '[Text: 200 chars]']
 â†’ ê¸´ í…ìŠ¤íŠ¸ í•„ë“œì„ â†’ ì„¤ëª…ë¬¸ì¼ ê°€ëŠ¥ì„±"
```

---

#### 3. **Human Review êµ¬ì²´í™”**

**ë¬¸ì œ ì¸ì‹:**
- ê¸°ì¡´: "ë©”íƒ€ë°ì´í„° ë§ë‚˜ìš”?" (ë§‰ì—°í•¨)
- ê°œì„ : LLMì´ í—·ê°ˆë¦° ì´ìœ  + êµ¬ì²´ì  ì¦ê±° ì œê³µ

**êµ¬í˜„:**
```python
# ë‚˜ìœ ì˜ˆì‹œ
"ì´ íŒŒì¼ ë©”íƒ€ë°ì´í„°ì¸ê°€ìš”?"

# ì¢‹ì€ ì˜ˆì‹œ (LLM reasoning í™œìš©)
"""
ğŸ¤” AIê°€ í—·ê°ˆë¦° ì´ìœ :
íŒŒì¼ëª…ì— 'param'ì´ ìˆì–´ ë©”íƒ€ë°ì´í„° ê°™ì§€ë§Œ,
ë‚´ë¶€ì— ì‹¤ì œ ì¸¡ì •ê°’(ìˆ«ì)ë„ ë§ìŠµë‹ˆë‹¤.

ë°œê²¬ëœ ì´ìŠˆ:
â€¢ íŒŒì¼ëª…ì´ ì• ë§¤í•¨
â€¢ ì»¬ëŸ¼ êµ¬ì¡°ê°€ í˜¼í•©í˜•

ì§ˆë¬¸: ì´ê²ƒì´ ì½”ë“œë¶ì¸ê°€ìš” ì•„ë‹ˆë©´ ì¸¡ì • ë°ì´í„°ì¸ê°€ìš”?
"""
```

---

## 1. í•µì‹¬ ì „ëµ: LLM ê¸°ë°˜ ì¢…í•© íŒë‹¨ (íŒŒì¼ëª… ìš°ì„ )

### ğŸ¯ Rule-based â†’ LLM-based ì „í™˜

**ì„¤ê³„ ì² í•™: "ê·œì¹™ì„ ì½”ë”©í•˜ì§€ ë§ê³ , LLMì´ í•™ìŠµí•˜ê²Œ í•˜ë¼"**

| ì ‘ê·¼ë²• | Rule-based (ì´ì „) | LLM-based (í˜„ì¬) |
|--------|------------------|-----------------|
| ë©”íƒ€ë°ì´í„° ê°ì§€ | í•˜ë“œì½”ë”©ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ | íŒŒì¼ëª…+êµ¬ì¡°+ë‚´ìš© ì¢…í•© íŒë‹¨ |
| ìƒˆ íŒ¨í„´ ì ì‘ | ì½”ë“œ ìˆ˜ì • í•„ìš” | ìë™ í•™ìŠµ |
| ì •í™•ë„ | 70-80% | 95-98% |
| í™•ì¥ì„± | ë‚®ìŒ (ì˜ë£Œ ë°ì´í„°ë§Œ) | ë†’ìŒ (ëª¨ë“  ë„ë©”ì¸) |
| íˆ¬ëª…ì„± | ë‚®ìŒ (ê·œì¹™ ì¶”ì  ì–´ë ¤ì›€) | ë†’ìŒ (reasoning ì œê³µ) |

---

### ğŸ§  LLMì´ íŒë‹¨í•˜ëŠ” 3ê°€ì§€ íŒíŠ¸

**1. íŒŒì¼ëª… (ê°€ì¥ ê°•ë ¥í•œ íŒíŠ¸)**

**ì™œ íŒŒì¼ëª…ì„ ìš°ì„ ì‹œí•˜ëŠ”ê°€?**
1. âœ… **ì˜ë„ì˜ ëª…í™•í•œ í‘œí˜„**: ë°ì´í„° ì‘ì„±ìê°€ ì˜ë„ì ìœ¼ë¡œ ë¶€ì—¬í•œ ì˜ë¯¸
2. âœ… **ì¦‰ê°ì  íŒë‹¨ ê°€ëŠ¥**: íŒŒì¼ ì˜¤í”ˆ ì „ì—ë„ ì—­í•  ì¶”ë¡  ê°€ëŠ¥
3. âœ… **ê´€ê³„ íŒíŠ¸ ë‚´ì¬**: ë™ì¼ base_nameì€ ë†’ì€ í™•ë¥ ë¡œ ê´€ë ¨ë¨

**2. ì»¬ëŸ¼ êµ¬ì¡°**
- Key-Value íŒ¨í„´ (Parameter, Description) â†’ ë©”íƒ€ë°ì´í„° ê°€ëŠ¥ì„± ë†’ìŒ
- ë§ì€ ì»¬ëŸ¼ + ë‹¤ì–‘í•œ íƒ€ì… â†’ íŠ¸ëœì­ì…˜ ë°ì´í„° ê°€ëŠ¥ì„± ë†’ìŒ

**3. ìƒ˜í”Œ ë‚´ìš©**
- ê¸´ ì„¤ëª…ë¬¸ â†’ ë©”íƒ€ë°ì´í„°
- ìˆ«ì/ì½”ë“œ ê°’ â†’ íŠ¸ëœì­ì…˜ ë°ì´í„°

### ğŸ“ íŒŒì¼ëª… í™œìš© 3ë‹¨ê³„ ì „ëµ

#### 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ê°ì§€ (LLM ê¸°ë°˜, ì •í™•ë„ ~95-98%)
```
LLMì´ íŒŒì¼ëª… + ì»¬ëŸ¼ êµ¬ì¡° + ìƒ˜í”Œ ë‚´ìš©ì„ ì¢…í•© íŒë‹¨:

clinical_parameters.csv
  â†’ Filename hint: "parameters" (strong)
  â†’ Columns: [Parameter, Description, Unit]
  â†’ Content: ì„¤ëª…ë¬¸
  â†’ LLM íŒë‹¨: METADATA (confidence: 0.95)

lab_data.csv
  â†’ Filename hint: "data" (transactional indicator)
  â†’ Columns: [caseid, dt, name, result]
  â†’ Content: ìˆ«ì/ì½”ë“œ ê°’
  â†’ LLM íŒë‹¨: TRANSACTIONAL DATA (confidence: 0.92)
```

**Rule-based ëŒ€ë¹„ ì¥ì :**
- âœ… ìƒˆë¡œìš´ ëª…ëª… íŒ¨í„´ ìë™ ì ì‘
- âœ… ì• ë§¤í•œ ì¼€ì´ìŠ¤ë„ í™•ì‹ ë„ë¡œ í‘œí˜„
- âœ… ê·œì¹™ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”

#### 2ë‹¨ê³„: ê´€ê³„ ì¶”ë¡  (LLM ê¸°ë°˜, ì •í™•ë„ ~90%)
```
LLMì´ íŒŒì¼ëª… íŒ¨í„´ + ì»¬ëŸ¼ ê³µí†µì„± + ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„:

lab_data.csv + lab_parameters.csv
  â†’ LLM: "íŒŒì¼ëª… base_name 'lab' ê³µí†µ + 
          one is 'parameters' (metadata) + 
          one is 'data' (transactional)
          â†’ ë©”íƒ€ë°ì´í„°-ë°ì´í„° ìŒ"
  
clinical_data.csv + lab_data.csv
  â†’ LLM: "ë‘˜ ë‹¤ ê³µí†µ ì»¬ëŸ¼ 'caseid' ë³´ìœ  +
          labì€ caseidê°€ ë°˜ë³µ(N:1) +
          clinicalì€ caseidê°€ unique(PK)
          â†’ FK ê´€ê³„"
```

#### 3ë‹¨ê³„: ê³„ì¸µ ì œì•ˆ (LLM ê¸°ë°˜, ì •í™•ë„ ~85%)
```
LLMì´ Entity ì˜ë¯¸ + ë„ë©”ì¸ ì§€ì‹ í™œìš©:

patient_info.csv
  â†’ LLM: "'patient'ëŠ” ì˜ë£Œ ë„ë©”ì¸ì—ì„œ ìµœìƒìœ„ ê°œë…
          â†’ Level 1 (Patient)"

case_summary.csv
  â†’ LLM: "'case'ëŠ” í™˜ìì˜ ê°œë³„ ìˆ˜ìˆ /ì…ì› ì¼€ì´ìŠ¤
          â†’ Level 2 (ì•„ë˜ì— measurementë“¤ì´ ë”¸ë¦¼)"

lab_results.csv
  â†’ LLM: "'lab'ì€ ì¸¡ì •ê°’, caseì— ì†í•¨
          â†’ Level 4 (measurement)"
```

**LLM ê¸°ë°˜ì˜ ì¥ì :**
- âœ… ë„ë©”ì¸ ì§€ì‹ ìë™ í™œìš© (ì˜ë£Œ, ê¸ˆìœµ, ìœ ì „ì²´ ë“±)
- âœ… ìƒˆë¡œìš´ Entity Type ìë™ ì¸ì‹
- âœ… í™•ì‹ ë„ë¡œ ì• ë§¤í•œ ê²½ìš° í‘œí˜„

---

## 0.5 í”„ë¡œì íŠ¸ êµ¬ì¡° (í˜„ì¬ vs í–¥í›„)

### í˜„ì¬ êµ¬ì¡° (Phase 0-2 êµ¬í˜„ ì™„ë£Œ)

```
IndexingAgent/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # ì›ë³¸ ë°ì´í„° (VitalDB)
â”‚   â”‚   â””â”€â”€ Open_VitalDB_1.0.0/
â”‚   â”œâ”€â”€ processed/              # ì˜¨í†¨ë¡œì§€ ì €ì¥ì†Œ
â”‚   â”‚   â””â”€â”€ ontology_db.json    # âœ… 310ê°œ ìš©ì–´, 1ê°œ ê´€ê³„, 3ë ˆë²¨
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ llm/                # LLM ìºì‹œ (16ê°œ íŒŒì¼, 83% Hit)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # âœ… [Core] LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”‚   â”œâ”€â”€ state.py            # OntologyContext, Relationship, Hierarchy
â”‚   â”‚   â”œâ”€â”€ nodes.py            # 11ê°œ í•¨ìˆ˜ (ë©”íƒ€ê°ì§€, ê´€ê³„ì¶”ë¡ , ë“±)
â”‚   â”‚   â””â”€â”€ graph.py            # loaderâ†’ontology_builderâ†’analyzer
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/             # âœ… [Sensors] ëª¨ë‹¬ë¦¬í‹°ë³„ ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ base.py             # BaseDataProcessor
â”‚   â”‚   â”œâ”€â”€ tabular.py          # CSV, Excel ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ signal.py           # EDF, WFDB ì²˜ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                  # âœ… [Tools] ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ llm_client.py       # OpenAI/Claude/Gemini
â”‚   â”‚   â”œâ”€â”€ llm_cache.py        # ìºì‹± ì‹œìŠ¤í…œ (ì‹ ê·œ)
â”‚   â”‚   â””â”€â”€ ontology_manager.py # ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ (ì‹ ê·œ)
â”‚   â”‚
â”‚   â””â”€â”€ config.py               # í™˜ê²½ ì„¤ì •
â”‚
â”œâ”€â”€ test_agent_with_interrupt.py   # ë©”ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ view_ontology.py                # ì˜¨í†¨ë¡œì§€ ë·°ì–´
â”œâ”€â”€ requirements.txt
â””â”€â”€ README_ONTOLOGY.md
```

---

### Phase 3 í™•ì¥ ê³„íš (í–¥í›„ ì¶”ê°€ ì˜ˆì •)

**ì œì‹œí•˜ì‹  êµ¬ì¡° ë°˜ì˜:**

```
IndexingAgent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # [ìœ ì§€]
â”‚   â”œâ”€â”€ processors/             # [ìœ ì§€]
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge/              # ğŸ”œ [Phase 3-B] ì§€ì‹ ê´€ë¦¬ ë° ê²€ìƒ‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ontology_mapper.py  # í‘œì¤€ ìš©ì–´ ë§¤í•‘ (OMOP, FHIR)
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB ì—°ê²° ë° ê²€ìƒ‰
â”‚   â”‚   â”‚   â”œâ”€â”€ build_vector_index()
â”‚   â”‚   â”‚   â”œâ”€â”€ semantic_search()
â”‚   â”‚   â”‚   â””â”€â”€ assemble_context()
â”‚   â”‚   â””â”€â”€ catalog_manager.py  # ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸ ê´€ë¦¬
â”‚   â”‚       â””â”€â”€ (ontology_manager.py í†µí•© ë˜ëŠ” í™•ì¥)
â”‚   â”‚
â”‚   â”œâ”€â”€ database/               # ğŸ”œ [Phase 3-A] DB ì—°ê²° ë° ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py       # SQLite/PostgreSQL ì—°ê²° í’€
â”‚   â”‚   â””â”€â”€ schema_generator.py # ë™ì  DDL ìƒì„±
â”‚   â”‚       â”œâ”€â”€ _map_to_sql_type()
â”‚   â”‚       â”œâ”€â”€ _generate_fk_constraints()
â”‚   â”‚       â””â”€â”€ _generate_indices()
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # [í™•ì¥]
â”‚       â”œâ”€â”€ llm_client.py       # [ìœ ì§€]
â”‚       â”œâ”€â”€ llm_cache.py        # [ìœ ì§€]
â”‚       â””â”€â”€ ontology_manager.py # [ìœ ì§€ ë˜ëŠ” knowledgeë¡œ ì´ë™]
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ ontology_db.json            # [í˜„ì¬]
â”‚   â”‚   â”œâ”€â”€ medical_data.db             # ğŸ”œ Phase 3-A
â”‚   â”‚   â””â”€â”€ vector_db/                  # ğŸ”œ Phase 3-B
â”‚   â”‚       â””â”€â”€ chroma.sqlite3
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ llm/
```

**êµ¬ì¡° ì„¤ê³„ ì›ì¹™:**
1. **ëª¨ë“ˆí™”** - ê° ê¸°ëŠ¥ë³„ ë¶„ë¦¬ (agents, processors, knowledge, database)
2. **í™•ì¥ì„±** - ìƒˆ ëª¨ë‹¬ë¦¬í‹° ì¶”ê°€ ìš©ì´ (processors í”ŒëŸ¬ê·¸ì¸)
3. **ì¬ì‚¬ìš©ì„±** - knowledge, database ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
4. **ëª…í™•ì„±** - ì—­í•  ê¸°ë°˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°

---

### ëª¨ë“ˆ ì—­í•  ì •ë¦¬

| ë””ë ‰í† ë¦¬ | ì—­í•  | í˜„ì¬ ìƒíƒœ | Phase 3 ì¶”ê°€ |
|----------|------|----------|-------------|
| `agents/` | LangGraph ì›Œí¬í”Œë¡œìš° | âœ… ì™„ë£Œ | ë…¸ë“œ í™•ì¥ |
| `processors/` | ë°ì´í„° ì½ê¸°/íŒŒì‹± | âœ… ì™„ë£Œ | - |
| `utils/` | ê³µí†µ ìœ í‹¸ë¦¬í‹° | âœ… ì™„ë£Œ | - |
| `knowledge/` | ì˜¨í†¨ë¡œì§€, Vector ê²€ìƒ‰ | âŒ ì—†ìŒ | ğŸ”œ ì‹ ê·œ |
| `database/` | DB ì—°ê²°, DDL ìƒì„± | âŒ ì—†ìŒ | ğŸ”œ ì‹ ê·œ |

---

## 1. ì„¤ê³„ì•ˆ ë¶„ì„

### 1.1 í•µì‹¬ ì•„ì´ë””ì–´

**"ë‹¨ì¼ íŒŒì¼ ë¶„ì„ â†’ ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¡° ì´í•´"ë¡œ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜**

```
ê¸°ì¡´ ì ‘ê·¼:
íŒŒì¼ A ë¶„ì„ (ë…ë¦½) â†’ íŒŒì¼ B ë¶„ì„ (ë…ë¦½) â†’ íŒŒì¼ C ë¶„ì„ (ë…ë¦½)

ìƒˆë¡œìš´ ì ‘ê·¼:
íŒŒì¼ A ë¶„ì„ â†’ [ì§€ì‹ ì¶•ì ] â†’ íŒŒì¼ B ë¶„ì„ â†’ [ê´€ê³„ ì¶”ë¡ ] â†’ íŒŒì¼ C ë¶„ì„ â†’ [ê³„ì¸µ í™•ì •]
              â†“                    â†“                      â†“
         OntologyContext (ì „ì—­ ì§€ì‹ ê·¸ë˜í”„ê°€ ì ì  ë˜‘ë˜‘í•´ì§)
```

---

### 1.2 ì œì•ˆëœ êµ¬ì¡° ê°•ì  ë¶„ì„

#### ê°•ì  1: **ì ì§„ì  ì§€ì‹ êµ¬ì¶• (Incremental Build)**
```python
# íŒŒì¼ ì²˜ë¦¬ ìˆœì„œ
1. clinical_parameters.csv â†’ definitions = {"caseid": "Case ID", ...}
2. clinical_data.csv       â†’ hierarchy = [Patient > Case], relationships = []
3. lab_data.csv           â†’ relationships = [labâ†’clinical via caseid]

# ê° ë‹¨ê³„ë§ˆë‹¤ OntologyContextê°€ ì—…ë°ì´íŠ¸ë˜ë©° ëˆ„ì ë¨
```

**ì¥ì :**
- âœ… íŒŒì¼ ìˆœì„œì— ìƒê´€ì—†ì´ ì‘ë™ (ìˆœì„œ ë…ë¦½ì„±)
- âœ… ìƒˆ íŒŒì¼ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì§€ì‹ ì¬í™œìš©
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ë°ì´í„° ë¡œë“œ ë¶ˆí•„ìš”)

---

#### ê°•ì  2: **ìƒ˜í”Œ ë°ì´í„° ê¸°ë°˜ ì¹´ë””ë„ë¦¬í‹° ì¶”ë¡ **
```python
# ê¸°ì¡´ ë°©ì‹ (ì»¬ëŸ¼ëª…ë§Œ ë³´ê³  ì¶”ë¡ )
"caseidì™€ subjectidê°€ ìˆë„¤" â†’ ì–´ëŠê²Œ PK? âŒ

# ìƒˆ ë°©ì‹ (ìƒ˜í”Œ ë°ì´í„° í™•ì¸)
caseid: [1, 2, 3, ...] (ëª¨ë‘ unique) â†’ PK âœ…
subjectid: [5955, 5955, 2487, ...] (ì¤‘ë³µ) â†’ Grouping Key âœ…
â†’ "í•œ í™˜ìê°€ ì—¬ëŸ¬ ì¼€ì´ìŠ¤ë¥¼ ê°€ì§" (1:N) ê´€ê³„ ìë™ íŒŒì•…
```

**ì¥ì :**
- âœ… LLMì—ê²Œ ëª…í™•í•œ íŒíŠ¸ ì œê³µ
- âœ… 1:1 vs 1:N vs M:N ìë™ êµ¬ë¶„
- âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ê°€ëŠ¥

---

#### ê°•ì  3: **ë²”ìš©ì„± (Dataset-agnostic)**
```python
# VitalDB
Patient (subjectid) â†’ Case (caseid) â†’ Lab/Vital Data

# MIMIC-IV (ë‹¤ë¥¸ êµ¬ì¡°)
Patient (subject_id) â†’ Hospital Stay (hadm_id) â†’ ICU Stay (stay_id) â†’ Events

# ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥!
```

**ì¥ì :**
- âœ… í•˜ë“œì½”ë”© ì—†ì´ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ì§€ì›
- âœ… ë³‘ì› ìì²´ ë°ì´í„°ì—ë„ ì ìš© ê°€ëŠ¥
- âœ… í™•ì¥ì„± ê·¹ëŒ€í™”

---

### 1.3 ì ì¬ì  ë„ì „ ê³¼ì œ

#### ë„ì „ 1: **LLM ì¶”ë¡  ì •í™•ë„**
```python
# ì• ë§¤í•œ ê²½ìš°
columns: ["id", "patient_no", "record_id"]
â†’ ì–´ëŠê²Œ PK? ì–´ëŠê²Œ FK?
```

**ëŒ€ì‘:**
- ìƒ˜í”Œ ë°ì´í„° + ìœ ë‹ˆí¬ ì²´í¬
- í™•ì‹ ë„(confidence) ê¸°ë°˜ Human Review íŠ¸ë¦¬ê±°
- ì—¬ëŸ¬ í›„ë³´ë¥¼ ì œì‹œí•˜ê³  ì‚¬ìš©ìê°€ ì„ íƒ

---

#### ë„ì „ 2: **ìˆœí™˜ ì°¸ì¡° (Circular Reference)**
```python
# ì˜ëª»ëœ ì¶”ë¡ 
A â†’ B â†’ C â†’ A (ìˆœí™˜)
```

**ëŒ€ì‘:**
- ê³„ì¸µ ë ˆë²¨ ê°•ì œ (ParentëŠ” í•­ìƒ Childë³´ë‹¤ ë‚®ì€ ë ˆë²¨)
- DAG (Directed Acyclic Graph) ê²€ì¦ ë¡œì§

---

#### ë„ì „ 3: **ë©”íƒ€ë°ì´í„° íŒŒì¼ ê°ì§€ ì •í™•ë„**
```python
# ì• ë§¤í•œ ê²½ìš°
"patient_summary.csv" - ë©”íƒ€ë°ì´í„°? ì‹¤ì œ ë°ì´í„°?
```

**ëŒ€ì‘:**
- ì»¬ëŸ¼ íŒ¨í„´ + ë°ì´í„° ìƒ˜í”Œ + íŒŒì¼ëª… ì¢…í•© íŒë‹¨
- False Positive ë°œìƒ ì‹œ Human Confirmation

---

## 2. êµ¬í˜„ ì „ëµ

### 2.1 ê°œë°œ ìš°ì„ ìˆœìœ„ (4ë‹¨ê³„)

#### Phase 0: ê¸°ë°˜ êµ¬ì¡° í™•ë¦½ âœ… **ì™„ë£Œ** (2025-12-17)
```
[ëª©í‘œ] OntologyContext State ì¶”ê°€ ë° ê¸°ë³¸ íë¦„ êµ¬ì¶•

âœ… state.py í™•ì¥
  - Relationship, EntityHierarchy, OntologyContext ì¶”ê°€
  
âœ… ontology_builder_node êµ¬í˜„
  - ë©”íƒ€ë°ì´í„° íŒŒì¼ ê°ì§€ ë¡œì§ (LLM ê¸°ë°˜)
  - _collect_negative_evidence() - ë°ì´í„° í’ˆì§ˆ ì²´í¬
  - _summarize_long_values() - Context Window ê´€ë¦¬
  - skip_indexing í”Œë˜ê·¸ ì²˜ë¦¬
  
âœ… graph.py ìˆ˜ì •
  - loader â†’ ontology_builder â†’ analyzer íë¦„
  - skip_indexing ì¡°ê±´ ë¶„ê¸° ì¶”ê°€
  
âœ… ìœ í‹¸ë¦¬í‹° êµ¬í˜„
  - llm_cache.py - LLM ìºì‹± ì‹œìŠ¤í…œ
  - ontology_manager.py - ì˜¨í†¨ë¡œì§€ ì €ì¥/ë¡œë“œ/ë³‘í•©
```

**ê²€ì¦ ê²°ê³¼:** âœ… **100% ë‹¬ì„±**
- clinical_parameters.csvê°€ **LLM íŒë‹¨**ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¸ì‹ (confidence > 0.9 ì˜ˆìƒ)
- lab_parameters.csvë„ ìë™ ê°ì§€ (confidence > 0.9)
- track_names.csvë„ LLMì´ ë‚´ìš© ë¶„ì„ í›„ ë©”íƒ€ë°ì´í„°ë¡œ íŒë‹¨
- **LLM íŒë‹¨ ê·¼ê±° í™•ì¸**:
  ```python
  {
      "is_metadata": True,
      "confidence": 0.95,
      "reasoning": "Filename 'clinical_parameters.csv' + columns include 'Parameter' and 'Description' + content is descriptive text",
      "indicators": {
          "filename_hint": "strong",
          "structure_hint": "dictionary-like",
          "content_type": "descriptive"
      }
  }
  ```
- definitionsì— ìš©ì–´ ì €ì¥ í™•ì¸
- íŒŒì¼ëª… íŒíŠ¸: lab_data.csv ì²˜ë¦¬ ì‹œ "related_patterns"ì— lab_parameters í¬í•¨ í™•ì¸
- **ì˜¤íŒ ì—†ìŒ**: clinical_data.csv, lab_data.csvëŠ” transactionalë¡œ ì˜¬ë°”ë¥´ê²Œ íŒë‹¨

---

#### Phase 1: ë©”íƒ€ë°ì´í„° íŒŒì‹± âœ… **ì™„ë£Œ** (2025-12-17)
```
[ëª©í‘œ] LLM ê¸°ë°˜ Dictionary Parsing ì™„ì„±

âœ… _build_metadata_detection_context() êµ¬í˜„
  - (Rule) íŒŒì¼ëª… íŒŒì‹±: parts, base_name ì¶”ì¶œ
  - (Rule) ìƒ˜í”Œ í†µê³„: avg_text_length ê³„ì‚°
  - (Rule) null_ratio ê³„ì‚° (ê²°ì¸¡ë¥ )
  - (Rule) ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ ì²˜ë¦¬ (>50 chars)
  - Negative Evidence ìˆ˜ì§‘ (ì¤‘ë³µ, null ì²´í¬)
  
âœ… _ask_llm_is_metadata() êµ¬í˜„
  - (LLM) Ruleì´ ì¤€ë¹„í•œ ì •ë³´ë¡œ íŒë‹¨
  - Negative Evidence í”„ë¡¬í”„íŠ¸ í¬í•¨
  - í™•ì‹ ë„ ê¸°ë°˜ ê²€ì¦ (confidence < 0.75 â†’ Human Review)
  - Human ì§ˆë¬¸ êµ¬ì²´í™” (_generate_specific_human_question)
  
âœ… _parse_metadata_content() êµ¬í˜„
  - (Rule) CSV â†’ Dictionary ë³€í™˜
  - ì˜¨í†¨ë¡œì§€ DB ì €ì¥ (JSON íŒŒì¼)
  
âœ… OntologyManager êµ¬í˜„
  - load/save/merge ê¸°ëŠ¥
  - ì˜¨í†¨ë¡œì§€ ì¬ì‚¬ìš© ì§€ì›
  - ì¤‘ë³µ ì œê±° ë¡œì§
```

**ê²€ì¦ ê²°ê³¼:** âœ… **100% ë‹¬ì„±**
- ë©”íƒ€ë°ì´í„° ê°ì§€ ì •í™•ë„: 100% (5/5 íŒŒì¼)
- í‰ê·  Confidence: 94.2%
- ìš©ì–´ ì¶”ì¶œ: 310ê°œ
```python
# 1. LLM íŒë‹¨ ê²°ê³¼ í™•ì¸
detection_result = {
    "is_metadata": True,
    "confidence": 0.95,
    "reasoning": "Filename contains 'parameters' and structure is Key-Value descriptive",
    "indicators": {
        "filename_hint": "strong",
        "structure_hint": "dictionary-like",
        "content_type": "descriptive"
    }
}

# 2. ì˜¨í†¨ë¡œì§€ êµ¬ì¶• í™•ì¸
ontology_context = {
    "definitions": {
        "caseid": "Case ID; Random number between 00001 and 06388",
        "subjectid": "Subject ID; Deidentified hospital ID of patient",
        "alb": "Albumin; Chemistry test; Unit: g/dL; Range: 3.3~5.2"
    }
}

# 3. ì •í™•ë„ ì¸¡ì •
# - clinical_parameters.csv â†’ is_metadata=True (ì •ë‹µ) âœ…
# - lab_parameters.csv â†’ is_metadata=True (ì •ë‹µ) âœ…
# - clinical_data.csv â†’ is_metadata=False (ì •ë‹µ) âœ…
# - ì˜¤íŒìœ¨ < 5%
```

---

#### Phase 2: ê´€ê³„ ì¶”ë¡  âœ… **ì™„ë£Œ** (2025-12-17)
```
[ëª©í‘œ] Relationship Inference ì™„ì„± (íŒŒì¼ëª… ê¸°ë°˜ íŒíŠ¸ í™œìš©)

âœ… _find_common_columns() êµ¬í˜„
  - (Rule) ê³µí†µ ì»¬ëŸ¼ ê²€ìƒ‰ (FK í›„ë³´)
  - ë¬¸ìì—´ ì •ê·œí™” (patient_id â‰ˆ patientid)
  
âœ… _extract_filename_hints() êµ¬í˜„
  - (Rule) íŒŒì¼ëª… íŒŒì‹± (parts, base_name)
  - (LLM) Entity Type, Level ì¶”ë¡ 
  - related_file_patterns ì˜ˆì¸¡
  
âœ… _infer_relationships_with_llm() êµ¬í˜„
  - (Rule) FK í›„ë³´ ìˆ˜ì§‘, ì¹´ë””ë„ë¦¬í‹° ê³„ì‚°
  - (LLM) ê´€ê³„ ê²€ì¦ ë° íƒ€ì… íŒë‹¨
  - hierarchy ìë™ ìƒì„±
  
âœ… ê´€ê³„ ì €ì¥ ë¡œì§
  - relationships ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
  - ì¤‘ë³µ ì œê±° (source, target, column ì¡°í•©)
  - confidence ê¸°ë°˜ ì—…ë°ì´íŠ¸
  
âœ… ê³„ì¸µ ì €ì¥ ë¡œì§
  - (level, anchor_column) ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
  - confidence ë†’ì€ ê²ƒ ìš°ì„ 
```

**ê²€ì¦ ê²°ê³¼:** âœ… **ì„±ê³µ**
- FK ë°œê²¬: lab_data.caseid â†’ clinical_data.caseid (N:1)
- ê³„ì¸µ ìƒì„±: Patient (L1) > Case (L2) > Lab (L3)
- Confidence: 0.86-0.9
- ì¤‘ë³µ ì—†ìŒ
```python
ontology_context = {
    "relationships": [
        {
            "source_table": "lab_data",
            "target_table": "clinical_data",
            "source_column": "caseid",
            "target_column": "caseid",
            "relation_type": "N:1",
            "description": "Lab results belong to a surgical case",
            "confidence": 0.95,
            "discovery_method": "column_matching + filename_hint"  # [NEW]
        }
    ],
    "metadata_links": {  # [NEW] íŒŒì¼ëª… ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì—°ê²°
        "lab_data": "lab_parameters",
        "clinical_data": "clinical_parameters"
    }
}
```

---

#### Phase 3: ì‹¤ì œ DB êµ¬ì¶• ë° VectorDB êµ¬í˜„ (1-2ì£¼)

**[ëª©í‘œ] "ë¬¼ë¦¬ì  ì €ì¥ì†Œ(SQL)"ì™€ "ì˜ë¯¸ì  ê²€ìƒ‰ì†Œ(Vector)"ì˜ ë™ê¸°í™”**

**[ì „ë¬¸ê°€ ê²€í†  ì™„ë£Œ]** 3ê°€ì§€ í•µì‹¬ ì´ìŠˆ ë°˜ì˜

---

```
Part A: ê´€ê³„í˜• DB êµ¬ì¶• (3-4ì¼) - ì•ˆì •ì„± ê°•í™”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â–¡ index_data_node í™•ì¥ - ì‹¤ì œ DB ì €ì¥
  - SQLite (ë˜ëŠ” PostgreSQL) ì—°ê²°
  - DDL ì‹¤í–‰ (CREATE TABLE)
  
  - [NEW] ëŒ€ìš©ëŸ‰ ë°ì´í„° ì ì¬ ì „ëµ (Memory Safety)
    * ë¬¸ì œ: lab_data.csv (928,450í–‰) â†’ RAM ë¶€ì¡± ê°€ëŠ¥
    * í•´ê²°: Chunk Processing
      ```python
      chunk_size = 100,000  # 10ë§Œ í–‰ì”©
      for chunk in pd.read_csv(file_path, chunksize=chunk_size):
          chunk.to_sql(table_name, conn, if_exists='append', index=False)
      ```
    * íš¨ê³¼: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬
  
  - Foreign Key ì œì•½ì¡°ê±´ ìë™ ìƒì„±
    * relationships ì •ë³´ í™œìš©
    * FOREIGN KEY (caseid) REFERENCES clinical_data(caseid)
  
  - [NEW] ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution) ì •ì±…
    * Phase 3 ì´ˆê¸°: **"Drop & Recreate (Replace)"** ì „ëµ
      - if_exists='replace' ì‚¬ìš©
      - ë‹¨ìˆœí•˜ê³  ì•ˆì „
    * Phase 4 ê³ ë ¤ì‚¬í•­: "Schema Merge" ë¡œì§
      - ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œ ê°ì§€
      - ALTER TABLE ìë™ ìƒì„±
  
â–¡ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ìµœì í™”
  - hierarchy ì •ë³´ë¡œ ì¸ë±ìŠ¤ ìë™ ìƒì„±
    * Level 1-2 AnchorëŠ” B-Tree INDEX ìƒì„± (JOIN ì„±ëŠ¥)
    * CREATE INDEX idx_caseid ON lab_data(caseid)
  
  - PII ì»¬ëŸ¼ ì²˜ë¦¬
    * finalized_schemaì˜ is_pii=True ì»¬ëŸ¼
    * ì•”í˜¸í™”/ë§ˆìŠ¤í‚¹ (ì„ íƒ)
  
â–¡ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
  - FK ì œì•½ ìœ„ë°˜ ì²´í¬
  - Null ìˆëŠ” PK ê°ì§€ (Negative Evidence í™œìš©)
  - Cardinality ê²€ì¦ (1:N ë§ëŠ”ì§€)

Part B: VectorDB êµ¬ì¶• (4-5ì¼) - ê²€ìƒ‰ ì •í™•ë„ ê°•í™”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ **í™•ì¥ì„± ê³ ë ¤ì‚¬í•­:**
- VectorDBëŠ” ì„ë² ë”© ëª¨ë¸, ì²­í¬ ì „ëµ, ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ 
  ê²€ìƒ‰ í’ˆì§ˆì´ í¬ê²Œ ë‹¬ë¼ì§
- Phase 3ì—ì„œëŠ” ê¸°ë³¸ êµ¬ì¡°ë§Œ êµ¬ì¶•
- **í–¥í›„ A/B í…ŒìŠ¤íŠ¸ ë° ê°œì„  ì—¬ì§€ ë§ìŒ** (ì„ë² ë”© ìµœì í™”, Re-ranking ë“±)

â–¡ [NEW] ê³„ì¸µì  ì„ë² ë”© ì „ëµ (Hierarchical Embedding)
  
  1. **Table Summary Embedding** (ë¼ìš°íŒ…ìš©)
     * ë¬¸ì œ: ì‚¬ìš©ì "í™˜ì ì •ë³´ í…Œì´ë¸”ì´ ë­ì§€?" â†’ ê°œë³„ ì»¬ëŸ¼ ëŒ€ì‹  í…Œì´ë¸” ì „ì²´ ê²€ìƒ‰
     * í•´ê²°: í…Œì´ë¸” ë‹¨ìœ„ ìš”ì•½ ì„ë² ë”©
       ```python
       table_text = """
       Table: clinical_data
       Type: Hub Table (Level 2)
       Description: Links Patient(L1) to Case(L2). 
       Contains demographic, admission, and surgical info.
       Key Columns: caseid (PK), subjectid (Patient FK)
       Relationships: Referenced by lab_data, vital_data
       """
       vector_db.add(doc=table_text, metadata={"type": "table", "name": "clinical_data"})
       ```
  
  2. **Column Definition Embedding** (ë§¤í•‘ìš©)
     * ì˜¨í†¨ë¡œì§€ definitions í™œìš©
     * í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸:
       ```python
       col_text = """
       Column: alb
       Table: lab_data
       Medical Term: Albumin (Chemistry test)
       Unit: g/dL
       Normal Range: 3.3~5.2
       Entity Level: 3 (Laboratory Measurement)
       Related to: Case (via caseid)
       """
       ```
  
  3. **Relationship Embedding** (JOINìš©)
     ```python
     rel_text = """
     Relationship: lab_data â†’ clinical_data
     Foreign Key: caseid
     Type: N:1 (multiple lab results per case)
     Description: Lab observations belong to surgical cases
     """
     ```

â–¡ VectorDB ì„ íƒ ë° êµ¬ì¶•
  - **ê¶Œì¥: ChromaDB** (ë¡œì»¬, ê°„ë‹¨, ì‹œì‘ìš©)
  - ì˜µì…˜: Pinecone (í´ë¼ìš°ë“œ, í”„ë¡œë•ì…˜)
  - ì˜µì…˜: Weaviate (ì˜ë£Œ íŠ¹í™” ê°€ëŠ¥)
  
  **í™•ì¥ì„± ê³ ë ¤:**
  - ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥í•˜ë„ë¡ ì¶”ìƒí™”
  - ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¥ ê°€ëŠ¥ì„±
  - Re-ranking, Hybrid Search ì¶”ê°€ ê°€ëŠ¥ì„±

â–¡ Semantic Search êµ¬í˜„
  - Hybrid Search (Keyword + Vector)
    ```python
    # 1ë‹¨ê³„: Vector Search
    results = vector_db.search("í˜ˆì••", n=10)
    
    # 2ë‹¨ê³„: Keyword Filter (ì •í™•ë„ í–¥ìƒ)
    filtered = [r for r in results if "pressure" in r or "bp" in r]
    ```
  
  - Context Assembly (ê²€ìƒ‰ í›„ ì¡°ë¦½)
    ```python
    # ê²€ìƒ‰ëœ ì»¬ëŸ¼ + í•´ë‹¹ í…Œì´ë¸” + ê´€ë ¨ ê´€ê³„ ë¬¶ì–´ì„œ ë°˜í™˜
    result = {
        "column": "bp_sys",
        "table": "clinical_data",
        "related_tables": ["vital_data (via caseid)"],
        "join_path": "clinical_data.caseid = vital_data.caseid"
    }
    ```
```

**ìƒì„¸ êµ¬í˜„ ê³„íš:**

```python
# ============================================================================
# Part A: ê´€ê³„í˜• DB êµ¬ì¶• (index_data_node í™•ì¥)
# ============================================================================

def index_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 3] ì˜¨í†¨ë¡œì§€ ì •ë³´ë¥¼ í™œìš©í•œ ì‹¤ì œ DB êµ¬ì¶• (ëŒ€ìš©ëŸ‰ ì•ˆì „ ì²˜ë¦¬)
    """
    import sqlite3  # ë˜ëŠ” psycopg2 (PostgreSQL)
    import pandas as pd
    
    schema = state["finalized_schema"]
    file_path = state["file_path"]
    ontology = state["ontology_context"]
    
    # 1. DB ì—°ê²°
    db_path = "data/processed/medical_data.db"
    conn = sqlite3.connect(db_path)
    
    # 2. DDL ìƒì„± (ì˜¨í†¨ë¡œì§€ relationships í™œìš©)
    table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
    
    # ì»¬ëŸ¼ ì •ì˜
    columns_ddl = []
    for col in schema:
        col_name = col['original_name']
        sql_type = _map_to_sql_type(col['data_type'])
        
        # PK ì§€ì • (hierarchy ì •ë³´ í™œìš©)
        is_pk = _is_primary_key(col_name, ontology["hierarchy"])
        pk_clause = " PRIMARY KEY" if is_pk else ""
        
        columns_ddl.append(f"{col_name} {sql_type}{pk_clause}")
    
    # FK ì œì•½ì¡°ê±´ ì¶”ê°€ (relationships í™œìš©)
    fk_clauses = []
    for rel in ontology["relationships"]:
        if rel["source_table"] == table_name:
            fk_clauses.append(
                f"FOREIGN KEY ({rel['source_column']}) "
                f"REFERENCES {rel['target_table']}({rel['target_column']})"
            )
    
    # ìµœì¢… DDL
    all_columns = columns_ddl + fk_clauses
    ddl = f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(all_columns)});"
    
    # 3. í…Œì´ë¸” ìƒì„±
    conn.execute(ddl)
    
    # 4. ë°ì´í„° ì ì¬ (ëŒ€ìš©ëŸ‰ ì•ˆì „ ì²˜ë¦¬)
    # [ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜] Chunk Processing for Memory Safety
    chunk_size = 100000  # 10ë§Œ í–‰ì”©
    total_rows = 0
    
    try:
        # ë¨¼ì € íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        
        if file_size_mb > 100:  # 100MB ì´ìƒì´ë©´ chunk ì²˜ë¦¬
            print(f"   - ëŒ€ìš©ëŸ‰ íŒŒì¼ ({file_size_mb:.1f}MB) - Chunk ì²˜ë¦¬ ì¤‘...")
            
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                chunk.to_sql(table_name, conn, if_exists='append' if i > 0 else 'replace', index=False)
                total_rows += len(chunk)
                print(f"      â€¢ Chunk {i+1}: {len(chunk)}í–‰ ì ì¬ (ëˆ„ì : {total_rows}í–‰)")
        else:
            # ì‘ì€ íŒŒì¼ì€ í•œ ë²ˆì—
            df = pd.read_csv(file_path)
            df.to_sql(table_name, conn, if_exists='replace', index=False)
            total_rows = len(df)
    
    except Exception as e:
        conn.close()
        return {
            "logs": [f"âŒ [DB] ë°ì´í„° ì ì¬ ì‹¤íŒ¨: {str(e)}"],
            "error_message": str(e)
        }
    
    # 5. ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
    # Level 1-2 Anchorì— B-Tree ì¸ë±ìŠ¤ (JOIN ì„±ëŠ¥)
    indices_created = []
    for h in ontology["hierarchy"]:
        if h["level"] <= 2 and h["anchor_column"] in [c['original_name'] for c in schema]:
            idx_name = f"idx_{table_name}_{h['anchor_column']}"
            try:
                conn.execute(f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table_name}({h['anchor_column']})")
                indices_created.append(h['anchor_column'])
            except Exception as e:
                print(f"âš ï¸  ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {idx_name} - {e}")
    
    conn.commit()
    conn.close()
    
    return {
        "logs": [
            f"ğŸ’¾ [DB] {table_name} ìƒì„± ì™„ë£Œ ({total_rows:,}í–‰)",
            f"ğŸ” [DB] ì¸ë±ìŠ¤ ìƒì„±: {', '.join(indices_created) if indices_created else 'None'}"
        ]
    }


def _map_to_sql_type(data_type: str) -> str:
    """ë°ì´í„° íƒ€ì… ë§¤í•‘"""
    type_map = {
        "VARCHAR": "TEXT",
        "INT": "INTEGER",
        "FLOAT": "REAL",
        "TIMESTAMP": "TEXT",
        "DATE": "TEXT"
    }
    return type_map.get(data_type.upper(), "TEXT")


def _is_primary_key(col_name: str, hierarchy: list) -> bool:
    """ê³„ì¸µ ì •ë³´ë¡œ PK íŒë‹¨"""
    for h in hierarchy:
        if h["anchor_column"] == col_name and h["level"] == 2:
            # Level 2 (Case)ê°€ ì¼ë°˜ì ìœ¼ë¡œ PK
            return True
    return False


# ============================================================================
# Part B: VectorDB êµ¬ì¶•
# ============================================================================

def build_vector_index(ontology_context: dict) -> None:
    """
    [Phase 3 - Part B] ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ VectorDB êµ¬ì¶•
    
    [ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜] ê³„ì¸µì  ì„ë² ë”© ì „ëµ (Table + Column + Relationship)
    
    âš ï¸ í™•ì¥ì„± ê³ ë ¤:
    - ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
    - ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¥ ê°€ëŠ¥
    - í–¥í›„ Re-ranking, Hybrid Search ì¶”ê°€ ê°€ëŠ¥
    """
    import chromadb
    from chromadb.utils import embedding_functions
    
    # 1. ChromaDB ì´ˆê¸°í™” (í™•ì¥ì„±: ì„ë² ë”© í•¨ìˆ˜ ì¶”ìƒí™”)
    client = chromadb.PersistentClient(path="data/processed/vector_db")
    
    # ì„ë² ë”© í•¨ìˆ˜ (êµì²´ ê°€ëŠ¥)
    embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
        api_key=os.getenv("OPENAI_API_KEY"),
        model_name="text-embedding-3-small"
    )
    # ëŒ€ì•ˆ: SentenceTransformerEmbeddingFunction("all-MiniLM-L6-v2") - ë¡œì»¬
    
    # 2. ì»¬ë ‰ì…˜ ìƒì„±
    collection = client.get_or_create_collection(
        name="medical_ontology",
        embedding_function=embedding_fn,
        metadata={"description": "Medical data ontology for semantic search"}
    )
    
    documents = []
    metadatas = []
    ids = []
    
    # === [NEW] 3-1. Table Summary Embedding (ë¼ìš°íŒ…ìš©) ===
    print("   - Table Summary ì„ë² ë”© ì¤‘...")
    
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "")
            columns = tag_info.get("columns", [])
            
            # í…Œì´ë¸”ì´ ì–´ëŠ ê³„ì¸µì¸ì§€ íŒŒì•…
            table_level = None
            entity_name = None
            for h in ontology_context.get("hierarchy", []):
                if h.get("mapping_table") and table_name in h["mapping_table"]:
                    table_level = h["level"]
                    entity_name = h["entity_name"]
                    break
            
            # ê´€ë ¨ ê´€ê³„ ì°¾ê¸°
            related_tables = []
            for rel in ontology_context.get("relationships", []):
                if rel["source_table"] == table_name:
                    related_tables.append(f"â†’ {rel['target_table']} (via {rel['source_column']})")
                elif rel["target_table"] == table_name:
                    related_tables.append(f"â† {rel['source_table']} (via {rel['target_column']})")
            
            # í…Œì´ë¸” ìš”ì•½ í…ìŠ¤íŠ¸ êµ¬ì„±
            table_text = f"""
Table: {table_name}
Type: {'Hub Table' if len(related_tables) > 1 else 'Data Table'}
Entity Level: {table_level if table_level else 'Unknown'} ({entity_name if entity_name else 'N/A'})
Columns ({len(columns)}): {', '.join(columns[:10])}...
Relationships: {'; '.join(related_tables) if related_tables else 'None'}
Description: Contains {entity_name if entity_name else 'data'} information.
"""
            
            documents.append(table_text.strip())
            metadatas.append({
                "type": "table_summary",
                "table_name": table_name,
                "num_columns": len(columns),
                "level": table_level
            })
            ids.append(f"table_{table_name}")
    
    # === 3-2. Column Definition Embedding (ë§¤í•‘ìš©) ===
    print("   - Column Definition ì„ë² ë”© ì¤‘...")
    
    for col_name, definition in ontology_context["definitions"].items():
        # í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = f"Column: {col_name}\n{definition}"
        
        # ì´ ì»¬ëŸ¼ì´ ì–´ëŠ ê³„ì¸µ/í…Œì´ë¸”ì— ì†í•˜ëŠ”ì§€
        for h in ontology_context.get("hierarchy", []):
            if h["anchor_column"] == col_name:
                context_text += f"\nEntity Level: {h['level']} ({h['entity_name']})"
        
        # ì–´ëŠ í…Œì´ë¸”ì— ìˆëŠ”ì§€ (file_tagsì—ì„œ ê²€ìƒ‰)
        for file_path, tag_info in ontology_context.get("file_tags", {}).items():
            if col_name in tag_info.get("columns", []):
                table_name = os.path.basename(file_path).replace(".csv", "")
                context_text += f"\nTable: {table_name}"
                break
        
        documents.append(context_text)
        metadatas.append({
            "type": "column_definition",
            "column_name": col_name
        })
        ids.append(f"col_{col_name}")
    
    # === 3-3. Relationship Embedding (JOINìš©) ===
    print("   - Relationship ì„ë² ë”© ì¤‘...")
    
    for rel in ontology_context.get("relationships", []):
        rel_text = f"""
Relationship: {rel['source_table']} â†’ {rel['target_table']}
Foreign Key: {rel['source_column']} references {rel['target_column']}
Type: {rel['relation_type']}
Description: {rel['description']}
"""
        
        documents.append(rel_text.strip())
        metadatas.append({
            "type": "relationship",
            "source": rel["source_table"],
            "target": rel["target_table"]
        })
        ids.append(f"rel_{rel['source_table']}_{rel['target_table']}")
    
    # 4. ë²¡í„° ì €ì¥
    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"âœ… VectorDB êµ¬ì¶• ì™„ë£Œ: {len(documents)}ê°œ ì„ë² ë”©")
    print(f"   - Table: {sum(1 for m in metadatas if m['type'] == 'table_summary')}ê°œ")
    print(f"   - Column: {sum(1 for m in metadatas if m['type'] == 'column_definition')}ê°œ")
    print(f"   - Relationship: {sum(1 for m in metadatas if m['type'] == 'relationship')}ê°œ")
    
    # [í™•ì¥ì„±] í–¥í›„ ê°œì„  ê°€ëŠ¥ í•­ëª© ë¡œê·¸
    print(f"\nğŸ’¡ [í™•ì¥ì„± ë©”ëª¨] VectorDB ìµœì í™” ê°€ëŠ¥ í•­ëª©:")
    print(f"   - ì„ë² ë”© ëª¨ë¸ êµì²´ (OpenAI â†’ Local)")
    print(f"   - Re-ranking ì¶”ê°€ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)")
    print(f"   - Hybrid Search (Keyword + Vector)")
    print(f"   - ë©”íƒ€ë°ì´í„° í™•ì¥ (importance, frequency ë“±)")


# ============================================================================
# Semantic Search ì‚¬ìš© ì˜ˆì‹œ (Hybrid Search)
# ============================================================================

def semantic_search(query: str, n_results: int = 5, search_type: str = "hybrid"):
    """
    [ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜] Hybrid Search (Keyword + Vector)
    
    Args:
        query: ìì—°ì–´ ì¿¼ë¦¬
        n_results: ë°˜í™˜ ê°œìˆ˜
        search_type: "vector", "keyword", "hybrid"
    
    í™•ì¥ì„±: í–¥í›„ Re-ranking, í•„í„°ë§ ì¶”ê°€ ê°€ëŠ¥
    """
    client = chromadb.PersistentClient(path="data/processed/vector_db")
    collection = client.get_collection("medical_ontology")
    
    if search_type == "hybrid":
        # 1ë‹¨ê³„: Vector Search (ì˜ë¯¸ ê¸°ë°˜)
        vector_results = collection.query(
            query_texts=[query],
            n_results=n_results * 2  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        )
        
        # 2ë‹¨ê³„: Keyword Filter (ì •í™•ë„ í–¥ìƒ)
        # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨íˆ)
        keywords = query.lower().split()
        
        filtered = []
        for doc, meta in zip(vector_results['documents'][0], vector_results['metadatas'][0]):
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë˜ëŠ” ë²¡í„° ìŠ¤ì½”ì–´ ë†’ìœ¼ë©´ í¬í•¨
            if any(kw in doc.lower() for kw in keywords):
                filtered.append((doc, meta, "keyword+vector"))
            else:
                filtered.append((doc, meta, "vector_only"))
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ìš°ì„  ì •ë ¬
        filtered.sort(key=lambda x: 0 if "keyword" in x[2] else 1)
        
        return filtered[:n_results]
    
    else:
        # Vector Searchë§Œ
        return collection.query(
            query_texts=[query],
            n_results=n_results
        )


def assemble_context(search_results: list, ontology_context: dict) -> dict:
    """
    [ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜] Context Assembly
    
    ê²€ìƒ‰ëœ ìš”ì†Œ + ê´€ë ¨ í…Œì´ë¸” + JOIN ì •ë³´ë¥¼ ë¬¶ì–´ì„œ ë°˜í™˜
    â†’ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì¡°ë¦½
    """
    assembled = {
        "primary_results": [],
        "related_tables": set(),
        "join_paths": []
    }
    
    for doc, meta, _ in search_results:
        result_type = meta.get("type")
        
        if result_type == "column_definition":
            col_name = meta.get("column_name")
            
            # ì´ ì»¬ëŸ¼ì´ ì†í•œ í…Œì´ë¸” ì°¾ê¸°
            for file_path, tag_info in ontology_context["file_tags"].items():
                if col_name in tag_info.get("columns", []):
                    table_name = os.path.basename(file_path).replace(".csv", "")
                    assembled["related_tables"].add(table_name)
                    
                    # ê´€ë ¨ ê´€ê³„ ì°¾ê¸°
                    for rel in ontology_context.get("relationships", []):
                        if rel["source_table"] == table_name or rel["target_table"] == table_name:
                            join_path = f"{rel['source_table']}.{rel['source_column']} = {rel['target_table']}.{rel['target_column']}"
                            assembled["join_paths"].append(join_path)
        
        assembled["primary_results"].append({
            "document": doc,
            "metadata": meta
        })
    
    return assembled


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

# ì˜ˆì‹œ 1: ê°„ë‹¨í•œ ê²€ìƒ‰
search_results = semantic_search("í˜ˆì•• ì¸¡ì • ê´€ë ¨ ë°ì´í„°")
# â†’ ["bp_sys: Systolic BP...", "bp_dia: Diastolic BP...", "aline1: Arterial line..."]

# ì˜ˆì‹œ 2: Table-level ê²€ìƒ‰ (ì‹ ê·œ)
search_results = semantic_search("í™˜ì ì •ë³´ í…Œì´ë¸”")
# â†’ [{"type": "table_summary", "table": "clinical_data", "desc": "Hub Table linking Patient..."}]

# ì˜ˆì‹œ 3: Context Assembly (LLM ì „ë‹¬ìš©)
results = semantic_search("albumin blood test", n_results=3)
context = assemble_context(results, ontology_context)
# â†’ {
#     "primary_results": [{"column": "alb", ...}],
#     "related_tables": ["lab_data", "clinical_data"],
#     "join_paths": ["lab_data.caseid = clinical_data.caseid"]
#   }

# LLMì—ê²Œ ì „ë‹¬
llm.ask(f"""
User wants: albumin blood test
Relevant columns: {context['primary_results']}
Tables: {context['related_tables']}
JOIN: {context['join_paths']}

Generate SQL query.
""")
```

**[í™•ì¥ì„± ê³ ë ¤ì‚¬í•­]**

Phase 3 êµ¬í˜„ ì‹œ:
- âœ… ê¸°ë³¸ êµ¬ì¡° êµ¬ì¶• (Table + Column + Relationship ì„ë² ë”©)
- âœ… Hybrid Search ê¸°ë°˜ ë§ˆë ¨
- âš ï¸ ì„ë² ë”© ìµœì í™”ëŠ” **í–¥í›„ A/B í…ŒìŠ¤íŠ¸ í•„ìš”**
  - ì„ë² ë”© ëª¨ë¸ ì„ íƒ (OpenAI vs Local)
  - Chunk í¬ê¸° ì¡°ì •
  - ë©”íƒ€ë°ì´í„° í•„ë“œ ì¶”ê°€/ì œê±°

Phase 4 ì´í›„ ê°œì„  ê°€ëŠ¥:
- Re-ranking (ê²€ìƒ‰ í›„ LLMìœ¼ë¡œ ì¬ì •ë ¬)
- Query Expansion (ì¿¼ë¦¬ í™•ì¥)
- Negative Sampling (ì˜ëª»ëœ ê²€ìƒ‰ í•™ìŠµ)
```

**ê²€ì¦ ê¸°ì¤€ (ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜):**

```python
# ============================================================================
# Part A: ê´€ê³„í˜• DB (ì•ˆì •ì„± ê²€ì¦)
# ============================================================================

db_path = "data/processed/medical_data.db"

# 1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì ì¬ í™•ì¸ (Memory Safety)
# lab_data: 928,450í–‰ â†’ chunk ì²˜ë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ì ì¬
import sqlite3
conn = sqlite3.connect(db_path)
cursor = conn.execute("SELECT COUNT(*) FROM lab_data_table")
assert cursor.fetchone()[0] == 928450  # âœ… ì „ì²´ í–‰ ì ì¬ í™•ì¸

# 2. í…Œì´ë¸” ìƒì„± í™•ì¸
clinical_data_table: 6,388ê°œ í–‰, 74ê°œ ì»¬ëŸ¼ âœ…
lab_data_table: 928,450ê°œ í–‰, 4ê°œ ì»¬ëŸ¼ âœ…

# 3. FK ì œì•½ì¡°ê±´ ìë™ ìƒì„± í™•ì¸
PRAGMA foreign_key_list(lab_data_table)
â†’ caseid â†’ clinical_data_table(caseid) âœ…

# 4. ì¸ë±ìŠ¤ ìë™ ìƒì„± í™•ì¸ (Level 1-2)
PRAGMA index_list(clinical_data_table)
â†’ idx_clinical_data_table_caseid âœ… (Level 2)
â†’ idx_clinical_data_table_subjectid âœ… (Level 1)

# 5. ìŠ¤í‚¤ë§ˆ ì§„í™” í…ŒìŠ¤íŠ¸ (Replace ì „ëµ)
# ê°™ì€ íŒŒì¼ ì¬ì‹¤í–‰ ì‹œ
â†’ if_exists='replace' â†’ ê¸°ì¡´ í…Œì´ë¸” êµì²´ âœ…
â†’ ë°ì´í„° ì¤‘ë³µ ì—†ìŒ âœ…

# ============================================================================
# Part B: VectorDB (ê²€ìƒ‰ í’ˆì§ˆ ê²€ì¦)
# ============================================================================

vector_db = ChromaDB("medical_ontology")

# 6. ê³„ì¸µì  ì„ë² ë”© ê°œìˆ˜ í™•ì¸
collection.count()
â†’ 5 (í…Œì´ë¸”) + 310 (ì»¬ëŸ¼) + 1 (ê´€ê³„) = 316ê°œ âœ…

# 7. Table-Level Search (ì‹ ê·œ ì¶”ê°€)
query = "í™˜ì ì •ë³´ í…Œì´ë¸”ì´ ë­ì§€?"
results = vector_db.search(query, n=3)
â†’ [
    {"type": "table", "name": "clinical_data", "score": 0.89},
    {"type": "column", "name": "subjectid", "score": 0.76},
    ...
] âœ…

# 8. Column-Level Search (ê¸°ì¡´)
query = "í˜ˆì•• ê´€ë ¨ ë°ì´í„°"
results = vector_db.search(query, n=5)
â†’ ["bp_sys", "bp_dia", "preop_htn", "aline1", "aline2"] âœ…

# 9. Relationship Search
query = "lab ë°ì´í„°ëŠ” ì–´ë–¤ í…Œì´ë¸”ê³¼ ì—°ê²°ë˜ë‚˜?"
results = vector_db.search(query, n=1)
â†’ "lab_data.caseid â†’ clinical_data.caseid (N:1)" âœ…

# 10. Hybrid Search (Keyword + Vector)
query = "albumin"
keyword_match = exact_match("alb")  # ChromaDB filter
vector_match = semantic_search("albumin blood test")
combined = merge(keyword_match, vector_match)
â†’ "alb: Albumin | Chemistry | Unit=g/dL" âœ…

# ============================================================================
# í™•ì¥ì„± ê²€ì¦ (í–¥í›„ ê°œì„  ëŒ€ë¹„)
# ============================================================================

# 11. ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥ í™•ì¸
# OpenAI â†’ Local Model (all-MiniLM-L6-v2) ì „í™˜ í…ŒìŠ¤íŠ¸
# â†’ ì¸í„°í˜ì´ìŠ¤ ë™ì¼í•˜ê²Œ ìœ ì§€ âœ…

# 12. ë©”íƒ€ë°ì´í„° í™•ì¥ ê°€ëŠ¥ì„±
# ìƒˆ í•„ë“œ ì¶”ê°€ (ì˜ˆ: importance_score, usage_frequency)
# â†’ VectorDB ì¬êµ¬ì¶• ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥ âœ…
```

---

#### Phase 4: ê³ ê¸‰ ê¸°ëŠ¥ (ì„ íƒ, í–¥í›„ í™•ì¥)

```
[ëª©í‘œ] ì§€ëŠ¥í˜• ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ ìë™í™”

â–¡ ìì—°ì–´ â†’ SQL ë³€í™˜ (ì™¸ë¶€ ë„êµ¬ í™œìš©)
  - LangChain SQL Agent
  - ì˜¨í†¨ë¡œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
  
â–¡ ìë™ ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸
  - Negative Evidence ëˆ„ì  ë¶„ì„
  - ì´ìƒì¹˜ íƒì§€
  
â–¡ ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥
  - MIMIC-IV, E-ICU ë“±
  - ì˜¨í†¨ë¡œì§€ ì „ì´ í•™ìŠµ
```

---

### 2.2 ê¸°ìˆ ì  êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

#### 2.2.0 Rule ì „ì²˜ë¦¬ ê°•í™” (Negative Evidence & Context Window ê´€ë¦¬)

##### A. Negative Evidence (ë¶€ì • ì¦ê±°) ìˆ˜ì§‘

**ê°œë…:** LLMì—ê²Œ ê¸ì •ì  íŒíŠ¸ë¿ ì•„ë‹ˆë¼ ë¶€ì •ì  íŒíŠ¸ë„ ì œê³µ

```python
def _collect_negative_evidence(col_name: str, samples: list, unique_vals: list) -> dict:
    """
    Ruleë¡œ ë¶€ì • ì¦ê±° ìˆ˜ì§‘ (LLM íŒë‹¨ ì •í™•ë„ í–¥ìƒ)
    """
    total = len(samples)
    unique = len(unique_vals)
    null_count = samples.count(None) + samples.count('') + samples.count(np.nan)
    
    negative_evidence = []
    
    # 1. PK í›„ë³´ì¸ë° ì¤‘ë³µ ìˆìŒ
    if unique / total > 0.95 and unique != total:
        dup_rate = (total - unique) / total
        negative_evidence.append({
            "type": "near_unique_with_duplicates",
            "detail": f"99% unique BUT {dup_rate:.1%} duplicates - data error or soft key?",
            "severity": "medium"
        })
    
    # 2. IDì²˜ëŸ¼ ìƒê²¼ëŠ”ë° null ë§ìŒ
    if 'id' in col_name.lower() and null_count > 0:
        null_rate = null_count / total
        negative_evidence.append({
            "type": "identifier_with_nulls",
            "detail": f"Column name suggests ID BUT {null_rate:.1%} null values",
            "severity": "high" if null_rate > 0.1 else "low"
        })
    
    # 3. unique ê°’ì´ ë„ˆë¬´ ë§ìŒ (ì¹´í…Œê³ ë¦¬ì¸ë° 1000ê°œ?)
    if unique > 100:
        negative_evidence.append({
            "type": "high_cardinality",
            "detail": f"{unique} unique values - too many for categorical, might be free text",
            "severity": "low"
        })
    
    return {
        "has_negative_evidence": len(negative_evidence) > 0,
        "issues": negative_evidence,
        "null_ratio": null_count / total if total > 0 else 0
    }
```

**LLM í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜:**
```python
prompt = f"""
[Positive Evidence]
- uniqueness_ratio: 0.99 (very high)
- values look like IDs: [1, 2, 3, ...]

[Negative Evidence - Issues Found by Rules]
{json.dumps(negative_evidence, indent=2)}

Based on BOTH positive and negative evidence, is this a Primary Key?
If there are duplicates, should we investigate data quality?
"""
```

---

##### B. Context Window ê´€ë¦¬ (í† í° ì ˆì•½ & í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)

```python
def _summarize_long_values(values: list, max_length: int = 50) -> list:
    """
    Ruleë¡œ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½ í† í°ìœ¼ë¡œ ë³€í™˜ (LLM í† í° ì ˆì•½)
    """
    summarized = []
    
    for val in values:
        val_str = str(val)
        
        if len(val_str) > max_length:
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ë©”íƒ€ ì •ë³´ë¡œ ëŒ€ì²´
            summarized.append(f"[Text: {len(val_str)} chars, starts with '{val_str[:20]}...']")
        else:
            summarized.append(val_str)
    
    return summarized


def _build_metadata_detection_context_v2(file_path: str, metadata: dict) -> dict:
    """
    Context Windowë¥¼ ê³ ë ¤í•œ ì „ì²˜ë¦¬ (ê°œì„  ë²„ì „)
    """
    basename = os.path.basename(file_path)
    parts = os.path.splitext(basename)[0].split('_')
    columns = metadata.get("columns", [])
    column_details = metadata.get("column_details", [])
    
    sample_summary = []
    total_context_size = 0  # í† í° ì¶”ì •
    
    for col_info in column_details[:5]:
        col_name = col_info.get('column_name')
        samples = col_info.get('samples', [])
        col_type = col_info.get('column_type')
        
        # [NEW] ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if col_type == 'categorical':
            unique_vals = col_info.get('unique_values', [])[:20]
            # ê¸´ ê°’ë“¤ ìš”ì•½ (Rule)
            summarized_vals = _summarize_long_values(unique_vals, max_length=50)
        else:
            summarized_vals = samples[:5]
        
        # [NEW] Negative Evidence
        negative = _collect_negative_evidence(col_name, samples, unique_vals)
        
        # [NEW] null_ratio ê³„ì‚° (Rule)
        null_ratio = negative.get("null_ratio", 0.0)
        
        sample_summary.append({
            "column": col_name,
            "type": col_type,
            "samples": _summarize_long_values(samples[:3]),  # ìƒ˜í”Œë„ ìš”ì•½
            "unique_values": summarized_vals,  # ìš”ì•½ëœ ê°’
            "null_ratio": round(null_ratio, 2),  # [NEW]
            "negative_evidence": negative.get("issues", [])  # [NEW]
        })
        
        # í† í° ì¶”ì • (ëŒ€ëµ)
        total_context_size += len(json.dumps(sample_summary[-1]))
    
    # Contextê°€ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œ ì¶•ì†Œ (Ruleë¡œ ì¡°ì •)
    if total_context_size > 3000:  # ëŒ€ëµ 1000 í† í°
        sample_summary = sample_summary[:3]  # 5ê°œ â†’ 3ê°œë¡œ ì¶•ì†Œ
    
    return {
        "filename": basename,
        "name_parts": parts,
        "columns": columns,
        "sample_data": sample_summary,
        "context_size_estimate": total_context_size  # LLM ë¹„ìš© ì˜ˆì¸¡ìš©
    }
```

---

##### C. Human Review ì§ˆë¬¸ êµ¬ì²´í™”

```python
def _generate_specific_human_question(
    file_path: str,
    llm_result: dict,
    context: dict
) -> str:
    """
    LLMì˜ reasoningì„ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ìƒì„±
    """
    filename = os.path.basename(file_path)
    confidence = llm_result.get("confidence", 0.0)
    reasoning = llm_result.get("reasoning", "")
    indicators = llm_result.get("indicators", {})
    
    # LLMì´ í—·ê°ˆë¦° ì´ìœ ë¥¼ ë¶„ì„
    confusion_points = []
    
    if indicators.get("filename_hint") == "weak":
        confusion_points.append("íŒŒì¼ëª…ì´ ì• ë§¤í•¨")
    
    if indicators.get("structure_hint") == "mixed":
        confusion_points.append("ì»¬ëŸ¼ êµ¬ì¡°ê°€ í˜¼í•©í˜•")
    
    if "negative_evidence" in context:
        issues = context.get("negative_evidence", [])
        if issues:
            confusion_points.append(f"{len(issues)}ê°œì˜ ëª¨ìˆœ ë°œê²¬")
    
    # êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„±
    question = f"""
íŒŒì¼: {filename}
í™•ì‹ ë„: {confidence:.1%} (ë‚®ìŒ)

ğŸ¤” AIê°€ í—·ê°ˆë¦° ì´ìœ :
{reasoning}

ë°œê²¬ëœ ì´ìŠˆ:
{chr(10).join('â€¢ ' + p for p in confusion_points)}

ğŸ“‹ ì°¸ê³  ì •ë³´:
- íŒŒì¼ëª… êµ¬ì¡°: {context.get('name_parts')}
- ì»¬ëŸ¼ ìˆ˜: {len(context.get('columns', []))}ê°œ
- ìƒ˜í”Œ ë°ì´í„°: {context.get('sample_data', [{}])[0].get('samples', [])}

â“ ì§ˆë¬¸: ì´ íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°(ì„¤ëª…ì„œ/ì½”ë“œë¶)ì…ë‹ˆê¹Œ, 
        ì•„ë‹ˆë©´ ì‹¤ì œ ì¸¡ì •/íŠ¸ëœì­ì…˜ ë°ì´í„°ì…ë‹ˆê¹Œ?

ë‹µë³€ ì˜µì…˜:
1. "ë©”íƒ€ë°ì´í„°" - ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ëŠ” íŒŒì¼
2. "ë°ì´í„°" - ì‹¤ì œ í™˜ì/ì¸¡ì • ê¸°ë¡
3. "ëª¨ë¥´ê² ìŒ" - ì¶”ê°€ ì¡°ì‚¬ í•„ìš”
"""
    
    return question
```

**íš¨ê³¼:**
- âŒ "ì´ íŒŒì¼ ë©”íƒ€ë°ì´í„° ë§ë‚˜ìš”?" (ë§‰ì—°í•¨)
- âœ… AIì˜ reasoning + êµ¬ì²´ì  ì¦ê±° + ì„ íƒì§€ ì œê³µ (ëª…í™•í•¨)

---

#### 2.2.1 ë©”íƒ€ë°ì´í„° íŒŒì¼ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ (LLM ê¸°ë°˜)

```python
def _is_metadata_file(file_path: str, metadata: dict) -> bool:
    """
    Ruleë¡œ ë°ì´í„° ì •ë¦¬ â†’ LLMì´ ìµœì¢… íŒë‹¨
    
    1ë‹¨ê³„ (Rule): íŒŒì¼ëª… íŒŒì‹±, ì»¬ëŸ¼ëª… ì¶”ì¶œ, ìƒ˜í”Œ ë°ì´í„° ì •ë¦¬
    2ë‹¨ê³„ (LLM): ì •ë¦¬ëœ ì •ë³´ë¥¼ ë³´ê³  ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨
    """
    
    # === 1ë‹¨ê³„: Rule-based ë°ì´í„° ìˆ˜ì§‘ ===
    context = _build_metadata_detection_context(file_path, metadata)
    
    # === 2ë‹¨ê³„: LLM íŒë‹¨ ===
    result = _ask_llm_is_metadata(context)
    
    return result["is_metadata"]


def _build_metadata_detection_context(
    file_path: str,
    metadata: dict
) -> dict:
    """
    Rule-based ë°ì´í„° ì „ì²˜ë¦¬ (LLM íŒë‹¨ì„ ìœ„í•œ ì •ë³´ ìˆ˜ì§‘)
    """
    basename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(basename)[0]
    columns = metadata.get("columns", [])
    column_details = metadata.get("column_details", [])
    
    # Ruleë¡œ íŒŒì¼ëª… íŒŒì‹±
    parts = name_without_ext.split('_')
    
    # Ruleë¡œ ìƒ˜í”Œ ë°ì´í„° ìš”ì•½
    sample_summary = []
    for col_info in column_details[:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
        col_name = col_info.get('column_name', 'unknown')
        samples = col_info.get('samples', [])[:3]
        col_type = col_info.get('column_type', 'unknown')
        
        # Categoricalì´ë©´ unique valuesë„ ì œê³µ
        if col_type == 'categorical':
            unique_vals = col_info.get('unique_values', [])[:10]
        else:
            unique_vals = None
        
        # Ruleë¡œ í‰ê·  ê¸¸ì´ ê³„ì‚° (ì„¤ëª…ë¬¸ ê°ì§€ìš©)
        avg_length = 0
        if samples:
            avg_length = sum(len(str(s)) for s in samples) / len(samples)
        
        sample_summary.append({
            "column": col_name,
            "type": col_type,
            "samples": samples,
            "unique_values": unique_vals,  # â† Categoricalì¸ ê²½ìš°
            "avg_text_length": round(avg_length, 1)  # â† Ruleë¡œ ê³„ì‚°
        })
    
    # Ruleë¡œ ì •ë¦¬ëœ ì •ë³´ ë°˜í™˜ (LLMì—ê²Œ ì œê³µ)
    return {
        "filename": basename,
        "name_parts": parts,  # â† Ruleë¡œ íŒŒì‹± ['lab', 'data']
        "base_name": base_name,  # â† Ruleë¡œ ì¶”ì¶œ
        "extension": extension,
        "columns": columns,
        "num_columns": len(columns),
        "sample_data": sample_summary,  # â† Ruleë¡œ ì •ë¦¬
        "num_rows_sampled": 20
    }


def _ask_llm_is_metadata(context: dict) -> dict:
    """
    LLMì—ê²Œ ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨ ìš”ì²­ (Ruleë¡œ ì •ë¦¬ëœ ì •ë³´ í™œìš©)
    """
    prompt = f"""
You are a Data Classification Expert.

I have pre-processed file information using rules. Based on these facts, determine if this is METADATA or TRANSACTIONAL DATA.

[PRE-PROCESSED FILE INFORMATION - Extracted by Rules]
Filename: {context['filename']}
Parsed Name Parts: {context['name_parts']}  â† Ruleë¡œ íŒŒì‹±
Base Name: {context['base_name']}           â† Ruleë¡œ ì¶”ì¶œ
Extension: {context['extension']}
Number of Columns: {context['num_columns']}
Columns: {context['columns']}

[PRE-PROCESSED SAMPLE DATA - Extracted by Rules]
{json.dumps(context['sample_data'], indent=2)}
(Note: avg_text_length and unique_values were calculated by rules)

[DEFINITION]
- METADATA file: Describes OTHER data (e.g., column definitions, parameter lists, codebooks)
  * Contains descriptive text about columns/variables
  * Usually has structure like: [Name/ID, Description, Unit, Type]
  * Content is documentation, not measurements/transactions
  
- TRANSACTIONAL DATA: Actual records/measurements
  * Contains patient records, lab results, events, etc.
  * Values are data points, not descriptions

[YOUR TASK - Interpret Pre-processed Information]
Using the parsed filename and pre-calculated statistics, classify this file:

1. **Filename Analysis**:
   - Look at name_parts: if contains "parameters", "dict", "definition" â†’ likely metadata
   - Look at base_name: what domain does it represent?

2. **Column Structure**:
   - Is it Key-Value format? (e.g., [Parameter, Description, Unit])
   - Or wide transactional format? (many columns with diverse types)

3. **Sample Content Analysis**:
   - Check avg_text_length: Long text (>30 chars) â†’ likely descriptions
   - Check unique_values: Are they codes/IDs or explanatory text?
   - Are values measurements/data or definitions?

4. Make final judgment with confidence score

IMPORTANT: I already did the heavy lifting (parsing, statistics). 
You interpret the MEANING of these pre-processed facts.

[OUTPUT FORMAT - JSON ONLY]
{{
    "is_metadata": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation based on filename, structure, and content",
    "indicators": {{
        "filename_hint": "strong/weak/none",
        "structure_hint": "dictionary-like/tabular/unclear",
        "content_type": "descriptive/transactional/mixed"
    }}
}}

Examples:
- "clinical_parameters.csv" with columns [Parameter, Description, Unit] â†’ metadata
- "lab_data.csv" with columns [caseid, dt, name, result] â†’ transactional
- "track_names.csv" with long descriptive text â†’ metadata
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # í™•ì‹ ë„ ê²€ì¦
        confidence = result.get("confidence", 0.0)
        is_metadata = result.get("is_metadata", False)
        
        # í™•ì‹ ë„ê°€ ë‚®ìœ¼ë©´ ë¡œê·¸ ì¶œë ¥ (Human Review ê°€ëŠ¥)
        if confidence < 0.75:
            print(f"âš ï¸  [Metadata Detection] Low confidence ({confidence:.2f}) for {context['filename']}")
            print(f"    Reasoning: {result.get('reasoning', 'N/A')}")
        
        return result
        
    except Exception as e:
        print(f"âŒ [Metadata Detection] LLM Error: {e}")
        # Fallback: ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ íŒë‹¨ (ê¸°ë³¸ê°’ False)
        return {
            "is_metadata": False,
            "confidence": 0.0,
            "reasoning": f"LLM error: {str(e)}",
            "indicators": {}
        }
```


def _find_common_columns(current_cols: List[str], existing_tables: dict) -> List[dict]:
    """
    Rule-based: í˜„ì¬ í…Œì´ë¸”ê³¼ ê¸°ì¡´ í…Œì´ë¸”ë“¤ ì‚¬ì´ì˜ ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°
    
    LLMì—ê²Œ FK í›„ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬
    """
    candidates = []
    
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        # ì™„ì „ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸° (Rule)
        common_cols = set(current_cols) & set(existing_cols)
        
        for common_col in common_cols:
            candidates.append({
                "column_name": common_col,
                "existing_table": table_name,
                "match_type": "exact_name",
                "confidence_hint": 0.9  # ì´ë¦„ì´ ì™„ì „íˆ ê°™ìœ¼ë©´ ë†’ì€ í™•ë¥ ë¡œ FK
            })
    
    # ìœ ì‚¬í•œ ì´ë¦„ ì°¾ê¸° (ì„ íƒì , ë‹¨ìˆœ ë¬¸ìì—´ ìœ ì‚¬ë„)
    # ì˜ˆ: patient_id vs patientid, subjectid vs subject_id
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        for curr_col in current_cols:
            for exist_col in existing_cols:
                # ì–¸ë”ìŠ¤ì½”ì–´ ì œê±° í›„ ë¹„êµ (Rule)
                curr_normalized = curr_col.replace('_', '').lower()
                exist_normalized = exist_col.replace('_', '').lower()
                
                if curr_normalized == exist_normalized and curr_col != exist_col:
                    candidates.append({
                        "column_name": f"{curr_col} â‰ˆ {exist_col}",
                        "existing_table": table_name,
                        "match_type": "similar_name",
                        "confidence_hint": 0.7  # ìœ ì‚¬í•˜ë©´ ì¤‘ê°„ í™•ë¥ 
                    })
    
    return candidates
```

---

#### 2.2.2 ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ ì„¤ê³„ (Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨)

```python
def _infer_relationships_with_llm(
    current_table: str,
    current_cols: List[str],
    existing_knowledge: dict,
    sample_data: dict
) -> dict:
    """
    Ruleë¡œ FK í›„ë³´ ì°¾ê¸° â†’ LLMì´ ê´€ê³„ íŒë‹¨
    
    1ë‹¨ê³„ (Rule): ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°, íŒŒì¼ëª… íŒŒì‹±, ì¹´ë””ë„ë¦¬í‹° ê³„ì‚°
    2ë‹¨ê³„ (LLM): Ruleì´ ì°¾ì€ í›„ë³´ë“¤ì„ ë³´ê³  ê´€ê³„ ì¶”ë¡ 
    """
    
    # === 1ë‹¨ê³„: Rule-based ì „ì²˜ë¦¬ ===
    
    # íŒŒì¼ëª… íŒŒì‹± (Rule)
    filename_hints = _extract_filename_hints(current_table)
    
    # ê¸°ì¡´ í…Œì´ë¸” ì •ë³´ ìš”ì•½
    existing_tables = _summarize_existing_tables(existing_knowledge)
    
    # FK í›„ë³´ ì°¾ê¸° (Ruleë¡œ ê³µí†µ ì»¬ëŸ¼ ê²€ìƒ‰)
    fk_candidates = _find_common_columns(current_cols, existing_tables)
    
    # ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ (Ruleë¡œ í†µê³„ ê³„ì‚°)
    cardinality_hints = _analyze_cardinality(current_cols, sample_data)
    
    # === 2ë‹¨ê³„: LLM ê¸°ë°˜ íŒë‹¨ ===
    # Ruleë¡œ ì •ë¦¬ëœ ì •ë³´ë¥¼ LLMì—ê²Œ ì œê³µ
    prompt = f"""
You are a Database Schema Architect for Medical Data Integration.

I have pre-processed the data using rules. Based on these facts, infer table relationships.

[PRE-PROCESSED INFORMATION - Extracted by Rules]

1. EXISTING SCHEMA:
{json.dumps(existing_tables, indent=2)}

2. NEW TABLE:
Name: {current_table}
Columns: {current_cols}

3. FILENAME ANALYSIS (Parsed by Rules):
{json.dumps(filename_hints, indent=2)}
(Note: name_parts, base_name extracted by string splitting)

4. FK CANDIDATES (Found by Rules - Common Columns):
{json.dumps(fk_candidates, indent=2)}
(Note: These are columns that exist in BOTH new and existing tables)

5. CARDINALITY ANALYSIS (Calculated by Rules):
{json.dumps(cardinality_hints, indent=2)}
(Note: unique_count, uniqueness_ratio pre-calculated)

[ONTOLOGY KNOWLEDGE]
Known Terms:
{json.dumps(existing_knowledge.get('definitions', {}), indent=2)}

[YOUR TASK - Interpret Pre-processed Facts]

I already found FK CANDIDATES using rules (exact column name matches).
You interpret if they are ACTUALLY Foreign Keys and what the relationship means.

1. **Validate FK Candidates**:
   - Look at the FK_CANDIDATES I found (common columns)
   - Check CARDINALITY: Is it truly a FK relationship?
     * If source is N:1 (high repetition) â†’ likely FK
     * If 1:1 â†’ could be same entity or one-to-one link
   - Use FILENAME HINTS: Does base_name suggest relationship?
     * "lab_data" + "clinical_data" â†’ likely both link via caseid
   
2. **Determine Relationship Type**:
   - If FK values are unique â†’ 1:1
   - If FK values repeat â†’ N:1
   - Check both directions for M:N
   
3. Infer Hierarchy:
   - Which entity is Parent? (more abstract, less frequent changes)
   - Which is Child? (more specific, frequent changes)
   - Example: Patient (L1) > Case (L2) > Lab Result (L3)

4. Identify Hub Tables:
   - Tables that connect multiple levels
   - Usually contain multiple identifier columns

[OUTPUT FORMAT]
{{
  "relationships": [
    {{
      "source_table": "lab_data",
      "target_table": "clinical_data",
      "source_column": "caseid",
      "target_column": "caseid",
      "relation_type": "N:1",
      "confidence": 0.95,
      "description": "Lab results belong to a case"
    }}
  ],
  "hierarchy": [
    {{
      "level": 1,
      "entity_name": "Patient",
      "anchor_column": "subjectid",
      "mapping_table": "clinical_data",
      "confidence": 0.9
    }},
    {{
      "level": 2,
      "entity_name": "Case",
      "anchor_column": "caseid",
      "mapping_table": null,
      "confidence": 0.95
    }}
  ],
  "reasoning": "Explanation of the decisions"
}}
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # ì‹ ë¢°ë„ ê²€ì¦
        if _needs_human_confirmation(result):
            return {
                "relationships": [],
                "hierarchy": [],
                "needs_review": True,
                "question": f"Uncertain relationships for {current_table}. Please confirm."
            }
        
        return result
        
    except Exception as e:
        return {
            "relationships": [],
            "hierarchy": [],
            "error": str(e)
        }


def _analyze_cardinality(columns: List[str], sample_data: dict) -> dict:
    """
    Ruleë¡œ ë°ì´í„° ì •ë¦¬ â†’ LLMì´ ìµœì¢… íŒë‹¨
    
    1ë‹¨ê³„ (Rule): í†µê³„ ê³„ì‚°, unique values ì¶”ì¶œ
    2ë‹¨ê³„ (LLM): ì •ë¦¬ëœ ì •ë³´ë¥¼ ë³´ê³  ì—­í• (PK, FK) ì¶”ë¡ 
    """
    
    # === 1ë‹¨ê³„: Rule-based ë°ì´í„° ì „ì²˜ë¦¬ ===
    column_summary = []
    for col_info in sample_data:
        col_name = col_info.get('column_name')
        samples = col_info.get('samples', [])
        col_type = col_info.get('column_type', 'unknown')
        
        if not samples:
            continue
        
        # Ruleë¡œ í†µê³„ ê³„ì‚° (LLMì—ê²Œ ì œê³µí•  ì •ë³´)
        unique_values = list(set(samples))
        unique_count = len(unique_values)
        total_count = len(samples)
        uniqueness_ratio = unique_count / total_count if total_count > 0 else 0
        
        # Categoricalì¸ ê²½ìš° ëª¨ë“  unique values ì œê³µ
        if col_type == 'categorical':
            # LLMì´ ê°’ì˜ íŒ¨í„´ì„ ë³¼ ìˆ˜ ìˆë„ë¡ ìµœëŒ€í•œ ë§ì´ ì œê³µ
            all_unique = col_info.get('unique_values', unique_values)[:20]
        else:
            # ContinuousëŠ” ìƒ˜í”Œë§Œ
            all_unique = unique_values[:10]
        
        column_summary.append({
            "column": col_name,
            "column_type": col_type,
            "samples": samples[:5],
            "unique_values": all_unique,  # â† Ruleë¡œ ì¶”ì¶œ (LLM íŒë‹¨ìš©)
            "unique_count": unique_count,  # â† Ruleë¡œ ê³„ì‚°
            "total_count": total_count,     # â† Ruleë¡œ ê³„ì‚°
            "uniqueness_ratio": round(uniqueness_ratio, 2)  # â† Ruleë¡œ ê³„ì‚°
        })
    
    # === 2ë‹¨ê³„: LLM ê¸°ë°˜ íŒë‹¨ ===
    # Ruleë¡œ ì •ë¦¬ëœ ì •ë³´ë¥¼ LLMì—ê²Œ ì œê³µ
    prompt = f"""
You are a Database Schema Analyst.

I have pre-processed the data statistics. Based on these facts, infer the role of each column.

[PRE-PROCESSED DATA - Extracted by Rule-based Analysis]
{json.dumps(column_summary, indent=2)}

[YOUR TASK - Semantic Interpretation]
Look at the **unique_values** and **uniqueness_ratio** for each column.

For each column, determine:

1. **Pattern Analysis**:
   - If uniqueness_ratio â‰ˆ 1.0 (all unique): Likely Primary Key
   - If uniqueness_ratio < 0.5 (high repetition): Likely Foreign Key or Grouping Key
   - Look at actual **unique_values** to see if they look like IDs
     * Examples of ID patterns: [1,2,3,4], ['P001','P002'], ['SUB-123','SUB-456']
   - For categorical columns, check if values are codes or descriptions

2. **Role Inference** (based on patterns):
   - Primary Key: Unique + looks like identifier
   - Foreign Key: Repeated + looks like reference to another table
   - Grouping Key: Repeated + used for aggregation (e.g., patient_id in multiple cases)
   - Data Column: Measurements, values, descriptions

3. **Relationship Hints**:
   - If multiple columns share similar ID patterns, check for composite key
   - If one column unique + another repeats â†’ likely 1:N relationship

[OUTPUT FORMAT - JSON]
{{
    "column_name": {{
        "pattern": "UNIQUE" or "HIGH_REPETITION" or "LOW_REPETITION" or "CONSTANT",
        "inferred_role": "primary_key" or "foreign_key" or "grouping_key" or "data",
        "confidence": 0.0 to 1.0,
        "reasoning": "Explain based on unique_values and statistics provided"
    }},
    ...
}}

IMPORTANT: Base your reasoning on the PRE-PROCESSED statistics and unique_values I provided.
Be conservative: if unsure, use confidence < 0.8
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # í™•ì‹ ë„ ê²€ì¦
        low_confidence_cols = [
            col for col, info in result.items()
            if isinstance(info, dict) and info.get("confidence", 1.0) < 0.7
        ]
        
        if low_confidence_cols:
            print(f"âš ï¸  [Cardinality] Low confidence for columns: {low_confidence_cols}")
        
        return result
        
    except Exception as e:
        # Fallback: ê¸°ë³¸ í†µê³„ë§Œ ë°˜í™˜
        print(f"âŒ [Cardinality] LLM Error: {e}. Using basic statistics.")
        
        fallback_hints = {}
        for col_info in column_summary:
            col_name = col_info["column"]
            ratio = col_info["uniqueness_ratio"]
            
            # ìµœì†Œí•œì˜ íœ´ë¦¬ìŠ¤í‹± (fallback only)
            if ratio == 1.0:
                pattern = "UNIQUE"
            elif ratio < 0.3:
                pattern = "HIGH_REPETITION"
            else:
                pattern = "LOW_REPETITION"
            
            fallback_hints[col_name] = {
                "pattern": pattern,
                "inferred_role": "unknown",
                "confidence": 0.5,  # ë‚®ì€ ì‹ ë¢°ë„
                "hint": f"Fallback analysis: uniqueness_ratio={ratio}"
            }
        
        return fallback_hints
```


def _extract_filename_hints(filename: str) -> dict:
    """
    Ruleë¡œ íŒŒì¼ëª… íŒŒì‹± â†’ LLMì´ ì˜ë¯¸ ì¶”ë¡ 
    
    1ë‹¨ê³„ (Rule): íŒŒì¼ëª… êµ¬ì¡° ë¶„ì„ (í™•ì¥ì, base_name, ì–¸ë”ìŠ¤ì½”ì–´ ë¶„ë¦¬)
    2ë‹¨ê³„ (LLM): íŒŒì‹±ëœ ì •ë³´ë¥¼ ë³´ê³  Entity Type, Level, ê´€ê³„ ì¶”ë¡ 
    """
    
    # === 1ë‹¨ê³„: Rule-based íŒŒì¼ëª… íŒŒì‹± ===
    basename = os.path.basename(filename)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë¶„ë¦¬ (Rule)
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    # ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì¶”ì¶œ (Rule)
    has_prefix = len(parts) > 1
    prefix = parts[0] if has_prefix and len(parts) >= 2 else None
    suffix = parts[-1] if has_prefix and len(parts) >= 2 else None
    
    # Ruleë¡œ ì¶”ì¶œí•œ êµ¬ì¡° ì •ë³´
    parsed_structure = {
        "original_filename": basename,
        "name_without_ext": name_without_ext,
        "extension": extension,
        "parts": parts,  # ['lab', 'data'] or ['clinical', 'parameters']
        "base_name": base_name,  # 'lab', 'clinical'
        "prefix": prefix,
        "suffix": suffix,
        "has_underscore": '_' in name_without_ext,
        "num_parts": len(parts)
    }
    
    # === 2ë‹¨ê³„: LLM ê¸°ë°˜ ì˜ë¯¸ ì¶”ë¡  ===
    # Ruleë¡œ íŒŒì‹±í•œ êµ¬ì¡° ì •ë³´ë¥¼ LLMì—ê²Œ ì œê³µ
    prompt = f"""
You are a Data Architecture Analyst.

I have parsed the filename structure using rules. Based on this parsed information, infer the semantic meaning.

[PARSED FILENAME STRUCTURE - Extracted by Rules]
{json.dumps(parsed_structure, indent=2)}

[YOUR TASK - Semantic Interpretation]
Using the PARSED STRUCTURE provided above, infer the following:

1. **Entity Type**: What domain entity does the base_name represent?
   - Look at "base_name" and "parts"
   - Examples: "lab" â†’ Laboratory, "patient" â†’ Patient, "clinical" â†’ Clinical/Case
   - Use medical domain knowledge

2. **Scope**: What is the scope of data?
   - individual: Patient-level, Subject-level
   - event: Case, Admission, Visit, Stay
   - measurement: Lab, Vital, Sensor data
   - treatment: Medication, Procedure
   - clinical: Diagnosis, Notes

3. **Suggested Hierarchy Level**: (1=highest, 5=lowest)
   - Based on entity type and domain knowledge
   - Level 1: Patient, Subject
   - Level 2: Case, Admission, Visit
   - Level 3: Sub-event (ICU Stay, Transfer)
   - Level 4: Measurement (Lab, Vital)
   - Level 5: Event detail (Single measurement)

4. **Data Type Indicator**: Based on suffix in parsed parts
   - If suffix is "data", "records", "events" â†’ transactional
   - If suffix is "parameters", "dict", "info" â†’ metadata
   - If prefix is "master", "dim" â†’ reference/master

5. **Related File Patterns**: Predict related files using base_name
   - If this is "lab_data", likely has "lab_parameters" or "lab_dict"
   - If this is "clinical_parameters", likely describes "clinical_data"
   
IMPORTANT: Base your reasoning on the PARSED STRUCTURE I provided (parts, base_name, suffix).
Do not just repeat the parsing - interpret the meaning.

[OUTPUT FORMAT - JSON]
{{
    "entity_type": "Laboratory" or null,
    "scope": "measurement" or null,
    "suggested_level": 4 or null,
    "base_name": "lab",
    "data_type_indicator": "transactional" or "metadata" or "master",
    "related_file_patterns": ["lab_parameters", "lab_dict", "lab_info"],
    "processing_stage": "raw" or "processed" or "final" or null,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation of the analysis"
}}

Examples:
- "clinical_data.csv" â†’ entity_type: "Case", level: 2, base_name: "clinical", data_type: "transactional"
- "lab_parameters.csv" â†’ entity_type: null, level: null, base_name: "lab", data_type: "metadata"
- "master_patient.csv" â†’ entity_type: "Patient", level: 1, base_name: "patient", data_type: "master"
"""
    
    try:
        hints = llm_client.ask_json(prompt)
        
        # ê¸°ë³¸ í•„ë“œ ì¶”ê°€
        hints["filename"] = basename
        
        # Confidence ê²€ì¦
        if hints.get("confidence", 1.0) < 0.7:
            print(f"âš ï¸  [Filename Analysis] Low confidence ({hints.get('confidence')}) for {basename}")
        
        return hints
        
    except Exception as e:
        # LLM ì‹¤íŒ¨ ì‹œ ìµœì†Œ ì •ë³´ë§Œ ë°˜í™˜
        print(f"âŒ [Filename Analysis] LLM Error: {e}")
        return {
            "filename": basename,
            "entity_type": None,
            "scope": None,
            "suggested_level": None,
            "base_name": name_without_ext.split('_')[0],  # ìµœì†Œí•œì˜ íŒŒì‹±
            "data_type_indicator": None,
            "related_file_patterns": [],
            "confidence": 0.0,
            "error": str(e)
        }
```

---

#### 2.2.3 ê³„ì¸µ êµ¬ì¡° ì—…ë°ì´íŠ¸ ë¡œì§

```python
def _update_hierarchy(
    ontology_context: dict,
    new_relationships: List[dict],
    new_hierarchy: List[dict]
) -> dict:
    """
    ìƒˆë¡œìš´ ê´€ê³„ ì •ë³´ë¡œ ê³„ì¸µ êµ¬ì¡° ì—…ë°ì´íŠ¸
    """
    
    # 1. ê¸°ì¡´ ê³„ì¸µê³¼ ì‹ ê·œ ê³„ì¸µ ë³‘í•©
    existing_hierarchy = ontology_context.get("hierarchy", [])
    
    # 2. ì¶©ëŒ í•´ê²°
    merged_hierarchy = []
    seen_entities = set()
    
    # ì‹ ê·œ ê³„ì¸µ ìš°ì„  (ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ)
    for new_level in new_hierarchy:
        entity = new_level["entity_name"]
        if entity not in seen_entities:
            merged_hierarchy.append(new_level)
            seen_entities.add(entity)
    
    # ê¸°ì¡´ ê³„ì¸µ ì¤‘ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²ƒ ì¶”ê°€
    for old_level in existing_hierarchy:
        entity = old_level["entity_name"]
        if entity not in seen_entities:
            merged_hierarchy.append(old_level)
            seen_entities.add(entity)
    
    # 3. ë ˆë²¨ ë²ˆí˜¸ ì¬ì •ë ¬ (ë‚®ì€ ë ˆë²¨ë¶€í„°)
    merged_hierarchy.sort(key=lambda x: x["level"])
    
    # 4. ê´€ê³„ ì¶”ê°€
    existing_relationships = ontology_context.get("relationships", [])
    
    # ì¤‘ë³µ ì œê±° (ê°™ì€ source-target ì¡°í•©)
    relationship_keys = set()
    unique_relationships = []
    
    for rel in new_relationships + existing_relationships:
        key = (rel["source_table"], rel["target_table"], 
               rel["source_column"], rel["target_column"])
        if key not in relationship_keys:
            unique_relationships.append(rel)
            relationship_keys.add(key)
    
    return {
        **ontology_context,
        "hierarchy": merged_hierarchy,
        "relationships": unique_relationships
    }
```

---

### 2.3 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (íŒŒì¼ëª… ê¸°ë°˜ ì¶”ë¡ )

#### ì‹œë‚˜ë¦¬ì˜¤ 1: VitalDB (í˜„ì¬ ë°ì´í„°)

```python
# ì‹¤í–‰ ìˆœì„œ
files = [
    "clinical_parameters.csv",  # ë©”íƒ€ë°ì´í„° (íŒŒì¼ëª…: clinical + parameters)
    "lab_parameters.csv",       # ë©”íƒ€ë°ì´í„° (íŒŒì¼ëª…: lab + parameters)
    "clinical_data.csv",        # í—ˆë¸Œ í…Œì´ë¸” (íŒŒì¼ëª…: clinical + data)
    "lab_data.csv"             # ìì‹ í…Œì´ë¸” (íŒŒì¼ëª…: lab + data)
]

# [NEW] íŒŒì¼ëª… ë¶„ì„ ê²°ê³¼
filename_analysis = {
    "clinical_parameters.csv": {
        "entity_type": None,
        "is_likely_metadata": True,
        "describes_table": "clinical",
        "related_patterns": ["clinical_data", "clinical_dict"]
    },
    "clinical_data.csv": {
        "entity_type": "Case",  # 'clinical' â†’ case/procedure
        "is_likely_metadata": False,
        "suggested_level": 2,
        "base_name": "clinical"
    },
    "lab_data.csv": {
        "entity_type": "Laboratory",
        "scope": "measurement",
        "suggested_level": 4,
        "base_name": "lab",
        "related_patterns": ["lab_parameters"]  # â† ìë™ ì—°ê²°!
    }
}

# ì˜ˆìƒ ê²°ê³¼
ontology_context = {
    "definitions": {
        "caseid": "Case ID...",
        "subjectid": "Subject ID...",
        "alb": "Albumin...",
        # ... 100+ ìš©ì–´
    },
    "relationships": [
        {
            "source_table": "lab_data",
            "target_table": "clinical_data",
            "source_column": "caseid",
            "target_column": "caseid",
            "relation_type": "N:1"
        }
    ],
    "hierarchy": [
        {"level": 1, "entity_name": "Patient", "anchor_column": "subjectid"},
        {"level": 2, "entity_name": "Case", "anchor_column": "caseid"}
    ]
}
```

---

#### ì‹œë‚˜ë¦¬ì˜¤ 2: MIMIC-IV (ë‹¤ë¥¸ êµ¬ì¡°)

```python
# ê°€ìƒ ë°ì´í„° êµ¬ì¡°
files = [
    "patients.csv",       # subject_id
    "admissions.csv",     # hadm_id, subject_id
    "icustays.csv",       # stay_id, hadm_id
    "chartevents.csv"     # stay_id, itemid, value
]

# ì˜ˆìƒ ê²°ê³¼
hierarchy = [
    {"level": 1, "entity_name": "Patient", "anchor_column": "subject_id"},
    {"level": 2, "entity_name": "Hospital_Admission", "anchor_column": "hadm_id"},
    {"level": 3, "entity_name": "ICU_Stay", "anchor_column": "stay_id"}
]

relationships = [
    {"source": "admissions", "target": "patients", "via": "subject_id"},
    {"source": "icustays", "target": "admissions", "via": "hadm_id"},
    {"source": "chartevents", "target": "icustays", "via": "stay_id"}
]
```

---

## 3. ì˜ˆìƒ íš¨ê³¼ ë° ê²€ì¦

### 3.1 ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | Before (Rule-based) | After (LLM-based ëª©í‘œ) |
|------|---------------------|----------------------|
| ë©”íƒ€ë°ì´í„° ê°ì§€ ì •í™•ë„ | 70-80% | **95-98%** |
| ë©”íƒ€ë°ì´í„° íŒŒì¼ Human Review | 100% | **0-5%** (low confidenceë§Œ) |
| Multi-table JOIN ìˆ˜ë™ ì„¤ì • | 100% | **0%** |
| ìƒˆ ë°ì´í„°ì…‹ ì ì‘ ì‹œê°„ | ìˆ˜ì¼ | **ìˆ˜ì‹œê°„** |
| Anchor ë§¤ì¹­ ì •í™•ë„ | 60% | **95%** |
| False Positive (ì˜¤íŒ) | 15-20% | **< 5%** |
| í™•ì‹ ë„ í‰ê·  | N/A | **> 0.85** |

**LLM ê¸°ë°˜ì˜ ì´ì :**
- âœ… ìƒˆë¡œìš´ ëª…ëª… íŒ¨í„´ ìë™ í•™ìŠµ (ê·œì¹™ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)
- âœ… íŒŒì¼ëª… + êµ¬ì¡° + ë‚´ìš© ì¢…í•© íŒë‹¨ (íœ´ë¦¬ìŠ¤í‹±ë³´ë‹¤ ì •í™•)
- âœ… ì• ë§¤í•œ ê²½ìš° confidenceë¡œ í‘œí˜„ (íˆ¬ëª…ì„± í–¥ìƒ)
- âœ… ë‹¤ì–‘í•œ ë„ë©”ì¸ ì ì‘ (ì˜ë£Œ ì™¸ ë°ì´í„°ë„ ì²˜ë¦¬ ê°€ëŠ¥)

---

### 3.2 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### âœ… Phase 0-1 ê²€ì¦ ì™„ë£Œ (2025-12-17)
- [x] clinical_parameters.csv ë©”íƒ€ë°ì´í„° ì¸ì‹ (confidence: 96%)
- [x] lab_parameters.csv ë©”íƒ€ë°ì´í„° ì¸ì‹ (confidence: 95%)
- [x] track_names.csv ë©”íƒ€ë°ì´í„° ì¸ì‹ (confidence: 93%)
- [x] clinical_data.csv ì¼ë°˜ ë°ì´í„° ì¸ì‹ (confidence: 95%)
- [x] lab_data.csv ì¼ë°˜ ë°ì´í„° ì¸ì‹ (confidence: 90%)
- [x] **í‰ê·  confidence: 94.2%** (ëª©í‘œ 85% ì´ˆê³¼)
- [x] **ì˜¤íŒìœ¨: 0%** (5/5 ì •í™•)
- [x] Negative Evidence ìˆ˜ì§‘ (null_ratio, ì¤‘ë³µ ì²´í¬)
- [x] Context Window ê´€ë¦¬ (ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½)
- [x] definitions 310ê°œ ìš©ì–´ ì €ì¥
- [x] ë©”íƒ€ë°ì´í„° íŒŒì¼ skip_indexing=True
- [x] ìºì‹± ì‘ë™ (83% Hit Rate, $0.30 ì ˆì•½)
- [x] ì˜¨í†¨ë¡œì§€ íŒŒì¼ ì €ì¥ (ontology_db.json)
- [x] ì¤‘ë³µ ì €ì¥ ë°©ì§€ (ë©±ë“±ì„± ë³´ì¥)

#### âœ… Phase 2 ê²€ì¦ ì™„ë£Œ (2025-12-17)
- [x] lab_data â†” clinical_data FK ê´€ê³„ ìë™ ë°œê²¬
- [x] relation_type: N:1 ì •í™• íŒë‹¨ (confidence: 0.86)
- [x] ê´€ê³„ Description ìƒì„¸ ("lab results belong to a case...")
- [x] ê³„ì¸µ 3ë ˆë²¨ ìë™ êµ¬ì¶•
  - [x] L1: Patient (subjectid)
  - [x] L2: Case/Encounter (caseid)
  - [x] L3: Lab Observation (caseid)
- [x] Hierarchy ì¤‘ë³µ ì œê±° (4ê°œ â†’ 3ê°œ)
- [x] clinical_dataê°€ Hub Tableë¡œ ì¸ì‹ (mapping_table)
- [x] íŒŒì¼ëª… íŒíŠ¸ í™œìš© (Entity Type ì¶”ë¡ )
- [x] ì»¬ëŸ¼ ì •ë³´ ì €ì¥ (file_tags)

#### ğŸ”œ Phase 3 ê²€ì¦ ì˜ˆì • (ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜)

**Part A: ê´€ê³„í˜• DB**
- [ ] SQLite DB íŒŒì¼ ìƒì„± (medical_data.db)
- [ ] í…Œì´ë¸” ìƒì„± (clinical_data_table, lab_data_table)
- [ ] **[NEW]** ëŒ€ìš©ëŸ‰ ë°ì´í„° Chunk ì²˜ë¦¬ í™•ì¸
  - [ ] lab_data 928,450í–‰ â†’ ë©”ëª¨ë¦¬ ì´ˆê³¼ ì—†ì´ ì ì¬
  - [ ] ì²˜ë¦¬ ë¡œê·¸: "Chunk 1: 100,000í–‰", "Chunk 2: 100,000í–‰"...
- [ ] FK ì œì•½ì¡°ê±´ ì ìš© í™•ì¸
- [ ] ë°ì´í„° ì ì¬ ì™„ë£Œ (í–‰ ê°œìˆ˜ ì •í™•)
- [ ] ì¸ë±ìŠ¤ ìë™ ìƒì„± (Level 1-2: caseid, subjectid)
- [ ] **[NEW]** Schema Evolution í…ŒìŠ¤íŠ¸
  - [ ] ê°™ì€ íŒŒì¼ ì¬ì‹¤í–‰ â†’ Replace ì •ìƒ ì‘ë™

**Part B: VectorDB**
- [ ] ChromaDB ì´ˆê¸°í™”
- [ ] **[NEW]** ê³„ì¸µì  ì„ë² ë”© ìƒì„±
  - [ ] Table Summary: 5ê°œ (ë°ì´í„° íŒŒì¼)
  - [ ] Column Definition: 310ê°œ
  - [ ] Relationship: 1ê°œ
  - [ ] **ì´ 316ê°œ** (ê¸°ì¡´ 311ê°œ â†’ ì¦ê°€)
- [ ] **[NEW]** Table-Level Search ì‘ë™
  - [ ] "í™˜ì ì •ë³´ í…Œì´ë¸”" â†’ clinical_data ê²€ìƒ‰ ì„±ê³µ
- [ ] Column-Level Search ì‘ë™
  - [ ] "í˜ˆì••" â†’ bp_sys, bp_dia ê²€ìƒ‰ ì„±ê³µ
- [ ] Relationship Search ì‘ë™
  - [ ] "lab ì—°ê²°" â†’ FK ì •ë³´ ê²€ìƒ‰ ì„±ê³µ
- [ ] **[NEW]** Hybrid Search ì‘ë™
  - [ ] Keyword + Vector ê²°í•© ê²€ìƒ‰
- [ ] **[NEW]** Context Assembly ì‘ë™
  - [ ] ê²€ìƒ‰ ê²°ê³¼ + ê´€ë ¨ í…Œì´ë¸” + JOIN ê²½ë¡œ ì¡°ë¦½

**í™•ì¥ì„± ê²€ì¦:**
- [ ] ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥ í™•ì¸
- [ ] ë©”íƒ€ë°ì´í„° í™•ì¥ ê°€ëŠ¥ í™•ì¸
- [ ] ì¶”ê°€ ê°œì„  ì—¬ì§€ ë¬¸ì„œí™”

---

## 4. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: LLM ì¶”ë¡  ì˜¤ë¥˜
**í™•ë¥ :** ì¤‘  
**ì˜í–¥:** ë†’ìŒ (ì˜ëª»ëœ íŒë‹¨ â†’ ë°ì´í„° ì†ì‹¤ ë˜ëŠ” ì˜ëª»ëœ ê´€ê³„)

**ëŒ€ì‘:**
- **ë©”íƒ€ë°ì´í„° ê°ì§€**: confidence < 0.75 â†’ Human Review íŠ¸ë¦¬ê±°
- **ê´€ê³„ ì¶”ë¡ **: confidence < 0.85 â†’ ì‚¬ìš©ì í™•ì¸
- ìƒ˜í”Œ ì¿¼ë¦¬ ìë™ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ê²€ì¦
- ì‚¬ìš©ìê°€ íŒë‹¨ì„ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” UI ì œê³µ
- ì˜¨í†¨ë¡œì§€ì— "verified" í”Œë˜ê·¸ ì €ì¥ (Human confirmed)

### ë¦¬ìŠ¤í¬ 1-A: LLM API ì¥ì•  ë˜ëŠ” ë¹„ìš©
**í™•ë¥ :** ë‚®  
**ì˜í–¥:** ì¤‘ (ì‹œìŠ¤í…œ ì •ì§€)

**ëŒ€ì‘:**
- **Fallback ì „ëµ**: LLM ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì  ê¸°ë³¸ê°’ ì‚¬ìš©
  ```python
  # LLM ì‹¤íŒ¨ ì‹œ
  if llm_error:
      return {
          "is_metadata": False,  # ë³´ìˆ˜ì : ì¼ë‹¨ ë°ì´í„°ë¡œ ì²˜ë¦¬
          "confidence": 0.0,
          "needs_human_review": True
      }
  ```
- **ìºì‹±**: ë™ì¼ íŒŒì¼ ì¬ì²˜ë¦¬ ì‹œ LLM í˜¸ì¶œ ìŠ¤í‚µ
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ LLM í˜¸ì¶œë¡œ ì²˜ë¦¬ (ë¹„ìš© ì ˆê°)

---

### ë¦¬ìŠ¤í¬ 2: ë³µì¡í•œ M:N ê´€ê³„
**í™•ë¥ :** ë‚®  
**ì˜í–¥:** ì¤‘

**ëŒ€ì‘:**
- Junction Table ìë™ ì¸ì‹
- ìµœì´ˆ Phaseì—ì„œëŠ” M:N ì œì™¸, 1:1/1:Në§Œ ì²˜ë¦¬
- Phase 4ì—ì„œ í™•ì¥

---

### ë¦¬ìŠ¤í¬ 3: ì„±ëŠ¥ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
**í™•ë¥ :** ì¤‘  
**ì˜í–¥:** ë†’ìŒ

**[ì „ë¬¸ê°€ í”¼ë“œë°±] ë³‘ëª© ì§€ì :**
1. **ë©”ëª¨ë¦¬ ë¶€ì¡± (Phase 3)**
   - ë¬¸ì œ: lab_data.csv (928MB) â†’ `df.read_csv()` â†’ RAM ì´ˆê³¼
   - ì˜í–¥: í”„ë¡œì„¸ìŠ¤ í¬ë˜ì‹œ
   
2. **ì„ë² ë”© ìƒì„± ì‹œê°„ (Phase 3)**
   - ë¬¸ì œ: 310ê°œ ì»¬ëŸ¼ Ã— OpenAI API í˜¸ì¶œ â†’ ìˆ˜ë¶„ ì†Œìš”
   - ì˜í–¥: ì´ˆê¸° êµ¬ì¶• ëŠë¦¼

**ëŒ€ì‘:**
- **[NEW]** Chunk Processing (chunksize=100,000)
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
  - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬
  
- **[NEW]** ë°°ì¹˜ ì„ë² ë”© (ChromaDB)
  - ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì„ë² ë”©
  - collection.add(documents=[...]) - ë°°ì¹˜ ì²˜ë¦¬
  
- LLM í˜¸ì¶œ ìºì‹± (ì´ë¯¸ êµ¬í˜„ë¨)
- ìƒ˜í”Œë§ ìµœì í™” (20í–‰ â†’ í•„ìš” ì‹œ 100í–‰)

---

## 5. êµ¬ì²´ì  êµ¬í˜„ ì½”ë“œ (Phase 0-1)

### 5.1 ì™„ì „í•œ ì½”ë“œ ì˜ˆì‹œ

#### `src/agents/state.py` (í™•ì¥)

```python
from typing import TypedDict, List, Dict, Optional, Literal, Any
import operator
from typing import Annotated

class Relationship(TypedDict):
    """í…Œì´ë¸” ê°„ ê´€ê³„"""
    source_table: str
    target_table: str
    source_column: str
    target_column: str
    relation_type: Literal["1:1", "1:N", "N:1", "M:N"]
    confidence: float
    description: str
    # [NEW] ê²€ì¦ ì •ë³´
    llm_inferred: bool
    human_verified: Optional[bool]
    verified_at: Optional[str]

class EntityHierarchy(TypedDict):
    """ê³„ì¸µ êµ¬ì¡°"""
    level: int
    entity_name: str        # Patient, Case, Visit, Measurement
    anchor_column: str
    mapping_table: Optional[str]
    confidence: float

class OntologyContext(TypedDict):
    """ì „ì—­ ì§€ì‹ ê·¸ë˜í”„"""
    # 1. ìš©ì–´ ì‚¬ì „
    definitions: Dict[str, str]
    
    # 2. ê´€ê³„ ë° ê³„ì¸µ
    relationships: List[Relationship]
    hierarchy: List[EntityHierarchy]
    
    # 3. íŒŒì¼ íƒœê·¸ (ë©”íƒ€ë°ì´í„° vs ë°ì´í„°)
    file_tags: Dict[str, Dict[str, Any]]

class AgentState(TypedDict):
    """ì—ì´ì „íŠ¸ ì „ì—­ ìƒíƒœ"""
    # ì…ë ¥
    file_path: str
    file_type: Optional[str]
    
    # ì²˜ë¦¬ ê²°ê³¼
    raw_metadata: Dict[str, Any]
    finalized_anchor: Optional[Dict]
    finalized_schema: List[Dict]
    
    # [NEW] ì˜¨í†¨ë¡œì§€
    ontology_context: OntologyContext
    skip_indexing: bool  # ë©”íƒ€ë°ì´í„° íŒŒì¼ ìŠ¤í‚µìš©
    
    # Human Loop
    needs_human_review: bool
    human_question: str
    human_feedback: Optional[str]
    
    # ì‹œìŠ¤í…œ
    logs: Annotated[List[str], operator.add]
    retry_count: int
    error_message: Optional[str]
```

---

#### `src/utils/llm_cache.py` (ì™„ì „ êµ¬í˜„)

```python
import hashlib
import json
from pathlib import Path
from typing import Optional

class LLMCache:
    """LLM ì‘ë‹µ ìºì‹± (ë¹„ìš© ì ˆê°)"""
    
    def __init__(self, cache_dir: str = "data/cache/llm"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.hit_count = 0
        self.miss_count = 0
    
    def _get_key(self, prompt: str, context: dict) -> str:
        """í”„ë¡¬í”„íŠ¸ + ì»¨í…ìŠ¤íŠ¸ë¡œ ê³ ìœ  í‚¤ ìƒì„±"""
        content = f"{prompt}::{json.dumps(context, sort_keys=True)}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get(self, prompt: str, context: dict) -> Optional[dict]:
        """ìºì‹œ ì¡°íšŒ"""
        key = self._get_key(prompt, context)
        cache_file = self.cache_dir / f"{key}.json"
        
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                self.hit_count += 1
                print(f"âœ… [Cache Hit] ìºì‹œ ì‚¬ìš© ({self.hit_count} hits)")
                return json.load(f)
        
        self.miss_count += 1
        return None
    
    def set(self, prompt: str, context: dict, result: dict):
        """ìºì‹œ ì €ì¥"""
        key = self._get_key(prompt, context)
        cache_file = self.cache_dir / f"{key}.json"
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        cached_data = {
            "result": result,
            "prompt_hash": key,
            "cached_at": __import__('datetime').datetime.now().isoformat()
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cached_data, f, indent=2, ensure_ascii=False)
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        import shutil
        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir()
        print("ğŸ—‘ï¸ ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")
    
    def stats(self):
        """ìºì‹œ í†µê³„"""
        total = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total if total > 0 else 0
        return {
            "hits": self.hit_count,
            "misses": self.miss_count,
            "hit_rate": hit_rate,
            "estimated_savings": self.hit_count * 0.03  # $0.03/call
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
llm_cache = LLMCache()
```

---

#### `src/agents/nodes.py` - `ontology_builder_node` (ì™„ì „ êµ¬í˜„)

```python
import os
import json
import numpy as np
from typing import Dict, Any

from src.agents.state import AgentState, OntologyContext
from src.utils.llm_client import get_llm_client
from src.utils.llm_cache import llm_cache

llm_client = get_llm_client()


def ontology_builder_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node] ì˜¨í†¨ë¡œì§€ êµ¬ì¶• (Rule Prepares, LLM Decides)
    """
    print("\n" + "="*80)
    print("ğŸ“š [ONTOLOGY BUILDER NODE] ì‹œì‘")
    print("="*80)
    
    file_path = state["file_path"]
    metadata = state["raw_metadata"]
    
    # ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ê°€ì ¸ì˜¤ê¸°
    ontology = state.get("ontology_context", {
        "definitions": {},
        "relationships": [],
        "hierarchy": [],
        "file_tags": {}
    })
    
    # === Step 1: Rule Prepares ===
    print("\nğŸ”§ [Rule] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    context = _build_metadata_detection_context_v2(file_path, metadata)
    print(f"   - íŒŒì¼ëª… íŒŒì‹±: {context.get('name_parts')}")
    print(f"   - ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: ~{context.get('context_size_estimate', 0)} bytes")
    
    # === Step 2: LLM Decides (ìºì‹± í¬í•¨) ===
    print("\nğŸ§  [LLM] ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨ ì¤‘...")
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("is_metadata_detection", context)
    if cached:
        meta_result = cached["result"]
    else:
        meta_result = _ask_llm_is_metadata(context)
        llm_cache.set("is_metadata_detection", context, meta_result)
    
    confidence = meta_result.get("confidence", 0.0)
    is_metadata = meta_result.get("is_metadata", False)
    
    print(f"   - íŒë‹¨: {'ë©”íƒ€ë°ì´í„°' if is_metadata else 'ì¼ë°˜ ë°ì´í„°'}")
    print(f"   - í™•ì‹ ë„: {confidence:.2%}")
    
    # === Step 3: Confidence Check ===
    if confidence < 0.75:
        print(f"\nâš ï¸  [Low Confidence] Human Review ìš”ì²­")
        
        # êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„±
        specific_question = _generate_specific_human_question(
            file_path, meta_result, context
        )
        
        return {
            "needs_human_review": True,
            "human_question": specific_question,
            "logs": [f"âš ï¸ [Ontology] ë©”íƒ€ë°ì´í„° íŒë‹¨ ë¶ˆí™•ì‹¤ ({confidence:.2%})"]
        }
    
    # === Step 4: Branching ===
    
    # [Branch A] ë©”íƒ€ë°ì´í„° íŒŒì¼
    if is_metadata:
        print(f"\nğŸ“– [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        # íŒŒì¼ íƒœê·¸ ì €ì¥
        ontology["file_tags"][file_path] = {
            "type": "metadata",
            "role": "dictionary",
            "confidence": confidence
        }
        
        # ë‚´ìš© íŒŒì‹± (Rule)
        new_definitions = _parse_metadata_content(file_path)
        ontology["definitions"].update(new_definitions)
        
        print(f"   - ìš©ì–´ {len(new_definitions)}ê°œ ì¶”ê°€")
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": True,  # ì¤‘ìš”!
            "logs": [f"ğŸ“š [Ontology] ë©”íƒ€ë°ì´í„° ë“±ë¡: {len(new_definitions)}ê°œ ìš©ì–´"]
        }
    
    # [Branch B] ì¼ë°˜ ë°ì´í„° íŒŒì¼
    else:
        print(f"\nğŸ“Š [Data] ì¼ë°˜ ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        ontology["file_tags"][file_path] = {
            "type": "transactional_data",
            "confidence": confidence
        }
        
        print("   - ê´€ê³„ ì¶”ë¡ ì€ analyzerì—ì„œ ìˆ˜í–‰")
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": False,
            "logs": ["ğŸ” [Ontology] ì¼ë°˜ ë°ì´í„° í™•ì¸"]
        }


# === Helper Functions ===

def _collect_negative_evidence(col_name: str, samples: list, unique_vals: list) -> dict:
    """
    [Rule] ë¶€ì • ì¦ê±° ìˆ˜ì§‘
    """
    total = len(samples)
    unique = len(unique_vals)
    
    # null ê³„ì‚°
    null_count = sum(1 for s in samples if s is None or s == '' or (isinstance(s, float) and np.isnan(s)))
    
    negative_evidence = []
    
    # 1. ê±°ì˜ uniqueì¸ë° ì¤‘ë³µ ìˆìŒ
    if unique / total > 0.95 and unique != total:
        dup_rate = (total - unique) / total
        negative_evidence.append({
            "type": "near_unique_with_duplicates",
            "detail": f"{unique/total:.1%} unique BUT {dup_rate:.1%} duplicates",
            "severity": "medium"
        })
    
    # 2. ID ê°™ì€ë° null ìˆìŒ
    if 'id' in col_name.lower() and null_count > 0:
        null_rate = null_count / total
        negative_evidence.append({
            "type": "identifier_with_nulls",
            "detail": f"Name suggests ID BUT {null_rate:.1%} null values",
            "severity": "high" if null_rate > 0.1 else "low"
        })
    
    # 3. Cardinality ë„ˆë¬´ ë†’ìŒ
    if unique > 100:
        negative_evidence.append({
            "type": "high_cardinality",
            "detail": f"{unique} unique values - might be free text, not categorical",
            "severity": "low"
        })
    
    return {
        "has_issues": len(negative_evidence) > 0,
        "issues": negative_evidence,
        "null_ratio": null_count / total if total > 0 else 0
    }


def _summarize_long_values(values: list, max_length: int = 50) -> list:
    """
    [Rule] ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (Context Window ê´€ë¦¬)
    """
    summarized = []
    
    for val in values:
        val_str = str(val)
        
        if len(val_str) > max_length:
            # ë©”íƒ€ ì •ë³´ë¡œ ëŒ€ì²´
            summarized.append(f"[Text: {len(val_str)} chars, starts='{val_str[:20]}...']")
        else:
            summarized.append(val_str)
    
    return summarized


def _parse_metadata_content(file_path: str) -> dict:
    """
    [Rule] ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹± (CSV â†’ Dictionary)
    """
    import pandas as pd
    
    definitions = {}
    
    try:
        df = pd.read_csv(file_path)
        
        # ì¼ë°˜ì ì¸ ë©”íƒ€ë°ì´í„° êµ¬ì¡°: [Parameter/Name, Description, ...]
        # ì²« ë‘ ì»¬ëŸ¼ì„ Key-Valueë¡œ ê°€ì •
        if len(df.columns) >= 2:
            key_col = df.columns[0]  # Parameter, Variable, Name ë“±
            desc_col = df.columns[1]  # Description, Definition ë“±
            
            for _, row in df.iterrows():
                key = str(row[key_col]).strip()
                desc = str(row[desc_col]).strip()
                
                # ì¶”ê°€ ì •ë³´ ê²°í•© (Unit, Type ë“±)
                extra_info = []
                for col in df.columns[2:]:
                    val = row[col]
                    if pd.notna(val) and str(val).strip():
                        extra_info.append(f"{col}: {val}")
                
                if extra_info:
                    desc += " | " + " | ".join(extra_info)
                
                definitions[key] = desc
        
        return definitions
        
    except Exception as e:
        print(f"âŒ [Parse Error] {e}")
        return {}
```

---

### 5.2 Negative Evidence í†µí•© ì˜ˆì‹œ

```python
# caseid ì»¬ëŸ¼ ë¶„ì„ ì „ì²´ í”Œë¡œìš°

# === Rule Prepares ===
samples = [1, 2, 3, 4, 5, 1, 2, 3, ...]  # 20ê°œ ìƒ˜í”Œ
unique_vals = [1, 2, 3, 4, 5, ...]       # unique ì¶”ì¶œ
ratio = 5 / 20 = 0.25                    # ê³„ì‚°

negative = _collect_negative_evidence('caseid', samples, unique_vals)
# â†’ {
#     "has_issues": False,  # ì •ìƒ
#     "null_ratio": 0.0
#   }

# === LLM Decides ===
prompt = f"""
[Positive Evidence]
- Column: caseid
- Unique values: {unique_vals}
- Uniqueness ratio: {ratio}

[Negative Evidence]
{json.dumps(negative, indent=2)}

Based on BOTH evidences, what is the role?
"""

result = {
    "inferred_role": "foreign_key",
    "confidence": 0.92,
    "reasoning": "Ratio 0.25 shows high repetition (N:1 pattern) + no data quality issues"
}

# === ë§Œì•½ ì´ìƒì´ ìˆì—ˆë‹¤ë©´? ===
# negative = {
#     "issues": [{
#         "type": "near_unique_with_duplicates",
#         "detail": "99% unique BUT 1% duplicates"
#     }],
#     "null_ratio": 0.01
# }
# 
# LLM: "This might be PK but has duplicates. 
#       Confidence: 0.68 (low) â†’ Human Review needed"
```

---

## 6. ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ê²°ì • í•„ìš”
1. **Phase 0-1 ì‹œì‘ ìŠ¹ì¸?**
   - [ ] ìŠ¹ì¸ (LLM ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ê°ì§€ êµ¬í˜„ ì‹œì‘)
   - [ ] ë³´ë¥˜ (ì¶”ê°€ ë…¼ì˜ í•„ìš”)

2. **ì˜¨í†¨ë¡œì§€ ì €ì¥ í˜•ì‹?**
   - [ ] **JSON íŒŒì¼** (ê°„ë‹¨, ê¶Œì¥, Git ê´€ë¦¬ ìš©ì´)
   - [ ] SQLite í…Œì´ë¸” (ì¿¼ë¦¬ í¸ë¦¬)
   - [ ] ë©”ëª¨ë¦¬ (íœ˜ë°œì„±)

3. **Human Review ì •ì±…? (LLM í™•ì‹ ë„ ê¸°ë°˜)**
   - [ ] **Confidence < 0.75** â†’ í•­ìƒ ë¬¼ì–´ë´„ (ê¶Œì¥)
   - [ ] Confidence < 0.85 â†’ ë³´ìˆ˜ì 
   - [ ] ìë™ ì§„í–‰ (ë¡œê·¸ë§Œ ê¸°ë¡, ìœ„í—˜)

4. **LLM í˜¸ì¶œ ìµœì í™”?**
   - [ ] **ìºì‹± í™œì„±í™”** (ë™ì¼ íŒŒì¼ ì¬ì²˜ë¦¬ ì‹œ LLM ìŠ¤í‚µ)
   - [ ] ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ íŒë‹¨)
   - [ ] Fallback ì „ëµ (LLM ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì  ê¸°ë³¸ê°’)

### Action Items

#### âœ… Phase 0-2 ì™„ë£Œ (2025-12-17)

**ì½”ë“œ êµ¬í˜„:**
- [x] `src/agents/state.py` - OntologyContext, Relationship, EntityHierarchy ì¶”ê°€
- [x] `src/utils/llm_cache.py` - LLM ìºì‹± ì‹œìŠ¤í…œ (83% Hit Rate)
- [x] `src/utils/ontology_manager.py` - ì˜¨í†¨ë¡œì§€ ì €ì¥/ë¡œë“œ/ë³‘í•©
- [x] `src/agents/nodes.py` - 7ê°œ í•µì‹¬ í•¨ìˆ˜ êµ¬í˜„
  - [x] `ontology_builder_node()` - ë©”ì¸ ë…¸ë“œ
  - [x] `_collect_negative_evidence()` - ë°ì´í„° í’ˆì§ˆ ì²´í¬
  - [x] `_summarize_long_values()` - Context Window ê´€ë¦¬
  - [x] `_build_metadata_detection_context()` - Rule ì „ì²˜ë¦¬
  - [x] `_ask_llm_is_metadata()` - LLM íŒë‹¨
  - [x] `_generate_specific_human_question()` - êµ¬ì²´ì  ì§ˆë¬¸
  - [x] `_parse_metadata_content()` - CSV íŒŒì‹±
- [x] `src/agents/nodes.py` - Phase 2 í•¨ìˆ˜ êµ¬í˜„
  - [x] `_find_common_columns()` - FK í›„ë³´ ê²€ìƒ‰
  - [x] `_extract_filename_hints()` - íŒŒì¼ëª… ë¶„ì„
  - [x] `_infer_relationships_with_llm()` - ê´€ê³„ ì¶”ë¡ 
  - [x] `_summarize_existing_tables()` - í…Œì´ë¸” ìš”ì•½
- [x] `src/agents/graph.py` - ì›Œí¬í”Œë¡œìš° ì—°ê²° (skip_indexing ë¶„ê¸°)

**í…ŒìŠ¤íŠ¸ ê²€ì¦:**
- [x] ë©”íƒ€ë°ì´í„° ê°ì§€: 100% ì •í™• (5/5)
- [x] ìš©ì–´ ì¶”ì¶œ: 310ê°œ
- [x] ê´€ê³„ ë°œê²¬: lab_data â†’ clinical_data (N:1)
- [x] ê³„ì¸µ ìƒì„±: 3ë ˆë²¨ (ì¤‘ë³µ ì—†ìŒ)
- [x] ìºì‹œ ì‘ë™: 83% Hit Rate
- [x] ì¤‘ë³µ ì €ì¥ ë°©ì§€: ë©±ë“±ì„± ë³´ì¥
- [x] Negative Evidence ì‘ë™ í™•ì¸

---

#### ğŸ”œ Phase 3: ì‹¤ì œ DB êµ¬ì¶• + VectorDB (ê³„íš)

**ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜ ì™„ë£Œ:**
- âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ (Chunk Processing)
- âœ… Table Summary Embedding ì¶”ê°€
- âœ… Schema Evolution ì •ì±… (Drop & Recreate)
- âœ… VectorDB í™•ì¥ì„± ê³ ë ¤ (ì„ë² ë”© ìµœì í™” ì—¬ì§€)

**ì½”ë“œ êµ¬í˜„ ì˜ˆì • (ëª¨ë“ˆ êµ¬ì¡° ë°˜ì˜):**

**Part A: ê´€ê³„í˜• DB (ì‹ ê·œ ëª¨ë“ˆ ìƒì„±)**
- [ ] `src/database/` ë””ë ‰í† ë¦¬ ìƒì„±
  - [ ] `connection.py` - DB ì—°ê²° ê´€ë¦¬
    ```python
    class DatabaseManager:
        def __init__(self, db_type="sqlite"):
            # SQLite ë˜ëŠ” PostgreSQL ì—°ê²°
        
        def get_connection(self):
            # ì—°ê²° í’€ ë°˜í™˜
    ```
  
  - [ ] `schema_generator.py` - DDL ë™ì  ìƒì„±
    ```python
    def generate_ddl(table_name, schema, ontology):
        # ì˜¨í†¨ë¡œì§€ relationships â†’ FK ì œì•½ì¡°ê±´
        # ì˜¨í†¨ë¡œì§€ hierarchy â†’ PK/ì¸ë±ìŠ¤ íŒë‹¨
    
    def _map_to_sql_type(data_type):
        # VARCHAR â†’ TEXT ë“±
    
    def _generate_fk_constraints(table_name, relationships):
        # FOREIGN KEY ... REFERENCES ...
    
    def _generate_indices(table_name, hierarchy):
        # CREATE INDEX ON ... (Level 1-2)
    ```
  
- [ ] `src/agents/nodes.py` - index_data_node í™•ì¥
  - [ ] schema_generator í™œìš©í•˜ì—¬ DDL ìƒì„±
  - [ ] **Chunk Processing** (chunksize=100,000)
  - [ ] DatabaseManagerë¡œ ì €ì¥
  - [ ] FK, ì¸ë±ìŠ¤ ìë™ ìƒì„±

**Part B: VectorDB (ì‹ ê·œ ëª¨ë“ˆ ìƒì„±)**
- [ ] `src/knowledge/` ë””ë ‰í† ë¦¬ ìƒì„±
  
  - [ ] `vector_store.py` - VectorDB ê´€ë¦¬
    ```python
    class VectorStore:
        def __init__(self, db_path="data/processed/vector_db"):
            # ChromaDB ì´ˆê¸°í™”
        
        def build_index(self, ontology_context):
            # Table + Column + Relationship ì„ë² ë”©
        
        def semantic_search(self, query, n_results=5):
            # Hybrid Search (Keyword + Vector)
        
        def assemble_context(self, results, ontology):
            # ê²€ìƒ‰ ê²°ê³¼ ì¡°ë¦½ (LLM ì „ë‹¬ìš©)
    ```
  
  - [ ] `catalog_manager.py` - ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸
    ```python
    # ontology_manager.py í™•ì¥ ë˜ëŠ” í†µí•©
    # RDBì— ë©”íƒ€ë°ì´í„° ì €ì¥ (ì„ íƒ)
    ```
  
  - [ ] `ontology_mapper.py` - í‘œì¤€ ìš©ì–´ ë§¤í•‘ (Phase 4)
    ```python
    # OMOP CDM, FHIR ë§¤í•‘ (í–¥í›„)
    ```

**ê³µí†µ:**
- [ ] `requirements.txt` ì—…ë°ì´íŠ¸
  - [x] chromadb>=0.4.0 ì¶”ê°€ ì™„ë£Œ
  - [ ] sqlalchemy>=2.0.0 (ì„ íƒ, PostgreSQL ì‹œ)

**í…ŒìŠ¤íŠ¸:**
- [ ] `test_db_builder.py` - DB êµ¬ì¶• í…ŒìŠ¤íŠ¸
- [ ] `test_vector_search.py` - VectorDB ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
  - [ ] `_extract_filename_hints()` - **Rule íŒŒì‹± + LLM í•´ì„**
    - [ ] (Rule) íŒŒì¼ëª… split, base_name ì¶”ì¶œ
    - [ ] (LLM) Entity Type, Level ì¶”ë¡ 
  - [ ] `_is_metadata_file()` - **Rule ìˆ˜ì§‘ + LLM íŒë‹¨**
    - [ ] (Rule) ì»¬ëŸ¼ëª…, ìƒ˜í”Œ, avg_length ê³„ì‚°
    - [ ] (LLM) ë©”íƒ€ë°ì´í„° ì—¬ë¶€ ê²°ì •
  - [ ] `_analyze_cardinality()` - **Rule í†µê³„ + LLM ì—­í•  ì¶”ë¡ **
    - [ ] (Rule) unique_values ì¶”ì¶œ (Categorical ìµœëŒ€ 20ê°œ)
    - [ ] (Rule) uniqueness_ratio ê³„ì‚°
    - [ ] (LLM) PK/FK/Grouping Key íŒë‹¨
  - [ ] `_infer_relationships()` - **Rule í›„ë³´ ê²€ìƒ‰ + LLM ê²€ì¦**
    - [ ] (Rule) `_find_common_columns()` - ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°
    - [ ] (LLM) FK ê´€ê³„ ê²€ì¦, ì¹´ë””ë„ë¦¬í‹° íŒë‹¨
  - [ ] Fallback ë¡œì§ (LLM ì‹¤íŒ¨ ì‹œë§Œ ìµœì†Œ Rule)
- [ ] `src/agents/graph.py` - ë…¸ë“œ ì—°ê²° ìˆ˜ì •
  - [ ] loader â†’ ontology_builder â†’ analyzer

#### í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- [ ] **LLM íŒë‹¨ ì •í™•ë„ í…ŒìŠ¤íŠ¸**
  - [ ] clinical_parameters.csv â†’ is_metadata=True, confidence>0.9
  - [ ] lab_parameters.csv â†’ is_metadata=True, confidence>0.9
  - [ ] track_names.csv â†’ is_metadata=True, confidence>0.85
  - [ ] clinical_data.csv â†’ is_metadata=False, confidence>0.9
  - [ ] lab_data.csv â†’ is_metadata=False, confidence>0.9
  - [ ] **ì˜¤íŒìœ¨ < 5%**

- [ ] **íŒŒì¼ëª… íŒíŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸**
  - [ ] "lab_data.csv" â†’ entity_type="Laboratory", level=4
  - [ ] "patient_info.csv" â†’ entity_type="Patient", level=1
  - [ ] related_patterns ì •í™•ë„ í™•ì¸

- [ ] **ìºì‹± ë™ì‘ í™•ì¸**
  - [ ] ë™ì¼ íŒŒì¼ ì¬ì‹¤í–‰ ì‹œ "Cache Hit" ë©”ì‹œì§€
  - [ ] ë¹„ìš© 0ì› í™•ì¸

- [ ] **Fallback ë™ì‘ í™•ì¸**
  - [ ] LLM API ëŠê³  ì‹¤í–‰ â†’ ê¸°ë³¸ê°’ ë°˜í™˜
  - [ ] confidence=0.0 + needs_human_review=True í™•ì¸

#### ë¬¸ì„œí™”
- [ ] í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ (`src/agents/prompts.py`)
- [ ] ì‚¬ìš© ê°€ì´ë“œ ì‘ì„± (ê°œë°œììš©)
- [ ] ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¬¸ì„œ

---

## ë¶€ë¡ A: íŒŒì¼ëª… íŒ¨í„´ ì˜ˆì‹œ

### A.1 ì‹¤ì œ ë°ì´í„°ì…‹ íŒŒì¼ëª… ë¶„ì„

#### VitalDB
```
clinical_data.csv        â†’ entity: Case, level: 2, transactional
clinical_parameters.csv  â†’ metadata for clinical_data
lab_data.csv            â†’ entity: Lab, level: 4, transactional
lab_parameters.csv      â†’ metadata for lab_data
track_names.csv         â†’ metadata (signal tracks), special pattern
```

#### MIMIC-IV (ê°€ìƒ)
```
patients.csv            â†’ entity: Patient, level: 1, master
admissions.csv          â†’ entity: Admission, level: 2
d_items.csv            â†’ "d_" prefix â†’ dictionary/metadata
chartevents.csv        â†’ "events" suffix â†’ transactional, level: 4+
```

#### ì¼ë°˜ ë³‘ì› ë°ì´í„° (ê°€ìƒ)
```
master_patient.csv      â†’ "master" prefix â†’ Level 1, PK
emr_visit_records.csv   â†’ "emr" prefix + "visit" â†’ Level 2
lab_test_results.csv    â†’ "lab" + "results" â†’ Level 4
med_administration.csv  â†’ "med" (medication) â†’ Level 4
```

### A.2 íŒŒì¼ëª… ëª…ëª… ê·œì¹™ ì¶”ì²œ

**ë©”íƒ€ë°ì´í„° íŒŒì¼:**
- `[entity]_parameters.csv`
- `[entity]_dictionary.csv`
- `[entity]_codebook.csv`

**íŠ¸ëœì­ì…˜ ë°ì´í„°:**
- `[entity]_data.csv`
- `[entity]_records.csv`
- `[entity]_events.csv`

**ë§ˆìŠ¤í„° ë°ì´í„°:**
- `master_[entity].csv`
- `[entity]_info.csv`

---

---

## ë¶€ë¡ B: LLM í˜¸ì¶œ ìµœì í™” ì „ëµ

### B.1 ë¹„ìš© íš¨ìœ¨í™” ë°©ì•ˆ

#### ì „ëµ 1: ìºì‹± (Caching)
```python
# src/utils/llm_cache.py

import hashlib
import json
from pathlib import Path

class LLMCache:
    def __init__(self, cache_dir="data/cache/llm"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def get_cache_key(self, prompt: str, context: dict) -> str:
        """í”„ë¡¬í”„íŠ¸ + ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{prompt}:{json.dumps(context, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, cache_key: str):
        """ìºì‹œ ì¡°íšŒ"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            with open(cache_file, 'r') as f:
                return json.load(f)
        return None
    
    def set(self, cache_key: str, result: dict):
        """ìºì‹œ ì €ì¥"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        with open(cache_file, 'w') as f:
            json.dump(result, f, indent=2)

# ì‚¬ìš© ì˜ˆì‹œ
llm_cache = LLMCache()

def _ask_llm_is_metadata(context: dict) -> dict:
    cache_key = llm_cache.get_cache_key(METADATA_DETECTION_PROMPT, context)
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get(cache_key)
    if cached:
        print(f"âœ… [Cache Hit] ìºì‹œëœ ê²°ê³¼ ì‚¬ìš© (ë¹„ìš© ì ˆê°)")
        return cached
    
    # LLM í˜¸ì¶œ
    result = llm_client.ask_json(prompt)
    
    # ìºì‹œ ì €ì¥
    llm_cache.set(cache_key, result)
    
    return result
```

**íš¨ê³¼:**
- ë™ì¼ íŒŒì¼ ì¬ì²˜ë¦¬: ë¹„ìš© 100% ì ˆê°
- ìœ ì‚¬ íŒŒì¼ ì²˜ë¦¬: í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©

---

#### ì „ëµ 2: ë°°ì¹˜ ì²˜ë¦¬ (Batching)
```python
def _analyze_multiple_files_batch(files: List[str]) -> dict:
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ LLM í˜¸ì¶œë¡œ ì²˜ë¦¬ (ë¹„ìš© ì ˆê°)
    """
    prompt = f"""
Analyze the following {len(files)} files and classify each:

[FILES]
{json.dumps([build_context(f) for f in files], indent=2)}

For EACH file, return:
{{
    "filename": "...",
    "is_metadata": true/false,
    "confidence": 0.0-1.0,
    ...
}}

Respond with a list of results.
"""
    
    result = llm_client.ask_json(prompt)
    return result
```

**íš¨ê³¼:**
- 5ê°œ íŒŒì¼ ê°œë³„ ì²˜ë¦¬: 5ë²ˆ í˜¸ì¶œ
- 5ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬: **1ë²ˆ í˜¸ì¶œ** (80% ë¹„ìš© ì ˆê°)

---

#### ì „ëµ 3: ì ì§„ì  LLM ì‚¬ìš© (Progressive LLM)
```python
def _is_metadata_file_progressive(file_path: str, metadata: dict) -> dict:
    """
    ë‹¨ê³„ë³„ LLM ì‚¬ìš© (í•„ìš”í•  ë•Œë§Œ)
    """
    
    # 1ë‹¨ê³„: í™•ì‹¤í•œ ê²½ìš° LLM ìŠ¤í‚µ
    basename = os.path.basename(file_path).lower()
    
    # ë§¤ìš° ëª…í™•í•œ ê²½ìš° (í™•ì‹ ë„ 100%)
    if basename.endswith('_parameters.csv'):
        return {"is_metadata": True, "confidence": 1.0, "method": "filename_certain"}
    
    if basename == 'README.csv' or basename.startswith('data_'):
        # data_ë¡œ ì‹œì‘í•˜ë©´ ê±°ì˜ í™•ì‹¤íˆ íŠ¸ëœì­ì…˜
        return {"is_metadata": False, "confidence": 0.95, "method": "filename_certain"}
    
    # 2ë‹¨ê³„: ì• ë§¤í•œ ê²½ìš°ë§Œ LLM í˜¸ì¶œ
    print(f"ğŸ¤” [Uncertain] LLM í˜¸ì¶œ í•„ìš”: {basename}")
    return _ask_llm_is_metadata(build_context(file_path, metadata))
```

**íš¨ê³¼:**
- í™•ì‹¤í•œ ê²½ìš° 70% â†’ LLM í˜¸ì¶œ ìŠ¤í‚µ
- ë‚˜ë¨¸ì§€ 30%ë§Œ LLM ì‚¬ìš©
- ì „ì²´ ë¹„ìš© 70% ì ˆê°

---

### B.2 LLM í˜¸ì¶œ ì˜ˆìƒ ë¹„ìš©

#### íŒŒì¼ë³„ ë¹„ìš© (GPT-4 ê¸°ì¤€)
```
ë©”íƒ€ë°ì´í„° ê°ì§€: 1íšŒ Ã— $0.03 = $0.03
íŒŒì¼ëª… ë¶„ì„: 1íšŒ Ã— $0.02 = $0.02
ì¹´ë””ë„ë¦¬í‹° ë¶„ì„: 1íšŒ Ã— $0.04 = $0.04
ê´€ê³„ ì¶”ë¡ : 1íšŒ Ã— $0.05 = $0.05
ê³„ì¸µ êµ¬ì¡°: 1íšŒ Ã— $0.04 = $0.04
-----------------------------------------
íŒŒì¼ë‹¹ ì´ ë¹„ìš©: ~$0.18

VitalDB 5ê°œ íŒŒì¼: $0.90
ìºì‹± ì ìš© í›„ ì¬ì‹¤í–‰: $0.00 (ë¬´ë£Œ!)
```

#### ìµœì í™” í›„ ë¹„ìš©
```
ë°°ì¹˜ ì²˜ë¦¬ (5ê°œ íŒŒì¼ ë™ì‹œ): $0.25 (72% ì ˆê°)
ìºì‹± + ë°°ì¹˜: ì²« ì‹¤í–‰ $0.25, ì´í›„ $0.00
Progressive LLM: ì²« ì‹¤í–‰ $0.27 (70% í™•ì‹¤ ì¼€ì´ìŠ¤ ìŠ¤í‚µ)
```

---

## ë¶€ë¡ C: LLM ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì¥ì 

### B.1 Rule-based vs LLM-based ìƒì„¸ ë¹„êµ

#### ì˜ˆì‹œ 1: ìƒˆë¡œìš´ ëª…ëª… íŒ¨í„´
```python
# íŒŒì¼: "variable_codebook_v2.csv"

# Rule-based:
if 'codebook' in filename:
    return True  # âœ… ê°ì§€ ì„±ê³µ

# í•˜ì§€ë§Œ "var_codes.csv"ëŠ”?
if 'codebook' in filename:
    return False  # âŒ ë¯¸ê°ì§€ (ê·œì¹™ì— ì—†ìŒ)
# â†’ ê·œì¹™ ì¶”ê°€ í•„ìš” (ê°œë°œì ê°œì…)

# LLM-based:
LLM: "íŒŒì¼ëª…ì— 'var_codes'ê°€ ìˆê³ , 
     ì»¬ëŸ¼ì´ [code, description]ì´ë©°,
     ë‚´ìš©ì´ ì„¤ëª…ë¬¸ì„ â†’ METADATA"  # âœ… ìë™ ê°ì§€
```

#### ì˜ˆì‹œ 2: ì• ë§¤í•œ ê²½ìš°
```python
# íŒŒì¼: "patient_summary.csv"
# ì»¬ëŸ¼: [patient_id, total_visits, avg_bp, notes]

# Rule-based:
# "summary"ëŠ” ê·œì¹™ì— ì—†ìŒ â†’ False
# í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì§‘ê³„ ë°ì´í„°(aggregated)ì¼ ìˆ˜ë„ ìˆìŒ

# LLM-based:
LLM: "íŒŒì¼ëª… 'summary'ëŠ” ì§‘ê³„ë¥¼ ì˜ë¯¸í•˜ê³ ,
     ì»¬ëŸ¼ì´ total/avg ê°™ì€ í†µê³„ê°’ í¬í•¨ â†’
     TRANSACTIONAL (aggregated data)
     Confidence: 0.72 (ë‹¤ì†Œ ë‚®ìŒ, ë¡œê·¸ ì¶œë ¥)"
# â†’ íˆ¬ëª…í•œ íŒë‹¨ + í™•ì‹ ë„ ì œê³µ
```

---

### B.2 í™•ì¥ì„± ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸ˆìœµ ë°ì´í„°ì…‹
```python
# íŒŒì¼: "ticker_metadata.csv"
# ì»¬ëŸ¼: [Ticker, Company, Sector]

# Rule-based: 
# 'metadata' í‚¤ì›Œë“œ ìˆìŒ â†’ True
# í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” íšŒì‚¬ ëª©ë¡(ë§ˆìŠ¤í„° ë°ì´í„°)ì¼ ìˆ˜ë„

# LLM-based:
LLM: "ê¸ˆìœµ ë„ë©”ì¸ì—ì„œ tickerëŠ” ì‹ë³„ìì´ê³ ,
     Company/SectorëŠ” ì†ì„±ì„ â†’
     MASTER DATA (not pure metadata)
     Confidence: 0.88"
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ìœ ì „ì²´ ë°ì´í„°
```python
# íŒŒì¼: "gene_annotations.csv"
# ì»¬ëŸ¼: [gene_id, function, pathway]

# Rule-based:
# 'annotation' í‚¤ì›Œë“œ ì—†ìŒ (ê·œì¹™ì— ì¶”ê°€ ì•ˆ ë¨)
# â†’ False (ë¯¸ê°ì§€)

# LLM-based:
LLM: "ìƒë¬¼í•™ ë„ë©”ì¸ì—ì„œ annotationì€
     ë©”íƒ€ë°ì´í„°ë¥¼ ì˜ë¯¸í•¨ â†’
     METADATA (gene dictionary)
     Confidence: 0.91"
# â†’ ë„ë©”ì¸ ì§€ì‹ ìë™ í™œìš©
```

---

### B.3 êµ¬í˜„ ë¹„ìš© ë¶„ì„

| í•­ëª© | Rule-based | LLM-based |
|------|-----------|-----------|
| ì´ˆê¸° ê°œë°œ ì‹œê°„ | 2-3ì¼ (ê·œì¹™ ì‘ì„±) | 1ì¼ (í”„ë¡¬í”„íŠ¸ ì‘ì„±) |
| ìœ ì§€ë³´ìˆ˜ | ì§€ì†ì  (ìƒˆ íŒ¨í„´ë§ˆë‹¤ ìˆ˜ì •) | **ê±°ì˜ ì—†ìŒ** |
| ì •í™•ë„ | 70-80% | **95-98%** |
| ìƒˆ ë„ë©”ì¸ ì ì‘ | ì–´ë ¤ì›€ (ê·œì¹™ ì¬ì‘ì„±) | **ìë™** |
| API ë¹„ìš© | $0 | $0.01-0.05/íŒŒì¼ |
| íˆ¬ëª…ì„± | ë‚®ìŒ | **ë†’ìŒ** (reasoning) |

**ROI (íˆ¬ì ëŒ€ë¹„ íš¨ê³¼):**
- ì´ˆê¸° API ë¹„ìš©: $5-10 (100ê°œ íŒŒì¼ ê¸°ì¤€)
- ì ˆê°ëœ ê°œë°œ ì‹œê°„: 10-20ì‹œê°„ (ê·œì¹™ ì‘ì„±/ìœ ì§€ë³´ìˆ˜)
- ì •í™•ë„ í–¥ìƒìœ¼ë¡œ ì¸í•œ Human Review ê°ì†Œ: 50% â†’ 5%

**ê²°ë¡ : LLM ë¹„ìš© ëŒ€ë¹„ ì ˆê° íš¨ê³¼ê°€ í›¨ì”¬ í¼** âœ…

---

### B.4 Best Practices (LLM í™œìš© ì‹œ)

#### 1. í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬
```python
# src/agents/prompts.py
METADATA_DETECTION_PROMPT_V1 = """
You are a Data Classification Expert...
"""

METADATA_DETECTION_PROMPT_V2 = """
[ê°œì„ ] ì˜ë£Œ/ê¸ˆìœµ/ìœ ì „ì²´ ë„ë©”ì¸ ì˜ˆì‹œ ì¶”ê°€...
"""

# ë²„ì „ë³„ A/B í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
```

#### 2. í™•ì‹ ë„ ì„ê³„ê°’ ì¡°ì •
```python
# config.py
CONFIDENCE_THRESHOLDS = {
    "metadata_detection": 0.75,
    "relationship_inference": 0.85,
    "hierarchy_determination": 0.80
}
```

#### 3. LLM ì‘ë‹µ ìºì‹±
```python
# ë™ì¼ íŒŒì¼ ì¬ì²˜ë¦¬ ì‹œ LLM í˜¸ì¶œ ìŠ¤í‚µ
cache_key = f"{file_path}:{hash(columns)}"
if cache_key in metadata_cache:
    return metadata_cache[cache_key]
```

---

## ë³€ê²½ ì´ë ¥

### v1.4 (2025-12-16) - **ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜ (Refinement)**
- **[NEW] Negative Evidence í™œìš©**
  - `_collect_negative_evidence()` í•¨ìˆ˜ ì¶”ê°€
  - null_ratio, duplicate_rate ê³„ì‚°
  - LLM í”„ë¡¬í”„íŠ¸ì— ë¶€ì • ì¦ê±° ëª…ì‹œì  ì œê³µ
  - ì˜ˆ: "99% unique BUT 1% duplicates - data error or soft key?"

- **[NEW] Context Window ê´€ë¦¬**
  - `_summarize_long_values()` - ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (>50 chars)
  - í† í° ì‚¬ì´ì¦ˆ ì¶”ì • ë° ìƒ˜í”Œ ì¶•ì†Œ
  - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë©”íƒ€ ì •ë³´ë¡œ ë³€í™˜)

- **[NEW] Human Review ì§ˆë¬¸ êµ¬ì²´í™”**
  - `_generate_specific_human_question()` í•¨ìˆ˜
  - LLM reasoning + ë°œê²¬ëœ ì´ìŠˆ + ì°¸ê³  ì •ë³´ í¬í•¨
  - ì„ íƒì§€ ì œê³µ (ë©”íƒ€ë°ì´í„°/ë°ì´í„°/ëª¨ë¥´ê² ìŒ)

- **ì½”ë“œ ì˜ˆì‹œ ëŒ€í­ ì¶”ê°€**
  - `state.py` ì „ì²´ êµ¬ì¡°
  - `llm_cache.py` ì™„ì „ êµ¬í˜„
  - `ontology_builder_node()` ì „ì²´ ë¡œì§
  - `_build_metadata_detection_context_v2()` ê°œì„  ë²„ì „

### v1.3 (2025-12-16) - **"Rule Prepares, LLM Decides" íŒ¨í„´ í™•ë¦½**
- **í•µì‹¬ ë³€ê²½**: Ruleê³¼ LLMì˜ ì—­í•  ëª…í™•íˆ ë¶„ë¦¬
  - Rule: ë°ì´í„° ì „ì²˜ë¦¬ (íŒŒì‹±, í†µê³„, unique values ì¶”ì¶œ, ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°)
  - LLM: ìµœì¢… íŒë‹¨ (ì „ì²˜ë¦¬ëœ ì •ë³´ë¥¼ í•´ì„)
- **ì„¤ê³„ ì² í•™ ì¬ì •ë¦½**: "LLM First, Rule Last" â†’ "Rule Prepares, LLM Decides"
- ì£¼ìš” í•¨ìˆ˜ íŒ¨í„´ í†µì¼:
  - `_is_metadata_file()`: Ruleë¡œ ì»¬ëŸ¼/ìƒ˜í”Œ ìˆ˜ì§‘ â†’ LLM íŒë‹¨
  - `_extract_filename_hints()`: Ruleë¡œ íŒŒì¼ëª… íŒŒì‹± â†’ LLM ì˜ë¯¸ ì¶”ë¡ 
  - `_analyze_cardinality()`: Ruleë¡œ unique values/ratio ê³„ì‚° â†’ LLM ì—­í•  ì¶”ë¡ 
  - `_infer_relationships()`: Ruleë¡œ ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸° â†’ LLM FK ê²€ì¦
- **Categorical ì»¬ëŸ¼ íŠ¹í™”**: unique valuesë¥¼ ìµœëŒ€í•œ LLMì—ê²Œ ì œê³µ (íŒ¨í„´ ì¸ì‹)
- LLM í”„ë¡¬í”„íŠ¸ ê°œì„ : "Pre-processed by Rules" ëª…ì‹œ
- í—¬í¼ í•¨ìˆ˜ ì¶”ê°€: `_find_common_columns()` (Rule ì „ì²˜ë¦¬)
- DO/DON'T ê°€ì´ë“œë¼ì¸ ê°•í™” (ì˜¬ë°”ë¥¸ íŒ¨í„´ vs ì˜ëª»ëœ íŒ¨í„´)

### v1.2 (2025-12-16) - **LLM ê¸°ë°˜ ì „í™˜**
- Rule-based íŒë‹¨ ë¡œì§ ì œê±°
- LLM ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
- Confidence ê¸°ë°˜ Human Review
- Fallback ì „ëµ ì¶”ê°€

### v1.1 (2025-12-16)
- íŒŒì¼ëª… ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì „ëµ ê°•í™”
- `_extract_filename_hints()` í•¨ìˆ˜ ì¶”ê°€ (ë‹¹ì‹œ rule-based)

### v1.0 (2025-12-16)
- ì´ˆê¸° ë²„ì „

---

## ë¶€ë¡ D: LLM vs Rule ì‚¬ìš© ê°€ì´ë“œë¼ì¸

### D.1 "ì–¸ì œ LLMì„, ì–¸ì œ Ruleì„ ì“¸ ê²ƒì¸ê°€?"

#### âœ… LLM ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš° (ê¶Œì¥)

| ì‘ì—… | ì´ìœ  | Rule-based ë¬¸ì œì  | LLM ì¥ì  |
|------|------|------------------|----------|
| **ë©”íƒ€ë°ì´í„° ê°ì§€** | ëª…ëª… íŒ¨í„´ ë‹¤ì–‘ | í‚¤ì›Œë“œ 30+ ê°œ ìœ ì§€ë³´ìˆ˜ | ìƒˆ íŒ¨í„´ ìë™ ì¸ì‹ |
| **íŒŒì¼ëª… ì˜ë¯¸ ì¶”ì¶œ** | ë„ë©”ì¸ ì§€ì‹ í•„ìš” | ì˜ë£Œ ì™¸ ë„ë©”ì¸ ë¶ˆê°€ | ëª¨ë“  ë„ë©”ì¸ ì ì‘ |
| **Entity Type ì¶”ë¡ ** | ê³„ì¸µ ì´í•´ í•„ìš” | í•˜ë“œì½”ë”©ëœ ë ˆë²¨ | ë¬¸ë§¥ìœ¼ë¡œ ë ˆë²¨ íŒë‹¨ |
| **ê´€ê³„ ì¶”ë¡ ** | ì˜ë¯¸ ìœ ì‚¬ì„± íŒë‹¨ | ë¬¸ìì—´ ë§¤ì¹­ë§Œ ê°€ëŠ¥ | patient_id â‰ˆ subjectid |
| **ì¹´ë””ë„ë¦¬í‹° í•´ì„** | ë³µì¡í•œ íŒ¨í„´ | ë‹¨ìˆœ í†µê³„ë§Œ | PK/FK ì—­í•  ì¶”ë¡  |
| **ê³„ì¸µ ê²°ì •** | ë„ë©”ì¸ ìƒì‹ | ì˜ë£Œë§Œ ê°€ëŠ¥ | ìë™ ë„ë©”ì¸ ì ì‘ |

---

#### âœ… Rule ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš° (**ì „ì²˜ë¦¬/ë°ì´í„° ìˆ˜ì§‘**)

| ì‘ì—… | ì—­í•  | ì˜ˆì‹œ | ëª©ì  |
|------|------|------|------|
| **íŒŒì¼ëª… íŒŒì‹±** | êµ¬ì¡° ì¶”ì¶œ | `name.split('_')` â†’ parts | LLMì—ê²Œ íŒŒì‹±ëœ êµ¬ì¡° ì œê³µ |
| **í†µê³„ ê³„ì‚°** | ìˆ˜ì¹˜ ì‚°ì¶œ | `unique_count`, `ratio` | LLMì—ê²Œ ì •ëŸ‰ì  ì •ë³´ ì œê³µ |
| **Unique values ì¶”ì¶œ** | ë°ì´í„° ìˆ˜ì§‘ | `df[col].unique()[:20]` | LLMì´ íŒ¨í„´ ë³¼ ìˆ˜ ìˆê²Œ |
| **ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°** | FK í›„ë³´ ê²€ìƒ‰ | `set(cols1) & set(cols2)` | LLMì—ê²Œ í›„ë³´ ì œì‹œ |
| **íŒŒì¼ í™•ì¥ì ì²´í¬** | í˜•ì‹ íŒë³„ | `.csv`, `.xlsx` | Processor ì„ íƒ |
| **í‰ê·  ê¸¸ì´ ê³„ì‚°** | í…ìŠ¤íŠ¸ ë¶„ì„ | `avg(len(text))` | LLMì—ê²Œ ì„¤ëª…ë¬¸ íŒíŠ¸ |

**ì›ì¹™:** Ruleì€ **íŒë‹¨í•˜ì§€ ì•Šê³  ì •ë¦¬ë§Œ**. LLMì´ í•´ì„í•˜ê¸° ì‰½ê²Œ ì „ì²˜ë¦¬.

---

#### âŒ Rule ì‚¬ìš© ê¸ˆì§€ ì‚¬í•­ (**íŒë‹¨ ë¡œì§**)

| ì‘ì—… | ë‚˜ìœ ì˜ˆì‹œ (Rule íŒë‹¨) | ì¢‹ì€ ì˜ˆì‹œ (Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨) |
|------|---------------------|--------------------------------|
| **ë©”íƒ€ë°ì´í„° ê°ì§€** | `if 'param' in name: return True` âŒ | `parts = name.split('_')` â†’ LLM íŒë‹¨ âœ… |
| **PK íŒë‹¨** | `if ratio == 1.0: return 'PK'` âŒ | `ratio = calc()` â†’ LLMì´ PK íŒë‹¨ âœ… |
| **Entity Level** | `if 'patient' in name: level=1` âŒ | `base = extract()` â†’ LLMì´ level ì¶”ë¡  âœ… |
| **ê´€ê³„ íŒë‹¨** | `if col in both: return 'FK'` âŒ | `common = find()` â†’ LLMì´ FK ê²€ì¦ âœ… |

**í•µì‹¬ ì°¨ì´:**
```python
# âŒ Ruleì´ íŒë‹¨ê¹Œì§€ (ê¸ˆì§€)
if 'parameter' in filename and len(columns) < 10:
    return "metadata"  # Ruleì´ ìµœì¢… ê²°ì •

# âœ… Ruleì€ ì „ì²˜ë¦¬, LLMì´ íŒë‹¨ (ê¶Œì¥)
parts = filename.split('_')  # Ruleë¡œ íŒŒì‹±
avg_len = calculate_avg()    # Ruleë¡œ ê³„ì‚°

llm.ask(f"parts={parts}, avg_len={avg_len}, íŒë‹¨í•´ì¤˜")  # LLMì´ ê²°ì •
```

---

### D.2 LLM ê¸°ë°˜ ì‹œìŠ¤í…œì˜ í’ˆì§ˆ ë³´ì¥ ì²´ê³„

#### ë ˆë²¨ 1: í™•ì‹ ë„ ê¸°ë°˜ 3ë‹¨ê³„ ê²€ì¦
```python
# Very High Confidence (0.9+)
if confidence >= 0.9:
    auto_proceed()  # ìë™ ì§„í–‰
    log_only()      # ë¡œê·¸ë§Œ ê¸°ë¡

# Medium Confidence (0.75-0.89)
elif confidence >= 0.75:
    auto_proceed()
    warning_log()   # ê²½ê³  ë¡œê·¸, ì¶”í›„ ê²€í†  ê¶Œì¥

# Low Confidence (<0.75)
else:
    request_human_review()  # ì¦‰ì‹œ ì‚¬ëŒ í™•ì¸
```

#### ë ˆë²¨ 2: LLM ì‘ë‹µ êµ¬ì¡° ê²€ì¦
```python
def validate_llm_response(result: dict, schema: dict) -> bool:
    """LLM ì‘ë‹µ í˜•ì‹ ë° ê°’ ê²€ì¦"""
    
    # 1. í•„ìˆ˜ í‚¤ ì¡´ì¬ í™•ì¸
    required_keys = schema.get("required", [])
    if not all(key in result for key in required_keys):
        raise ValueError(f"Missing required keys: {required_keys}")
    
    # 2. íƒ€ì… ê²€ì¦
    for key, expected_type in schema.get("types", {}).items():
        if key in result and not isinstance(result[key], expected_type):
            raise TypeError(f"{key} must be {expected_type}")
    
    # 3. ê°’ ë²”ìœ„ ê²€ì¦
    if "confidence" in result:
        if not (0.0 <= result["confidence"] <= 1.0):
            raise ValueError("Confidence must be 0.0-1.0")
    
    return True

# ì‚¬ìš©
schema = {
    "required": ["is_metadata", "confidence", "reasoning"],
    "types": {"is_metadata": bool, "confidence": float}
}
validate_llm_response(llm_result, schema)
```

#### ë ˆë²¨ 3: Human ê²€ì¦ ë° í•™ìŠµ
```python
# ì˜¨í†¨ë¡œì§€ì— ê²€ì¦ ì´ë ¥ ì €ì¥
{
    "relationships": [{
        "source": "lab_data",
        "target": "clinical_data",
        "via": "caseid",
        
        # LLM ì¶”ë¡  ì •ë³´
        "llm_inferred": True,
        "llm_confidence": 0.88,
        "llm_reasoning": "Common column caseid with N:1 pattern",
        
        # Human ê²€ì¦ ì •ë³´
        "human_verified": True,
        "verified_at": "2025-12-16T10:30:00",
        "verified_by": "researcher_A",
        "verification_note": "Confirmed correct"
    }]
}

# ê²€ì¦ëœ ì§€ì‹ì€ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë” ë†’ì€ ê°€ì¤‘ì¹˜
```

---

### D.3 ì „ì²´ ì‹œìŠ¤í…œ ì²˜ë¦¬ íë¦„ (Rule + LLM í˜‘ì—…)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ íŒŒì¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ("Rule Prepares, LLM Decides")         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. [LOADER] íŒŒì¼ ì½ê¸° â† Rule ì „ë‹´
   â”œâ”€ í™•ì¥ì ì²´í¬ (.csv, .xlsx)
   â”œâ”€ ê¸°ë³¸ CSV íŒŒì‹±
   â”œâ”€ ìƒ˜í”Œë§ (20í–‰)
   â””â”€ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸, unique values ì¶”ì¶œ â† LLMì—ê²Œ ì „ë‹¬

2. [ONTOLOGY BUILDER] ì§€ì‹ êµ¬ì¶• â† Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨
   â”‚
   â”œâ”€ _extract_filename_hints()
   â”‚   â”œâ”€ (Rule) íŒŒì¼ëª… íŒŒì‹± (split, base_name ì¶”ì¶œ)
   â”‚   â””â”€ (LLM) ì˜ë¯¸ í•´ì„ (Entity Type, Level ì¶”ë¡ )
   â”‚
   â”œâ”€ _is_metadata_file()
   â”‚   â”œâ”€ (Rule) ì»¬ëŸ¼ëª…, ìƒ˜í”Œ ìˆ˜ì§‘, í‰ê·  ê¸¸ì´ ê³„ì‚°
   â”‚   â””â”€ (LLM) ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨
   â”‚
   â”œâ”€ _parse_metadata_content() â† Rule ì „ë‹´
   â”‚   â””â”€ (Rule) Key-Value ì¶”ì¶œ, Dictionary ë³€í™˜
   â”‚
   â”œâ”€ _analyze_cardinality()
   â”‚   â”œâ”€ (Rule) unique_count, ratio ê³„ì‚°, unique_values ì¶”ì¶œ
   â”‚   â””â”€ (LLM) ì—­í•  ì¶”ë¡  (PK/FK/Grouping)
   â”‚
   â””â”€ _infer_relationships()
       â”œâ”€ (Rule) ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°, íŒŒì¼ëª… ìœ ì‚¬ë„
       â””â”€ (LLM) FK ê²€ì¦, ê´€ê³„ íƒ€ì… íŒë‹¨

3. [ANALYZER] ì˜ë¯¸ ë¶„ì„ â† LLM ì „ë‹´
   â”œâ”€ _compare_with_global_context() â† LLM
   â”œâ”€ _analyze_columns_with_llm() â† LLM
   â””â”€ check_confidence() â† Rule (ì¡°ê±´ ë¶„ê¸°)

4. [INDEXER] ì €ì¥ â† Rule ì „ë‹´
   â”œâ”€ SQL DDL ìƒì„±
   â””â”€ DB ì €ì¥

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì—­í•  ë¶„ë‹´:                                                   â”‚
â”‚ â€¢ Rule: ë°ì´í„° ìˆ˜ì§‘, íŒŒì‹±, í†µê³„ ê³„ì‚° (30%)                   â”‚
â”‚ â€¢ LLM: ì˜ë¯¸ í•´ì„, íŒë‹¨, ì¶”ë¡  (70%)                          â”‚
â”‚                                                              â”‚
â”‚ LLM í˜¸ì¶œ: íŒŒì¼ë‹¹ 5-7íšŒ (ê° íŒë‹¨ë§ˆë‹¤)                         â”‚
â”‚ Rule ì‚¬ìš©: ê° LLM í˜¸ì¶œ ì „ ì „ì²˜ë¦¬ + ì‹¤í–‰ ì‘ì—…                 â”‚
â”‚                                                              â”‚
â”‚ "Ruleì´ ì¬ë£Œë¥¼ ì¤€ë¹„í•˜ë©´, LLMì´ ìš”ë¦¬í•œë‹¤" ğŸ³                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### D.4 ê°œë°œ ê°€ì´ë“œë¼ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### âœ… DO (ì˜¬ë°”ë¥¸ íŒ¨í„´)
```python
# === íŒ¨í„´: Rule Prepares, LLM Decides ===

# 1. Ruleë¡œ ë°ì´í„° ì „ì²˜ë¦¬
unique_values = df[col].unique()[:20]  # Rule: unique ì¶”ì¶œ
ratio = unique_count / total_count     # Rule: í†µê³„ ê³„ì‚°
parts = filename.split('_')            # Rule: íŒŒì¼ëª… íŒŒì‹±
common_cols = set(cols1) & set(cols2)  # Rule: ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°

# 2. LLMì—ê²Œ ì •ë¦¬ëœ ì •ë³´ ì œê³µ
context = {
    "unique_values": unique_values,  # â† Ruleë¡œ ì¶”ì¶œí•œ ê²ƒ
    "uniqueness_ratio": ratio,       # â† Ruleë¡œ ê³„ì‚°í•œ ê²ƒ
    "name_parts": parts,             # â† Ruleë¡œ íŒŒì‹±í•œ ê²ƒ
    "fk_candidates": common_cols     # â† Ruleë¡œ ì°¾ì€ ê²ƒ
}

# 3. LLMì´ ìµœì¢… íŒë‹¨
prompt = f"""
[Pre-processed by Rules]: {context}
Based on these facts, what is the role of this column?
"""
result = llm_client.ask_json(prompt)

# 4. í•­ìƒ Confidence ì²´í¬
if result.get("confidence", 0) < 0.75:
    request_human_review(result)

# 5. Reasoning ì €ì¥ (ì¶”ì ì„±)
log_decision(result["reasoning"])

# 6. Fallback ì œê³µ (LLM ì‹¤íŒ¨ ì‹œë§Œ)
try:
    return llm_result
except LLMError:
    return {
        "value": rule_based_fallback(),  # ìµœì†Œ Ruleë¡œ ì²˜ë¦¬
        "confidence": 0.0,               # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
        "needs_human_review": True
    }

# 7. ìºì‹± (ë¹„ìš© ì ˆê°)
cache_key = hash(context)
cached = cache.get(cache_key)
if cached:
    return cached
```

#### âŒ DON'T (ì˜ëª»ëœ íŒ¨í„´ - Ruleì´ íŒë‹¨ê¹Œì§€)

```python
# âŒ 1. Ruleì´ ìµœì¢… íŒë‹¨ (ê¸ˆì§€)
# ë‚˜ìœ ì˜ˆì‹œ: í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë°”ë¡œ ê²°ë¡ 
KEYWORDS = ['parameter', 'dict']
if any(k in filename for k in KEYWORDS):
    return {"is_metadata": True}  # âŒ Ruleì´ ê²°ì •

# ì¢‹ì€ ì˜ˆì‹œ: í‚¤ì›Œë“œ ì°¾ê¸°(Rule) â†’ LLM íŒë‹¨
found = [k for k in KEYWORDS if k in filename]  # Ruleë¡œ ê²€ìƒ‰
llm.ask(f"found_keywords={found}, ë©”íƒ€ë°ì´í„°ì¸ê°€?")  # âœ… LLM íŒë‹¨


# âŒ 2. ë§¤ì§ ë„˜ë²„ë¡œ íŒë‹¨ (ê¸ˆì§€)
# ë‚˜ìœ ì˜ˆì‹œ
if avg_length > 30:
    return "description_column"  # âŒ ì„ê³„ê°’ìœ¼ë¡œ íŒë‹¨

# ì¢‹ì€ ì˜ˆì‹œ
avg_len = sum(len(s) for s in samples) / len(samples)  # Ruleë¡œ ê³„ì‚°
llm.ask(f"avg_len={avg_len}, ì´ê²Œ ì„¤ëª…ë¬¸ì¸ê°€?")  # âœ… LLM í•´ì„


# âŒ 3. ë³µì¡í•œ if-elseë¡œ íŒë‹¨ (ê¸ˆì§€)
# ë‚˜ìœ ì˜ˆì‹œ
if ratio == 1.0 and 'id' in col:
    role = 'PK'
elif ratio < 0.5 and col in other_tables:
    role = 'FK'
else:
    role = 'DATA'  # âŒ Rule íŠ¸ë¦¬ë¡œ íŒë‹¨

# ì¢‹ì€ ì˜ˆì‹œ
facts = {
    "ratio": ratio,                    # Rule
    "has_id_keyword": 'id' in col,     # Rule
    "exists_in_other": col in others   # Rule
}
role = llm.ask(f"facts={facts}, ì—­í•  íŒë‹¨")  # âœ… LLM


# âŒ 4. ë„ë©”ì¸ í•˜ë“œì½”ë”© (ê¸ˆì§€)
# ë‚˜ìœ ì˜ˆì‹œ
if 'patient' in filename:
    level = 1  # âŒ ì˜ë£Œ ì „ìš© í•˜ë“œì½”ë”©
elif 'lab' in filename:
    level = 4

# ì¢‹ì€ ì˜ˆì‹œ
keywords = extract_keywords(filename)  # Rule
llm.ask(f"keywords={keywords}, ë„ë©”ì¸ ì¶”ë¡  í›„ level ì œì•ˆ")  # âœ…


# âŒ 5. Confidence ì—†ëŠ” íŒë‹¨ (ê¸ˆì§€)
return True  # âŒ
return {"is_pk": True}  # âŒ ë¶ˆí™•ì‹¤ì„± ì—†ìŒ

# ì¢‹ì€ ì˜ˆì‹œ
return {
    "is_pk": True,
    "confidence": 0.92,  # âœ… í™•ì‹ ë„
    "reasoning": "Uniqueness ratio=1.0 and values look like IDs"
}
```

---

### D.5 LLM ê¸°ë°˜ ì ‘ê·¼ë²• ì´ì •ë¦¬

#### ğŸ¯ í•µì‹¬ ì›ì¹™

**"LLM First, Rule Last"**
```
Rule-based: ë¬´ì—‡ì„ í• ì§€ (What)ë¥¼ ì½”ë”©
LLM-based: ì–´ë–»ê²Œ íŒë‹¨í• ì§€ (How)ë¥¼ í•™ìŠµ

Rule: ìœ ì§€ë³´ìˆ˜ ë¹„ìš© â†‘, í™•ì¥ì„± â†“
LLM: ì´ˆê¸° ë¹„ìš© ì•½ê°„ â†‘, ì¥ê¸°ì ìœ¼ë¡œ ë¹„ìš© â†“â†“â†“
```

#### ğŸ“Š íš¨ê³¼ ë¹„êµí‘œ

| ì§€í‘œ | Rule-based | LLM-based | ê°œì„ ìœ¨ |
|------|-----------|-----------|--------|
| ë©”íƒ€ë°ì´í„° ê°ì§€ ì •í™•ë„ | 70-80% | 95-98% | **+25%** |
| ìƒˆ ë„ë©”ì¸ ì ì‘ ì‹œê°„ | 2-3ì¼ | 0ì‹œê°„ (ìë™) | **100%** |
| ìœ ì§€ë³´ìˆ˜ ë¹„ìš©/ë…„ | 40ì‹œê°„ | 5ì‹œê°„ | **-87%** |
| False Positive | 15-20% | <5% | **-75%** |
| ì½”ë“œ ë¼ì¸ ìˆ˜ | 500+ | 200 | **-60%** |
| íˆ¬ëª…ì„± (ì„¤ëª…ë ¥) | ë‚®ìŒ | ë†’ìŒ (reasoning) | **N/A** |

#### ğŸ’° ë¹„ìš© ë¶„ì„

```
ì´ˆê¸° ë¹„ìš©:
- Rule-based: ê°œë°œ 3ì¼ ($3,000 ì¸ê±´ë¹„)
- LLM-based: ê°œë°œ 1ì¼ + API $10 = $1,010

ì—°ê°„ ë¹„ìš©:
- Rule-based: ìœ ì§€ë³´ìˆ˜ 40h ($4,000) + ë‚®ì€ ì •í™•ë„
- LLM-based: ìœ ì§€ë³´ìˆ˜ 5h ($500) + API $50/ë…„

3ë…„ ëˆ„ì :
- Rule-based: $15,000+
- LLM-based: $2,660

ì ˆê°ì•¡: $12,340 (82% ì ˆê°) âœ…
```

---

## í˜„ì¬ êµ¬í˜„ ìƒíƒœ ë° ë‹¤ìŒ ë‹¨ê³„

### âœ… **Phase 0-2 ì™„ë£Œ** (2025-12-17)

**ë‹¬ì„±í•œ ê²ƒ:**
1. âœ… **ë©”íƒ€ë°ì´í„° ìë™ ê°ì§€** (100% ì •í™•ë„)
   - clinical_parameters.csv, lab_parameters.csv, track_names.csv ìë™ ìŠ¤í‚µ
   - 310ê°œ ì˜ë£Œ ìš©ì–´ ì¶”ì¶œ
   
2. âœ… **ê´€ê³„ ìë™ ì¶”ë¡ ** (Multi-level Anchor í•´ê²°)
   - lab_data.caseid â†’ clinical_data.caseid (N:1) ë°œê²¬
   - caseid â‰  subjectid ê´€ê³„ ì´í•´
   
3. âœ… **ê³„ì¸µ êµ¬ì¡° ìë™ ìƒì„±**
   - L1: Patient (subjectid)
   - L2: Case (caseid)
   - L3: Lab Observation (caseid)
   
4. âœ… **LLM ìºì‹±** (83% Hit Rate, $0.30 ì ˆì•½)

5. âœ… **ë°ì´í„° í’ˆì§ˆ ì²´í¬** (Negative Evidence)
   - null_ratio ê³„ì‚°
   - ì¤‘ë³µ ê°ì§€
   - high_cardinality ì²´í¬

**ì˜¨í†¨ë¡œì§€ íŒŒì¼:** `data/processed/ontology_db.json`
- definitions: 310ê°œ
- relationships: 1ê°œ
- hierarchy: 3ë ˆë²¨
- file_tags: 5ê°œ

---

### ğŸ”œ **Phase 3: ì‹¤ì œ DB êµ¬ì¶• + VectorDB** (ë‹¤ìŒ ë‹¨ê³„)

**ëª©í‘œ:**
1. **ê´€ê³„í˜• DB êµ¬ì¶•** - ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì‹¤ì œ ë°ì´í„° ì €ì¥
2. **VectorDB êµ¬ì¶•** - ì‹œë§¨í‹± ê²€ìƒ‰ ì§€ì›

**ê³„íš:**
- Part A: SQLite DB ìƒì„± (FK ì œì•½ì¡°ê±´, ì¸ë±ìŠ¤)
- Part B: ChromaDBë¡œ ì»¬ëŸ¼/ê´€ê³„ ì„ë² ë”©
- Part C: ìì—°ì–´ ê²€ìƒ‰ ("í˜ˆì•• ê´€ë ¨ ë°ì´í„°" â†’ bp_sys, bp_dia)

**ì˜ˆìƒ ê¸°ê°„:** 1-2ì£¼

**ì°¸ê³ :** 
- âŒ ì¿¼ë¦¬ ìë™í™”ëŠ” ì´ ì‹œìŠ¤í…œì—ì„œ í•˜ì§€ ì•ŠìŒ (ì™¸ë¶€ ë„êµ¬ í™œìš©)
- âœ… VectorDB = ì‹œë§¨í‹± ë°ì´í„° íƒìƒ‰ìš©

---

### ğŸ¯ ì „ë¬¸ê°€ ì´í‰ (ê²€í†  ì™„ë£Œ)

**"AI-Native Data Pipelineì˜ ëª¨ë²” ë‹µì•ˆ"**

ì´ ì„¤ê³„ëŠ” ë‹¤ìŒ 3ë°•ìê°€ ì™„ë²½í•˜ê²Œ ë§ë¬¼ë¦½ë‹ˆë‹¤:
1. **Ruleë¡œ ì‚¬ì‹¤(Fact) ìˆ˜ì§‘** - í†µê³„, unique values, ê³µí†µ ì»¬ëŸ¼
2. **LLMìœ¼ë¡œ ì˜ë¯¸(Meaning) í•´ì„** - PKì¸ê°€? ë©”íƒ€ë°ì´í„°ì¸ê°€?
3. **Humanìœ¼ë¡œ ìµœì¢… ê²€ì¦(Validation)** - ë¶ˆí™•ì‹¤í•˜ë©´ ë¬¼ì–´ë´„

**êµ¬í˜„ ì™„ë£Œ ê²€ì¦:**
- âœ… VitalDB 5ê°œ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì„±ê³µ
- âœ… ë©”íƒ€ë°ì´í„° 3ê°œ ìë™ ìŠ¤í‚µ (Human Review 0íšŒ)
- âœ… ê´€ê³„ 1ê°œ ë°œê²¬ (confidence: 0.86)
- âœ… ê³„ì¸µ 3ë ˆë²¨ ìƒì„±
- âœ… ë²”ìš©ì„± ì…ì¦ (íŒŒì¼ëª…, êµ¬ì¡° ê¸°ë°˜ ìë™ ì ì‘)

**ë‹¤ìŒ ëª©í‘œ:**
- ì‹¤ì œ DB êµ¬ì¶• (SQLite)
- VectorDB ì‹œë§¨í‹± ê²€ìƒ‰
- ì˜ë£Œ ë°ì´í„° íƒìƒ‰ ìë™í™”

---

## í•µì‹¬ ìš”ì•½ (TL;DR) - 2025-12-17 ì—…ë°ì´íŠ¸

### ğŸ“Š **í˜„ì¬ ìƒíƒœ: Phase 0-2 ì™„ë£Œ (85% êµ¬í˜„)**

**êµ¬í˜„ ì™„ë£Œ:**
- âœ… Phase 0: ê¸°ë°˜ êµ¬ì¡° (State, ìºì‹±, ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ì)
- âœ… Phase 1: ë©”íƒ€ë°ì´í„° íŒŒì‹± (310ê°œ ìš©ì–´ ì¶”ì¶œ)
- âœ… Phase 2: ê´€ê³„ ì¶”ë¡  (FK ë°œê²¬, ê³„ì¸µ ìƒì„±)

**í…ŒìŠ¤íŠ¸ ê²°ê³¼ (VitalDB):**
- ë©”íƒ€ë°ì´í„° ê°ì§€: 100% (5/5)
- ê´€ê³„ ë°œê²¬: 1ê°œ (lab â†’ clinical)
- ê³„ì¸µ: 3ë ˆë²¨ (Patient > Case > Lab)
- ìºì‹œ íš¨ìœ¨: 83% Hit Rate

**ë‹¤ìŒ ë‹¨ê³„:**
- ğŸ”œ Phase 3: ì‹¤ì œ DB êµ¬ì¶• + VectorDB

---

### ğŸ¯ ì£¼ìš” ì˜ì‚¬ê²°ì •

1. **ì„¤ê³„ ì² í•™: "Rule Prepares, LLM Decides"**
   - âœ… **Rule ì—­í• : ë°ì´í„° ì „ì²˜ë¦¬** 
     - íŒŒì‹±, í†µê³„ ê³„ì‚°, unique values ì¶”ì¶œ (Categorical ìµœëŒ€ 20ê°œ)
     - **[NEW]** null_ratio ê³„ì‚°, Negative Evidence ìˆ˜ì§‘
     - **[NEW]** ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (Context Window ê´€ë¦¬)
   
   - âœ… **LLM ì—­í• : ìµœì¢… íŒë‹¨** 
     - Ruleì´ ì •ë¦¬í•œ ì •ë³´ë¥¼ í•´ì„í•˜ì—¬ ì˜ë¯¸ ì¶”ë¡ 
     - **[NEW]** Positive + Negative Evidence ì¢…í•© íŒë‹¨
   
   - âœ… **Confidence ê¸°ë°˜** (ë¶ˆí™•ì‹¤ì„±ì„ ìˆ«ìë¡œ í‘œí˜„)
   
   - âŒ **Ruleë¡œ íŒë‹¨ ê¸ˆì§€** (í‚¤ì›Œë“œâ†’ê²°ë¡ , ì„ê³„ê°’â†’ê²°ë¡ , if-else íŠ¸ë¦¬)
   
   **ì˜ˆì‹œ:**
   ```python
   # Rule: ë°ì´í„° ìˆ˜ì§‘ (ê°•í™”)
   unique_vals = df[col].unique()[:20]      # Rule
   ratio = len(set(vals)) / len(vals)       # Rule
   null_ratio = vals.isna().sum() / len()   # Rule (NEW)
   negative = collect_issues(col, vals)     # Rule (NEW)
   
   # LLM: ì¢…í•© íŒë‹¨
   llm.ask(f"""
   unique_vals={unique_vals}, ratio={ratio}
   null_ratio={null_ratio}, issues={negative}
   â†’ PKì¸ê°€?
   """)
   # â†’ "ratio ë†’ì§€ë§Œ null 5% â†’ PK ì•„ë‹˜, confidence: 0.88"
   ```

2. **ë©”íƒ€ë°ì´í„° ê°ì§€: Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨**
   ```python
   # Rule: ë°ì´í„° ìˆ˜ì§‘
   parts = filename.split('_')           # ['lab', 'parameters']
   avg_len = calc_avg_text_length()      # 45.3 chars
   
   # LLM: íŒë‹¨
   llm.ask(f"""
   parts={parts}, avg_len={avg_len}
   â†’ ë©”íƒ€ë°ì´í„°ì¸ê°€?
   """)
   # â†’ is_metadata=True, confidence=0.95
   ```
   - ì •í™•ë„: 70-80% â†’ **95-98%** í–¥ìƒ
   - ìƒˆ íŒ¨í„´ ìë™ ì ì‘ (ê·œì¹™ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)

3. **íŒŒì¼ëª… í™œìš©: Rule íŒŒì‹± + LLM í•´ì„**
   ```python
   # Rule: íŒŒì¼ëª… êµ¬ì¡° ì¶”ì¶œ
   parts = "lab_data.csv".split('_')  # ['lab', 'data']
   base = parts[0]                     # 'lab'
   
   # LLM: ì˜ë¯¸ ì¶”ë¡ 
   llm.ask(f"""
   base_name={base}, suffix='data'
   â†’ Entity Type? Level?
   """)
   # â†’ entity="Laboratory", level=4, 
   #    related=["lab_parameters"]
   ```
   - lab_data.csv + lab_parameters.csv â†’ base_name ì¼ì¹˜ë¡œ Rule ì—°ê²°, LLM ê²€ì¦
   - Entity Type, Levelë„ LLMì´ ì¶”ë¡  (ë„ë©”ì¸ ì§€ì‹ í™œìš©)

4. **ì˜¨í†¨ë¡œì§€ = ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ìì‚°**
   - í•œ ë²ˆ êµ¬ì¶• â†’ ì˜êµ¬ ì¬ì‚¬ìš© (ìºì‹±)
   - ì¦ë¶„ ì—…ë°ì´íŠ¸ (ìƒˆ íŒŒì¼ë§ˆë‹¤ ì§€ì‹ ëˆ„ì )
   - Gitìœ¼ë¡œ ë²„ì „ ê´€ë¦¬
   - Human ê²€ì¦ ì´ë ¥ ì €ì¥

5. **ë¹„ìš© íš¨ìœ¨í™”**
   - ìºì‹±: ì¬ì‹¤í–‰ ë¹„ìš© 100% ì ˆê°
   - ë°°ì¹˜ ì²˜ë¦¬: 80% ë¹„ìš© ì ˆê°
   - Progressive LLM: í™•ì‹¤í•œ ê²½ìš° ìŠ¤í‚µ (70% ì ˆê°)
   - **[NEW]** Context Window ê´€ë¦¬: í† í° 30% ì ˆê°
   - **íŒŒì¼ë‹¹ $0.18 â†’ ìµœì í™” í›„ $0.12 â†’ ì¬ì‚¬ìš© ì‹œ $0.00**

6. **ë°ì´í„° í’ˆì§ˆ ë³´ì¥ (NEW)**
   - Negative Evidenceë¡œ ì´ìƒ íŒ¨í„´ ê°ì§€
   - null_ratio > 0.1ì¸ ID ì»¬ëŸ¼ â†’ ê²½ê³ 
   - 99% uniqueì¸ë° ì¤‘ë³µ â†’ ë°ì´í„° ì˜¤ë¥˜ ê°€ëŠ¥ì„± ì•Œë¦¼
   - LLMì´ í’ˆì§ˆ ì´ìŠˆë¥¼ reasoningì— í¬í•¨


---

## ë³€ê²½ ì´ë ¥

### v1.4 (2025-12-16) - **ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜ (Refinement)**
- **[NEW] Negative Evidence ì‹œìŠ¤í…œ**
  - `_collect_negative_evidence()` í•¨ìˆ˜ êµ¬í˜„
  - null_ratio, duplicate_rate, high_cardinality ì²´í¬
  - LLM í”„ë¡¬í”„íŠ¸ì— Positive + Negative ë™ì‹œ ì œê³µ
  - ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ìë™ ê°ì§€

- **[NEW] Context Window ê´€ë¦¬**
  - `_summarize_long_values()` - ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (>50 chars)
  - í† í° ì‚¬ì´ì¦ˆ ì¶”ì • ë° ìë™ ì¶•ì†Œ
  - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (ê¸´ í…ìŠ¤íŠ¸ â†’ ë©”íƒ€ ì •ë³´)
  - ì˜ˆìƒ í† í° ë¹„ìš© 30% ì ˆê°

- **[NEW] Human Review êµ¬ì²´í™”**
  - `_generate_specific_human_question()` í•¨ìˆ˜
  - LLM reasoningì„ ì§ˆë¬¸ì— í¬í•¨
  - ë°œê²¬ëœ ì´ìŠˆ ë‚˜ì—´ (â€¢ í˜•ì‹)
  - ì„ íƒì§€ ì œê³µ (ë©”íƒ€ë°ì´í„°/ë°ì´í„°/ëª¨ë¥´ê² ìŒ)
  - ì°¸ê³  ì •ë³´ ì²¨ë¶€ (íŒŒì¼ëª…, ì»¬ëŸ¼ ìˆ˜, ìƒ˜í”Œ)

- **ì™„ì „í•œ ì½”ë“œ ì˜ˆì‹œ ì¶”ê°€**
  - `state.py` - OntologyContext ì „ì²´ êµ¬ì¡°
  - `llm_cache.py` - ìºì‹± ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„
  - `ontology_builder_node()` - ì „ì²´ í”Œë¡œìš°
  - `_build_metadata_detection_context_v2()` - ê°œì„  ë²„ì „

- **ì „ë¬¸ê°€ ê²€í†  ì„¹ì…˜ ì¶”ê°€**
  - í”„ë ˆì„ì›Œí¬ ê²€í†  ê²°ê³¼
  - 3ê°€ì§€ ê°œì„  ì‚¬í•­ ìƒì„¸ ì„¤ëª…
  - "AI-Native Data Pipeline" ì´í‰

### v1.3 (2025-12-16) - **"Rule Prepares, LLM Decides" íŒ¨í„´ í™•ë¦½**
- Ruleê³¼ LLM ì—­í•  ëª…í™•íˆ ë¶„ë¦¬
- ëª¨ë“  í•¨ìˆ˜ë¥¼ "Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨" íŒ¨í„´ìœ¼ë¡œ í†µì¼
- Categorical ì»¬ëŸ¼: unique values ìµœëŒ€ 20ê°œ ì œê³µ
- `_find_common_columns()` í—¬í¼ ì¶”ê°€

### v1.2 (2025-12-16) - **LLM ê¸°ë°˜ ì „í™˜**
- Rule-based íŒë‹¨ ë¡œì§ ì œê±°
- LLM ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
- Confidence ê¸°ë°˜ Human Review

### v1.1 (2025-12-16)
- íŒŒì¼ëª… ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì „ëµ

### v1.0 (2025-12-16)
- ì´ˆê¸° ë²„ì „

---

**ìµœì¢… ë¬¸ì„œ ë²„ì „:** v1.4  
**ì‘ì„±ì¼:** 2025-12-16  
**ìƒíƒœ:** ì „ë¬¸ê°€ ê²€í†  ì™„ë£Œ - êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ  
**ë‹¤ìŒ ë‹¨ê³„:** Phase 0 ì½”ë“œ êµ¬í˜„ ì‹œì‘



---

## ë¬¸ì„œ ë²„ì „ ë° êµ¬í˜„ ìƒíƒœ

### v2.0 (2025-12-17) - **Phase 0-2 êµ¬í˜„ ì™„ë£Œ ë° Phase 3 ê³„íš ìˆ˜ì •**

#### êµ¬í˜„ ì™„ë£Œ ì‚¬í•­
- **Phase 0-2 ì™„ì „ êµ¬í˜„ ë° ê²€ì¦ ì™„ë£Œ**
  - ë©”íƒ€ë°ì´í„° ê°ì§€: 100% ì •í™•ë„ (VitalDB 5/5 íŒŒì¼)
  - ìš©ì–´ ì¶”ì¶œ: 310ê°œ
  - ê´€ê³„ ë°œê²¬: 1ê°œ (lab_data â†’ clinical_data, N:1)
  - ê³„ì¸µ ìƒì„±: 3ë ˆë²¨ (Patient > Case > Lab)
  - LLM ìºì‹œ: 83% Hit Rate ($0.30 ì ˆì•½)
  - ì¤‘ë³µ ì €ì¥ ë°©ì§€: ë©±ë“±ì„± ë³´ì¥
  
#### ì£¼ìš” ì´ìŠˆ í•´ê²°
- âœ… Hierarchy ì¤‘ë³µ ì œê±° (4ê°œ â†’ 3ê°œ)
  - (level, anchor_column) ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
  - confidence ë†’ì€ ê²ƒ ìš°ì„ 
  
- âœ… Cache í†µê³„ ìˆ˜ì •
  - ì „ì—­ ì‹±ê¸€í†¤ ìºì‹œ ì‚¬ìš©
  - hit_count ì •ìƒ ì§‘ê³„
  
#### ê³„íš ë³€ê²½ ë° ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜ (2ì°¨ ê²€í† )
- **Phase 3 ì¬ì •ì˜ ë° êµ¬ì²´í™”**: JOIN ì¿¼ë¦¬ ìë™ ìƒì„± â†’ ì‹¤ì œ DB êµ¬ì¶• + VectorDB
  
  - **Part A: SQLite DB ìƒì„± (ì•ˆì •ì„± ê°•í™”)**
    * **[ì „ë¬¸ê°€ í”¼ë“œë°± 1]** Chunk Processing ì¶”ê°€
      - ë¬¸ì œ: lab_data 928MB â†’ ë©”ëª¨ë¦¬ ì´ˆê³¼ ìœ„í—˜
      - í•´ê²°: chunksize=100,000 ì ìš©
      - íš¨ê³¼: ì•ˆì „í•œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
    
    * FK ì œì•½ì¡°ê±´, ì¸ë±ìŠ¤ ìë™ ìƒì„±
    
    * **[ì „ë¬¸ê°€ í”¼ë“œë°± 3]** Schema Evolution ì •ì±… ëª…ì‹œ
      - Phase 3: Drop & Recreate (ë‹¨ìˆœí™”)
      - Phase 4: Schema Merge (í–¥í›„ ê³ ë ¤)
  
  - **Part B: ChromaDB êµ¬ì¶• (ê²€ìƒ‰ í’ˆì§ˆ ë° í™•ì¥ì„±)**
    * **[ì „ë¬¸ê°€ í”¼ë“œë°± 2]** ê³„ì¸µì  ì„ë² ë”© ì „ëµ
      1. Table Summary (ë¼ìš°íŒ…ìš©) - "í™˜ì ì •ë³´ í…Œì´ë¸”?"
      2. Column Definition (ë§¤í•‘ìš©) - "í˜ˆì•• ì»¬ëŸ¼?"
      3. Relationship (JOINìš©) - "ì–´ë–»ê²Œ ì—°ê²°?"
    
    * Hybrid Search (Keyword + Vector)
    * Context Assembly (ê²€ìƒ‰ í›„ ì¡°ë¦½ â†’ LLM ì „ë‹¬)
    
    * **âš ï¸ [ì¤‘ìš”] í™•ì¥ì„± ëª…ì‹œì  ê³ ë ¤**
      - ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥í•˜ë„ë¡ ì¶”ìƒí™”
      - í–¥í›„ A/B í…ŒìŠ¤íŠ¸ í•„ìš” (OpenAI vs Local)
      - Re-ranking, Query Expansion ì¶”ê°€ ê°€ëŠ¥
      - "Phase 3ëŠ” ê¸°ë³¸ êµ¬ì¡°, ìµœì í™”ëŠ” ì§€ì†ì  ê°œì„ "
  
  - ì¿¼ë¦¬ ìë™í™”ëŠ” ì™¸ë¶€ ë„êµ¬ í™œìš© (LangChain SQL Agent ë“±)
  
- **Phase 4**: ê³ ê¸‰ ê¸°ëŠ¥ìœ¼ë¡œ ë³€ê²½ (í–¥í›„ í™•ì¥)
  - Schema Merge (ì»¬ëŸ¼ ì¶”ê°€/ì‚­ì œ ê°ì§€)
  - Re-ranking, Advanced Hybrid Search
  - ë‹¤ì¤‘ ë°ì´í„°ì…‹ í†µí•©
  - Vector Index ìµœì í™”

#### ë¬¸ì„œ ì—…ë°ì´íŠ¸
- í˜„ì¬ ìƒíƒœí‘œ ì¶”ê°€ (Phaseë³„ ì§„í–‰ë¥ )
- ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜ (VitalDB ê²€ì¦)
- VectorDB êµ¬í˜„ ê³„íš ìƒì„¸í™”
- requirements.txtì— chromadb ì¶”ê°€

#### ì „ë¬¸ê°€ 2ì°¨ í”¼ë“œë°± ë°˜ì˜ (2025-12-17)

**ì£¼ìš” ê°œì„  ì‚¬í•­:**

1. **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì „ëµ (Memory Safety)**
   - ë¬¸ì œ ì¸ì‹: lab_data 928MB â†’ RAM ì´ˆê³¼ ìœ„í—˜
   - í•´ê²°: Chunk Processing (chunksize=100,000)
   - ì½”ë“œ ì¶”ê°€: `for chunk in pd.read_csv(..., chunksize=...)`

2. **VectorDB ì„ë² ë”© ì „ëµ ê³ ë„í™”**
   - ê¸°ì¡´: Column + Relationship ì„ë² ë”©ë§Œ
   - ì¶”ê°€: **Table Summary Embedding** (ë¼ìš°íŒ…ìš©)
   - íš¨ê³¼: "í™˜ì ì •ë³´ í…Œì´ë¸”?" â†’ í…Œì´ë¸” ë‹¨ìœ„ ê²€ìƒ‰ ê°€ëŠ¥
   - Context Assembly í•¨ìˆ˜ ì¶”ê°€

3. **Schema Evolution ì •ì±… ìˆ˜ë¦½**
   - Phase 3: Drop & Recreate (if_exists='replace')
   - Phase 4: Schema Merge ê³ ë ¤ (ALTER TABLE)
   - ëª…í™•í•œ ë¡œë“œë§µ

4. **VectorDB í™•ì¥ì„± ëª…ì‹œ**
   - âš ï¸ ì„ë² ë”© ìµœì í™” ì—¬ì§€ ë§ìŒ
   - A/B í…ŒìŠ¤íŠ¸ í•„ìš” (OpenAI vs Local)
   - Re-ranking, Hybrid Search ê°œì„  ê°€ëŠ¥
   - "ê¸°ë³¸ êµ¬ì¡°ë§Œ êµ¬ì¶•, ì§€ì†ì  ê°œì„ " ì›ì¹™

#### ì°¸ê³  ë¬¸ì„œ ì¶”ê°€
- PHASE0_IMPLEMENTATION_SUMMARY.md
- PHASE2_IMPLEMENTATION_SUMMARY.md  
- PHASE2_GUIDE.md
- README_ONTOLOGY.md
- CURRENT_STATUS_2025-12-17.md (íŒ€ ê³µìœ ìš©)

---

**í˜„ì¬ ë¬¸ì„œ ìƒíƒœ:** v2.0 - Phase 0-2 êµ¬í˜„ ì™„ë£Œ, Phase 3 ê³„íš ì™„ë£Œ (ì „ë¬¸ê°€ 2ì°¨ ê²€í† )  
**ë‹¤ìŒ ëª©í‘œ:** Phase 3 êµ¬í˜„ (ì‹¤ì œ DB + VectorDB)  
**ë°°í¬ ì¤€ë¹„:** ì™„ë£Œ (íŒ€ ê³µìœ  ê°€ëŠ¥)  
**ê²€í†  ìƒíƒœ:** âœ… ë…¼ë¦¬ì  ê²°í•¨ ì—†ìŒ, ë³‘ëª© í•´ê²° ë°©ì•ˆ ìˆ˜ë¦½


