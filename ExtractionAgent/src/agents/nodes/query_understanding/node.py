# src/agents/nodes/query_understanding/node.py
"""
QueryUnderstandingNode - [100] ë™ì  ì»¨í…ìŠ¤íŠ¸ ë¡œë”© + ì¿¼ë¦¬ ë¶„ì„

Responsibilities:
1. SchemaContextBuilderë¡œ DB ë©”íƒ€ë°ì´í„° ë¡œë”©
2. ë™ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
3. ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„ (intent, parameters, filters, temporal)
"""

import sys
from pathlib import Path
from typing import Dict, Any

# shared íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.agents.base import BaseNode
from src.agents.registry import register_node
from src.agents.context import SchemaContextBuilder
from src.config import get_config

# shared LLM client
from shared.llm.client import get_llm_client

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
from .prompts import build_system_prompt, build_user_prompt


@register_node
class QueryUnderstandingNode(BaseNode):
    """
    [100] ì¿¼ë¦¬ ì´í•´ ë…¸ë“œ
    
    DB ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ìƒì„± + LLM ì¿¼ë¦¬ ë¶„ì„
    
    Input:
        - user_query: str
    
    Output:
        - schema_context: Dict (cohort_sources, signal_groups, parameters, relationships)
        - intent: str ("data_retrieval")
        - requested_parameters: List[Dict] (term, normalized, candidates)
        - cohort_filters: List[Dict] (column, operator, value)
        - temporal_context: Dict (type, start_column, end_column, margin_seconds)
    """
    
    name = "query_understanding"
    description = "ë™ì  ì»¨í…ìŠ¤íŠ¸ ë¡œë”© + ì¿¼ë¦¬ ë¶„ì„"
    order = 100
    requires_llm = True
    requires_db = True
    
    def __init__(self):
        super().__init__()
        self._context_builder = None
        self._llm_client = None
        self._config = get_config()
    
    @property
    def context_builder(self) -> SchemaContextBuilder:
        """SchemaContextBuilder ì¸ìŠ¤í„´ìŠ¤ (lazy loading)"""
        if self._context_builder is None:
            self._context_builder = SchemaContextBuilder()
        return self._context_builder
    
    @property
    def llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (lazy loading)"""
        if self._llm_client is None:
            self._llm_client = get_llm_client()
        return self._llm_client
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute query understanding
        
        1. SchemaContextBuilderë¡œ DB ë©”íƒ€ë°ì´í„° ë¡œë”©
        2. ë™ì  ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        3. LLM í˜¸ì¶œ ë° ì‘ë‹µ íŒŒì‹±
        """
        user_query = state.get("user_query", "")
        
        self.log(f"ì…ë ¥ ì¿¼ë¦¬: {user_query}", "ğŸ“")
        
        # 1. ìŠ¤í‚¤ë§ˆ ì»¨í…ìŠ¤íŠ¸ ë¡œë”©
        self.log("ìŠ¤í‚¤ë§ˆ ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì¤‘...", "ğŸ“Š")
        schema_context = self._load_schema_context()
        
        cohort_count = len(schema_context.get("cohort_sources", []))
        group_count = len(schema_context.get("signal_groups", []))
        param_categories = len(schema_context.get("parameters", {}))
        rel_count = len(schema_context.get("relationships", []))
        
        self.log(f"ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì™„ë£Œ: cohort={cohort_count}, groups={group_count}, param_categories={param_categories}, rels={rel_count}", "âœ…", indent=1)
        
        # 2. LLM í˜¸ì¶œë¡œ ì¿¼ë¦¬ ë¶„ì„
        self.log("LLM ì¿¼ë¦¬ ë¶„ì„ ì¤‘...", "ğŸ¤–")
        llm_result = self._analyze_query_with_llm(user_query, schema_context)
        
        # 3. ê²°ê³¼ ì¶”ì¶œ
        intent = llm_result.get("intent", "data_retrieval")
        requested_parameters = llm_result.get("requested_parameters", [])
        cohort_filters = llm_result.get("cohort_filters", [])
        temporal_context = llm_result.get("temporal_context", {
            "type": "full_record",
            "margin_seconds": 0
        })
        reasoning = llm_result.get("reasoning", "")
        
        self.log(f"ì˜ë„ ë¶„ì„: {intent}", "ğŸ¯", indent=1)
        self.log(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {len(requested_parameters)}ê°œ", "ğŸ“‹", indent=1)
        for param in requested_parameters:
            self.log(f"  - {param.get('term')} â†’ {param.get('normalized')}", "", indent=2)
        
        self.log(f"í•„í„°: {len(cohort_filters)}ê°œ", "ğŸ”", indent=1)
        for f in cohort_filters:
            self.log(f"  - {f.get('column')} {f.get('operator')} {f.get('value')}", "", indent=2)
        
        self.log(f"ì‹œê°„ ë²”ìœ„: {temporal_context.get('type')}", "â°", indent=1)
        
        if reasoning:
            self.log(f"ë¶„ì„ ê·¼ê±°: {reasoning[:100]}...", "ğŸ’­", indent=1)
        
        return {
            "query_understanding_result": {
                "status": "success",
                "node": self.name,
                "user_query": user_query,
                "context_loaded": True,
                "llm_reasoning": reasoning
            },
            "schema_context": schema_context,
            "intent": intent,
            "requested_parameters": requested_parameters,
            "cohort_filters": cohort_filters,
            "temporal_context": temporal_context
        }
    
    def _load_schema_context(self) -> Dict[str, Any]:
        """
        SchemaContextBuilderë¡œ ìŠ¤í‚¤ë§ˆ ì»¨í…ìŠ¤íŠ¸ ë¡œë”©
        
        Returns:
            {
                "cohort_sources": [...],
                "signal_groups": [...],
                "parameters": {...},
                "relationships": [...],
                "context_text": "..."
            }
        """
        max_params = self._config.query_understanding.max_parameters_in_context
        return self.context_builder.build_context(max_parameters=max_params)
    
    def _analyze_query_with_llm(
        self, 
        user_query: str, 
        schema_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ë¶„ì„
        
        Args:
            user_query: ì‚¬ìš©ì ìì—°ì–´ ì¿¼ë¦¬
            schema_context: ìŠ¤í‚¤ë§ˆ ì»¨í…ìŠ¤íŠ¸ (context_text í¬í•¨)
        
        Returns:
            {
                "intent": "data_retrieval",
                "requested_parameters": [...],
                "cohort_filters": [...],
                "temporal_context": {...},
                "reasoning": "..."
            }
        """
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        context_text = schema_context.get("context_text", "")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¶”ì¶œ (ë™ì )
        category_guide = schema_context.get("category_guide", {})
        available_categories = list(category_guide.keys()) if category_guide else None
        
        system_prompt = build_system_prompt(context_text, available_categories)
        user_prompt = build_user_prompt(user_query)
        
        # LLM í˜¸ì¶œ
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        try:
            response = self.llm_client.ask_json(full_prompt)
            
            # ì—ëŸ¬ ì²´í¬
            if "error" in response:
                self.log(f"LLM ì—ëŸ¬: {response.get('error')}", "âš ï¸")
                return self._get_default_response()
            
            return self._validate_and_normalize_response(response)
            
        except Exception as e:
            self.log(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}", "âŒ")
            return self._get_default_response()
    
    def _validate_and_normalize_response(
        self, 
        response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µ ê²€ì¦ ë° ì •ê·œí™”
        
        - í•„ìˆ˜ í•„ë“œ í™•ì¸
        - ê¸°ë³¸ê°’ ì„¤ì •
        - íƒ€ì… ê²€ì¦
        """
        result = {
            "intent": response.get("intent", "data_retrieval"),
            "requested_parameters": [],
            "cohort_filters": [],
            "temporal_context": {
                "type": "full_record",
                "margin_seconds": 0
            },
            "reasoning": response.get("reasoning", "")
        }
        
        # requested_parameters ì •ê·œí™”
        params = response.get("requested_parameters", [])
        if isinstance(params, list):
            for param in params:
                if isinstance(param, dict):
                    normalized_param = {
                        "term": param.get("term", ""),
                        "normalized": param.get("normalized", param.get("term", "")),
                        "candidates": param.get("candidates", []),
                        "expected_categories": param.get("expected_categories", [])  # Option B: ì¹´í…Œê³ ë¦¬ íŒíŠ¸
                    }
                    # candidatesê°€ ì—†ìœ¼ë©´ termê³¼ normalizedë¥¼ ì‚¬ìš©
                    if not normalized_param["candidates"]:
                        candidates = [normalized_param["term"]]
                        if normalized_param["normalized"] != normalized_param["term"]:
                            candidates.append(normalized_param["normalized"])
                        normalized_param["candidates"] = candidates
                    
                    result["requested_parameters"].append(normalized_param)
        
        # cohort_filters ì •ê·œí™”
        filters = response.get("cohort_filters", [])
        if isinstance(filters, list):
            for f in filters:
                if isinstance(f, dict) and f.get("column"):
                    normalized_filter = {
                        "column": f.get("column"),
                        "operator": f.get("operator", "="),
                        "value": f.get("value", "")
                    }
                    result["cohort_filters"].append(normalized_filter)
        
        # temporal_context ì •ê·œí™”
        temporal = response.get("temporal_context", {})
        if isinstance(temporal, dict):
            result["temporal_context"] = {
                "type": temporal.get("type", "full_record"),
                "margin_seconds": temporal.get("margin_seconds", 0),
                "start_column": temporal.get("start_column"),
                "end_column": temporal.get("end_column")
            }
        
        return result
    
    def _get_default_response(self) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        return {
            "intent": "data_retrieval",
            "requested_parameters": [],
            "cohort_filters": [],
            "temporal_context": {
                "type": "full_record",
                "margin_seconds": 0
            },
            "reasoning": "LLM í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"
        }
