#!/usr/bin/env python
"""
Signal ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. ExtractionAgentë¥¼ í†µí•´ "ì „ì²´ signal ë°ì´í„°ë¥¼ ë¡œë“œí•´ì¤˜" ì¿¼ë¦¬ ì²˜ë¦¬
2. DataContextì— execution plan ë¡œë“œ
3. ì‹¤ì œ signal ë°ì´í„° ë¡œë“œ
4. ë¡œë“œëœ ë°ì´í„° ê²€ì¦ ë° ì¶œë ¥
"""

import sys
import os
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def print_header(text: str, char: str = "="):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{char * 70}")
    print(f"  {text}")
    print(f"{char * 70}\n")


def print_subheader(text: str):
    """ì„œë¸Œí—¤ë” ì¶œë ¥"""
    print(f"\n{'â”€' * 70}")
    print(f"  {text}")
    print(f"{'â”€' * 70}\n")


def main():
    print_header("Signal ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸")
    
    # =========================================================================
    # Step 1: ExtractionAgent íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # =========================================================================
    print_subheader("Step 1: ExtractionAgent íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    
    try:
        from src.agents.graph import build_agent
        # ë…¸ë“œ importë¡œ ìë™ ë“±ë¡
        from src.agents.nodes import (
            QueryUnderstandingNode,
            ParameterResolverNode,
            PlanBuilderNode
        )
        
        # íŒŒì´í”„ë¼ì¸ ë¹Œë“œ (checkpointer ì—†ì´)
        workflow = build_agent()
        print("âœ… íŒŒì´í”„ë¼ì¸ ë¹Œë“œ ì„±ê³µ")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ë¹Œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (íŒŒë¼ë¯¸í„° ëª…ì‹œ ì—†ì´ - ConceptCategory ê¸°ë°˜ ì¶”ë¡  í•„ìš”)
    test_query = "ìˆ˜ìˆ  í™˜ìì˜ vital signal ë°ì´í„°ë¥¼ ì „ë¶€ ì¶”ì¶œí•´ì¤˜"
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: \"{test_query}\"")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    print("â³ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
    start_time = time.time()
    
    try:
        initial_state = {
            "user_query": test_query,  # ExtractionStateì˜ í‚¤ ì´ë¦„
            "logs": []
        }
        
        result = workflow.invoke(initial_state)
        elapsed = time.time() - start_time
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ({elapsed:.2f}s)")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Execution Plan í™•ì¸
    execution_plan = result.get("execution_plan", {})
    if not execution_plan:
        print("âŒ Execution Planì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸ“‹ ìƒì„±ëœ Execution Plan:")
    print(f"   - Version: {execution_plan.get('version')}")
    print(f"   - Agent: {execution_plan.get('agent')}")
    
    plan = execution_plan.get("execution_plan", {})
    cohort = plan.get("cohort_source", {})
    signal = plan.get("signal_source", {})
    
    print(f"   - Cohort: {cohort.get('file_name')}")
    print(f"   - Signal Group: {signal.get('group_name')}")
    print(f"   - Parameters: {len(signal.get('parameters', []))}ê°œ")
    
    for i, p in enumerate(signal.get("parameters", []), 1):
        print(f"      [{i}] {p.get('term')}: {p.get('param_keys')}")
    
    # =========================================================================
    # Step 2: DataContextì— Plan ë¡œë“œ
    # =========================================================================
    print_subheader("Step 2: DataContextì— Plan ë¡œë“œ")
    
    try:
        from shared.data import DataContext
        
        ctx = DataContext()
        ctx.load_from_plan(execution_plan)
        print("âœ… DataContextì— Plan ë¡œë“œ ì„±ê³µ")
        
        # ìƒíƒœ í™•ì¸
        print("\nğŸ“Š DataContext ìƒíƒœ:")
        print(f"   - cohort_file_id: {ctx._cohort_file_id}")
        print(f"   - cohort_file_path: {ctx._cohort_file_path}")
        print(f"   - signal_group_id: {ctx._signal_group_id}")
        print(f"   - signal_files ìˆ˜: {len(ctx._signal_files)}")
        if ctx._signal_files:
            print(f"   - signal_files ìƒ˜í”Œ: {ctx._signal_files[:3]}")
        print(f"   - param_keys: {ctx._param_keys}")
        print(f"   - temporal_type: {ctx._temporal_config.get('type')}")
        print(f"   - is_loaded: {ctx.is_loaded()}")
        
    except Exception as e:
        print(f"âŒ DataContext ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # =========================================================================
    # Step 3: ì‹¤ì œ Signal ë°ì´í„° ë¡œë“œ
    # =========================================================================
    print_subheader("Step 3: Signal ë°ì´í„° ë¡œë“œ")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ ID í™•ì¸
    print("ğŸ“‹ ì¼€ì´ìŠ¤ ID ì¡°íšŒ ì¤‘...")
    try:
        cohort_case_ids = ctx.get_case_ids(signals_only=False)
        signal_case_ids = ctx.get_case_ids(signals_only=True)  # ê¸°ë³¸ê°’
        available_case_ids = ctx.get_available_case_ids()  # êµì§‘í•©
        
        print(f"   - Cohort ì „ì²´ ì¼€ì´ìŠ¤: {len(cohort_case_ids)}ê°œ")
        print(f"   - Signal íŒŒì¼ ìˆëŠ” ì¼€ì´ìŠ¤: {len(signal_case_ids)}ê°œ")
        print(f"   - ë¶„ì„ ê°€ëŠ¥ ì¼€ì´ìŠ¤ (êµì§‘í•©): {len(available_case_ids)}ê°œ")
        
        if signal_case_ids:
            print(f"   - Signal ì¼€ì´ìŠ¤ ID: {signal_case_ids[:5]}{'...' if len(signal_case_ids) > 5 else ''}")
    except Exception as e:
        print(f"   âš ï¸ ì¼€ì´ìŠ¤ ID ì¡°íšŒ ì‹¤íŒ¨: {e}")
        signal_case_ids = []
    
    # Signal íŒŒì¼ ìƒì„¸ ì •ë³´
    print(f"\nğŸ“‹ Signal íŒŒì¼ ìƒì„¸:")
    print(f"   - Signal files ìˆ˜: {len(ctx._signal_files)}")
    
    if ctx._signal_files:
        for i, sf in enumerate(ctx._signal_files[:3]):
            print(f"   - [{i+1}] caseid={sf.get('caseid')}, path={sf.get('file_path')}")
    
    # Signal fileì´ ìˆëŠ” ì¼€ì´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸
    test_case_id = None
    test_file_path = None
    if signal_case_ids:
        test_case_id = signal_case_ids[0]
        # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        for f in ctx._signal_files:
            if f.get("caseid") == test_case_id:
                test_file_path = f.get("file_path")
                break
        print(f"\nğŸ” ì¼€ì´ìŠ¤ {test_case_id}ì˜ Signal ë°ì´í„° ë¡œë“œ ì¤‘...")
        print(f"   íŒŒì¼: {test_file_path}")
    else:
        print(f"\nâš ï¸ Signal íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
    
    if test_case_id and test_file_path:
        # .vital íŒŒì¼ ì „ì²´ íŠ¸ë™ ì •ë³´ í™•ì¸
        print(f"\nğŸ“‚ .vital íŒŒì¼ ì „ì²´ íŠ¸ë™ ì •ë³´:")
        try:
            import vitaldb
            vf = vitaldb.VitalFile(test_file_path)
            all_tracks = list(vf.trks.keys())
            print(f"   - íŒŒì¼ ë‚´ ì „ì²´ íŠ¸ë™ ìˆ˜: {len(all_tracks)}")
            print(f"   - ì „ì²´ íŠ¸ë™ ëª©ë¡:")
            for i, trk in enumerate(all_tracks):
                print(f"      [{i+1}] {trk}")
        except Exception as e:
            print(f"   âš ï¸ íŠ¸ë™ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ìš”ì²­í•œ param_keys vs ì‹¤ì œ ë¡œë“œ
        print(f"\nğŸ“‹ ìš”ì²­ param_keys vs ì‹¤ì œ ë¡œë“œ:")
        print(f"   - ìš”ì²­ëœ param_keys ({len(ctx._param_keys)}ê°œ): {ctx._param_keys}")
        
        start_time = time.time()
        try:
            signals = ctx.get_signals(caseid=test_case_id)
            elapsed = time.time() - start_time
            
            if signals is not None:
                print(f"âœ… Signal ë°ì´í„° ë¡œë“œ ì„±ê³µ ({elapsed:.2f}s)")
                print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ì •ë³´:")
                print(f"   - type: {type(signals).__name__}")
                print(f"   - shape: {signals.shape if hasattr(signals, 'shape') else 'N/A'}")
                print(f"   - columns: {list(signals.columns) if hasattr(signals, 'columns') else 'N/A'}")
                
                if hasattr(signals, 'head') and len(signals) > 0:
                    # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” í–‰ ì°¾ê¸° (nanì´ ì•„ë‹Œ ê°’)
                    numeric_cols = [c for c in signals.columns if c != 'Time']
                    
                    print(f"\nğŸ“‹ ì „ì²´ ë°ì´í„° ê°œìš”:")
                    print(f"   - ì´ í–‰ ìˆ˜: {len(signals)}")
                    print(f"   - ì‹œê°„ ë²”ìœ„: {signals['Time'].min():.1f}s ~ {signals['Time'].max():.1f}s")
                    
                    # ê° ì»¬ëŸ¼ë³„ ìœ íš¨ ë°ì´í„° ìˆ˜ í™•ì¸
                    print(f"\nğŸ“Š ì»¬ëŸ¼ë³„ ìœ íš¨ ë°ì´í„° ìˆ˜:")
                    for col in numeric_cols:
                        # [nan] ê°™ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„° ì²˜ë¦¬
                        if signals[col].dtype == object:
                            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„°ì¸ ê²½ìš°
                            valid_count = signals[col].apply(
                                lambda x: x is not None and (not isinstance(x, list) or (len(x) > 0 and x[0] == x[0]))
                            ).sum()
                        else:
                            valid_count = signals[col].notna().sum()
                        print(f"   - {col}: {valid_count}/{len(signals)} ({valid_count/len(signals)*100:.1f}%)")
                    
                    # nanì´ ì•„ë‹Œ ë°ì´í„°ê°€ ìˆëŠ” í–‰ ì°¾ê¸°
                    print(f"\nğŸ“‹ ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ (ìœ íš¨í•œ ê°’ì´ ìˆëŠ” í–‰):")
                    
                    # ì¤‘ê°„ ì§€ì ì˜ ë°ì´í„° ì¶œë ¥ (ë³´í†µ ìˆ˜ìˆ  ì¤‘ ë°ì´í„°)
                    mid_idx = len(signals) // 2
                    sample_range = signals.iloc[mid_idx:mid_idx+10]
                    
                    # ì¶œë ¥ í˜•ì‹ ê°œì„ 
                    print(f"   (ì‹œê°„ {sample_range['Time'].iloc[0]:.0f}s ~ {sample_range['Time'].iloc[-1]:.0f}s)")
                    print(sample_range.to_string(index=False))
                    
                    # í†µê³„ ì •ë³´ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ)
                    if len(numeric_cols) > 0:
                        print(f"\nğŸ“Š ë°ì´í„° í†µê³„:")
                        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜
                        stats_df = signals.copy()
                        for col in numeric_cols:
                            if stats_df[col].dtype == object:
                                stats_df[col] = stats_df[col].apply(
                                    lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None
                                )
                        
                        numeric_stats = stats_df[numeric_cols].describe()
                        print(numeric_stats.to_string())
                        
                elif len(signals) == 0:
                    print(f"\nâš ï¸ Signal ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (param_keysê°€ ì—†ê±°ë‚˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ)")
            else:
                print(f"âš ï¸ Signal ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ Signal ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # =========================================================================
    # Step 4: Cohort ë°ì´í„° ë¡œë“œ í™•ì¸
    # =========================================================================
    print_subheader("Step 4: Cohort ë°ì´í„° ë¡œë“œ í™•ì¸")
    
    try:
        cohort_data = ctx.get_cohort()
        
        if cohort_data is not None:
            print(f"âœ… Cohort ë°ì´í„° ë¡œë“œ ì„±ê³µ")
            print(f"\nğŸ“Š Cohort ë°ì´í„° ì •ë³´:")
            print(f"   - type: {type(cohort_data).__name__}")
            print(f"   - shape: {cohort_data.shape if hasattr(cohort_data, 'shape') else 'N/A'}")
            print(f"   - columns: {list(cohort_data.columns)[:10]}..." if hasattr(cohort_data, 'columns') and len(cohort_data.columns) > 10 else f"   - columns: {list(cohort_data.columns) if hasattr(cohort_data, 'columns') else 'N/A'}")
            
            if hasattr(cohort_data, 'head'):
                print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3í–‰, ì£¼ìš” ì»¬ëŸ¼ë§Œ):")
                display_cols = ['caseid', 'age', 'sex', 'department'] if all(c in cohort_data.columns for c in ['caseid', 'age', 'sex', 'department']) else list(cohort_data.columns)[:6]
                print(cohort_data[display_cols].head(3).to_string(index=False))
        else:
            print(f"âš ï¸ Cohort ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ Cohort ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    # =========================================================================
    # Step 5: ì „ì²´ ì¼€ì´ìŠ¤ ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬
    # =========================================================================
    print_subheader("Step 5: ì „ì²´ ì¼€ì´ìŠ¤ ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬")
    
    # ìºì‹œ í´ë¦¬ì–´ í›„ ì „ì²´ ë¡œë“œ
    DataContext.clear_cache("signals")
    print("ğŸ—‘ï¸ Signal ìºì‹œ í´ë¦¬ì–´")
    
    def print_cache_status():
        print(f"   - cohort_cache: {len(DataContext._cohort_cache)}ê°œ")
        print(f"   - signal_cache: {len(DataContext._signal_cache)}ê°œ")
        if DataContext._signal_cache:
            print(f"   - ìºì‹œëœ caseid: {list(DataContext._signal_cache.keys())}")
    
    print(f"\nğŸ“Š ì´ˆê¸° ìºì‹œ ìƒíƒœ:")
    print_cache_status()
    
    # ìˆœì°¨ì ìœ¼ë¡œ ì¼€ì´ìŠ¤ ë¡œë“œ
    print(f"\nğŸ”„ ì¼€ì´ìŠ¤ë³„ ìˆœì°¨ ë¡œë“œ:")
    for i, cid in enumerate(signal_case_ids):
        start_time = time.time()
        df = ctx.get_signals(caseid=cid)
        elapsed = time.time() - start_time
        print(f"\n   [{i+1}] ì¼€ì´ìŠ¤ {cid}: {df.shape}, {elapsed:.2f}s")
        print_cache_status()
    
    # ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸
    if signal_case_ids:
        print(f"\nğŸš€ ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸ (ë™ì¼ ì¼€ì´ìŠ¤ ì¬ìš”ì²­):")
        start_time = time.time()
        df = ctx.get_signals(caseid=signal_case_ids[0])
        elapsed = time.time() - start_time
        print(f"   ì¼€ì´ìŠ¤ {signal_case_ids[0]} ì¬ë¡œë“œ: {elapsed:.4f}s (ìºì‹œ íˆíŠ¸)")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    print(f"\nğŸ“Š ì¼€ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    total_memory = 0
    for cid, df in DataContext._signal_cache.items():
        mem = df.memory_usage(deep=True).sum()
        total_memory += mem
        print(f"   - ì¼€ì´ìŠ¤ {cid}: {mem/1024/1024:.2f} MB ({len(df)} rows)")
    print(f"\n   ğŸ’¾ ì´ ìºì‹œ ë©”ëª¨ë¦¬: {total_memory/1024/1024:.2f} MB")
    
    # =========================================================================
    # Step 6: ë¡œë“œ ìƒíƒœ ìµœì¢… í™•ì¸
    # =========================================================================
    print_subheader("Step 6: ë¡œë“œ ìƒíƒœ ìµœì¢… í™•ì¸")
    
    print("ğŸ“Š DataContext ìµœì¢… ìƒíƒœ:")
    print(f"   - is_loaded(): {ctx.is_loaded()}")
    
    # Summary ì¶œë ¥
    try:
        summary = ctx.summary()
        print(f"\nğŸ“‹ Summary:")
        print(f"   - loaded_at: {summary.get('loaded_at')}")
        print(f"   - cohort.loaded: {summary.get('cohort', {}).get('loaded')}")
        print(f"   - signals.total_files: {summary.get('signals', {}).get('total_files')}")
        print(f"   - signals.cached_count: {summary.get('signals', {}).get('cached_count')}")
        print(f"   - cache_stats: {summary.get('cache_stats')}")
    except Exception as e:
        print(f"   âš ï¸ Summary ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # Analysis Context
    print_subheader("Step 7: Analysis Context ìƒì„±")
    
    try:
        analysis_ctx = ctx.get_analysis_context()
        print("âœ… Analysis Context ìƒì„± ì„±ê³µ")
        print(f"\nğŸ“‹ Analysis Context:")
        print(f"   - description: {analysis_ctx.get('description', '')[:100]}...")
        print(f"   - original_query: {analysis_ctx.get('original_query')}")
        print(f"   - cohort.total_cases: {analysis_ctx.get('cohort', {}).get('total_cases')}")
        print(f"   - signals.param_keys: {analysis_ctx.get('signals', {}).get('param_keys')}")
    except Exception as e:
        print(f"âŒ Analysis Context ìƒì„± ì‹¤íŒ¨: {e}")
    
    # =========================================================================
    # ì™„ë£Œ
    # =========================================================================
    print_header("í…ŒìŠ¤íŠ¸ ì™„ë£Œ", "â•")
    print("ğŸ‰ Signal ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()

