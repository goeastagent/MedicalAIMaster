# ExtractionAgent v2 ì•„í‚¤í…ì²˜ ë° êµ¬í˜„ ëª…ì„¸

## ğŸ“– ê°œìš”

ExtractionAgent v2ëŠ” IndexingAgentê°€ êµ¬ì¶•í•œ **PostgreSQL ë©”íƒ€ë°ì´í„°**ì™€ **Neo4j ì˜¨í†¨ë¡œì§€**ë¥¼ í™œìš©í•˜ì—¬:
1. ì‚¬ìš©ìì˜ **ìì—°ì–´ ì§ˆì˜**ë¥¼ ë¶„ì„
2. ë°ì´í„° ìœ„ì¹˜ì™€ ì ‘ê·¼ ë°©ë²•ì„ ë‹´ì€ **Execution Plan JSON**ì„ ìƒì„±

í•˜ëŠ” ê³„íš ìˆ˜ë¦½(Planning) ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

### í•µì‹¬ ì² í•™

```
"ìš”ë¦¬(ë¶„ì„)ë¥¼ ìœ„í•œ ì™„ë²½í•œ ë ˆì‹œí”¼ì™€ ì¬ë£Œ ìœ„ì¹˜ë¥¼ ì œê³µí•œë‹¤"

- ë°ì´í„° ìì²´(Values)ê°€ ì•„ë‹Œ ë°ì´í„° í•¸ë“¤(Handle)ì„ ë°˜í™˜
- Analysis Agentê°€ Planì„ ë°›ì•„ ì‹¤ì œ ë°ì´í„° ë¡œë“œ/ì²˜ë¦¬ ìˆ˜í–‰
- Signal ë°ì´í„°(GB ë‹¨ìœ„)ë¥¼ ì§ì ‘ ì „ì†¡í•˜ì§€ ì•ŠìŒ
```

### IndexingAgent vs ExtractionAgent

| êµ¬ë¶„ | IndexingAgent | ExtractionAgent v2 |
|------|---------------|-------------------|
| **ì—­í• ** | ë°ì´í„° â†’ ë©”íƒ€ë°ì´í„° êµ¬ì¶• | ì¿¼ë¦¬ â†’ ì‹¤í–‰ ê³„íš ìƒì„± |
| **DB ì ‘ê·¼** | Write + Read | **Read Only** |
| **LLM ì‚¬ìš©** | ë¶„ë¥˜/ì¶”ë¡  | ì¿¼ë¦¬ ì´í•´/ë§¤í•‘ |
| **ì¶œë ¥** | PostgreSQL + Neo4j | Execution Plan JSON |

---

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì•„í‚¤í…ì²˜

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          ì…ë ¥: ìì—°ì–´ ì¿¼ë¦¬                                            â”ƒ
â”ƒ             "2023ë…„ ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜(HR) ë°ì´í„°ë¥¼ ì¤˜"                                â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 1: ì¿¼ë¦¬ ì´í•´ (LLM-based)                               â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [100] query_understanding ğŸ¤– â”‚â”€â”€â”€â”€â–¶â”‚ State: intent, extracted_entities       â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Intent Classification      â”‚     â”‚        resolution_strategy              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Entity Extraction (NER)    â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Resolution Strategy ê²°ì •   â”‚     â”‚ Intent: data_retrieval                 â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Entities: [diagnosis, temporal, param] â”‚   â”ƒ
â”ƒ                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 2: ì‹œë§¨í‹± í•´ì„ (LLM + DB)                              â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [200] semantic_resolver ğŸ¤–ğŸ“ â”‚â”€â”€â”€â”€â–¶â”‚ State: resolved_parameters              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Neo4j Parameter ê²€ìƒ‰       â”‚     â”‚        resolved_filters, ambiguities   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ PostgreSQL parameter ê²€ìƒ‰  â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Term â†’ Column ë§¤í•‘         â”‚     â”‚ "ì‹¬ë°•ìˆ˜" â†’ [Solar8000/HR, BIS/HR]      â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ëª¨í˜¸ì„± íƒì§€                â”‚     â”‚ "ìœ„ì•”" â†’ diagnosis column              â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 3: í† í´ë¡œì§€ íƒìƒ‰ (Rule-based)                          â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [300] topology_navigator ğŸ“  â”‚â”€â”€â”€â”€â–¶â”‚ State: data_topology                    â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Cohort Source ì‹ë³„         â”‚     â”‚   - cohort_source (clinical_data.csv)  â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Target Sources ì‹ë³„        â”‚     â”‚   - target_sources (vital_case_records)â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Join Path íƒìƒ‰             â”‚     â”‚   - join_paths (caseid)                â”‚   â”ƒ
â”ƒ   â”‚  â€¢ FileGroup í•´ì„             â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 4: ì½”í˜¸íŠ¸ ë¶„ì„ (Rule + LLM)                            â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [400] cohort_analyzer ğŸ“ğŸ¤–   â”‚â”€â”€â”€â”€â–¶â”‚ State: cohort_definition                â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„° ê°€ëŠ¥ì„± â”‚     â”‚   - strategy: partial_metadata          â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Filter Logic ìƒì„±          â”‚     â”‚   - filter_logic: [{col, op, val}]     â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ìŠ¤ìº” í•„ìš” ì—¬ë¶€ íŒë‹¨        â”‚     â”‚   - estimated_cohort_size: 150         â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 5: ê³„íš ìˆ˜ë¦½ (Rule-based)                              â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [500] plan_builder ğŸ“        â”‚â”€â”€â”€â”€â–¶â”‚ State: execution_plan                   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Execution Plan JSON ì¡°ë¦½   â”‚     â”‚   - cohort_source                      â”‚   â”ƒ
â”ƒ   â”‚  â€¢ File Path ë§¤í•‘             â”‚     â”‚   - data_sources                       â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Reader Type ê²°ì •           â”‚     â”‚   - join_specification                 â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Delegated Tasks ì •ì˜       â”‚     â”‚   - delegated_tasks                    â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 6: ê²€ì¦ (Rule + LLM)                                   â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [600] plan_validator ğŸ“ğŸ¤–    â”‚â”€â”€â”€â”€â–¶â”‚ State: validated_plan                   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ íŒŒì¼ ì¡´ì¬ í™•ì¸             â”‚     â”‚        validation_warnings              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Join Path ìœ íš¨ì„± ê²€ì¦      â”‚     â”‚        overall_confidence               â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Confidence ê³„ì‚°            â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Human Review í•„ìš” íŒë‹¨     â”‚     â”‚ Confidence: 0.92                       â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          ì¶œë ¥: Execution Plan JSON                                    â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   {                                                                                   â”ƒ
â”ƒ     "intent": "data_retrieval",                                                       â”ƒ
â”ƒ     "execution_plan": {                                                               â”ƒ
â”ƒ       "cohort_source": { file_path, filter_logic, result_identifier },               â”ƒ
â”ƒ       "data_sources": [{ group_id, target_parameters, join_key }],                   â”ƒ
â”ƒ       "join_specification": { paths }                                                 â”ƒ
â”ƒ     },                                                                                â”ƒ
â”ƒ     "validation": { confidence: 0.92 }                                               â”ƒ
â”ƒ   }                                                                                   â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

---

## ğŸ“Š ë…¸ë“œë³„ ìƒì„¸ ëª…ì„¸

### ğŸ”· [100] QueryUnderstandingNode

**ì—­í• **: ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ Intentì™€ Entityë¡œ ë³€í™˜

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 100 |
| **Type** | ğŸ¤– LLM-based |
| **Input** | `user_query` (ìì—°ì–´ ë¬¸ìì—´) |
| **Output** | `intent`, `extracted_entities`, `resolution_strategy` |
| **DB ì ‘ê·¼** | ì—†ìŒ |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### Intent Types

```python
class Intent(Enum):
    DATA_RETRIEVAL = "data_retrieval"      # ë°ì´í„° ì¶”ì¶œ (ê°€ì¥ ì¼ë°˜ì )
    AGGREGATION = "aggregation"             # ì§‘ê³„/í†µê³„ (í‰ê· , ìµœëŒ€ê°’ ë“±)
    EXPLORATION = "exploration"             # íƒìƒ‰ (ì–´ë–¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€?)
    RELATIONSHIP = "relationship"           # ê´€ê³„ íƒìƒ‰ (Aì™€ Bì˜ ê´€ê³„ëŠ”?)
    METADATA_LOOKUP = "metadata_lookup"     # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì»¬ëŸ¼ ì •ì˜ ë“±)
```

#### Entity Types

```python
class EntityType(Enum):
    PARAMETER = "parameter"           # ì¸¡ì • íŒŒë¼ë¯¸í„° (HR, SpO2, BP)
    DIAGNOSIS = "diagnosis"           # ì§„ë‹¨ëª… (ìœ„ì•”, ë‹¹ë‡¨)
    TEMPORAL = "temporal"             # ì‹œê°„ ì¡°ê±´ (2023ë…„, ìµœê·¼ 24ì‹œê°„)
    DEMOGRAPHIC = "demographic"       # ì¸êµ¬í†µê³„ (ë‚¨ì„±, 60ì„¸ ì´ìƒ)
    IDENTIFIER = "identifier"         # ì‹ë³„ì (caseid=123)
    PROCEDURE = "procedure"           # ì‹œìˆ /ìˆ˜ìˆ  (ë³µê°•ê²½ ìˆ˜ìˆ )
    CONDITION = "condition"           # ì¡°ê±´ (SBP < 90)
```

#### Extracted Entity ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class ExtractedEntity:
    type: EntityType
    value: str                           # ì›ë¬¸ í…ìŠ¤íŠ¸ ("ì‹¬ë°•ìˆ˜", "2023ë…„")
    normalized: Union[str, Dict]         # ì •ê·œí™”ëœ ê°’
    condition_type: Optional[str]        # "exact", "like", "range", "comparison"
    operator: Optional[str]              # "<", ">", "BETWEEN", "LIKE"
    confidence: float                    # LLM ì‹ ë¢°ë„
```

#### LLM í”„ë¡¬í”„íŠ¸ (query_understanding/prompts.py)

```python
SYSTEM_PROMPT = """
You are a medical data query analyzer for a surgical/clinical database.

Your task is to:
1. Classify the user's intent (what they want to do)
2. Extract entities (medical terms, conditions, parameters)
3. Normalize extracted entities to standard forms

Available Entity Types:
- parameter: Medical measurements (HR, SpO2, Blood Pressure, etc.)
- diagnosis: Disease/condition names (Stomach Cancer, Diabetes, etc.)
- temporal: Time constraints (2023, last 24 hours, during surgery)
- demographic: Patient demographics (male, age > 60)
- identifier: Specific IDs (caseid=123, patient_id=456)
- condition: Value-based conditions (SBP < 90, HR > 100)

Output JSON format:
{
    "intent": "data_retrieval" | "aggregation" | "exploration" | "relationship" | "metadata_lookup",
    "entities": [
        {
            "type": "parameter",
            "value": "ì‹¬ë°•ìˆ˜",
            "normalized": "Heart Rate",
            "candidates": ["HR", "Heart Rate", "heart_rate"],
            "condition_type": null,
            "confidence": 0.95
        },
        {
            "type": "diagnosis",
            "value": "ìœ„ì•”",
            "normalized": "Stomach Cancer",
            "condition_type": "like",
            "confidence": 0.9
        },
        {
            "type": "temporal",
            "value": "2023ë…„",
            "normalized": {"start": "2023-01-01", "end": "2023-12-31"},
            "condition_type": "range",
            "confidence": 0.95
        }
    ],
    "reasoning": "User wants to retrieve Heart Rate data for stomach cancer patients in 2023"
}
"""

USER_PROMPT_TEMPLATE = """
Analyze the following query:

Query: {user_query}

Extract the intent and all entities.
"""
```

#### Resolution Strategy ê²°ì • ë¡œì§

```python
def _determine_resolution_strategy(self, entities: List[ExtractedEntity]) -> str:
    """
    Entity íŠ¹ì„±ì— ë”°ë¼ í•´ê²° ì „ëµ ê²°ì •
    
    Rules:
    1. ê°’ ë²”ìœ„/ë¹„êµ ì¡°ê±´ (SBP < 90) â†’ scan_required
    2. Categorical í•„í„°ë§Œ (diagnosis=ìœ„ì•”) â†’ metadata_only ê°€ëŠ¥
    3. íŒŒë¼ë¯¸í„° ì¶”ì¶œë§Œ â†’ metadata_only
    """
    has_value_condition = any(
        e.type == EntityType.CONDITION or 
        e.condition_type in ["comparison", "range"]
        for e in entities
    )
    
    if has_value_condition:
        return "scan_required"
    
    has_filter = any(
        e.type in [EntityType.DIAGNOSIS, EntityType.TEMPORAL, EntityType.DEMOGRAPHIC]
        for e in entities
    )
    
    if has_filter:
        return "partial_metadata"
    
    return "metadata_only"
```

---

### ğŸ”· [200] SemanticResolverNode

**ì—­í• **: ì¶”ì¶œëœ Entityë¥¼ ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ(param_key, column_name)ì— ë§¤í•‘

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 200 |
| **Type** | ğŸ¤–ğŸ“ Hybrid (LLM + Rule) |
| **Input** | `extracted_entities` |
| **Output** | `resolved_parameters`, `resolved_filters`, `ambiguities` |
| **DB ì ‘ê·¼** | parameter, column_metadata, file_catalog |
| **Neo4j ì ‘ê·¼** | Parameter, ConceptCategory ë…¸ë“œ |

#### PostgreSQL ì¿¼ë¦¬

```sql
-- Parameter ê²€ìƒ‰ (semantic_name ë˜ëŠ” param_keyë¡œ)
SELECT 
    p.param_id,
    p.param_key,
    p.semantic_name,
    p.unit,
    p.concept_category,
    p.source_type,
    p.file_id,
    p.group_id,
    fg.group_name
FROM parameter p
LEFT JOIN file_group fg ON p.group_id = fg.group_id
WHERE (
    p.semantic_name ILIKE '%{term}%' 
    OR p.param_key ILIKE '%{term}%'
)
ORDER BY p.llm_confidence DESC NULLS LAST
LIMIT 10;

-- í•„í„° ì»¬ëŸ¼ ê²€ìƒ‰ (diagnosis, op_date ë“±)
SELECT DISTINCT
    cm.original_name,
    cm.column_type,
    cm.column_role,
    cm.value_distribution,
    fc.file_id,
    fc.file_path
FROM column_metadata cm
JOIN file_catalog fc ON cm.file_id = fc.file_id
WHERE cm.original_name ILIKE '%{column_hint}%'
  AND fc.is_metadata = FALSE;
```

#### Neo4j ì¿¼ë¦¬

```cypher
-- Semantic Nameìœ¼ë¡œ Parameter ê²€ìƒ‰
MATCH (p:Parameter)
WHERE toLower(p.semantic_name) CONTAINS toLower($term)
   OR toLower(p.key) CONTAINS toLower($term)
OPTIONAL MATCH (c:ConceptCategory)-[:CONTAINS]->(p)
OPTIONAL MATCH (fg:FileGroup)-[:HAS_COMMON_PARAM]->(p)
RETURN 
    p.key as param_key,
    p.semantic_name as semantic_name,
    p.unit as unit,
    c.name as concept_category,
    p.source_type as source_type,
    fg.group_id as group_id,
    fg.name as group_name
LIMIT 10;

-- ConceptCategory ê¸°ë°˜ ê²€ìƒ‰
MATCH (c:ConceptCategory {name: $category})-[:CONTAINS]->(p:Parameter)
RETURN p.key, p.semantic_name, p.unit
ORDER BY p.semantic_name;
```

#### ëª¨í˜¸ì„± ì²˜ë¦¬ ë¡œì§

```python
def _resolve_with_ambiguity_check(
    self, 
    entity: ExtractedEntity,
    pg_results: List[Dict],
    neo4j_results: List[Dict]
) -> ResolvedEntity:
    """
    ì—¬ëŸ¬ í›„ë³´ ì¤‘ ìµœì  ë§¤í•‘ ì„ íƒ, ëª¨í˜¸í•œ ê²½ìš° í‘œì‹œ
    """
    candidates = self._merge_candidates(pg_results, neo4j_results)
    
    if len(candidates) == 0:
        return ResolvedEntity(
            original=entity,
            resolved=None,
            confidence=0.0,
            status="not_found"
        )
    
    if len(candidates) == 1:
        return ResolvedEntity(
            original=entity,
            resolved=candidates[0],
            confidence=0.95,
            status="resolved"
        )
    
    # ì—¬ëŸ¬ í›„ë³´ â†’ LLMìœ¼ë¡œ ìµœì  ì„ íƒ ë˜ëŠ” ëª¨í˜¸ì„± í‘œì‹œ
    if self._are_semantically_similar(candidates):
        # ë™ì¼ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‚¤ (Solar8000/HR, BIS/HR ë‘˜ ë‹¤ Heart Rate)
        return ResolvedEntity(
            original=entity,
            resolved=candidates,  # ëª¨ë‘ í¬í•¨
            confidence=0.85,
            status="multiple_valid"
        )
    else:
        # ì˜ë¯¸ê°€ ë‹¤ë¥¸ í›„ë³´ë“¤ â†’ ì‚¬ìš©ì í™•ì¸ í•„ìš”
        return ResolvedEntity(
            original=entity,
            resolved=candidates,
            confidence=0.5,
            status="ambiguous",
            needs_human_review=True
        )
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class ResolvedParameter:
    semantic_term: str                   # ì›ë³¸ ê²€ìƒ‰ì–´ ("ì‹¬ë°•ìˆ˜")
    param_keys: List[str]                # ë§¤í•‘ëœ param_keyë“¤ ["Solar8000/HR", "BIS/HR"]
    concept_category: str                # "Vital Signs"
    source_type: str                     # "group_common"
    file_id: Optional[str]               # ê°œë³„ íŒŒì¼ íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°
    group_id: Optional[str]              # ê·¸ë£¹ íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°
    unit: Optional[str]                  # "bpm"
    confidence: float

@dataclass
class ResolvedFilter:
    entity_type: str                     # "diagnosis", "temporal"
    semantic_term: str                   # "ìœ„ì•”"
    column_name: str                     # "diagnosis"
    file_id: str                         # í•„í„°ê°€ ì ìš©ë  íŒŒì¼
    operator: str                        # "LIKE"
    value: Any                           # "%Stomach Cancer%"
    confidence: float
```

---

### ğŸ”· [300] TopologyNavigatorNode

**ì—­í• **: ë°ì´í„° ì†ŒìŠ¤ ê°„ ì—°ê²° ê²½ë¡œ(Join Path) íƒìƒ‰

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 300 |
| **Type** | ğŸ“ Rule-based |
| **Input** | `resolved_parameters`, `resolved_filters` |
| **Output** | `data_topology` |
| **DB ì ‘ê·¼** | file_catalog, file_group, table_entities, table_relationships |
| **Neo4j ì ‘ê·¼** | RowEntity LINKS_TO ê´€ê³„ |

#### Cohort Source ì‹ë³„ ë¡œì§

```python
def _identify_cohort_source(self, resolved_filters: List[ResolvedFilter]) -> Dict:
    """
    í•„í„° ì¡°ê±´ì´ ì ìš©ë˜ëŠ” íŒŒì¼(Cohort Source) ì‹ë³„
    
    Logic:
    1. resolved_filtersì—ì„œ íŒŒì¼ ID ì¶”ì¶œ
    2. table_entitiesì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ entity ì •ë³´ ì¡°íšŒ
    3. entity_identifier í™•ì¸ (Join Keyë¡œ ì‚¬ìš©)
    """
    if not resolved_filters:
        return None
    
    # í•„í„°ê°€ ìˆëŠ” íŒŒì¼ ID
    file_ids = list(set(f.file_id for f in resolved_filters))
    
    # ê°€ì¥ ì í•©í•œ Cohort Source ì„ íƒ
    # (ì—¬ëŸ¬ íŒŒì¼ì— í•„í„°ê°€ ìˆìœ¼ë©´ ìƒìœ„ Entity íŒŒì¼ ì„ íƒ)
    query = """
    SELECT 
        fc.file_id, fc.file_path, fc.file_name,
        te.row_represents, te.entity_identifier
    FROM file_catalog fc
    JOIN table_entities te ON fc.file_id = te.file_id
    WHERE fc.file_id = ANY(%s)
    ORDER BY 
        CASE te.row_represents 
            WHEN 'patient' THEN 1
            WHEN 'surgery' THEN 2
            WHEN 'case' THEN 3
            ELSE 10
        END
    LIMIT 1;
    """
    result = self.db.execute(query, [file_ids])
    
    return {
        "file_id": str(result["file_id"]),
        "file_path": result["file_path"],
        "file_name": result["file_name"],
        "entity_type": result["row_represents"],
        "identifier_column": result["entity_identifier"]
    }
```

#### Join Path íƒìƒ‰

```python
def _find_join_paths(
    self, 
    cohort_source: Dict, 
    target_sources: List[Dict]
) -> List[Dict]:
    """
    table_relationshipsë¥¼ ì‚¬ìš©í•´ Join Path íƒìƒ‰
    """
    paths = []
    
    for target in target_sources:
        # PostgreSQLì—ì„œ ê´€ê³„ ê²€ìƒ‰
        query = """
        SELECT 
            tr.source_column, tr.target_column, tr.cardinality,
            fc_s.file_name as source_name,
            fc_t.file_name as target_name,
            COALESCE(fg.group_name, fc_t.file_name) as target_display
        FROM table_relationships tr
        JOIN file_catalog fc_s ON tr.source_file_id = fc_s.file_id
        LEFT JOIN file_catalog fc_t ON tr.target_file_id = fc_t.file_id
        LEFT JOIN file_group fg ON fc_t.group_id = fg.group_id
        WHERE fc_s.file_id = %s
          AND (fc_t.file_id = %s OR fc_t.group_id = %s)
        """
        results = self.db.execute(query, [
            cohort_source["file_id"],
            target.get("file_id"),
            target.get("group_id")
        ])
        
        for r in results:
            paths.append({
                "from_file": cohort_source["file_name"],
                "to_target": r["target_display"],
                "source_column": r["source_column"],
                "target_column": r["target_column"],
                "cardinality": r["cardinality"]
            })
    
    # ì§ì ‘ ì—°ê²°ì´ ì—†ìœ¼ë©´ Neo4jì—ì„œ ê°„ì ‘ ê²½ë¡œ íƒìƒ‰
    if not paths:
        paths = self._find_indirect_paths_neo4j(cohort_source, target_sources)
    
    return paths
```

#### Neo4j ê°„ì ‘ ê²½ë¡œ íƒìƒ‰

```cypher
-- 2-hop ì´ë‚´ ê²½ë¡œ íƒìƒ‰
MATCH path = shortestPath(
    (source:RowEntity {source_table: $source_file})
    -[:LINKS_TO*1..2]-
    (target:RowEntity)
)
WHERE target.source_table = $target_file 
   OR target.group_name = $target_group
RETURN 
    [node in nodes(path) | node.name] as entities,
    [rel in relationships(path) | {
        type: type(rel),
        cardinality: rel.cardinality,
        join_column: rel.join_column
    }] as relationships,
    length(path) as hops
ORDER BY hops
LIMIT 1;
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class DataTopology:
    cohort_source: Dict[str, Any]        # Cohortë¥¼ ì •ì˜í•˜ëŠ” íŒŒì¼
    # {
    #   "file_id": "uuid",
    #   "file_path": "/data/clinical_data.csv",
    #   "entity_type": "surgery",
    #   "identifier_column": "caseid"
    # }
    
    target_sources: List[Dict[str, Any]] # ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ì†ŒìŠ¤ë“¤
    # [{
    #   "type": "file_group",
    #   "group_id": "uuid",
    #   "group_name": "vital_case_records",
    #   "param_keys": ["Solar8000/HR", "BIS/HR"]
    # }]
    
    join_paths: List[Dict[str, Any]]     # Join ê²½ë¡œ
    # [{
    #   "from_file": "clinical_data.csv",
    #   "to_target": "vital_case_records",
    #   "source_column": "caseid",
    #   "target_column": "filename_values.caseid",
    #   "cardinality": "1:N"
    # }]
```

---

### ğŸ”· [400] CohortAnalyzerNode

**ì—­í• **: ì½”í˜¸íŠ¸ í•„í„°ì˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•´ê²° ê°€ëŠ¥ì„± ë¶„ì„

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 400 |
| **Type** | ğŸ“ğŸ¤– Hybrid |
| **Input** | `resolved_filters`, `data_topology` |
| **Output** | `cohort_definition` |
| **DB ì ‘ê·¼** | column_metadata (value_distribution, column_info) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### í•„í„° ë¶„ì„ ë¡œì§

```python
def _analyze_filter_feasibility(
    self, 
    filter_: ResolvedFilter, 
    cohort_source: Dict
) -> Dict:
    """
    ê°œë³„ í•„í„°ì˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•´ê²° ê°€ëŠ¥ì„± ë¶„ì„
    
    ê²°ì • ê¸°ì¤€:
    1. Categorical + LIKE/Exact â†’ value_distribution í™•ì¸
    2. Range/Comparison â†’ scan í•„ìš”
    3. Temporal â†’ column_infoì˜ min/max í™•ì¸
    """
    # column_metadata ì¡°íšŒ
    query = """
    SELECT column_type, column_info, value_distribution
    FROM column_metadata
    WHERE file_id = %s AND original_name = %s
    """
    meta = self.db.execute_one(query, [
        cohort_source["file_id"], 
        filter_.column_name
    ])
    
    if not meta:
        return {
            "column": filter_.column_name,
            "resolvable_by_metadata": False,
            "requires_scan": True,
            "reason": "Column metadata not found"
        }
    
    column_type = meta["column_type"]
    condition_type = filter_.condition_type
    
    # Case 1: Categorical ì»¬ëŸ¼ + exact/like ì¡°ê±´
    if column_type == "categorical" and condition_type in ["exact", "like"]:
        value_dist = meta.get("value_distribution", {})
        unique_values = value_dist.get("unique_values", [])
        
        # ê°’ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        target = filter_.value.replace("%", "")  # LIKE íŒ¨í„´ ì œê±°
        found = any(target.lower() in str(v).lower() for v in unique_values)
        
        if found:
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": True,
                "requires_scan": False,
                "metadata_hint": f"Value '{target}' found in distribution"
            }
        else:
            # unique_countê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ë˜ì—ˆì„ ìˆ˜ ìˆìŒ
            unique_count = value_dist.get("unique_count", 0)
            if unique_count > len(unique_values):
                return {
                    "column": filter_.column_name,
                    "resolvable_by_metadata": False,
                    "requires_scan": True,
                    "reason": f"Distribution sampled ({len(unique_values)}/{unique_count})"
                }
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": False,
                "requires_scan": True,
                "reason": "Value not in distribution"
            }
    
    # Case 2: Temporal ì»¬ëŸ¼ + range ì¡°ê±´
    if column_type == "datetime" and condition_type == "range":
        column_info = meta.get("column_info", {})
        min_date = column_info.get("min")
        max_date = column_info.get("max")
        
        if min_date and max_date:
            # ë²”ìœ„ê°€ ì™„ì „íˆ ë²—ì–´ë‚˜ë©´ ë¹ˆ ê²°ê³¼ ì˜ˆì¸¡
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": True,
                "requires_scan": True,  # ì‹¤ì œ í•„í„°ë§ì€ ìŠ¤ìº” í•„ìš”
                "metadata_hint": f"Date range: {min_date} ~ {max_date}"
            }
    
    # Case 3: ë¹„êµ ì¡°ê±´ (< > <= >=) â†’ ìŠ¤ìº” í•„ìš”
    if condition_type == "comparison":
        return {
            "column": filter_.column_name,
            "resolvable_by_metadata": False,
            "requires_scan": True,
            "reason": f"Comparison condition ({filter_.operator}) requires scan"
        }
    
    return {
        "column": filter_.column_name,
        "resolvable_by_metadata": False,
        "requires_scan": True,
        "reason": "Unknown condition type"
    }
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class CohortDefinition:
    strategy: str                        # "metadata_resolvable" | "scan_required"
    filter_logic: List[Dict[str, Any]]   # Analysis Agentê°€ ì ìš©í•  í•„í„°
    # [{
    #   "column": "diagnosis",
    #   "operator": "LIKE",
    #   "value": "%Stomach Cancer%"
    # }]
    
    filter_analyses: List[Dict[str, Any]]  # ê° í•„í„°ì˜ ë¶„ì„ ê²°ê³¼
    estimated_cohort_size: Optional[int]   # ì¶”ì • ì½”í˜¸íŠ¸ í¬ê¸°
    scan_reason: Optional[str]             # ìŠ¤ìº”ì´ í•„ìš”í•œ ì´ìœ 
```

---

### ğŸ”· [500] PlanBuilderNode

**ì—­í• **: ìµœì¢… Execution Plan JSON ì¡°ë¦½

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 500 |
| **Type** | ğŸ“ Rule-based |
| **Input** | ëª¨ë“  ì´ì „ ë…¸ë“œ ê²°ê³¼ |
| **Output** | `execution_plan` |
| **DB ì ‘ê·¼** | file_catalog (íŒŒì¼ ê²½ë¡œ ì¡°íšŒ) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### Execution Plan ì¡°ë¦½

```python
def execute(self, state: ExtractionAgentState) -> Dict[str, Any]:
    plan = {
        "version": "1.0",
        "generated_at": datetime.now().isoformat(),
        "intent": state["intent"],
        "original_query": state["user_query"],
        
        "entities": self._build_entities_section(state),
        
        "execution_plan": {
            "cohort_source": self._build_cohort_source(
                state["data_topology"],
                state["cohort_definition"]
            ),
            "data_sources": self._build_data_sources(
                state["resolved_parameters"],
                state["data_topology"]
            ),
            "join_specification": self._build_join_spec(
                state["data_topology"]
            )
        },
        
        "resolution_strategy": state["cohort_definition"]["strategy"],
        "delegated_tasks": self._build_delegated_tasks(
            state["cohort_definition"],
            state["resolved_parameters"]
        )
    }
    
    return {
        "plan_builder_result": {"status": "success"},
        "execution_plan": plan
    }
```

#### Reader Type ê²°ì •

```python
READER_MAP = {
    ".csv": "pandas_csv",
    ".parquet": "pandas_parquet",
    ".xlsx": "pandas_excel",
    ".vital": "vitaldb_reader",
    ".edf": "pyedflib_reader",
    ".wfdb": "wfdb_reader",
    ".json": "json_reader",
}

def _get_reader_type(self, file_info: Dict) -> str:
    """íŒŒì¼ í™•ì¥ì ë˜ëŠ” processor_type ê¸°ë°˜ Reader ê²°ì •"""
    # file_catalog.processor_type í™•ì¸
    if file_info.get("processor_type") == "signal":
        ext = file_info.get("file_extension", "").lower()
        return READER_MAP.get(ext, "generic_signal_reader")
    else:
        ext = file_info.get("file_extension", "").lower()
        return READER_MAP.get(ext, "pandas_csv")
```

#### Delegated Tasks ì •ì˜

```python
def _build_delegated_tasks(
    self, 
    cohort_def: CohortDefinition,
    params: List[ResolvedParameter]
) -> List[Dict]:
    """
    Analysis Agentê°€ ìˆ˜í–‰í•  ì‘ì—… ì •ì˜
    """
    tasks = []
    
    # Task 1: Cohort í•„í„°ë§ (ìŠ¤ìº” í•„ìš” ì‹œ)
    if cohort_def.strategy == "scan_required":
        tasks.append({
            "task_id": "filter_cohort",
            "task_type": "file_scan_filter",
            "description": "íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì¡°ê±´ì— ë§ëŠ” ID ì¶”ì¶œ",
            "input": "cohort_source",
            "filter_logic": cohort_def.filter_logic,
            "output": "cohort_ids"
        })
    
    # Task 2: ë°ì´í„° ë¡œë“œ
    tasks.append({
        "task_id": "load_target_data",
        "task_type": "data_load",
        "description": "ì§€ì •ëœ íŒŒë¼ë¯¸í„° ë°ì´í„° ë¡œë“œ",
        "input": "data_sources",
        "parameters": [p.param_keys for p in params],
        "join_with": "cohort_ids",
        "output": "extracted_data"
    })
    
    return tasks
```

---

### ğŸ”· [600] PlanValidatorNode

**ì—­í• **: ìƒì„±ëœ Planì˜ ìœ íš¨ì„± ê²€ì¦ ë° ì‹ ë¢°ë„ í‰ê°€

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 600 |
| **Type** | ğŸ“ğŸ¤– Hybrid |
| **Input** | `execution_plan` |
| **Output** | `validated_plan`, `validation_warnings`, `overall_confidence` |
| **DB ì ‘ê·¼** | file_catalog (íŒŒì¼ ì¡´ì¬ í™•ì¸) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### ê²€ì¦ í•­ëª©

```python
def execute(self, state: ExtractionAgentState) -> Dict[str, Any]:
    plan = state["execution_plan"]
    warnings = []
    
    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    file_warnings = self._validate_file_existence(plan)
    warnings.extend(file_warnings)
    
    # 2. Join Path ìœ íš¨ì„± í™•ì¸
    join_warnings = self._validate_join_paths(plan)
    warnings.extend(join_warnings)
    
    # 3. Parameter ì»¤ë²„ë¦¬ì§€ í™•ì¸
    param_warnings = self._validate_parameter_coverage(plan, state)
    warnings.extend(param_warnings)
    
    # 4. ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± í™•ì¸
    type_warnings = self._validate_data_types(plan)
    warnings.extend(type_warnings)
    
    # 5. Confidence ê³„ì‚°
    confidence = self._calculate_confidence(plan, warnings, state)
    
    # 6. Human Review í•„ìš” ì—¬ë¶€ íŒë‹¨
    needs_review = (
        confidence < 0.7 or 
        any(w["severity"] == "high" for w in warnings) or
        len(state.get("ambiguities", [])) > 0
    )
    
    return {
        "plan_validator_result": {"status": "success"},
        "validated_plan": {**plan, "validation": {...}},
        "validation_warnings": warnings,
        "overall_confidence": confidence,
        "needs_human_review": needs_review,
        "human_review_type": self._get_review_type(warnings, state) if needs_review else None,
        "human_question": self._generate_review_question(warnings, state) if needs_review else None
    }
```

#### ì‹ ë¢°ë„ ê³„ì‚°

```python
def _calculate_confidence(
    self, 
    plan: Dict, 
    warnings: List[Dict], 
    state: Dict
) -> float:
    """
    ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚° (0.0 ~ 1.0)
    
    ê°ì  ìš”ì†Œ:
    - High severity warning: -0.3
    - Medium severity warning: -0.1
    - Low severity warning: -0.05
    - ëª¨í˜¸í•œ ë§¤í•‘: -0.1 per ambiguity
    - Join ê²½ë¡œ ì—†ìŒ: -0.2
    """
    confidence = 1.0
    
    # ê²½ê³ ì— ë”°ë¥¸ ê°ì 
    severity_penalty = {
        "high": 0.3,
        "medium": 0.1,
        "low": 0.05
    }
    for w in warnings:
        confidence -= severity_penalty.get(w["severity"], 0.05)
    
    # ëª¨í˜¸ì„±ì— ë”°ë¥¸ ê°ì 
    ambiguities = state.get("ambiguities", [])
    confidence -= len(ambiguities) * 0.1
    
    # Join ê²½ë¡œ ì—†ìœ¼ë©´ ê°ì 
    if not plan.get("execution_plan", {}).get("join_specification", {}).get("paths"):
        confidence -= 0.2
    
    return max(0.0, min(1.0, confidence))
```

---

## ğŸ“Š ExtractionAgentState ì „ì²´ ìŠ¤í‚¤ë§ˆ

```python
from typing import TypedDict, List, Dict, Any, Optional, Literal, Annotated
import operator


class ExtractionAgentState(TypedDict):
    """
    ExtractionAgent ì›Œí¬í”Œë¡œìš° ì „ì²´ ìƒíƒœ
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Input
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    user_query: str                           # ì›ë³¸ ìì—°ì–´ ì¿¼ë¦¬
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [100] query_understanding
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    query_understanding_result: Optional[Dict[str, Any]]  # ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    
    intent: Optional[Literal[
        "data_retrieval", "aggregation", "exploration", 
        "relationship", "metadata_lookup"
    ]]
    
    extracted_entities: List[Dict[str, Any]]
    # [{
    #   "type": "parameter",
    #   "value": "ì‹¬ë°•ìˆ˜",
    #   "normalized": "Heart Rate",
    #   "candidates": ["HR", "Heart Rate"],
    #   "condition_type": null,
    #   "confidence": 0.95
    # }]
    
    resolution_strategy: Optional[Literal[
        "metadata_only", "partial_metadata", "scan_required"
    ]]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [200] semantic_resolver
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    semantic_resolver_result: Optional[Dict[str, Any]]
    
    resolved_parameters: List[Dict[str, Any]]
    # [{
    #   "semantic_term": "ì‹¬ë°•ìˆ˜",
    #   "param_keys": ["Solar8000/HR", "BIS/HR"],
    #   "concept_category": "Vital Signs",
    #   "source_type": "group_common",
    #   "group_id": "uuid-...",
    #   "unit": "bpm",
    #   "confidence": 0.9
    # }]
    
    resolved_filters: List[Dict[str, Any]]
    # [{
    #   "entity_type": "diagnosis",
    #   "semantic_term": "ìœ„ì•”",
    #   "column_name": "diagnosis",
    #   "file_id": "uuid-...",
    #   "operator": "LIKE",
    #   "value": "%Stomach Cancer%",
    #   "confidence": 0.85
    # }]
    
    ambiguities: List[Dict[str, Any]]
    # [{
    #   "type": "parameter",
    #   "term": "BP",
    #   "candidates": [
    #     {"key": "NIBP", "name": "Non-Invasive BP"},
    #     {"key": "ABP", "name": "Arterial BP"}
    #   ],
    #   "reason": "Multiple BP types available"
    # }]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [300] topology_navigator
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    topology_navigator_result: Optional[Dict[str, Any]]
    
    data_topology: Optional[Dict[str, Any]]
    # {
    #   "cohort_source": {
    #     "file_id": "uuid",
    #     "file_path": "/data/clinical_data.csv",
    #     "entity_type": "surgery",
    #     "identifier_column": "caseid"
    #   },
    #   "target_sources": [{
    #     "type": "file_group",
    #     "group_id": "uuid",
    #     "group_name": "vital_case_records"
    #   }],
    #   "join_paths": [{
    #     "from_file": "clinical_data.csv",
    #     "to_target": "vital_case_records",
    #     "source_column": "caseid",
    #     "target_column": "filename_values.caseid",
    #     "cardinality": "1:N"
    #   }]
    # }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [400] cohort_analyzer
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    cohort_analyzer_result: Optional[Dict[str, Any]]
    
    cohort_definition: Optional[Dict[str, Any]]
    # {
    #   "strategy": "partial_metadata",
    #   "filter_logic": [
    #     {"column": "diagnosis", "operator": "LIKE", "value": "%Stomach Cancer%"}
    #   ],
    #   "estimated_cohort_size": 150,
    #   "scan_reason": null
    # }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [500] plan_builder
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    plan_builder_result: Optional[Dict[str, Any]]
    
    execution_plan: Optional[Dict[str, Any]]   # ìµœì¢… Execution Plan JSON
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [600] plan_validator
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    plan_validator_result: Optional[Dict[str, Any]]
    
    validated_plan: Optional[Dict[str, Any]]   # ê²€ì¦ ì™„ë£Œëœ Plan
    validation_warnings: List[Dict[str, Any]]  # ê²½ê³  ëª©ë¡
    overall_confidence: float                   # ì „ì²´ ì‹ ë¢°ë„ (0.0 ~ 1.0)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Human-in-the-Loop
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    needs_human_review: bool
    human_review_type: Optional[Literal[
        "ambiguous_parameter",
        "ambiguous_join_path",
        "low_confidence",
        "missing_data_source"
    ]]
    human_question: Optional[str]
    human_feedback: Optional[str]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # System
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    logs: Annotated[List[str], operator.add]
    error_message: Optional[str]
    retry_count: int
```

---

## ğŸ“„ ìµœì¢… Output: Execution Plan JSON Schema

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ExtractionAgent Execution Plan",
  "type": "object",
  "required": ["version", "intent", "execution_plan"],
  "properties": {
    "version": {
      "type": "string",
      "const": "1.0"
    },
    "generated_at": {
      "type": "string",
      "format": "date-time"
    },
    "intent": {
      "type": "string",
      "enum": ["data_retrieval", "aggregation", "exploration", "relationship", "metadata_lookup"]
    },
    "original_query": {
      "type": "string"
    },
    
    "entities": {
      "type": "object",
      "description": "ì¶”ì¶œëœ Entityë“¤ì˜ ì •ë¦¬ëœ í˜•íƒœ",
      "additionalProperties": true
    },
    
    "execution_plan": {
      "type": "object",
      "required": ["cohort_source", "data_sources"],
      "properties": {
        "cohort_source": {
          "type": "object",
          "required": ["type", "file_id", "file_path"],
          "properties": {
            "type": {"type": "string", "enum": ["tabular_file", "file_group"]},
            "file_id": {"type": "string", "format": "uuid"},
            "file_path": {"type": "string"},
            "file_name": {"type": "string"},
            "reader": {"type": "string"},
            "filter_logic": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "column": {"type": "string"},
                  "operator": {"type": "string"},
                  "value": {},
                  "values": {"type": "array"}
                }
              }
            },
            "result_identifier": {"type": "string"},
            "estimated_rows": {"type": "integer"}
          }
        },
        
        "data_sources": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "type": {"type": "string", "enum": ["file_group", "single_file"]},
              "group_id": {"type": "string", "format": "uuid"},
              "group_name": {"type": "string"},
              "file_id": {"type": "string", "format": "uuid"},
              "file_path": {"type": "string"},
              "reader": {"type": "string"},
              "file_pattern": {"type": "string"},
              "file_count": {"type": "integer"},
              "sample_paths": {"type": "array", "items": {"type": "string"}},
              "target_parameters": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "param_key": {"type": "string"},
                    "semantic_name": {"type": "string"},
                    "unit": {"type": "string"}
                  }
                }
              },
              "join_key": {
                "type": "object",
                "properties": {
                  "source": {"type": "string"},
                  "target": {"type": "string"}
                }
              }
            }
          }
        },
        
        "join_specification": {
          "type": "object",
          "properties": {
            "paths": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "from": {"type": "string"},
                  "to": {"type": "string"},
                  "via": {"type": "string"},
                  "cardinality": {"type": "string"}
                }
              }
            }
          }
        }
      }
    },
    
    "resolution_strategy": {
      "type": "string",
      "enum": ["metadata_only", "partial_metadata", "scan_required"]
    },
    
    "delegated_tasks": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "task_id": {"type": "string"},
          "task_type": {"type": "string"},
          "description": {"type": "string"},
          "input": {"type": "string"},
          "output": {"type": "string"}
        }
      }
    },
    
    "validation": {
      "type": "object",
      "properties": {
        "validated_at": {"type": "string", "format": "date-time"},
        "warnings": {"type": "array"},
        "confidence": {"type": "number", "minimum": 0, "maximum": 1}
      }
    }
  }
}
```

---

## ğŸ¯ ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ

**ì¿¼ë¦¬**: "2023ë…„ ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜(HR) ë°ì´í„°ë¥¼ ì¤˜"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â”œâ”€ diagnosis: "ìœ„ì•”" â†’ "Stomach Cancer"
â”‚   â”œâ”€ temporal: "2023ë…„" â†’ {2023-01-01 ~ 2023-12-31}
â”‚   â””â”€ parameter: "ì‹¬ë°•ìˆ˜" â†’ "Heart Rate"
â””â”€ Strategy: partial_metadata

[200] semantic_resolver
â”œâ”€ Parameters:
â”‚   â””â”€ "ì‹¬ë°•ìˆ˜" â†’ [Solar8000/HR, BIS/HR] (group_common)
â”œâ”€ Filters:
â”‚   â”œâ”€ diagnosis â†’ column "diagnosis" in clinical_data.csv
â”‚   â””â”€ op_date â†’ column "op_date" in clinical_data.csv
â””â”€ Ambiguities: []

[300] topology_navigator
â”œâ”€ Cohort Source: clinical_data.csv (identifier: caseid)
â”œâ”€ Target Sources: vital_case_records (FileGroup)
â””â”€ Join Path: clinical_data.caseid â†’ vital.filename_values.caseid

[400] cohort_analyzer
â”œâ”€ Strategy: partial_metadata
â”œâ”€ Filter Logic: [diagnosis LIKE, op_date BETWEEN]
â””â”€ Estimated Size: ~150 cases

[500] plan_builder
â””â”€ Execution Plan JSON ìƒì„±

[600] plan_validator
â”œâ”€ Warnings: []
â””â”€ Confidence: 0.92
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ê°’ ì¡°ê±´ í¬í•¨

**ì¿¼ë¦¬**: "ìˆ˜ìˆ  ì¤‘ ì €í˜ˆì••(SBP < 90)ì´ ë°œìƒí•œ í™˜ìì˜ Vital íŒŒì¼"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â”œâ”€ condition: "SBP < 90" â†’ {column: SBP, operator: <, value: 90}
â”‚   â””â”€ parameter: "Vital" â†’ (all vital signs)
â””â”€ Strategy: scan_required  â† ê°’ ì¡°ê±´ì´ë¯€ë¡œ ìŠ¤ìº” í•„ìš”

[400] cohort_analyzer
â”œâ”€ Strategy: scan_required
â”œâ”€ Scan Reason: "Value comparison (SBP < 90) requires file scan"
â””â”€ Delegated Task: "Scan vital files, filter by SBP < 90"
```

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
ExtractionAgent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                        # LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë”
â”‚   â”‚   â”œâ”€â”€ state.py                        # ExtractionAgentState
â”‚   â”‚   â”œâ”€â”€ registry.py                     # NodeRegistry (sharedì—ì„œ import ê°€ëŠ¥)
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ node.py                     # BaseNode
â”‚   â”‚   â”‚   â””â”€â”€ mixins.py                   # LLMMixin, DatabaseMixin
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enums.py                    # Intent, EntityType, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_responses.py            # LLM ì‘ë‹µ Pydantic ëª¨ë¸
â”‚   â”‚   â”‚   â””â”€â”€ execution_plan.py           # ExecutionPlan ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â””â”€â”€ nodes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ query_understanding/        # [100]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ semantic_resolver/          # [200]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ topology_navigator/         # [300]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ node.py
â”‚   â”‚       â”œâ”€â”€ cohort_analyzer/            # [400]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ plan_builder/               # [500]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ node.py
â”‚   â”‚       â””â”€â”€ plan_validator/             # [600]
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â”œâ”€â”€ node.py
â”‚   â”‚           â””â”€â”€ prompts.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_query_understanding.py
â”‚   â”œâ”€â”€ test_semantic_resolver.py
â”‚   â”œâ”€â”€ test_topology_navigator.py
â”‚   â”œâ”€â”€ test_full_pipeline.py
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_queries.json
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_extraction.py
â”‚   â”œâ”€â”€ complex_query.py
â”‚   â””â”€â”€ example_outputs/
â”‚
â”œâ”€â”€ ARCHITECTURE_V2.md                      # ì´ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ ì„¤ì •

### Nodeë³„ ì„¤ì •

```python
# src/config.py

class QueryUnderstandingConfig:
    """[100] QueryUnderstanding ë…¸ë“œ ì„¤ì •"""
    MAX_ENTITIES = 10                    # ìµœëŒ€ ì¶”ì¶œ Entity ìˆ˜
    CONFIDENCE_THRESHOLD = 0.7           # ìµœì†Œ ì‹ ë¢°ë„


class SemanticResolverConfig:
    """[200] SemanticResolver ë…¸ë“œ ì„¤ì •"""
    MAX_CANDIDATES = 10                  # ìµœëŒ€ í›„ë³´ ìˆ˜
    AMBIGUITY_THRESHOLD = 0.6            # ëª¨í˜¸ì„± íŒë‹¨ ê¸°ì¤€
    CACHE_ENABLED = True                 # ìºì‹± í™œì„±í™”
    CACHE_TTL = 3600                     # ìºì‹œ ìœ íš¨ ì‹œê°„ (ì´ˆ)


class TopologyNavigatorConfig:
    """[300] TopologyNavigator ë…¸ë“œ ì„¤ì •"""
    MAX_JOIN_HOPS = 3                    # ìµœëŒ€ Join í™‰ ìˆ˜


class CohortAnalyzerConfig:
    """[400] CohortAnalyzer ë…¸ë“œ ì„¤ì •"""
    VALUE_DISTRIBUTION_SAMPLE_THRESHOLD = 100  # ì´ ì´ìƒì´ë©´ ìƒ˜í”Œë§ë¨


class PlanBuilderConfig:
    """[500] PlanBuilder ë…¸ë“œ ì„¤ì •"""
    MAX_SAMPLE_PATHS = 5                 # ìƒ˜í”Œ íŒŒì¼ ê²½ë¡œ ìˆ˜


class PlanValidatorConfig:
    """[600] PlanValidator ë…¸ë“œ ì„¤ì •"""
    CONFIDENCE_THRESHOLD_FOR_REVIEW = 0.7  # Human Review ê¸°ì¤€
    VERIFY_FILE_EXISTENCE = True           # íŒŒì¼ ì¡´ì¬ í™•ì¸ ì—¬ë¶€
```

---

## ğŸ”— shared íŒ¨í‚¤ì§€ ì˜ì¡´ì„±

ExtractionAgentëŠ” ë‹¤ìŒ shared ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# Database (Read Only)
from shared.database import (
    get_db_manager,
    ParameterReader,          # Read-only, cached
    TopologyReader,           # Read-only
    FileRepository,           # Read-only ë©”ì„œë“œë§Œ ì‚¬ìš©
)

# Neo4j
from shared.neo4j import (
    get_neo4j_connection,
    ParameterQueryBuilder,
    TopologyQueryBuilder,
)

# Models
from shared.models import (
    ConceptCategory,
    SourceType,
    ColumnRole,
)

# LLM
from shared.llm import (
    get_llm_client,
)

# Config
from shared.config import (
    DatabaseConfig,
    Neo4jConfig,
    LLMConfig,
)
```

---

## ğŸ“ ë³€ê²½ ì´ë ¥

### v2.0 (Current)
- ì™„ì „íˆ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¡œ ì¬ì„¤ê³„
- Text-to-SQL ë°©ì‹ì—ì„œ **Execution Plan ìƒì„±** ë°©ì‹ìœ¼ë¡œ ë³€ê²½
- 6ê°œ ë…¸ë“œ íŒŒì´í”„ë¼ì¸ (query_understanding â†’ plan_validator)
- shared íŒ¨í‚¤ì§€ ì‚¬ìš©ìœ¼ë¡œ IndexingAgentì™€ ì¸í”„ë¼ ê³µìœ 
- Read-Only Repository íŒ¨í„´ ë„ì…

### v1.0 (Legacy)
- Text-to-SQL ì—ì´ì „íŠ¸
- ì§ì ‘ ë°ì´í„° ë°˜í™˜ ë°©ì‹
- ë³„ë„ì˜ database/ ëª¨ë“ˆ ë³´ìœ  (ì¤‘ë³µ)

