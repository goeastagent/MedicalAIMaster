# ExtractionAgent v2 ì•„í‚¤í…ì²˜ ë° êµ¬í˜„ ëª…ì„¸

## ğŸ“– ê°œìš”

ExtractionAgent v2ëŠ” IndexingAgentê°€ êµ¬ì¶•í•œ **PostgreSQL ë©”íƒ€ë°ì´í„°**ì™€ **Neo4j ì˜¨í†¨ë¡œì§€**ë¥¼ í™œìš©í•˜ì—¬:
1. ì‚¬ìš©ìì˜ **ìì—°ì–´ ì§ˆì˜**ë¥¼ ë¶„ì„
2. ë°ì´í„° ìœ„ì¹˜ì™€ ì ‘ê·¼ ë°©ë²•ì„ ë‹´ì€ **Execution Plan JSON**ì„ ìƒì„±

í•˜ëŠ” ê³„íš ìˆ˜ë¦½(Planning) ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

### í•µì‹¬ ì² í•™

```
"ìš”ë¦¬(ë¶„ì„)ë¥¼ ìœ„í•œ ì™„ë²½í•œ ë ˆì‹œí”¼ì™€ ì¬ë£Œ ìœ„ì¹˜ë¥¼ ì œê³µí•œë‹¤"

- ë°ì´í„° ìì²´(Values)ê°€ ì•„ë‹Œ ë°ì´í„° í•¸ë“¤(Handle)ì„ ë°˜í™˜
- Analysis Agentê°€ Planì„ ë°›ì•„ ì‹¤ì œ ë°ì´í„° ë¡œë“œ/ì²˜ë¦¬ ìˆ˜í–‰
- Signal ë°ì´í„°(GB ë‹¨ìœ„)ë¥¼ ì§ì ‘ ì „ì†¡í•˜ì§€ ì•ŠìŒ
```

### IndexingAgent vs ExtractionAgent

| êµ¬ë¶„ | IndexingAgent | ExtractionAgent v2 |
|------|---------------|-------------------|
| **ì—­í• ** | ë°ì´í„° â†’ ë©”íƒ€ë°ì´í„° êµ¬ì¶• | ì¿¼ë¦¬ â†’ ì‹¤í–‰ ê³„íš ìƒì„± |
| **DB ì ‘ê·¼** | Write + Read | **Read Only** |
| **LLM ì‚¬ìš©** | ë¶„ë¥˜/ì¶”ë¡  | ì¿¼ë¦¬ ì´í•´/ë§¤í•‘ |
| **ì¶œë ¥** | PostgreSQL + Neo4j | Execution Plan JSON |

---

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì•„í‚¤í…ì²˜

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          ì…ë ¥: ìì—°ì–´ ì¿¼ë¦¬                                            â”ƒ
â”ƒ             "2023ë…„ ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜(HR) ë°ì´í„°ë¥¼ ì¤˜"                                â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 1: ì¿¼ë¦¬ ì´í•´ (LLM-based)                               â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [100] query_understanding ğŸ¤– â”‚â”€â”€â”€â”€â–¶â”‚ State: intent, extracted_entities       â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Intent Classification      â”‚     â”‚        resolution_strategy              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Entity Extraction (NER)    â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Resolution Strategy ê²°ì •   â”‚     â”‚ Intent: data_retrieval                 â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Entities: [diagnosis, temporal, param] â”‚   â”ƒ
â”ƒ                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 2: ì‹œë§¨í‹± í•´ì„ (LLM + DB)                              â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [200] semantic_resolver ğŸ¤–ğŸ“ â”‚â”€â”€â”€â”€â–¶â”‚ State: resolved_parameters              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Neo4j Parameter ê²€ìƒ‰       â”‚     â”‚        resolved_filters, ambiguities   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ PostgreSQL parameter ê²€ìƒ‰  â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Term â†’ Column ë§¤í•‘         â”‚     â”‚ "ì‹¬ë°•ìˆ˜" â†’ [Solar8000/HR, BIS/HR]      â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ëª¨í˜¸ì„± íƒì§€                â”‚     â”‚ "ìœ„ì•”" â†’ diagnosis column              â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 3: í† í´ë¡œì§€ íƒìƒ‰ (Rule-based)                          â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [300] topology_navigator ğŸ“  â”‚â”€â”€â”€â”€â–¶â”‚ State: data_topology                    â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Cohort Source ì‹ë³„         â”‚     â”‚   - cohort_source (clinical_data.csv)  â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Target Sources ì‹ë³„        â”‚     â”‚   - target_sources (vital_case_records)â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Join Path íƒìƒ‰             â”‚     â”‚   - join_paths (caseid)                â”‚   â”ƒ
â”ƒ   â”‚  â€¢ FileGroup í•´ì„             â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 4: ì½”í˜¸íŠ¸ ë¶„ì„ (Rule + LLM)                            â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [400] cohort_analyzer ğŸ“ğŸ¤–   â”‚â”€â”€â”€â”€â–¶â”‚ State: cohort_definition                â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„° ê°€ëŠ¥ì„± â”‚     â”‚   - strategy: partial_metadata          â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Filter Logic ìƒì„±          â”‚     â”‚   - filter_logic: [{col, op, val}]     â”‚   â”ƒ
â”ƒ   â”‚  â€¢ ìŠ¤ìº” í•„ìš” ì—¬ë¶€ íŒë‹¨        â”‚     â”‚   - estimated_cohort_size: 150         â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 5: ê³„íš ìˆ˜ë¦½ (Rule-based)                              â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [500] plan_builder ğŸ“        â”‚â”€â”€â”€â”€â–¶â”‚ State: execution_plan                   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Execution Plan JSON ì¡°ë¦½   â”‚     â”‚   - cohort_source                      â”‚   â”ƒ
â”ƒ   â”‚  â€¢ File Path ë§¤í•‘             â”‚     â”‚   - data_sources                       â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Reader Type ê²°ì •           â”‚     â”‚   - join_specification                 â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Delegated Tasks ì •ì˜       â”‚     â”‚   - delegated_tasks                    â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          PHASE 6: ê²€ì¦ (Rule + LLM)                                   â”ƒ
â”ƒâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ   â”‚ [600] plan_validator ğŸ“ğŸ¤–    â”‚â”€â”€â”€â”€â–¶â”‚ State: validated_plan                   â”‚   â”ƒ
â”ƒ   â”‚  â€¢ íŒŒì¼ ì¡´ì¬ í™•ì¸             â”‚     â”‚        validation_warnings              â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Join Path ìœ íš¨ì„± ê²€ì¦      â”‚     â”‚        overall_confidence               â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Confidence ê³„ì‚°            â”‚     â”‚                                        â”‚   â”ƒ
â”ƒ   â”‚  â€¢ Human Review í•„ìš” íŒë‹¨     â”‚     â”‚ Confidence: 0.92                       â”‚   â”ƒ
â”ƒ   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                                             â”‚
                                             â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                          ì¶œë ¥: Execution Plan JSON                                    â”ƒ
â”ƒ                                                                                       â”ƒ
â”ƒ   {                                                                                   â”ƒ
â”ƒ     "intent": "data_retrieval",                                                       â”ƒ
â”ƒ     "execution_plan": {                                                               â”ƒ
â”ƒ       "cohort_source": { file_path, filter_logic, result_identifier },               â”ƒ
â”ƒ       "data_sources": [{ group_id, target_parameters, join_key }],                   â”ƒ
â”ƒ       "join_specification": { paths }                                                 â”ƒ
â”ƒ     },                                                                                â”ƒ
â”ƒ     "validation": { confidence: 0.92 }                                               â”ƒ
â”ƒ   }                                                                                   â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

---

## ğŸ“Š ë…¸ë“œë³„ ìƒì„¸ ëª…ì„¸

### ğŸ”· [100] QueryUnderstandingNode

**ì—­í• **: ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ Intentì™€ Entityë¡œ ë³€í™˜

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 100 |
| **Type** | ğŸ¤– LLM-based |
| **Input** | `user_query` (ìì—°ì–´ ë¬¸ìì—´) |
| **Output** | `intent`, `extracted_entities`, `resolution_strategy` |
| **DB ì ‘ê·¼** | ì—†ìŒ |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### Intent Types

```python
class Intent(Enum):
    DATA_RETRIEVAL = "data_retrieval"      # ë°ì´í„° ì¶”ì¶œ (ê°€ì¥ ì¼ë°˜ì )
    AGGREGATION = "aggregation"             # ì§‘ê³„/í†µê³„ (í‰ê· , ìµœëŒ€ê°’ ë“±)
    EXPLORATION = "exploration"             # íƒìƒ‰ (ì–´ë–¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€?)
    RELATIONSHIP = "relationship"           # ê´€ê³„ íƒìƒ‰ (Aì™€ Bì˜ ê´€ê³„ëŠ”?)
    METADATA_LOOKUP = "metadata_lookup"     # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì»¬ëŸ¼ ì •ì˜ ë“±)
```

#### Entity Types

```python
class EntityType(Enum):
    PARAMETER = "parameter"           # ì¸¡ì • íŒŒë¼ë¯¸í„° (HR, SpO2, BP)
    DIAGNOSIS = "diagnosis"           # ì§„ë‹¨ëª… (ìœ„ì•”, ë‹¹ë‡¨)
    TEMPORAL = "temporal"             # ì‹œê°„ ì¡°ê±´ (2023ë…„, ìˆ˜ìˆ  ì¤‘, ë§ˆì·¨ ìœ ë„ í›„)
    DEMOGRAPHIC = "demographic"       # ì¸êµ¬í†µê³„ (ë‚¨ì„±, 60ì„¸ ì´ìƒ)
    IDENTIFIER = "identifier"         # ì‹ë³„ì (caseid=123)
    PROCEDURE = "procedure"           # ì‹œìˆ /ìˆ˜ìˆ  (ë³µê°•ê²½ ìˆ˜ìˆ )
    CONDITION = "condition"           # ì¡°ê±´ (SBP < 90)


class TemporalType(Enum):
    """Temporal Entityì˜ ì„¸ë¶€ ìœ í˜• (LLMì´ íŒë‹¨)"""
    ABSOLUTE_RANGE = "absolute_range"      # "2023ë…„" â†’ ì ˆëŒ€ ì‹œê°„ ë²”ìœ„
    RELATIVE_WINDOW = "relative_window"    # "ìˆ˜ìˆ  ì¤‘" â†’ ì´ë²¤íŠ¸ ê¸°ì¤€ ìƒëŒ€ êµ¬ê°„
    EVENT_BASED = "event_based"            # "ë§ˆì·¨ ìœ ë„ í›„ 5ë¶„" â†’ ì´ë²¤íŠ¸ ê¸°ì¤€ ì˜¤í”„ì…‹
    DURATION = "duration"                  # "ìµœê·¼ 24ì‹œê°„" â†’ í˜„ì¬ ê¸°ì¤€ ê¸°ê°„
```

#### Extracted Entity ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class ExtractedEntity:
    type: EntityType
    value: str                           # ì›ë¬¸ í…ìŠ¤íŠ¸ ("ì‹¬ë°•ìˆ˜", "2023ë…„")
    normalized: Union[str, Dict]         # ì •ê·œí™”ëœ ê°’
    condition_type: Optional[str]        # "exact", "like", "range", "comparison"
    operator: Optional[str]              # "<", ">", "BETWEEN", "LIKE"
    confidence: float                    # LLM ì‹ ë¢°ë„


@dataclass
class TemporalEntity(ExtractedEntity):
    """Temporal Entity í™•ì¥ ìŠ¤í‚¤ë§ˆ - LLMì´ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ í’ë¶€í•˜ê²Œ ì¶”ì¶œ"""
    temporal_type: TemporalType          # absolute_range, relative_window, event_based
    reference_event: Optional[str]       # "op_start", "anesthesia_start", "admission"
    window_start: Optional[Union[str, int]]  # ì‹œì‘ì  (ì»¬ëŸ¼ëª… ë˜ëŠ” ì´ˆ ë‹¨ìœ„ ì˜¤í”„ì…‹)
    window_end: Optional[Union[str, int]]    # ì¢…ë£Œì  (ì»¬ëŸ¼ëª… ë˜ëŠ” ì´ˆ ë‹¨ìœ„ ì˜¤í”„ì…‹)
    margin_seconds: Optional[int]        # ì•ë’¤ ì—¬ìœ  ì‹œê°„ (Buffer)
```

#### LLM í”„ë¡¬í”„íŠ¸ (query_understanding/prompts.py)

```python
SYSTEM_PROMPT = """
You are a medical data query analyzer for a surgical/clinical database.

Your task is to:
1. Classify the user's intent (what they want to do)
2. Extract entities (medical terms, conditions, parameters)
3. Normalize extracted entities to standard forms
4. For temporal entities, identify the temporal context type and time boundaries

Available Entity Types:
- parameter: Medical measurements (HR, SpO2, Blood Pressure, etc.)
- diagnosis: Disease/condition names (Stomach Cancer, Diabetes, etc.)
- temporal: Time constraints with detailed context (see below)
- demographic: Patient demographics (male, age > 60)
- identifier: Specific IDs (caseid=123, patient_id=456)
- condition: Value-based conditions (SBP < 90, HR > 100)

## Temporal Entity Types (IMPORTANT):
For temporal entities, you MUST identify the temporal_type:

1. "absolute_range": Specific date/time range
   - Example: "2023ë…„" â†’ {"start": "2023-01-01", "end": "2023-12-31"}

2. "relative_window": Time window relative to a clinical event
   - Example: "ìˆ˜ìˆ  ì¤‘" â†’ reference_event: "surgery", window_start: "op_start", window_end: "op_end"
   - Example: "ë§ˆì·¨ ì¤‘" â†’ reference_event: "anesthesia", window_start: "ane_start", window_end: "ane_end"

3. "event_based": Offset from a specific event
   - Example: "ë§ˆì·¨ ìœ ë„ í›„ 5ë¶„" â†’ reference_event: "ane_start", window_start: 0, window_end: 300

4. "duration": Duration from current time or relative period
   - Example: "ìµœê·¼ 24ì‹œê°„" â†’ duration_seconds: 86400

## Complex Filter Logic:
For complex conditions with AND/OR logic, generate a SQL-like filter_expression:
- Example: "(ìœ„ì•” OR ëŒ€ì¥ì•”) í™˜ì ì¤‘ ë‚˜ì´ 60ì„¸ ì´ìƒ"
  â†’ filter_expression: "(diagnosis LIKE '%Stomach Cancer%' OR diagnosis LIKE '%Colon Cancer%') AND age >= 60"

Output JSON format:
{
    "intent": "data_retrieval" | "aggregation" | "exploration" | "relationship" | "metadata_lookup",
    "entities": [
        {
            "type": "parameter",
            "value": "ì‹¬ë°•ìˆ˜",
            "normalized": "Heart Rate",
            "candidates": ["HR", "Heart Rate", "heart_rate"],
            "condition_type": null,
            "confidence": 0.95
        },
        {
            "type": "diagnosis",
            "value": "ìœ„ì•”",
            "normalized": "Stomach Cancer",
            "condition_type": "like",
            "confidence": 0.9
        },
        {
            "type": "temporal",
            "value": "ìˆ˜ìˆ  ì¤‘",
            "temporal_type": "relative_window",
            "reference_event": "surgery",
            "window_start": "op_start",
            "window_end": "op_end",
            "margin_seconds": 300,
            "confidence": 0.95
        }
    ],
    "filter_expression": "diagnosis LIKE '%Stomach Cancer%'",
    "filter_format": "sql_where",
    "reasoning": "User wants to retrieve Heart Rate data during surgery for stomach cancer patients"
}
"""

USER_PROMPT_TEMPLATE = """
Analyze the following query:

Query: {user_query}

Extract the intent and all entities. Pay special attention to:
1. Temporal context - identify if it's absolute dates, relative to clinical events, or event-based offsets
2. Complex filter conditions - generate appropriate filter_expression for AND/OR logic
"""
```

#### Resolution Strategy ê²°ì • ë¡œì§

```python
def _determine_resolution_strategy(self, entities: List[ExtractedEntity]) -> str:
    """
    Entity íŠ¹ì„±ì— ë”°ë¼ í•´ê²° ì „ëµ ê²°ì •
    
    Rules:
    1. ê°’ ë²”ìœ„/ë¹„êµ ì¡°ê±´ (SBP < 90) â†’ scan_required
    2. Categorical í•„í„°ë§Œ (diagnosis=ìœ„ì•”) â†’ metadata_only ê°€ëŠ¥
    3. íŒŒë¼ë¯¸í„° ì¶”ì¶œë§Œ â†’ metadata_only
    """
    has_value_condition = any(
        e.type == EntityType.CONDITION or 
        e.condition_type in ["comparison", "range"]
        for e in entities
    )
    
    if has_value_condition:
        return "scan_required"
    
    has_filter = any(
        e.type in [EntityType.DIAGNOSIS, EntityType.TEMPORAL, EntityType.DEMOGRAPHIC]
        for e in entities
    )
    
    if has_filter:
        return "partial_metadata"
    
    return "metadata_only"
```

---

### ğŸ”· [200] SemanticResolverNode

**ì—­í• **: ì¶”ì¶œëœ Entityë¥¼ ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ(param_key, column_name)ì— ë§¤í•‘

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 200 |
| **Type** | ğŸ¤–ğŸ“ Hybrid (LLM + Rule) |
| **Input** | `extracted_entities` |
| **Output** | `resolved_parameters`, `resolved_filters`, `ambiguities` |
| **DB ì ‘ê·¼** | parameter, column_metadata, file_catalog |
| **Neo4j ì ‘ê·¼** | Parameter, ConceptCategory ë…¸ë“œ |

#### PostgreSQL ì¿¼ë¦¬

```sql
-- Parameter ê²€ìƒ‰ (semantic_name ë˜ëŠ” param_keyë¡œ)
SELECT 
    p.param_id,
    p.param_key,
    p.semantic_name,
    p.unit,
    p.concept_category,
    p.source_type,
    p.file_id,
    p.group_id,
    fg.group_name
FROM parameter p
LEFT JOIN file_group fg ON p.group_id = fg.group_id
WHERE (
    p.semantic_name ILIKE '%{term}%' 
    OR p.param_key ILIKE '%{term}%'
)
ORDER BY p.llm_confidence DESC NULLS LAST
LIMIT 10;

-- í•„í„° ì»¬ëŸ¼ ê²€ìƒ‰ (diagnosis, op_date ë“±)
SELECT DISTINCT
    cm.original_name,
    cm.column_type,
    cm.column_role,
    cm.value_distribution,
    fc.file_id,
    fc.file_path
FROM column_metadata cm
JOIN file_catalog fc ON cm.file_id = fc.file_id
WHERE cm.original_name ILIKE '%{column_hint}%'
  AND fc.is_metadata = FALSE;
```

#### Neo4j ì¿¼ë¦¬

```cypher
-- Semantic Nameìœ¼ë¡œ Parameter ê²€ìƒ‰
MATCH (p:Parameter)
WHERE toLower(p.semantic_name) CONTAINS toLower($term)
   OR toLower(p.key) CONTAINS toLower($term)
OPTIONAL MATCH (c:ConceptCategory)-[:CONTAINS]->(p)
OPTIONAL MATCH (fg:FileGroup)-[:HAS_COMMON_PARAM]->(p)
RETURN 
    p.key as param_key,
    p.semantic_name as semantic_name,
    p.unit as unit,
    c.name as concept_category,
    p.source_type as source_type,
    fg.group_id as group_id,
    fg.name as group_name
LIMIT 10;

-- ConceptCategory ê¸°ë°˜ ê²€ìƒ‰
MATCH (c:ConceptCategory {name: $category})-[:CONTAINS]->(p:Parameter)
RETURN p.key, p.semantic_name, p.unit
ORDER BY p.semantic_name;
```

#### Resolution Mode (LLMì´ ê²°ì •)

```python
class ResolutionMode(Enum):
    """íŒŒë¼ë¯¸í„° í•´ì„ ëª¨ë“œ - LLMì´ ì‚¬ìš©ì ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ê²°ì •"""
    ALL_SOURCES = "all_sources"      # ë™ì¼ ê°œë…ì˜ ëª¨ë“  ì†ŒìŠ¤ í¬í•¨ (ê¸°ë³¸ê°’)
    SPECIFIC = "specific"            # íŠ¹ì • ì†ŒìŠ¤ë§Œ ì„ íƒ
    BEST_MATCH = "best_match"        # ê°€ì¥ ì í•©í•œ í•˜ë‚˜ ì„ íƒ
    NEEDS_CLARIFICATION = "clarify"  # ì‚¬ìš©ì í™•ì¸ í•„ìš”
```

#### ëª¨í˜¸ì„± ì²˜ë¦¬ ë¡œì§ (LLM ê¸°ë°˜)

```python
def _resolve_with_ambiguity_check(
    self, 
    entity: ExtractedEntity,
    pg_results: List[Dict],
    neo4j_results: List[Dict]
) -> ResolvedEntity:
    """
    ì—¬ëŸ¬ í›„ë³´ ì¤‘ ìµœì  ë§¤í•‘ ì„ íƒ - LLMì´ Resolution Mode ê²°ì •
    
    ê¸°ë³¸ ë™ì‘: ë™ì¼ ConceptCategoryì˜ íŒŒë¼ë¯¸í„°ëŠ” ALL_SOURCESë¡œ ëª¨ë‘ í¬í•¨
    """
    candidates = self._merge_candidates(pg_results, neo4j_results)
    
    if len(candidates) == 0:
        return ResolvedEntity(
            original=entity,
            resolved=None,
            confidence=0.0,
            status="not_found"
        )
    
    if len(candidates) == 1:
        return ResolvedEntity(
            original=entity,
            resolved=candidates[0],
            confidence=0.95,
            status="resolved",
            resolution_mode=ResolutionMode.SPECIFIC
        )
    
    # ì—¬ëŸ¬ í›„ë³´ â†’ LLMì—ê²Œ Resolution Mode ê²°ì • ìœ„ì„
    resolution_decision = self._ask_llm_for_resolution(entity, candidates)
    
    if resolution_decision["mode"] == "ALL":
        # ë™ì¼ ì˜ë¯¸ì˜ ë‹¤ë¥¸ ì†ŒìŠ¤ë“¤ â†’ ëª¨ë‘ í¬í•¨ (ê¸°ë³¸ ë™ì‘)
        return ResolvedEntity(
            original=entity,
            resolved=candidates,
            confidence=0.9,
            status="multiple_valid",
            resolution_mode=ResolutionMode.ALL_SOURCES,
            resolution_reason=resolution_decision["reason"]
        )
    elif resolution_decision["mode"] == "PICK":
        # LLMì´ íŠ¹ì • í›„ë³´ë¥¼ ì„ íƒ
        selected = resolution_decision["selected"]
        return ResolvedEntity(
            original=entity,
            resolved=selected,
            confidence=0.85,
            status="resolved",
            resolution_mode=ResolutionMode.BEST_MATCH,
            resolution_reason=resolution_decision["reason"]
        )
    else:
        # ì˜ë¯¸ê°€ ë‹¤ë¥¸ í›„ë³´ë“¤ â†’ ì‚¬ìš©ì í™•ì¸ í•„ìš”
        return ResolvedEntity(
            original=entity,
            resolved=candidates,
            confidence=0.5,
            status="ambiguous",
            resolution_mode=ResolutionMode.NEEDS_CLARIFICATION,
            needs_human_review=True,
            clarification_question=resolution_decision["question"]
        )


def _ask_llm_for_resolution(
    self, 
    entity: ExtractedEntity, 
    candidates: List[Dict]
) -> Dict:
    """
    LLMì—ê²Œ Resolution Mode ê²°ì • ìœ„ì„
    """
    prompt = f"""
    User requested: "{entity.value}" (normalized: {entity.normalized})
    
    Found multiple candidates:
    {json.dumps([{
        'key': c['param_key'],
        'name': c['semantic_name'],
        'category': c['concept_category'],
        'source': c.get('group_name', c.get('file_name'))
    } for c in candidates], indent=2)}
    
    Determine the resolution mode:
    
    1. "ALL" - Include all candidates (they measure the same thing from different sources/devices)
       Use when: Same concept measured by different equipment (e.g., HR from multiple monitors)
       
    2. "PICK" - Select the best candidate
       Use when: One candidate clearly matches user intent better than others
       
    3. "CLARIFY" - Need user clarification
       Use when: Candidates represent genuinely different concepts
    
    Respond in JSON:
    {{
        "mode": "ALL" | "PICK" | "CLARIFY",
        "selected": <param_key if PICK>,
        "reason": "<brief explanation>",
        "question": "<clarification question if CLARIFY>"
    }}
    """
    return self.llm.invoke_json(prompt)
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class ResolvedParameter:
    semantic_term: str                   # ì›ë³¸ ê²€ìƒ‰ì–´ ("ì‹¬ë°•ìˆ˜")
    param_keys: List[str]                # ë§¤í•‘ëœ param_keyë“¤ ["Solar8000/HR", "BIS/HR"]
    concept_category: str                # "Vital Signs"
    source_type: str                     # "group_common"
    file_id: Optional[str]               # ê°œë³„ íŒŒì¼ íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°
    group_id: Optional[str]              # ê·¸ë£¹ íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°
    unit: Optional[str]                  # "bpm"
    confidence: float
    
    # v2.1 ì¶”ê°€: LLM ê¸°ë°˜ Resolution
    resolution_mode: ResolutionMode      # ALL_SOURCES, SPECIFIC, BEST_MATCH
    resolution_reason: Optional[str]     # LLMì´ ì„¤ëª…í•œ ì„ íƒ ì´ìœ 

@dataclass
class ResolvedFilter:
    entity_type: str                     # "diagnosis", "temporal"
    semantic_term: str                   # "ìœ„ì•”"
    column_name: str                     # "diagnosis"
    file_id: str                         # í•„í„°ê°€ ì ìš©ë  íŒŒì¼
    operator: str                        # "LIKE"
    value: Any                           # "%Stomach Cancer%"
    confidence: float


@dataclass
class ResolvedTemporalFilter(ResolvedFilter):
    """ì‹œê°„ í•„í„° í™•ì¥ ìŠ¤í‚¤ë§ˆ - Signal ë°ì´í„° ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©"""
    temporal_type: TemporalType          # relative_window, event_based, etc.
    reference_event: Optional[str]       # "surgery", "anesthesia"
    window_start: Optional[Union[str, int]]
    window_end: Optional[Union[str, int]]
    margin_seconds: Optional[int]
```

---

### ğŸ”· [300] TopologyNavigatorNode

**ì—­í• **: ë°ì´í„° ì†ŒìŠ¤ ê°„ ì—°ê²° ê²½ë¡œ(Join Path) íƒìƒ‰

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 300 |
| **Type** | ğŸ“ Rule-based |
| **Input** | `resolved_parameters`, `resolved_filters` |
| **Output** | `data_topology` |
| **DB ì ‘ê·¼** | file_catalog, file_group, table_entities, table_relationships |
| **Neo4j ì ‘ê·¼** | RowEntity LINKS_TO ê´€ê³„ |

#### Cohort Source ì‹ë³„ ë¡œì§

```python
def _identify_cohort_source(self, resolved_filters: List[ResolvedFilter]) -> Dict:
    """
    í•„í„° ì¡°ê±´ì´ ì ìš©ë˜ëŠ” íŒŒì¼(Cohort Source) ì‹ë³„
    
    Logic:
    1. resolved_filtersì—ì„œ íŒŒì¼ ID ì¶”ì¶œ
    2. table_entitiesì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ entity ì •ë³´ ì¡°íšŒ
    3. entity_identifier í™•ì¸ (Join Keyë¡œ ì‚¬ìš©)
    """
    if not resolved_filters:
        return None
    
    # í•„í„°ê°€ ìˆëŠ” íŒŒì¼ ID
    file_ids = list(set(f.file_id for f in resolved_filters))
    
    # ê°€ì¥ ì í•©í•œ Cohort Source ì„ íƒ
    # (ì—¬ëŸ¬ íŒŒì¼ì— í•„í„°ê°€ ìˆìœ¼ë©´ ìƒìœ„ Entity íŒŒì¼ ì„ íƒ)
    query = """
    SELECT 
        fc.file_id, fc.file_path, fc.file_name,
        te.row_represents, te.entity_identifier
    FROM file_catalog fc
    JOIN table_entities te ON fc.file_id = te.file_id
    WHERE fc.file_id = ANY(%s)
    ORDER BY 
        CASE te.row_represents 
            WHEN 'patient' THEN 1
            WHEN 'surgery' THEN 2
            WHEN 'case' THEN 3
            ELSE 10
        END
    LIMIT 1;
    """
    result = self.db.execute(query, [file_ids])
    
    return {
        "file_id": str(result["file_id"]),
        "file_path": result["file_path"],
        "file_name": result["file_name"],
        "entity_type": result["row_represents"],
        "identifier_column": result["entity_identifier"]
    }
```

#### Join Path íƒìƒ‰

```python
def _find_join_paths(
    self, 
    cohort_source: Dict, 
    target_sources: List[Dict]
) -> List[Dict]:
    """
    table_relationshipsë¥¼ ì‚¬ìš©í•´ Join Path íƒìƒ‰
    """
    paths = []
    
    for target in target_sources:
        # PostgreSQLì—ì„œ ê´€ê³„ ê²€ìƒ‰
        query = """
        SELECT 
            tr.source_column, tr.target_column, tr.cardinality,
            fc_s.file_name as source_name,
            fc_t.file_name as target_name,
            COALESCE(fg.group_name, fc_t.file_name) as target_display
        FROM table_relationships tr
        JOIN file_catalog fc_s ON tr.source_file_id = fc_s.file_id
        LEFT JOIN file_catalog fc_t ON tr.target_file_id = fc_t.file_id
        LEFT JOIN file_group fg ON fc_t.group_id = fg.group_id
        WHERE fc_s.file_id = %s
          AND (fc_t.file_id = %s OR fc_t.group_id = %s)
        """
        results = self.db.execute(query, [
            cohort_source["file_id"],
            target.get("file_id"),
            target.get("group_id")
        ])
        
        for r in results:
            paths.append({
                "from_file": cohort_source["file_name"],
                "to_target": r["target_display"],
                "source_column": r["source_column"],
                "target_column": r["target_column"],
                "cardinality": r["cardinality"]
            })
    
    # ì§ì ‘ ì—°ê²°ì´ ì—†ìœ¼ë©´ Neo4jì—ì„œ ê°„ì ‘ ê²½ë¡œ íƒìƒ‰
    if not paths:
        paths = self._find_indirect_paths_neo4j(cohort_source, target_sources)
    
    return paths
```

#### Neo4j ê°„ì ‘ ê²½ë¡œ íƒìƒ‰

```cypher
-- 2-hop ì´ë‚´ ê²½ë¡œ íƒìƒ‰
MATCH path = shortestPath(
    (source:RowEntity {source_table: $source_file})
    -[:LINKS_TO*1..2]-
    (target:RowEntity)
)
WHERE target.source_table = $target_file 
   OR target.group_name = $target_group
RETURN 
    [node in nodes(path) | node.name] as entities,
    [rel in relationships(path) | {
        type: type(rel),
        cardinality: rel.cardinality,
        join_column: rel.join_column
    }] as relationships,
    length(path) as hops
ORDER BY hops
LIMIT 1;
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class DataTopology:
    cohort_source: Dict[str, Any]        # Cohortë¥¼ ì •ì˜í•˜ëŠ” íŒŒì¼
    # {
    #   "file_id": "uuid",
    #   "file_path": "/data/clinical_data.csv",
    #   "entity_type": "surgery",
    #   "identifier_column": "caseid"
    # }
    
    target_sources: List[Dict[str, Any]] # ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ì†ŒìŠ¤ë“¤
    # [{
    #   "type": "file_group",
    #   "group_id": "uuid",
    #   "group_name": "vital_case_records",
    #   "param_keys": ["Solar8000/HR", "BIS/HR"]
    # }]
    
    join_paths: List[Dict[str, Any]]     # Join ê²½ë¡œ
    # [{
    #   "from_file": "clinical_data.csv",
    #   "to_target": "vital_case_records",
    #   "source_column": "caseid",
    #   "target_column": "filename_values.caseid",
    #   "cardinality": "1:N"
    # }]
```

---

### ğŸ”· [400] CohortAnalyzerNode

**ì—­í• **: ì½”í˜¸íŠ¸ í•„í„°ì˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•´ê²° ê°€ëŠ¥ì„± ë¶„ì„

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 400 |
| **Type** | ğŸ“ğŸ¤– Hybrid |
| **Input** | `resolved_filters`, `data_topology` |
| **Output** | `cohort_definition` |
| **DB ì ‘ê·¼** | column_metadata (value_distribution, column_info) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### í•„í„° ë¶„ì„ ë¡œì§

```python
def _analyze_filter_feasibility(
    self, 
    filter_: ResolvedFilter, 
    cohort_source: Dict
) -> Dict:
    """
    ê°œë³„ í•„í„°ì˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•´ê²° ê°€ëŠ¥ì„± ë¶„ì„
    
    ê²°ì • ê¸°ì¤€:
    1. Categorical + LIKE/Exact â†’ value_distribution í™•ì¸
    2. Range/Comparison â†’ scan í•„ìš”
    3. Temporal â†’ column_infoì˜ min/max í™•ì¸
    """
    # column_metadata ì¡°íšŒ
    query = """
    SELECT column_type, column_info, value_distribution
    FROM column_metadata
    WHERE file_id = %s AND original_name = %s
    """
    meta = self.db.execute_one(query, [
        cohort_source["file_id"], 
        filter_.column_name
    ])
    
    if not meta:
        return {
            "column": filter_.column_name,
            "resolvable_by_metadata": False,
            "requires_scan": True,
            "reason": "Column metadata not found"
        }
    
    column_type = meta["column_type"]
    condition_type = filter_.condition_type
    
    # Case 1: Categorical ì»¬ëŸ¼ + exact/like ì¡°ê±´
    if column_type == "categorical" and condition_type in ["exact", "like"]:
        value_dist = meta.get("value_distribution", {})
        unique_values = value_dist.get("unique_values", [])
        
        # ê°’ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        target = filter_.value.replace("%", "")  # LIKE íŒ¨í„´ ì œê±°
        found = any(target.lower() in str(v).lower() for v in unique_values)
        
        if found:
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": True,
                "requires_scan": False,
                "metadata_hint": f"Value '{target}' found in distribution"
            }
        else:
            # unique_countê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ë˜ì—ˆì„ ìˆ˜ ìˆìŒ
            unique_count = value_dist.get("unique_count", 0)
            if unique_count > len(unique_values):
                return {
                    "column": filter_.column_name,
                    "resolvable_by_metadata": False,
                    "requires_scan": True,
                    "reason": f"Distribution sampled ({len(unique_values)}/{unique_count})"
                }
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": False,
                "requires_scan": True,
                "reason": "Value not in distribution"
            }
    
    # Case 2: Temporal ì»¬ëŸ¼ + range ì¡°ê±´
    if column_type == "datetime" and condition_type == "range":
        column_info = meta.get("column_info", {})
        min_date = column_info.get("min")
        max_date = column_info.get("max")
        
        if min_date and max_date:
            # ë²”ìœ„ê°€ ì™„ì „íˆ ë²—ì–´ë‚˜ë©´ ë¹ˆ ê²°ê³¼ ì˜ˆì¸¡
            return {
                "column": filter_.column_name,
                "resolvable_by_metadata": True,
                "requires_scan": True,  # ì‹¤ì œ í•„í„°ë§ì€ ìŠ¤ìº” í•„ìš”
                "metadata_hint": f"Date range: {min_date} ~ {max_date}"
            }
    
    # Case 3: ë¹„êµ ì¡°ê±´ (< > <= >=) â†’ ìŠ¤ìº” í•„ìš”
    if condition_type == "comparison":
        return {
            "column": filter_.column_name,
            "resolvable_by_metadata": False,
            "requires_scan": True,
            "reason": f"Comparison condition ({filter_.operator}) requires scan"
        }
    
    return {
        "column": filter_.column_name,
        "resolvable_by_metadata": False,
        "requires_scan": True,
        "reason": "Unknown condition type"
    }
```

#### Output ìŠ¤í‚¤ë§ˆ

```python
@dataclass
class CohortDefinition:
    strategy: str                        # "metadata_resolvable" | "scan_required"
    
    # í•„í„° í‘œí˜„ (ë‘ ê°€ì§€ í˜•íƒœ ëª¨ë‘ ì§€ì›)
    filter_logic: List[Dict[str, Any]]   # ë‹¨ìˆœ í•„í„° ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜)
    # [{
    #   "column": "diagnosis",
    #   "operator": "LIKE",
    #   "value": "%Stomach Cancer%"
    # }]
    
    filter_expression: Optional[str]     # LLMì´ ìƒì„±í•œ ë³µí•© ë…¼ë¦¬ í‘œí˜„ì‹
    # "(diagnosis LIKE '%Stomach Cancer%' OR diagnosis LIKE '%Colon Cancer%') AND age >= 60"
    
    filter_format: str                   # "sql_where" | "pandas_query"
    # Analysis Agentê°€ í‘œí˜„ì‹ì„ ì–´ë–»ê²Œ í•´ì„í• ì§€ ê²°ì •
    
    filter_analyses: List[Dict[str, Any]]  # ê° í•„í„°ì˜ ë¶„ì„ ê²°ê³¼
    estimated_cohort_size: Optional[int]   # ì¶”ì • ì½”í˜¸íŠ¸ í¬ê¸°
    scan_reason: Optional[str]             # ìŠ¤ìº”ì´ í•„ìš”í•œ ì´ìœ 
```

#### Filter Expression ì²˜ë¦¬

```python
def _build_filter_expression(
    self, 
    state: ExtractionAgentState
) -> Tuple[str, str]:
    """
    LLMì´ ìƒì„±í•œ filter_expressionì„ ê²€ì¦í•˜ê³  ë°˜í™˜
    
    Returns:
        Tuple[filter_expression, filter_format]
    """
    # QueryUnderstandingì—ì„œ LLMì´ ìƒì„±í•œ í‘œí˜„ì‹ ì‚¬ìš©
    if state.get("filter_expression"):
        expression = state["filter_expression"]
        format_type = state.get("filter_format", "sql_where")
        
        # êµ¬ë¬¸ ê²€ì¦ (ê°„ë‹¨í•œ validation)
        if self._validate_filter_syntax(expression, format_type):
            return expression, format_type
        else:
            # êµ¬ë¬¸ ì˜¤ë¥˜ ì‹œ ë‹¨ìˆœ í•„í„°ë¡œ í´ë°±
            return self._convert_to_simple_expression(state["resolved_filters"])
    
    # filter_expressionì´ ì—†ìœ¼ë©´ resolved_filtersì—ì„œ ìƒì„±
    return self._convert_to_simple_expression(state["resolved_filters"])


def _convert_to_simple_expression(
    self, 
    resolved_filters: List[ResolvedFilter]
) -> Tuple[str, str]:
    """
    ë‹¨ìˆœ í•„í„° ë¦¬ìŠ¤íŠ¸ë¥¼ SQL WHERE í‘œí˜„ì‹ìœ¼ë¡œ ë³€í™˜
    (AND ì—°ê²°)
    """
    conditions = []
    for f in resolved_filters:
        if f.operator == "LIKE":
            conditions.append(f"{f.column_name} LIKE '{f.value}'")
        elif f.operator == "BETWEEN":
            conditions.append(f"{f.column_name} BETWEEN '{f.value[0]}' AND '{f.value[1]}'")
        else:
            conditions.append(f"{f.column_name} {f.operator} {repr(f.value)}")
    
    expression = " AND ".join(conditions)
    return expression, "sql_where"
```

---

### ğŸ”· [500] PlanBuilderNode

**ì—­í• **: ìµœì¢… Execution Plan JSON ì¡°ë¦½

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 500 |
| **Type** | ğŸ“ Rule-based |
| **Input** | ëª¨ë“  ì´ì „ ë…¸ë“œ ê²°ê³¼ |
| **Output** | `execution_plan` |
| **DB ì ‘ê·¼** | file_catalog (íŒŒì¼ ê²½ë¡œ ì¡°íšŒ) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### Execution Plan ì¡°ë¦½

```python
def execute(self, state: ExtractionAgentState) -> Dict[str, Any]:
    plan = {
        "version": "1.0",
        "generated_at": datetime.now().isoformat(),
        "intent": state["intent"],
        "original_query": state["user_query"],
        
        "entities": self._build_entities_section(state),
        
        "execution_plan": {
            "cohort_source": self._build_cohort_source(
                state["data_topology"],
                state["cohort_definition"]
            ),
            "data_sources": self._build_data_sources(
                state["resolved_parameters"],
                state["data_topology"]
            ),
            "join_specification": self._build_join_spec(
                state["data_topology"]
            )
        },
        
        "resolution_strategy": state["cohort_definition"]["strategy"],
        "delegated_tasks": self._build_delegated_tasks(
            state["cohort_definition"],
            state["resolved_parameters"]
        )
    }
    
    return {
        "plan_builder_result": {"status": "success"},
        "execution_plan": plan
    }
```

#### Reader Type ê²°ì •

```python
READER_MAP = {
    ".csv": "pandas_csv",
    ".parquet": "pandas_parquet",
    ".xlsx": "pandas_excel",
    ".vital": "vitaldb_reader",
    ".edf": "pyedflib_reader",
    ".wfdb": "wfdb_reader",
    ".json": "json_reader",
}

def _get_reader_type(self, file_info: Dict) -> str:
    """íŒŒì¼ í™•ì¥ì ë˜ëŠ” processor_type ê¸°ë°˜ Reader ê²°ì •"""
    # file_catalog.processor_type í™•ì¸
    if file_info.get("processor_type") == "signal":
        ext = file_info.get("file_extension", "").lower()
        return READER_MAP.get(ext, "generic_signal_reader")
    else:
        ext = file_info.get("file_extension", "").lower()
        return READER_MAP.get(ext, "pandas_csv")
```

#### Delegated Tasks ì •ì˜

```python
def _build_delegated_tasks(
    self, 
    cohort_def: CohortDefinition,
    params: List[ResolvedParameter]
) -> List[Dict]:
    """
    Analysis Agentê°€ ìˆ˜í–‰í•  ì‘ì—… ì •ì˜
    """
    tasks = []
    
    # Task 1: Cohort í•„í„°ë§ (ìŠ¤ìº” í•„ìš” ì‹œ)
    if cohort_def.strategy == "scan_required":
        tasks.append({
            "task_id": "filter_cohort",
            "task_type": "file_scan_filter",
            "description": "íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì¡°ê±´ì— ë§ëŠ” ID ì¶”ì¶œ",
            "input": "cohort_source",
            "filter_logic": cohort_def.filter_logic,
            "output": "cohort_ids"
        })
    
    # Task 2: ë°ì´í„° ë¡œë“œ
    tasks.append({
        "task_id": "load_target_data",
        "task_type": "data_load",
        "description": "ì§€ì •ëœ íŒŒë¼ë¯¸í„° ë°ì´í„° ë¡œë“œ",
        "input": "data_sources",
        "parameters": [p.param_keys for p in params],
        "join_with": "cohort_ids",
        "output": "extracted_data"
    })
    
    return tasks
```

---

### ğŸ”· [600] PlanValidatorNode

**ì—­í• **: ìƒì„±ëœ Planì˜ ìœ íš¨ì„± ê²€ì¦ ë° ì‹ ë¢°ë„ í‰ê°€

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Order** | 600 |
| **Type** | ğŸ“ğŸ¤– Hybrid |
| **Input** | `execution_plan` |
| **Output** | `validated_plan`, `validation_warnings`, `overall_confidence` |
| **DB ì ‘ê·¼** | file_catalog (íŒŒì¼ ì¡´ì¬ í™•ì¸) |
| **Neo4j ì ‘ê·¼** | ì—†ìŒ |

#### ê²€ì¦ í•­ëª©

```python
def execute(self, state: ExtractionAgentState) -> Dict[str, Any]:
    plan = state["execution_plan"]
    warnings = []
    
    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìƒ˜í”Œë§ ë°©ì‹ìœ¼ë¡œ ìµœì í™”)
    file_warnings = self._validate_file_existence(plan)
    warnings.extend(file_warnings)
    
    # 2. Join Path ìœ íš¨ì„± í™•ì¸
    join_warnings = self._validate_join_paths(plan)
    warnings.extend(join_warnings)
    
    # 3. Parameter ì»¤ë²„ë¦¬ì§€ í™•ì¸
    param_warnings = self._validate_parameter_coverage(plan, state)
    warnings.extend(param_warnings)
    
    # 4. ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± í™•ì¸
    type_warnings = self._validate_data_types(plan)
    warnings.extend(type_warnings)
    
    # 5. Filter Expression êµ¬ë¬¸ ê²€ì¦ (LLM ìƒì„± í‘œí˜„ì‹ ê²€ì¦)
    filter_warnings = self._validate_filter_expression(plan)
    warnings.extend(filter_warnings)
    
    # 6. Confidence ê³„ì‚°
    confidence = self._calculate_confidence(plan, warnings, state)
    
    # 7. Human Review í•„ìš” ì—¬ë¶€ íŒë‹¨
    needs_review = (
        confidence < 0.7 or 
        any(w["severity"] == "high" for w in warnings) or
        len(state.get("ambiguities", [])) > 0
    )
    
    return {
        "plan_validator_result": {"status": "success"},
        "validated_plan": {**plan, "validation": {...}},
        "validation_warnings": warnings,
        "overall_confidence": confidence,
        "needs_human_review": needs_review,
        "human_review_type": self._get_review_type(warnings, state) if needs_review else None,
        "human_question": self._generate_review_question(warnings, state) if needs_review else None
    }
```

#### íŒŒì¼ ì¡´ì¬ í™•ì¸ ìµœì í™” (Sampling Validation)

```python
def _validate_file_existence(self, plan: Dict) -> List[Dict]:
    """
    íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ - ìƒ˜í”Œë§ ë°©ì‹ìœ¼ë¡œ ìµœì í™”
    
    ëŒ€ê·œëª¨ ì½”í˜¸íŠ¸(10,000+)ì˜ ê²½ìš° ì „ìˆ˜ ì¡°ì‚¬ ëŒ€ì‹  ìƒ˜í”Œë§í•˜ì—¬
    ëŒ€í‘œì„± ìˆëŠ” ê²€ì¦ë§Œ ìˆ˜í–‰. ë‚˜ë¨¸ì§€ëŠ” Analysis Agentì—ì„œ ì—ëŸ¬ í•¸ë“¤ë§.
    """
    warnings = []
    config = PlanValidatorConfig()
    
    if not config.VERIFY_FILE_EXISTENCE:
        return warnings
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    file_paths = self._collect_file_paths(plan)
    total_files = len(file_paths)
    
    if total_files == 0:
        return warnings
    
    # ìƒ˜í”Œë§ ê²°ì •
    if config.FILE_VALIDATION_MODE == "all" or total_files <= config.FILE_VALIDATION_SAMPLE_SIZE:
        # ì „ìˆ˜ ì¡°ì‚¬ (ì†Œê·œëª¨)
        paths_to_check = file_paths
    elif config.FILE_VALIDATION_MODE == "sample":
        # ëœë¤ ìƒ˜í”Œë§ (ëŒ€ê·œëª¨)
        import random
        paths_to_check = random.sample(file_paths, config.FILE_VALIDATION_SAMPLE_SIZE)
    else:  # "skip"
        return warnings
    
    # ìƒ˜í”Œ ê²€ì¦
    missing_count = 0
    for path in paths_to_check:
        if not os.path.exists(path):
            missing_count += 1
    
    if missing_count > 0:
        missing_ratio = missing_count / len(paths_to_check)
        severity = "high" if missing_ratio > 0.5 else "medium" if missing_ratio > 0.1 else "low"
        
        warnings.append({
            "type": "file_existence",
            "severity": severity,
            "message": f"Sample validation: {missing_count}/{len(paths_to_check)} files not found",
            "details": {
                "sample_size": len(paths_to_check),
                "total_files": total_files,
                "missing_in_sample": missing_count,
                "estimated_missing_ratio": missing_ratio
            },
            "recommendation": "Analysis Agent will handle missing files gracefully with try-catch"
        })
    
    return warnings


def _validate_filter_expression(self, plan: Dict) -> List[Dict]:
    """
    LLMì´ ìƒì„±í•œ filter_expression êµ¬ë¬¸ ê²€ì¦
    """
    warnings = []
    
    cohort_source = plan.get("execution_plan", {}).get("cohort_source", {})
    filter_expr = cohort_source.get("filter_expression")
    filter_format = cohort_source.get("filter_format", "sql_where")
    
    if not filter_expr:
        return warnings
    
    try:
        if filter_format == "sql_where":
            # SQL WHERE êµ¬ë¬¸ ê¸°ë³¸ ê²€ì¦
            self._validate_sql_syntax(filter_expr)
        elif filter_format == "pandas_query":
            # pandas query êµ¬ë¬¸ ê²€ì¦
            self._validate_pandas_query_syntax(filter_expr)
    except SyntaxError as e:
        warnings.append({
            "type": "filter_expression_syntax",
            "severity": "high",
            "message": f"Filter expression syntax error: {str(e)}",
            "details": {
                "expression": filter_expr,
                "format": filter_format
            },
            "recommendation": "Will fallback to simple filter_logic if available"
        })
    
    return warnings
```

#### ì‹ ë¢°ë„ ê³„ì‚°

```python
def _calculate_confidence(
    self, 
    plan: Dict, 
    warnings: List[Dict], 
    state: Dict
) -> float:
    """
    ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚° (0.0 ~ 1.0)
    
    ê°ì  ìš”ì†Œ:
    - High severity warning: -0.3
    - Medium severity warning: -0.1
    - Low severity warning: -0.05
    - ëª¨í˜¸í•œ ë§¤í•‘: -0.1 per ambiguity
    - Join ê²½ë¡œ ì—†ìŒ: -0.2
    """
    confidence = 1.0
    
    # ê²½ê³ ì— ë”°ë¥¸ ê°ì 
    severity_penalty = {
        "high": 0.3,
        "medium": 0.1,
        "low": 0.05
    }
    for w in warnings:
        confidence -= severity_penalty.get(w["severity"], 0.05)
    
    # ëª¨í˜¸ì„±ì— ë”°ë¥¸ ê°ì 
    ambiguities = state.get("ambiguities", [])
    confidence -= len(ambiguities) * 0.1
    
    # Join ê²½ë¡œ ì—†ìœ¼ë©´ ê°ì 
    if not plan.get("execution_plan", {}).get("join_specification", {}).get("paths"):
        confidence -= 0.2
    
    return max(0.0, min(1.0, confidence))
```

---

## ğŸ“Š ExtractionAgentState ì „ì²´ ìŠ¤í‚¤ë§ˆ

```python
from typing import TypedDict, List, Dict, Any, Optional, Literal, Annotated
import operator


class ExtractionAgentState(TypedDict):
    """
    ExtractionAgent ì›Œí¬í”Œë¡œìš° ì „ì²´ ìƒíƒœ
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Input
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    user_query: str                           # ì›ë³¸ ìì—°ì–´ ì¿¼ë¦¬
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [100] query_understanding
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    query_understanding_result: Optional[Dict[str, Any]]  # ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    
    intent: Optional[Literal[
        "data_retrieval", "aggregation", "exploration", 
        "relationship", "metadata_lookup"
    ]]
    
    extracted_entities: List[Dict[str, Any]]
    # [{
    #   "type": "parameter",
    #   "value": "ì‹¬ë°•ìˆ˜",
    #   "normalized": "Heart Rate",
    #   "candidates": ["HR", "Heart Rate"],
    #   "condition_type": null,
    #   "confidence": 0.95
    # }]
    # 
    # Temporal Entity ì˜ˆì‹œ (í™•ì¥):
    # {
    #   "type": "temporal",
    #   "value": "ìˆ˜ìˆ  ì¤‘",
    #   "temporal_type": "relative_window",
    #   "reference_event": "surgery",
    #   "window_start": "op_start",
    #   "window_end": "op_end",
    #   "margin_seconds": 300,
    #   "confidence": 0.95
    # }
    
    # LLMì´ ìƒì„±í•œ ë³µí•© í•„í„° í‘œí˜„ì‹ (AND/OR ì§€ì›)
    filter_expression: Optional[str]
    # "(diagnosis LIKE '%Stomach Cancer%' OR diagnosis LIKE '%Colon Cancer%') AND age >= 60"
    
    filter_format: Optional[Literal["sql_where", "pandas_query"]]
    
    resolution_strategy: Optional[Literal[
        "metadata_only", "partial_metadata", "scan_required"
    ]]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [200] semantic_resolver
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    semantic_resolver_result: Optional[Dict[str, Any]]
    
    resolved_parameters: List[Dict[str, Any]]
    # [{
    #   "semantic_term": "ì‹¬ë°•ìˆ˜",
    #   "param_keys": ["Solar8000/HR", "BIS/HR"],
    #   "concept_category": "Vital Signs",
    #   "source_type": "group_common",
    #   "group_id": "uuid-...",
    #   "unit": "bpm",
    #   "confidence": 0.9,
    #   "resolution_mode": "all_sources",        # v2.1: LLM ê²°ì •
    #   "resolution_reason": "Same concept from multiple monitoring devices"
    # }]
    
    resolved_filters: List[Dict[str, Any]]
    # [{
    #   "entity_type": "diagnosis",
    #   "semantic_term": "ìœ„ì•”",
    #   "column_name": "diagnosis",
    #   "file_id": "uuid-...",
    #   "operator": "LIKE",
    #   "value": "%Stomach Cancer%",
    #   "confidence": 0.85
    # }]
    #
    # Temporal Filter ì˜ˆì‹œ (í™•ì¥):
    # {
    #   "entity_type": "temporal",
    #   "temporal_type": "relative_window",
    #   "reference_event": "surgery",
    #   "window_start": "op_start",
    #   "window_end": "op_end",
    #   "margin_seconds": 300
    # }
    
    ambiguities: List[Dict[str, Any]]
    # [{
    #   "type": "parameter",
    #   "term": "BP",
    #   "candidates": [
    #     {"key": "NIBP", "name": "Non-Invasive BP"},
    #     {"key": "ABP", "name": "Arterial BP"}
    #   ],
    #   "reason": "Multiple BP types available",
    #   "clarification_question": "Which blood pressure do you need: non-invasive (cuff) or invasive (arterial line)?"
    # }]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [300] topology_navigator
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    topology_navigator_result: Optional[Dict[str, Any]]
    
    data_topology: Optional[Dict[str, Any]]
    # {
    #   "cohort_source": {
    #     "file_id": "uuid",
    #     "file_path": "/data/clinical_data.csv",
    #     "entity_type": "surgery",
    #     "identifier_column": "caseid",
    #     "temporal_columns": ["op_start", "op_end", "ane_start", "ane_end"]  # v2.1
    #   },
    #   "target_sources": [{
    #     "type": "file_group",
    #     "group_id": "uuid",
    #     "group_name": "vital_case_records",
    #     "time_column": "Time"  # v2.1: Signal íŒŒì¼ì˜ ì‹œê°„ ì¶•
    #   }],
    #   "join_paths": [{
    #     "from_file": "clinical_data.csv",
    #     "to_target": "vital_case_records",
    #     "source_column": "caseid",
    #     "target_column": "filename_values.caseid",
    #     "cardinality": "1:N"
    #   }]
    # }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [400] cohort_analyzer
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    cohort_analyzer_result: Optional[Dict[str, Any]]
    
    cohort_definition: Optional[Dict[str, Any]]
    # {
    #   "strategy": "partial_metadata",
    #   "filter_logic": [
    #     {"column": "diagnosis", "operator": "LIKE", "value": "%Stomach Cancer%"}
    #   ],
    #   "filter_expression": "(diagnosis LIKE '%Stomach Cancer%') AND age >= 60",  # v2.1
    #   "filter_format": "sql_where",  # v2.1
    #   "estimated_cohort_size": 150,
    #   "scan_reason": null
    # }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [500] plan_builder
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    plan_builder_result: Optional[Dict[str, Any]]
    
    execution_plan: Optional[Dict[str, Any]]   # ìµœì¢… Execution Plan JSON
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [600] plan_validator
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    plan_validator_result: Optional[Dict[str, Any]]
    
    validated_plan: Optional[Dict[str, Any]]   # ê²€ì¦ ì™„ë£Œëœ Plan
    validation_warnings: List[Dict[str, Any]]  # ê²½ê³  ëª©ë¡
    overall_confidence: float                   # ì „ì²´ ì‹ ë¢°ë„ (0.0 ~ 1.0)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Human-in-the-Loop
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    needs_human_review: bool
    human_review_type: Optional[Literal[
        "ambiguous_parameter",
        "ambiguous_join_path",
        "low_confidence",
        "missing_data_source"
    ]]
    human_question: Optional[str]
    human_feedback: Optional[str]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # System
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    logs: Annotated[List[str], operator.add]
    error_message: Optional[str]
    retry_count: int
```

---

## ğŸ“„ ìµœì¢… Output: Execution Plan JSON Schema

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ExtractionAgent Execution Plan",
  "type": "object",
  "required": ["version", "intent", "execution_plan"],
  "properties": {
    "version": {
      "type": "string",
      "const": "1.0"
    },
    "generated_at": {
      "type": "string",
      "format": "date-time"
    },
    "intent": {
      "type": "string",
      "enum": ["data_retrieval", "aggregation", "exploration", "relationship", "metadata_lookup"]
    },
    "original_query": {
      "type": "string"
    },
    
    "entities": {
      "type": "object",
      "description": "ì¶”ì¶œëœ Entityë“¤ì˜ ì •ë¦¬ëœ í˜•íƒœ",
      "additionalProperties": true
    },
    
    "execution_plan": {
      "type": "object",
      "required": ["cohort_source", "data_sources"],
      "properties": {
        "cohort_source": {
          "type": "object",
          "required": ["type", "file_id", "file_path"],
          "properties": {
            "type": {"type": "string", "enum": ["tabular_file", "file_group"]},
            "file_id": {"type": "string", "format": "uuid"},
            "file_path": {"type": "string"},
            "file_name": {"type": "string"},
            "reader": {"type": "string"},
            "filter_logic": {
              "type": "array",
              "description": "ë‹¨ìˆœ í•„í„° ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜)",
              "items": {
                "type": "object",
                "properties": {
                  "column": {"type": "string"},
                  "operator": {"type": "string"},
                  "value": {},
                  "values": {"type": "array"}
                }
              }
            },
            "filter_expression": {
              "type": "string",
              "description": "LLMì´ ìƒì„±í•œ ë³µí•© ë…¼ë¦¬ í‘œí˜„ì‹ (AND/OR ì§€ì›)"
            },
            "filter_format": {
              "type": "string",
              "enum": ["sql_where", "pandas_query"],
              "description": "í‘œí˜„ì‹ í•´ì„ ë°©ì‹"
            },
            "result_identifier": {"type": "string"},
            "estimated_rows": {"type": "integer"}
          }
        },
        
        "data_sources": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "type": {"type": "string", "enum": ["file_group", "single_file"]},
              "group_id": {"type": "string", "format": "uuid"},
              "group_name": {"type": "string"},
              "file_id": {"type": "string", "format": "uuid"},
              "file_path": {"type": "string"},
              "reader": {"type": "string"},
              "file_pattern": {"type": "string"},
              "file_count": {"type": "integer"},
              "sample_paths": {"type": "array", "items": {"type": "string"}},
              "target_parameters": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "param_key": {"type": "string"},
                    "semantic_name": {"type": "string"},
                    "unit": {"type": "string"},
                    "resolution_mode": {
                      "type": "string",
                      "enum": ["all_sources", "specific", "best_match"],
                      "description": "LLMì´ ê²°ì •í•œ íŒŒë¼ë¯¸í„° í•´ì„ ëª¨ë“œ"
                    }
                  }
                }
              },
              "join_key": {
                "type": "object",
                "properties": {
                  "source": {"type": "string"},
                  "target": {"type": "string"}
                }
              },
              "temporal_alignment": {
                "type": "object",
                "description": "Signal ë°ì´í„° ì‹œê°„ ë™ê¸°í™” ì •ë³´ (LLMì´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ)",
                "properties": {
                  "type": {
                    "type": "string",
                    "enum": ["absolute_range", "relative_window", "event_based", "duration"],
                    "description": "ì‹œê°„ ì¡°ê±´ ìœ í˜•"
                  },
                  "reference_event": {
                    "type": "string",
                    "description": "ê¸°ì¤€ ì´ë²¤íŠ¸ (surgery, anesthesia, admission ë“±)"
                  },
                  "window_start": {
                    "oneOf": [{"type": "string"}, {"type": "integer"}],
                    "description": "ì‹œì‘ì  (ì»¬ëŸ¼ëª… ë˜ëŠ” ì´ˆ ë‹¨ìœ„ ì˜¤í”„ì…‹)"
                  },
                  "window_end": {
                    "oneOf": [{"type": "string"}, {"type": "integer"}],
                    "description": "ì¢…ë£Œì  (ì»¬ëŸ¼ëª… ë˜ëŠ” ì´ˆ ë‹¨ìœ„ ì˜¤í”„ì…‹)"
                  },
                  "margin_seconds": {
                    "type": "integer",
                    "description": "ì•ë’¤ ì—¬ìœ  ì‹œê°„ (Buffer)"
                  },
                  "absolute_start": {
                    "type": "string",
                    "format": "date-time",
                    "description": "ì ˆëŒ€ ì‹œì‘ ì‹œê°„ (absolute_rangeì¸ ê²½ìš°)"
                  },
                  "absolute_end": {
                    "type": "string",
                    "format": "date-time",
                    "description": "ì ˆëŒ€ ì¢…ë£Œ ì‹œê°„ (absolute_rangeì¸ ê²½ìš°)"
                  }
                }
              }
            }
          }
        },
        
        "join_specification": {
          "type": "object",
          "properties": {
            "paths": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "from": {"type": "string"},
                  "to": {"type": "string"},
                  "via": {"type": "string"},
                  "cardinality": {"type": "string"}
                }
              }
            }
          }
        }
      }
    },
    
    "resolution_strategy": {
      "type": "string",
      "enum": ["metadata_only", "partial_metadata", "scan_required"]
    },
    
    "delegated_tasks": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "task_id": {"type": "string"},
          "task_type": {"type": "string"},
          "description": {"type": "string"},
          "input": {"type": "string"},
          "output": {"type": "string"}
        }
      }
    },
    
    "validation": {
      "type": "object",
      "properties": {
        "validated_at": {"type": "string", "format": "date-time"},
        "warnings": {"type": "array"},
        "confidence": {"type": "number", "minimum": 0, "maximum": 1}
      }
    }
  }
}
```

---

## ğŸ¯ ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ

**ì¿¼ë¦¬**: "2023ë…„ ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜(HR) ë°ì´í„°ë¥¼ ì¤˜"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â”œâ”€ diagnosis: "ìœ„ì•”" â†’ "Stomach Cancer"
â”‚   â”œâ”€ temporal: "2023ë…„" â†’ {type: absolute_range, 2023-01-01 ~ 2023-12-31}
â”‚   â””â”€ parameter: "ì‹¬ë°•ìˆ˜" â†’ "Heart Rate"
â”œâ”€ filter_expression: "diagnosis LIKE '%Stomach Cancer%' AND op_date BETWEEN '2023-01-01' AND '2023-12-31'"
â””â”€ Strategy: partial_metadata

[200] semantic_resolver
â”œâ”€ Parameters:
â”‚   â””â”€ "ì‹¬ë°•ìˆ˜" â†’ [Solar8000/HR, BIS/HR] (resolution_mode: ALL_SOURCES)
â”œâ”€ Filters:
â”‚   â”œâ”€ diagnosis â†’ column "diagnosis" in clinical_data.csv
â”‚   â””â”€ op_date â†’ column "op_date" in clinical_data.csv
â””â”€ Ambiguities: []

[300] topology_navigator
â”œâ”€ Cohort Source: clinical_data.csv (identifier: caseid)
â”œâ”€ Target Sources: vital_case_records (FileGroup)
â””â”€ Join Path: clinical_data.caseid â†’ vital.filename_values.caseid

[400] cohort_analyzer
â”œâ”€ Strategy: partial_metadata
â”œâ”€ filter_expression: "diagnosis LIKE '%Stomach Cancer%' AND op_date BETWEEN '2023-01-01' AND '2023-12-31'"
â””â”€ Estimated Size: ~150 cases

[500] plan_builder
â””â”€ Execution Plan JSON ìƒì„±

[600] plan_validator
â”œâ”€ File Validation: sample (10/150), all found
â”œâ”€ Warnings: []
â””â”€ Confidence: 0.92
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ìˆ˜ìˆ  ì¤‘ ì‹œê°„ ë™ê¸°í™” (Temporal Alignment)

**ì¿¼ë¦¬**: "ìˆ˜ìˆ  ì¤‘ ì €í˜ˆì••(SBP < 90)ì´ ë°œìƒí•œ í™˜ìì˜ ì‹¬ë°•ìˆ˜"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â”œâ”€ temporal: "ìˆ˜ìˆ  ì¤‘"
â”‚   â”‚   â””â”€ temporal_type: relative_window
â”‚   â”‚   â””â”€ reference_event: "surgery"
â”‚   â”‚   â””â”€ window_start: "op_start"
â”‚   â”‚   â””â”€ window_end: "op_end"
â”‚   â”‚   â””â”€ margin_seconds: 300
â”‚   â”œâ”€ condition: "SBP < 90" â†’ {column: SBP, operator: <, value: 90}
â”‚   â””â”€ parameter: "ì‹¬ë°•ìˆ˜" â†’ "Heart Rate"
â””â”€ Strategy: scan_required  â† ê°’ ì¡°ê±´ì´ë¯€ë¡œ ìŠ¤ìº” í•„ìš”

[200] semantic_resolver
â”œâ”€ Parameters:
â”‚   â”œâ”€ "ì‹¬ë°•ìˆ˜" â†’ [Solar8000/HR, BIS/HR] (resolution_mode: ALL_SOURCES)
â”‚   â””â”€ "SBP" â†’ [Solar8000/NIBP_SBP, ART/SBP] (resolution_mode: ALL_SOURCES)
â””â”€ LLM Decision: "Include all BP sources for comprehensive hypotension detection"

[500] plan_builder
â””â”€ Execution Plan:
    {
      "data_sources": [{
        "group_name": "vital_case_records",
        "target_parameters": ["Solar8000/HR", "BIS/HR"],
        "temporal_alignment": {
          "type": "relative_window",
          "reference_event": "surgery",
          "window_start": "op_start",
          "window_end": "op_end",
          "margin_seconds": 300
        }
      }]
    }

[600] plan_validator
â”œâ”€ Temporal Alignment: validated (op_start, op_end columns exist in cohort)
â””â”€ Confidence: 0.88
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë³µí•© ë…¼ë¦¬ ì¡°ê±´ (OR/AND)

**ì¿¼ë¦¬**: "(ìœ„ì•” OR ëŒ€ì¥ì•”) í™˜ì ì¤‘ 60ì„¸ ì´ìƒì´ê³  ìˆ˜ìˆ ì‹œê°„ì´ 3ì‹œê°„ ì´ìƒì¸ í™˜ì"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â”œâ”€ diagnosis: ["ìœ„ì•”", "ëŒ€ì¥ì•”"]
â”‚   â”œâ”€ demographic: "60ì„¸ ì´ìƒ"
â”‚   â””â”€ condition: "ìˆ˜ìˆ ì‹œê°„ >= 3ì‹œê°„"
â”œâ”€ filter_expression: "(diagnosis LIKE '%Stomach Cancer%' OR diagnosis LIKE '%Colon Cancer%') AND age >= 60 AND op_duration >= 180"
â”œâ”€ filter_format: "sql_where"
â””â”€ Strategy: scan_required

[400] cohort_analyzer
â”œâ”€ Strategy: partial_metadata (age, diagnosis) + scan (op_duration ê³„ì‚° í•„ìš”)
â”œâ”€ filter_expression: "(diagnosis LIKE '%Stomach Cancer%' OR diagnosis LIKE '%Colon Cancer%') AND age >= 60 AND op_duration >= 180"
â””â”€ Note: op_duration may need to be calculated from op_start/op_end

[600] plan_validator
â”œâ”€ Filter Expression Syntax: âœ“ valid SQL WHERE
â”œâ”€ Column Validation: diagnosis âœ“, age âœ“, op_duration âš ï¸ (may need derivation)
â””â”€ Confidence: 0.85
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° ì†ŒìŠ¤ (Resolution Mode)

**ì¿¼ë¦¬**: "ì‹¬ë°•ìˆ˜ ë°ì´í„°"

```
[100] query_understanding
â”œâ”€ Intent: data_retrieval
â”œâ”€ Entities:
â”‚   â””â”€ parameter: "ì‹¬ë°•ìˆ˜" â†’ "Heart Rate"
â””â”€ Strategy: metadata_only

[200] semantic_resolver
â”œâ”€ Found Candidates:
â”‚   â”œâ”€ Solar8000/HR (Heart Rate from Solar8000 monitor)
â”‚   â”œâ”€ BIS/HR (Heart Rate from BIS monitor)
â”‚   â””â”€ Philips/HR (Heart Rate from Philips monitor)
â”‚
â”œâ”€ LLM Resolution Decision:
â”‚   â””â”€ Mode: "ALL" - "All candidates measure the same physiological parameter (heart rate) 
â”‚                     from different monitoring devices. User likely wants comprehensive data."
â”‚
â””â”€ Result:
    param_keys: ["Solar8000/HR", "BIS/HR", "Philips/HR"]
    resolution_mode: ALL_SOURCES
    resolution_reason: "Same concept from multiple sources"
```

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
ExtractionAgent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph.py                        # LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë”
â”‚   â”‚   â”œâ”€â”€ state.py                        # ExtractionAgentState
â”‚   â”‚   â”œâ”€â”€ registry.py                     # NodeRegistry (sharedì—ì„œ import ê°€ëŠ¥)
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ node.py                     # BaseNode
â”‚   â”‚   â”‚   â””â”€â”€ mixins.py                   # LLMMixin, DatabaseMixin
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enums.py                    # Intent, EntityType, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_responses.py            # LLM ì‘ë‹µ Pydantic ëª¨ë¸
â”‚   â”‚   â”‚   â””â”€â”€ execution_plan.py           # ExecutionPlan ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â””â”€â”€ nodes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ query_understanding/        # [100]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ semantic_resolver/          # [200]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ topology_navigator/         # [300]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ node.py
â”‚   â”‚       â”œâ”€â”€ cohort_analyzer/            # [400]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ node.py
â”‚   â”‚       â”‚   â””â”€â”€ prompts.py
â”‚   â”‚       â”œâ”€â”€ plan_builder/               # [500]
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ node.py
â”‚   â”‚       â””â”€â”€ plan_validator/             # [600]
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â”œâ”€â”€ node.py
â”‚   â”‚           â””â”€â”€ prompts.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_query_understanding.py
â”‚   â”œâ”€â”€ test_semantic_resolver.py
â”‚   â”œâ”€â”€ test_topology_navigator.py
â”‚   â”œâ”€â”€ test_full_pipeline.py
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_queries.json
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_extraction.py
â”‚   â”œâ”€â”€ complex_query.py
â”‚   â””â”€â”€ example_outputs/
â”‚
â”œâ”€â”€ ARCHITECTURE_V2.md                      # ì´ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ ì„¤ì •

### Nodeë³„ ì„¤ì •

```python
# src/config.py

class QueryUnderstandingConfig:
    """[100] QueryUnderstanding ë…¸ë“œ ì„¤ì •"""
    MAX_ENTITIES = 10                    # ìµœëŒ€ ì¶”ì¶œ Entity ìˆ˜
    CONFIDENCE_THRESHOLD = 0.7           # ìµœì†Œ ì‹ ë¢°ë„


class SemanticResolverConfig:
    """[200] SemanticResolver ë…¸ë“œ ì„¤ì •"""
    MAX_CANDIDATES = 10                  # ìµœëŒ€ í›„ë³´ ìˆ˜
    AMBIGUITY_THRESHOLD = 0.6            # ëª¨í˜¸ì„± íŒë‹¨ ê¸°ì¤€
    CACHE_ENABLED = True                 # ìºì‹± í™œì„±í™”
    CACHE_TTL = 3600                     # ìºì‹œ ìœ íš¨ ì‹œê°„ (ì´ˆ)


class TopologyNavigatorConfig:
    """[300] TopologyNavigator ë…¸ë“œ ì„¤ì •"""
    MAX_JOIN_HOPS = 3                    # ìµœëŒ€ Join í™‰ ìˆ˜


class CohortAnalyzerConfig:
    """[400] CohortAnalyzer ë…¸ë“œ ì„¤ì •"""
    VALUE_DISTRIBUTION_SAMPLE_THRESHOLD = 100  # ì´ ì´ìƒì´ë©´ ìƒ˜í”Œë§ë¨


class PlanBuilderConfig:
    """[500] PlanBuilder ë…¸ë“œ ì„¤ì •"""
    MAX_SAMPLE_PATHS = 5                 # ìƒ˜í”Œ íŒŒì¼ ê²½ë¡œ ìˆ˜


class PlanValidatorConfig:
    """[600] PlanValidator ë…¸ë“œ ì„¤ì •"""
    CONFIDENCE_THRESHOLD_FOR_REVIEW = 0.7  # Human Review ê¸°ì¤€
    VERIFY_FILE_EXISTENCE = True           # íŒŒì¼ ì¡´ì¬ í™•ì¸ ì—¬ë¶€
    FILE_VALIDATION_MODE = "sample"        # "all" | "sample" | "skip"
    FILE_VALIDATION_SAMPLE_SIZE = 10       # ìƒ˜í”Œ ê²€ì¦ ì‹œ í™•ì¸í•  íŒŒì¼ ìˆ˜
```

---

## ğŸ”— shared íŒ¨í‚¤ì§€ ì˜ì¡´ì„±

ExtractionAgentëŠ” ë‹¤ìŒ shared ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# Database (Read Only)
from shared.database import (
    get_db_manager,
    ParameterReader,          # Read-only, cached
    TopologyReader,           # Read-only
    FileRepository,           # Read-only ë©”ì„œë“œë§Œ ì‚¬ìš©
)

# Neo4j
from shared.neo4j import (
    get_neo4j_connection,
    ParameterQueryBuilder,
    TopologyQueryBuilder,
)

# Models
from shared.models import (
    ConceptCategory,
    SourceType,
    ColumnRole,
)

# LLM
from shared.llm import (
    get_llm_client,
)

# Config
from shared.config import (
    DatabaseConfig,
    Neo4jConfig,
    LLMConfig,
)
```

---

## ğŸ“ ë³€ê²½ ì´ë ¥

### v2.1 (Current) - LLM ì¤‘ì‹¬ ì„¤ê³„ ê°•í™”

#### ğŸ†• Temporal Alignment (ì‹œê°„ ë™ê¸°í™”)
- `TemporalType` enum ì¶”ê°€: `absolute_range`, `relative_window`, `event_based`, `duration`
- `TemporalEntity` í™•ì¥ ìŠ¤í‚¤ë§ˆ: `reference_event`, `window_start`, `window_end`, `margin_seconds`
- LLMì´ ì¿¼ë¦¬ ì´í•´ ë‹¨ê³„ì—ì„œ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ í’ë¶€í•˜ê²Œ ì¶”ì¶œ
- Execution Planì˜ `data_sources`ì— `temporal_alignment` í•„ë“œ ì¶”ê°€

#### ğŸ†• ë³µí•© ë…¼ë¦¬ ì¡°ê±´ ì§€ì› (Filter Expression)
- `filter_expression`: LLMì´ ìƒì„±í•œ SQL-like WHERE í‘œí˜„ì‹ (AND/OR ì§€ì›)
- `filter_format`: `sql_where` | `pandas_query`
- ë‹¨ìˆœ `filter_logic` ë¦¬ìŠ¤íŠ¸ì™€ í•˜ìœ„ í˜¸í™˜ ìœ ì§€
- Plan Validatorì—ì„œ êµ¬ë¬¸ ê²€ì¦

#### ğŸ†• Resolution Mode (íŒŒë¼ë¯¸í„° í•´ì„ ëª¨ë“œ)
- `ResolutionMode` enum: `ALL_SOURCES`, `SPECIFIC`, `BEST_MATCH`, `NEEDS_CLARIFICATION`
- ë™ì¼ ê°œë…ì˜ ì—¬ëŸ¬ ì†ŒìŠ¤ â†’ LLMì´ "ALL" ê²°ì • (ê¸°ë³¸ ë™ì‘)
- `_ask_llm_for_resolution()`: LLMì—ê²Œ Resolution Mode ê²°ì • ìœ„ì„

#### ğŸ†• Plan Validator ìµœì í™”
- `FILE_VALIDATION_MODE`: `all` | `sample` | `skip`
- ëŒ€ê·œëª¨ ì½”í˜¸íŠ¸ì—ì„œ ìƒ˜í”Œë§ ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì¡´ì¬ í™•ì¸
- Filter Expression êµ¬ë¬¸ ê²€ì¦ ì¶”ê°€

#### ì„¤ê³„ ì² í•™
- **"LLMì´ ì•Œì•„ì„œ ê²°ì •"**: ê·œì¹™ ê¸°ë°˜ ë¡œì§ ì¶”ê°€ë³´ë‹¤ LLM í”„ë¡¬í”„íŠ¸ ê°•í™”
- ë³µì¡í•œ Tree êµ¬ì¡° ëŒ€ì‹  LLMì´ í‘œí˜„ì‹ ì§ì ‘ ìƒì„±
- ëª¨í˜¸ì„± ì²˜ë¦¬ë¥¼ LLMì—ê²Œ ìœ„ì„í•˜ì—¬ Human Review ìµœì†Œí™”

### v2.0
- ì™„ì „íˆ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¡œ ì¬ì„¤ê³„
- Text-to-SQL ë°©ì‹ì—ì„œ **Execution Plan ìƒì„±** ë°©ì‹ìœ¼ë¡œ ë³€ê²½
- 6ê°œ ë…¸ë“œ íŒŒì´í”„ë¼ì¸ (query_understanding â†’ plan_validator)
- shared íŒ¨í‚¤ì§€ ì‚¬ìš©ìœ¼ë¡œ IndexingAgentì™€ ì¸í”„ë¼ ê³µìœ 
- Read-Only Repository íŒ¨í„´ ë„ì…

### v1.0 (Legacy)
- Text-to-SQL ì—ì´ì „íŠ¸
- ì§ì ‘ ë°ì´í„° ë°˜í™˜ ë°©ì‹
- ë³„ë„ì˜ database/ ëª¨ë“ˆ ë³´ìœ  (ì¤‘ë³µ)

---

## ğŸ¯ ì„¤ê³„ ì›ì¹™: LLM ì¤‘ì‹¬ ì˜ì‚¬ê²°ì •

ë³¸ ì•„í‚¤í…ì²˜ëŠ” **"LLMì´ ì•Œì•„ì„œ ê²°ì •"**í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

### ê·œì¹™ ê¸°ë°˜ vs LLM ê¸°ë°˜ ë¹„êµ

| ê¸°ëŠ¥ | ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ | LLM ì¤‘ì‹¬ ì ‘ê·¼ (ì±„íƒ) |
|------|---------------|---------------------|
| Temporal Alignment | ë³„ë„ `TemporalSlicer` ë¡œì§ + Neo4j í™•ì¥ | LLMì´ ì¿¼ë¦¬ ì´í•´ ì‹œ `temporal_type`, `reference_event` ì¶”ì¶œ |
| Complex Filter | Filter Tree êµ¬ì¡° íŒŒì‹± | LLMì´ SQL/pandas í‘œí˜„ì‹ ì§ì ‘ ìƒì„± |
| ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° ì†ŒìŠ¤ | ê·œì¹™ ê¸°ë°˜ `_are_semantically_similar()` | LLMì´ ì»¨í…ìŠ¤íŠ¸ ë³´ê³  ALL/PICK/CLARIFY ê²°ì • |
| ëª¨í˜¸ì„± ì²˜ë¦¬ | Enum ê¸°ë°˜ ë¶„ë¥˜ | LLMì´ ìì—°ì–´ë¡œ clarification question ìƒì„± |

### LLM ì¤‘ì‹¬ ì„¤ê³„ì˜ ì¥ì 

1. **ìœ ì—°ì„±**: ìƒˆë¡œìš´ ì¿¼ë¦¬ íŒ¨í„´ì— ê·œì¹™ ì¶”ê°€ ì—†ì´ ëŒ€ì‘
2. **ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ë¡ **: ë³µì¡í•œ ì˜ë£Œ ìš©ì–´/ë§¥ë½ì„ LLMì´ ì´í•´
3. **ìœ ì§€ë³´ìˆ˜ ìš©ì´**: í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ë§Œìœ¼ë¡œ ë™ì‘ ë³€ê²½ ê°€ëŠ¥
4. **Human Review ìµœì†Œí™”**: LLMì´ í•©ë¦¬ì  ê¸°ë³¸ê°’ ì„ íƒ

### ì£¼ì˜ì‚¬í•­

1. **ê²€ì¦ í•„ìˆ˜**: LLM ìƒì„± í‘œí˜„ì‹ì€ Plan Validatorì—ì„œ êµ¬ë¬¸ ê²€ì¦
2. **í´ë°± ì „ëµ**: êµ¬ë¬¸ ì˜¤ë¥˜ ì‹œ ë‹¨ìˆœ `filter_logic`ìœ¼ë¡œ í´ë°±
3. **Confidence ë°˜ì˜**: LLM ê²°ì •ì˜ ë¶ˆí™•ì‹¤ì„±ì„ `overall_confidence`ì— ë°˜ì˜

