#!/usr/bin/env python
"""
End-to-End Test: Full Pipeline HR Analysis
===========================================

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
  ExtractionAgent â†’ DataContext â†’ AnalysisAgent

ì‚¬ìš©ë²•:
    python test_e2e_hr_mean.py
    python test_e2e_hr_mean.py --verbose
    python test_e2e_hr_mean.py --query "ì‹¬ë°•ìˆ˜ í‰ê· ì„ êµ¬í•´ì¤˜"
"""

import sys
import os
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable
import time
import numpy as np
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%H:%M:%S"
    )
    
    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì ˆ
    for logger_name in ["httpx", "httpcore", "openai", "urllib3"]:
        logging.getLogger(logger_name).setLevel(logging.WARNING)


def run_full_pipeline_test(
    queries: List[tuple], 
    verbose: bool = False,
    validate: bool = True
) -> bool:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ExtractionAgent â†’ DataContext â†’ AnalysisAgent
    
    Args:
        queries: (ì¿¼ë¦¬ ë¬¸ìì—´, validator í•¨ìˆ˜) íŠœí”Œ ëª©ë¡
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        validate: Ground Truthì™€ ë¹„êµ ì—¬ë¶€
    
    Returns:
        ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
    """
    from OrchestrationAgent.src.orchestrator import Orchestrator
    from shared.data.context import DataContext
    
    # ìºì‹œ í´ë¦¬ì–´ (ì¬í˜„ì„± ë³´ì¥)
    DataContext.clear_cache()
    logging.info("ğŸ—‘ï¸ Cache cleared for reproducibility")
    
    logging.info("=" * 70)
    logging.info("  Full Pipeline End-to-End Test")
    logging.info("=" * 70)
    logging.info("  Pipeline: ExtractionAgent â†’ DataContext â†’ AnalysisAgent")
    if validate:
        logging.info("  Mode: With Ground Truth Validation âœ“")
        logging.info(f"  Test Cases: {TEST_CASE_IDS}")
    logging.info("=" * 70)
    
    orchestrator = Orchestrator()
    test_results = []
    
    for i, query_item in enumerate(queries, 1):
        # ì¿¼ë¦¬ì™€ validator ë¶„ë¦¬
        if isinstance(query_item, tuple):
            query, validator = query_item
        else:
            query, validator = query_item, None
        
        logging.info(f"\n{'='*60}")
        logging.info(f"[Test {i}/{len(queries)}] {query}")
        logging.info("=" * 60)
        
        start_time = time.time()
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = orchestrator.run(query)
        
        elapsed = time.time() - start_time
        
        if result.status == "success":
            logging.info(f"âœ… Execution SUCCESS ({elapsed:.2f}s)")
            logging.info(f"   Result: {result.result}")
            
            # Ground Truth ê²€ì¦
            validation_passed = True
            validation_msg = ""
            
            if validate and validator:
                logging.info("\n   ğŸ“Š Ground Truth Validation:")
                validation_passed, validation_msg = validator(result.result)
                
                for line in validation_msg.split("\n"):
                    logging.info(f"   {line}")
                
                if validation_passed:
                    logging.info("   âœ… VALIDATION PASSED")
                else:
                    logging.error("   âŒ VALIDATION FAILED")
            
            if verbose and result.generated_code:
                logging.info(f"\n   Generated Code:")
                logging.info("-" * 40)
                for line in result.generated_code.split("\n"):
                    logging.info(f"   {line}")
            
            test_results.append((query, validation_passed, result, validation_msg))
        else:
            logging.error(f"âŒ Execution FAILED ({elapsed:.2f}s)")
            logging.error(f"   Error Stage: {result.error_stage}")
            logging.error(f"   Error: {result.error_message}")
            test_results.append((query, False, result, "Execution failed"))
    
    # ê²°ê³¼ ìš”ì•½
    logging.info("\n" + "=" * 70)
    logging.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logging.info("=" * 70)
    
    passed_count = sum(1 for _, passed, _, _ in test_results if passed)
    total_count = len(test_results)
    
    for query, passed, result, val_msg in test_results:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        short_query = query[:50] + "..." if len(query) > 50 else query
        logging.info(f"  {status} | {short_query}")
    
    logging.info("-" * 70)
    logging.info(f"  Total: {passed_count}/{total_count} passed")
    
    all_passed = passed_count == total_count
    
    logging.info("\n" + "=" * 70)
    if all_passed:
        logging.info("  âœ… ALL TESTS PASSED (with Ground Truth validation)!")
    else:
        logging.info("  âŒ SOME TESTS FAILED")
    logging.info("=" * 70)
    
    return all_passed


# =============================================================================
# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ì¼€ì´ìŠ¤ ID (ì¬í˜„ì„±ì„ ìœ„í•´ ê³ ì •)
# =============================================================================

TEST_CASE_IDS = ['1', '11', '32', '197', '198', '199', '200', '574', '575', '576']
TEST_CASE_IDS_STR = ", ".join(TEST_CASE_IDS)  # "1, 11, 32, 197, 198, 199, 200, 574, 575, 576"


# =============================================================================
# Ground Truth ì •ì˜ (10ê°œ ì¼€ì´ìŠ¤ ê¸°ì¤€: 1, 11, 32, 197, 198, 199, 200, 574, 575, 576)
# =============================================================================

# ì¼€ì´ìŠ¤ë³„ HR í‰ê·  (Ground Truth)
GROUND_TRUTH_CASE_MEANS = {
    '1': 77.192667,
    '11': 69.065425,
    '32': 65.264281,
    '197': 64.691626,
    '198': 66.415616,
    '199': 61.723522,
    '200': 77.371204,
    '574': 72.946735,
    '575': 73.755811,
    '576': 93.802846,
}

# ì¼€ì´ìŠ¤ë³„ í†µê³„ (Ground Truth)
GROUND_TRUTH_CASE_STATS = {
    '1':   {'mean': 77.192667, 'std': 14.608640, 'min': 57.0, 'max': 139.0},
    '11':  {'mean': 69.065425, 'std': 8.158767,  'min': 46.0, 'max': 96.0},
    '32':  {'mean': 65.264281, 'std': 14.055257, 'min': 49.0, 'max': 153.0},
    '197': {'mean': 64.691626, 'std': 7.509587,  'min': 45.0, 'max': 175.0},
    '198': {'mean': 66.415616, 'std': 9.338637,  'min': 52.0, 'max': 123.0},
    '199': {'mean': 61.723522, 'std': 8.620957,  'min': 36.0, 'max': 133.0},
    '200': {'mean': 77.371204, 'std': 9.977991,  'min': 50.0, 'max': 112.0},
    '574': {'mean': 72.946735, 'std': 6.293863,  'min': 62.0, 'max': 105.0},
    '575': {'mean': 73.755811, 'std': 6.728994,  'min': 60.0, 'max': 106.0},
    '576': {'mean': 93.802846, 'std': 9.963528,  'min': 79.0, 'max': 131.0},
}

# ì „ì²´ í†µê³„ (Ground Truth)
GROUND_TRUTH_OVERALL = {
    'mean': 72.222973,  # ì¼€ì´ìŠ¤ í‰ê· ì˜ í‰ê· 
    'std': 9.525622,    # ì¼€ì´ìŠ¤ stdì˜ í‰ê· 
    'min': 36.0,
    'max': 175.0,
}


# =============================================================================
# Ground Truth ë¹„êµ í•¨ìˆ˜
# =============================================================================

def compare_numeric(result: Any, expected: float, tolerance: float = 0.01) -> tuple:
    """
    ìˆ«ì ê²°ê³¼ ë¹„êµ (ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›)
    
    Returns:
        (is_match, message)
    """
    # DataFrame/Seriesì—ì„œ ê°’ ì¶”ì¶œ ì‹œë„
    if isinstance(result, pd.DataFrame):
        if 'mean' in result.columns:
            result = result['mean'].mean()
        elif len(result.columns) == 1:
            result = result.iloc[:, 0].mean()
        else:
            return False, f"Cannot extract numeric from DataFrame: {result.columns.tolist()}"
    elif isinstance(result, pd.Series):
        result = result.mean()
    elif isinstance(result, dict):
        # dictì¸ ê²½ìš° - ë‹¨ì¼ ìˆ«ì ê°’ì´ë©´ ê·¸ ê°’ ì‚¬ìš©, ì•„ë‹ˆë©´ 'mean' í‚¤ ì°¾ê¸°
        if len(result) == 1:
            result = list(result.values())[0]
        elif 'mean' in result:
            result = result['mean']
    
    if result is None or (isinstance(result, float) and np.isnan(result)):
        return False, f"Result is None/NaN, expected {expected:.4f}"
    
    try:
        result_val = float(result)
    except (TypeError, ValueError):
        return False, f"Cannot convert result to float: {result}"
    
    diff = abs(result_val - expected)
    rel_diff = diff / abs(expected) if expected != 0 else diff
    
    if rel_diff <= tolerance:
        return True, f"âœ… {result_val:.4f} â‰ˆ {expected:.4f} (diff: {rel_diff*100:.2f}%)"
    else:
        return False, f"âŒ {result_val:.4f} â‰  {expected:.4f} (diff: {rel_diff*100:.2f}%)"


def compare_dict_means(result: Any, expected: Dict[str, float], tolerance: float = 0.01) -> tuple:
    """
    ì¼€ì´ìŠ¤ë³„ í‰ê·  ë”•ì…”ë„ˆë¦¬ ë¹„êµ (DataFrameë„ ì§€ì›)
    
    Returns:
        (is_match, message)
    """
    # DataFrameì„ dictë¡œ ë³€í™˜
    if isinstance(result, pd.DataFrame):
        # caseid ì»¬ëŸ¼ê³¼ mean ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'caseid' in result.columns and 'mean' in result.columns:
            result = dict(zip(result['caseid'].astype(str), result['mean']))
        # indexê°€ caseidì¸ ê²½ìš°
        elif 'mean' in result.columns:
            result = result['mean'].to_dict()
            result = {str(k): v for k, v in result.items()}
        else:
            return False, f"Cannot extract case means from DataFrame: {result.columns.tolist()}"
    
    if not isinstance(result, dict):
        return False, f"Result is not a dict: {type(result)}"
    
    messages = []
    all_match = True
    matched = 0
    total = len(expected)
    
    for case_id, exp_val in expected.items():
        if case_id not in result:
            messages.append(f"  âŒ Case {case_id}: MISSING")
            all_match = False
            continue
        
        res_val = result[case_id]
        if isinstance(res_val, float) and np.isnan(res_val):
            messages.append(f"  âŒ Case {case_id}: NaN (expected {exp_val:.4f})")
            all_match = False
            continue
        
        try:
            res_val = float(res_val)
        except (TypeError, ValueError):
            messages.append(f"  âŒ Case {case_id}: Cannot convert {res_val}")
            all_match = False
            continue
        
        diff = abs(res_val - exp_val)
        rel_diff = diff / abs(exp_val) if exp_val != 0 else diff
        
        if rel_diff <= tolerance:
            messages.append(f"  âœ… Case {case_id}: {res_val:.4f} â‰ˆ {exp_val:.4f}")
            matched += 1
        else:
            messages.append(f"  âŒ Case {case_id}: {res_val:.4f} â‰  {exp_val:.4f} (diff: {rel_diff*100:.2f}%)")
            all_match = False
    
    summary = f"Matched: {matched}/{total}"
    return all_match, summary + "\n" + "\n".join(messages)


def compare_stats(result: Any, expected_case_stats: Dict, expected_overall: Dict, tolerance: float = 0.01) -> tuple:
    """
    í†µê³„ ê²°ê³¼ ë¹„êµ (ì¼€ì´ìŠ¤ë³„ + ì „ì²´)
    DataFrame, dict ëª¨ë‘ ì§€ì›
    
    Returns:
        (is_match, message)
    """
    messages = []
    all_match = True
    
    # DataFrameì„ dictë¡œ ë³€í™˜ (caseidê°€ ì»¬ëŸ¼ì¸ ê²½ìš°)
    if isinstance(result, pd.DataFrame):
        if 'caseid' in result.columns:
            # caseid ì»¬ëŸ¼ì´ ìˆëŠ” DataFrame -> per_case dict í˜•íƒœë¡œ ë³€í™˜
            result = {'per_case': result}
        else:
            # index ê¸°ë°˜ DataFrame
            result = {'per_case': result}
    
    # ê²°ê³¼ í˜•íƒœ í™•ì¸
    if not isinstance(result, dict):
        return False, f"Result is not a dict: {type(result)}"
    
    # 1. ì¼€ì´ìŠ¤ë³„ í†µê³„ ë¹„êµ (per_caseê°€ ìˆëŠ” ê²½ìš°)
    per_case = result.get('per_case')
    if per_case is not None:
        messages.append("=== Per-Case Statistics ===")
        
        # DataFrameì¸ ê²½ìš°
        if isinstance(per_case, pd.DataFrame):
            # caseid ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰
            has_caseid_col = 'caseid' in per_case.columns
            
            for case_id, exp_stats in expected_case_stats.items():
                # í•´ë‹¹ ì¼€ì´ìŠ¤ row ì°¾ê¸°
                if has_caseid_col:
                    # caseid ì»¬ëŸ¼ ê¸°ì¤€
                    mask = per_case['caseid'].astype(str) == str(case_id)
                    if not mask.any():
                        mask = per_case['caseid'] == int(case_id)
                    if not mask.any():
                        messages.append(f"  âŒ Case {case_id}: MISSING")
                        all_match = False
                        continue
                    row = per_case[mask].iloc[0]
                else:
                    # index ê¸°ì¤€
                    if case_id not in per_case.index and int(case_id) not in per_case.index:
                        messages.append(f"  âŒ Case {case_id}: MISSING")
                        all_match = False
                        continue
                    idx = case_id if case_id in per_case.index else int(case_id)
                    row = per_case.loc[idx]
                
                for stat_name in ['mean', 'std', 'min', 'max']:
                    exp_val = exp_stats[stat_name]
                    res_val = row.get(stat_name, np.nan) if isinstance(row, dict) else row[stat_name] if stat_name in row.index else np.nan
                    
                    if pd.isna(res_val):
                        messages.append(f"  âŒ Case {case_id} {stat_name}: NaN")
                        all_match = False
                    else:
                        rel_diff = abs(res_val - exp_val) / abs(exp_val) if exp_val != 0 else abs(res_val - exp_val)
                        if rel_diff > tolerance:
                            messages.append(f"  âŒ Case {case_id} {stat_name}: {res_val:.2f} â‰  {exp_val:.2f}")
                            all_match = False
        
        # Dictì¸ ê²½ìš°
        elif isinstance(per_case, dict):
            for case_id, exp_stats in expected_case_stats.items():
                if case_id not in per_case:
                    messages.append(f"  âŒ Case {case_id}: MISSING")
                    all_match = False
                    continue
                
                case_result = per_case[case_id]
                for stat_name in ['mean', 'std', 'min', 'max']:
                    exp_val = exp_stats[stat_name]
                    res_val = case_result.get(stat_name, np.nan)
                    
                    if pd.isna(res_val):
                        messages.append(f"  âŒ Case {case_id} {stat_name}: NaN")
                        all_match = False
                    else:
                        rel_diff = abs(res_val - exp_val) / abs(exp_val) if exp_val != 0 else abs(res_val - exp_val)
                        if rel_diff > tolerance:
                            messages.append(f"  âŒ Case {case_id} {stat_name}: {res_val:.2f} â‰  {exp_val:.2f}")
                            all_match = False
        
        if all_match:
            messages.append("  âœ… All per-case statistics match!")
    
    # 2. ì „ì²´ í†µê³„ ë¹„êµ
    messages.append("=== Overall Statistics ===")
    overall_keys = ['overall', 'overall_across_cases', 'overall_stats']
    overall_result = None
    for key in overall_keys:
        if key in result:
            overall_result = result[key]
            break
    
    if overall_result is None:
        # ë‹¨ì¼ ê°’ ê²°ê³¼ì¸ ê²½ìš° (meanë§Œ)
        if 'mean' in result:
            overall_result = result
    
    if overall_result:
        for stat_name, exp_val in expected_overall.items():
            res_val = overall_result.get(stat_name, np.nan) if isinstance(overall_result, dict) else np.nan
            
            if pd.isna(res_val):
                messages.append(f"  âš ï¸ Overall {stat_name}: Not found in result")
            else:
                rel_diff = abs(res_val - exp_val) / abs(exp_val) if exp_val != 0 else abs(res_val - exp_val)
                if rel_diff <= tolerance:
                    messages.append(f"  âœ… Overall {stat_name}: {res_val:.4f} â‰ˆ {exp_val:.4f}")
                else:
                    messages.append(f"  âŒ Overall {stat_name}: {res_val:.4f} â‰  {exp_val:.4f}")
                    all_match = False
    else:
        messages.append("  âš ï¸ No overall statistics found in result")
    
    return all_match, "\n".join(messages)


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
# =============================================================================

def validate_query1_result(result: Any) -> tuple:
    """ì¿¼ë¦¬ 1: ëª¨ë“  ìˆ˜ìˆ  í™˜ìì˜ ì‹¬ë°•ìˆ˜(HR) í‰ê· """
    expected = GROUND_TRUTH_OVERALL['mean']
    return compare_numeric(result, expected)


def validate_query2_result(result: Any) -> tuple:
    """ì¿¼ë¦¬ 2: ê° í™˜ìë³„ HR í‰ê· ì„ dictionaryë¡œ"""
    return compare_dict_means(result, GROUND_TRUTH_CASE_MEANS)


def validate_query3_result(result: Any) -> tuple:
    """ì¿¼ë¦¬ 3: HRì˜ ê¸°ë³¸ í†µê³„"""
    return compare_stats(result, GROUND_TRUTH_CASE_STATS, GROUND_TRUTH_OVERALL)


# =============================================================================
# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (ì¼€ì´ìŠ¤ ID ëª…ì‹œë¡œ ì¬í˜„ì„± ë³´ì¥)
# =============================================================================

DEFAULT_QUERIES = [
    # Query 1: ì „ì²´ í‰ê·  - ë‹¨ì¼ float ë°˜í™˜ ëª…ì‹œ
    (f"caseidê°€ {TEST_CASE_IDS_STR}ì¸ í™˜ìë“¤ì˜ ì‹¬ë°•ìˆ˜(HR) ì „ì²´ í‰ê· ì„ êµ¬í•´ì„œ ë‹¨ì¼ float ê°’ìœ¼ë¡œ ë°˜í™˜í•´ì¤˜", validate_query1_result),
    # Query 2: ì¼€ì´ìŠ¤ë³„ í‰ê·  - dict í˜•íƒœ ëª…ì‹œ (ê¸°ì¡´ê³¼ ë™ì¼)
    (f"caseidê°€ {TEST_CASE_IDS_STR}ì¸ ê° í™˜ìë³„ HR í‰ê· ì„ {{caseid: mean}} í˜•íƒœì˜ dictionaryë¡œ ë°˜í™˜í•´ì¤˜", validate_query2_result),
    # Query 3: ê¸°ë³¸ í†µê³„ - DataFrame í˜•íƒœ ëª…ì‹œ
    (f"caseidê°€ {TEST_CASE_IDS_STR}ì¸ í™˜ìë“¤ì˜ HR ê¸°ë³¸ í†µê³„(í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ, ìµœëŒ€)ë¥¼ caseidë³„ë¡œ DataFrameìœ¼ë¡œ ë°˜í™˜í•´ì¤˜", validate_query3_result),
]


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Full Pipeline End-to-End Test",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (3ê°œ ì¿¼ë¦¬ + Ground Truth ê²€ì¦)
    python test_e2e_hr_mean.py
    
    # ì»¤ìŠ¤í…€ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (ê²€ì¦ ì—†ì´)
    python test_e2e_hr_mean.py --query "ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· "
    
    # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
    python test_e2e_hr_mean.py --verbose
    
    # Ground Truth ê²€ì¦ ë¹„í™œì„±í™”
    python test_e2e_hr_mean.py --no-validate
        """
    )
    parser.add_argument(
        "--query", "-q",
        type=str,
        nargs="+",
        help="í…ŒìŠ¤íŠ¸í•  ìì—°ì–´ ì¿¼ë¦¬ (ë³µìˆ˜ ê°€ëŠ¥, ê²€ì¦ ì—†ì´ ì‹¤í–‰)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ë° ìƒì„±ëœ ì½”ë“œ ì¶œë ¥"
    )
    parser.add_argument(
        "--no-validate",
        action="store_true",
        help="Ground Truth ê²€ì¦ ë¹„í™œì„±í™”"
    )
    args = parser.parse_args()
    
    setup_logging(args.verbose)
    
    # ì¿¼ë¦¬ ê²°ì •
    if args.query:
        # ì»¤ìŠ¤í…€ ì¿¼ë¦¬ëŠ” validator ì—†ì´ ì‹¤í–‰
        queries = [(q, None) for q in args.query]
        validate = False
    else:
        queries = DEFAULT_QUERIES
        validate = not args.no_validate
    
    try:
        success = run_full_pipeline_test(
            queries=queries, 
            verbose=args.verbose,
            validate=validate
        )
        sys.exit(0 if success else 1)
    
    except Exception as e:
        logging.exception(f"Test failed with exception: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
