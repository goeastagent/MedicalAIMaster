#!/usr/bin/env python
"""
End-to-End Test: Signal Segmentation Mean Analysis
===================================================

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
  ExtractionAgent â†’ DataContext â†’ AnalysisAgent

í…ŒìŠ¤íŠ¸ ëª©í‘œ:
  - ì¼€ì´ìŠ¤ ë³„ë¡œ SBP(NIBP_SBP) ê°’ì„ 10ë¶„ ë‹¨ìœ„ë¡œ segmentation
  - ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , segment í‰ê· ë“¤ì˜ í‰ê· ì„ ê³„ì‚°
  - í™˜ìë³„ SBP í‰ê· ì„ êµ¬í•¨
  - ë™ì¼ í™˜ìê°€ ì—¬ëŸ¬ ì¼€ì´ìŠ¤ë¥¼ ê°€ì§„ ê²½ìš°, ëª¨ë“  segmentë¥¼ í•©ì³ì„œ í‰ê· 

í…ŒìŠ¤íŠ¸ ëª¨ë“œ:
  - ê¸°ë³¸ ëª¨ë“œ: íŠ¹ì • subjectid (1, 2, 4, 5, 6, 7, 32, 150) ëŒ€ìƒ, ì‚¬ì „ì •ì˜ëœ Ground Truth ì‚¬ìš©
  - ì „ì²´ ëª¨ë“œ (--full): ëª¨ë“  í™˜ì ëŒ€ìƒ, Ground Truth ë™ì  ê³„ì‚°

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ëª¨ë“œ (íŠ¹ì • í™˜ì, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    python test_e2e_signal_segmentation_mean.py
    
    # ì „ì²´ ëª¨ë“œ (ëª¨ë“  í™˜ì, Ground Truth ë™ì  ê³„ì‚°)
    python test_e2e_signal_segmentation_mean.py --full
    
    # ì „ì²´ ëª¨ë“œ + ìƒì„¸ ë¡œê·¸
    python test_e2e_signal_segmentation_mean.py --full --verbose
"""

import sys
import os
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable, Tuple
import time
import numpy as np
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%H:%M:%S"
    )
    
    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì ˆ
    for logger_name in ["httpx", "httpcore", "openai", "urllib3"]:
        logging.getLogger(logger_name).setLevel(logging.WARNING)


# =============================================================================
# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ì¼€ì´ìŠ¤ - ê¸°ë³¸ ëª¨ë“œ (subjectid ê¸°ì¤€)
# =============================================================================

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ í™˜ì (subjectid) - ê¸°ë³¸ ëª¨ë“œ
# - ë‹¤ì¤‘ìˆ˜ìˆ : 32 (2 surgeries), 150 (2 surgeries)
# - ë‹¨ì¼ìˆ˜ìˆ : 1, 2, 4, 5, 6, 7
DEFAULT_TEST_SUBJECT_IDS = [1, 2, 4, 5, 6, 7, 32, 150]
DEFAULT_TEST_SUBJECT_IDS_STR = "1, 2, 4, 5, 6, 7, 32, 150"

# í™˜ìë³„ ì¼€ì´ìŠ¤ ë§¤í•‘ (ì°¸ê³ ìš©)
SUBJECT_CASES = {
    1: [3594],
    2: [121],
    4: [3417],
    5: [734],
    6: [1580],
    7: [1579],
    32: [1598, 1845],   # ë‹¤ì¤‘ìˆ˜ìˆ 
    150: [316, 5359],   # ë‹¤ì¤‘ìˆ˜ìˆ 
}


# =============================================================================
# Ground Truth ì •ì˜ - DEPRECATED (ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
# =============================================================================
# 
# ì•„ë˜ ìƒìˆ˜ë“¤ì€ ì´ì „ì— í•˜ë“œì½”ë”©ëœ ê°’ìœ¼ë¡œ, ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# í˜„ì¬ëŠ” compute_full_ground_truth()ë¥¼ í†µí•´ ë™ì ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
# ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
#
# ë¬¸ì œì : ì´ì „ ê³„ì‚° ë°©ì‹ê³¼ í˜„ì¬ ì•Œê³ ë¦¬ì¦˜ì˜ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ê°’ì´ ë¶€ì •í™•í–ˆìŒ
# í•´ê²°: ë™ì  ê³„ì‚°ìœ¼ë¡œ ì „í™˜í•˜ì—¬ LLM ìƒì„± ì½”ë“œì™€ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©

# [DEPRECATED] ì¼€ì´ìŠ¤ë³„ í‰ê·  (ì°¸ê³ ìš©)
_DEPRECATED_GROUND_TRUTH_CASE_MEANS = {
    '121': 116.360474,   # ì‹¤ì œ: ~128.57
    '316': 105.333336,
    '734': 124.750000,
    '1579': 112.615158,
    '1580': 112.800003,
    '1598': 179.949997,
    '1845': 138.772766,
    '3417': 113.357964,
    '3594': 129.399994,
    '5359': 99.666664,
}

# [DEPRECATED] í™˜ìë³„ ìµœì¢… í‰ê·  (Ground Truth)
_DEPRECATED_GROUND_TRUTH_SUBJECT_MEANS = {
    '1': 129.399994,
    '2': 116.360474,    # ì‹¤ì œ: ~128.57
    '4': 113.357964,
    '5': 124.750000,
    '6': 112.800003,
    '7': 112.615158,
    '32': 152.498505,
    '150': 101.555557,  # ì‹¤ì œ: ~108.42
}

# [DEPRECATED] ì „ì²´ í‰ê· 
_DEPRECATED_GROUND_TRUTH_OVERALL_MEAN = 120.417206


# =============================================================================
# ì¼€ì´ìŠ¤ ìƒ˜í”Œë§ (ë²”ìš© ìœ í‹¸ë¦¬í‹°)
# =============================================================================

def sample_valid_case_ids(
    cohort_path: str = None,
    signal_base_path: str = None,
    sample_size: int = 100,
    seed: int = 42,
) -> List[int]:
    """
    ìœ íš¨í•œ ì¼€ì´ìŠ¤ ID ì¤‘ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    
    ë²”ìš©ì  ì„¤ê³„:
    - ì‹œê·¸ë„ íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ìœ íš¨í•œ ì‹œê°„ ìœˆë„ìš°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ì„ íƒ
    - ì¬í˜„ì„±ì„ ìœ„í•´ seed ê³ ì •
    
    Args:
        cohort_path: ì½”í˜¸íŠ¸ ë©”íƒ€ë°ì´í„° CSV ê²½ë¡œ
        signal_base_path: ì‹œê·¸ë„ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        sample_size: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜
        seed: ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)
        
    Returns:
        ì„ íƒëœ ì¼€ì´ìŠ¤ ID ë¦¬ìŠ¤íŠ¸
    """
    import random
    
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    if signal_base_path is None:
        signal_base_path = PROJECT_ROOT / "IndexingAgent" / "data" / "raw" / "Open_VitalDB_1.0.0" / "vital_files"
    if cohort_path is None:
        cohort_path = PROJECT_ROOT / "IndexingAgent" / "data" / "raw" / "Open_VitalDB_1.0.0" / "clinical_data.csv"
    
    signal_base_path = Path(signal_base_path)
    cohort_path = Path(cohort_path)
    
    logging.info(f"ğŸ“‹ Sampling {sample_size} valid cases (seed={seed})...")
    
    # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê·¸ë„ íŒŒì¼ ID ìˆ˜ì§‘
    vital_files = list(signal_base_path.glob("*.vital"))
    available_case_ids = set()
    for vf in vital_files:
        try:
            stem = vf.stem.lstrip('0') or '0'
            caseid = int(stem)
            available_case_ids.add(caseid)
        except ValueError:
            continue
    
    logging.info(f"   Available signal files: {len(available_case_ids)}")
    
    # 2. ì‹œê·¸ë„ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§ (ì‹œê°„ ìœˆë„ìš° ê²€ì¦ ì—†ìŒ)
    cohort = pd.read_csv(cohort_path)
    valid_case_ids = []
    
    for _, row in cohort.iterrows():
        case_id = int(row['caseid'])
        
        # ì¡°ê±´: ì‹œê·¸ë„ íŒŒì¼ ì¡´ì¬
        if case_id not in available_case_ids:
            continue
        
        valid_case_ids.append(case_id)
    
    logging.info(f"   Valid cases (with signal file): {len(valid_case_ids)}")
    
    # 3. ëœë¤ ìƒ˜í”Œë§
    random.seed(seed)
    if sample_size >= len(valid_case_ids):
        selected = valid_case_ids
        logging.info(f"   âš ï¸ Requested {sample_size} but only {len(valid_case_ids)} available, using all")
    else:
        selected = random.sample(valid_case_ids, sample_size)
        logging.info(f"   âœ… Sampled {len(selected)} cases")
    
    # ì •ë ¬ (ì¬í˜„ì„±)
    selected = sorted(selected)
    logging.info(f"   Sample range: {min(selected)} ~ {max(selected)}")
    
    return selected


# =============================================================================
# ì „ì²´ ëª¨ë“œ Ground Truth ê³„ì‚° (ë™ì )
# =============================================================================

def _process_single_case_ground_truth(args: Tuple) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš© ì›Œì»¤ í•¨ìˆ˜)
    
    ë²”ìš©ì  ì„¤ê³„:
    - íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”
    - ê²°ê³¼ëŠ” í‘œì¤€í™”ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    - ì „ì²´ ì‹ í˜¸ ë°ì´í„° ì‚¬ìš© (ì‹œê°„ í•„í„°ë§ ì—†ìŒ)
    
    Args:
        args: (case_info, caseid_to_file, signal_column, segment_duration)
        
    Returns:
        {
            'subj_id': str,
            'case_id': int,
            'segment_means': List[float],
            'status': 'success' | 'no_file' | 'no_signal' | 'error',
            'error': Optional[str]
        }
    """
    import vitaldb
    
    case_info, caseid_to_file, signal_column, segment_duration = args
    
    subj_id = case_info['subjectid']
    case_id = case_info['caseid']
    
    result = {
        'subj_id': subj_id,
        'case_id': case_id,
        'segment_means': [],
        'status': 'unknown',
        'error': None
    }
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if case_id not in caseid_to_file:
        result['status'] = 'no_file'
        return result
    
    vital_path = caseid_to_file[case_id]
    
    try:
        # VitalDB íŒŒì¼ ë¡œë“œ (ë°ì´í„°ì†ŒìŠ¤ íŠ¹í™” ë¶€ë¶„ - ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œëŠ” ì´ ë¶€ë¶„ë§Œ êµì²´)
        vf = vitaldb.read_vital(str(vital_path), [signal_column])
        vals = vf.to_numpy([signal_column], 1)  # 1ì´ˆ ê°„ê²© ìƒ˜í”Œë§
        
        # 2D -> 1D ë³€í™˜
        if vals is not None and hasattr(vals, 'ndim') and vals.ndim == 2:
            vals = vals.flatten() if vals.shape[1] == 1 else vals[:, 0]
        
        if vals is None or len(vals) == 0:
            result['status'] = 'no_signal'
            return result
        
        # ================================================================
        # Time ê°’ ê¸°ë°˜ Segmentation (LLM ìƒì„± ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹)
        # ================================================================
        # Time ë°°ì—´ ìƒì„± (1ì´ˆ ê°„ê²© ë¦¬ìƒ˜í”Œë§ì´ë¯€ë¡œ ì¸ë±ìŠ¤ = ì‹œê°„(ì´ˆ))
        time_vals = np.arange(len(vals))
        
        # ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ segment í• ë‹¹ (Time // segment_duration)
        segment_indices = (time_vals // segment_duration).astype(int)
        unique_segments = np.unique(segment_indices)
        
        segment_means = []
        for seg in unique_segments:
            # í•´ë‹¹ segmentì˜ ê°’ë“¤ ì¶”ì¶œ
            segment_vals = vals[segment_indices == seg]
            # NaN ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
            valid_vals = segment_vals[~np.isnan(segment_vals)]
            if len(valid_vals) > 0:
                segment_means.append(float(np.mean(valid_vals)))
        
        result['segment_means'] = segment_means
        result['status'] = 'success' if segment_means else 'no_segments'
        return result
        
    except Exception as e:
        result['status'] = 'error'
        result['error'] = str(e)
        return result


def compute_full_ground_truth(
    signal_base_path: str = None,
    cohort_path: str = None,
    segment_duration_seconds: float = 600,
    signal_column: str = "Solar8000/NIBP_SBP",
    max_workers: int = 8,
    case_ids: List[int] = None,
) -> Tuple[Dict[str, float], float, Dict[str, Any]]:
    """
    ì „ì²´ ì‹œê·¸ë„ì—ì„œ Ground Truth ë™ì  ê³„ì‚° (ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „)
    
    ë²”ìš©ì  ì„¤ê³„:
    - ì¼€ì´ìŠ¤ ë‹¨ìœ„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ I/O ë³‘ëª© ìµœì†Œí™”
    - ê²°ê³¼ ì§‘ê³„ëŠ” entity_id(subject) ê¸°ì¤€ìœ¼ë¡œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
    - ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì ìš© ì‹œ _process_single_case_ground_truthì˜ íŒŒì¼ ë¡œë“œ ë¶€ë¶„ë§Œ ìˆ˜ì •
    
    Args:
        signal_base_path: ì‹œê·¸ë„ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        cohort_path: ì½”í˜¸íŠ¸ ë©”íƒ€ë°ì´í„° CSV ê²½ë¡œ
        segment_duration_seconds: segmentation ë‹¨ìœ„ (ì´ˆ)
        signal_column: ë¶„ì„í•  ì‹œê·¸ë„ ì»¬ëŸ¼ëª…
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 8)
        case_ids: ì²˜ë¦¬í•  ì¼€ì´ìŠ¤ ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        
    Returns:
        (subject_means, overall_mean, stats)
        - subject_means: {subjectid: mean} ë”•ì…”ë„ˆë¦¬
        - overall_mean: ì „ì²´ í™˜ì í‰ê· ì˜ í‰ê· 
        - stats: í†µê³„ ì •ë³´ (selected_case_ids í¬í•¨)
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from collections import defaultdict
    import vitaldb
    
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (VitalDB íŠ¹í™” - ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œëŠ” ë³€ê²½)
    if signal_base_path is None:
        signal_base_path = PROJECT_ROOT / "IndexingAgent" / "data" / "raw" / "Open_VitalDB_1.0.0" / "vital_files"
    if cohort_path is None:
        cohort_path = PROJECT_ROOT / "IndexingAgent" / "data" / "raw" / "Open_VitalDB_1.0.0" / "clinical_data.csv"
    
    signal_base_path = Path(signal_base_path)
    cohort_path = Path(cohort_path)
    
    # case_idsë¥¼ setìœ¼ë¡œ ë³€í™˜ (ë¹ ë¥¸ lookup)
    case_ids_filter = set(case_ids) if case_ids else None
    
    logging.info("=" * 60)
    logging.info("ğŸ”¬ Computing Full Ground Truth (Parallel Processing)")
    logging.info("=" * 60)
    logging.info(f"   Signal Path: {signal_base_path}")
    logging.info(f"   Cohort Path: {cohort_path}")
    logging.info(f"   Segment Duration: {segment_duration_seconds} seconds")
    logging.info(f"   Signal Column: {signal_column}")
    logging.info(f"   Workers: {max_workers}")
    if case_ids_filter:
        logging.info(f"   ğŸ¯ FILTERED: {len(case_ids_filter)} cases selected")
    logging.info("=" * 60)
    
    # 1. Cohort ë¡œë“œ
    logging.info("ğŸ“‚ Loading cohort data...")
    cohort = pd.read_csv(cohort_path)
    logging.info(f"   Total cases in cohort: {len(cohort)}")
    
    # 2. ì‹œê·¸ë„ íŒŒì¼ ë§¤í•‘ (VitalDB íŠ¹í™” - ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œëŠ” ë³€ê²½)
    vital_files = list(signal_base_path.glob("*.vital"))
    logging.info(f"   Total signal files: {len(vital_files)}")
    
    caseid_to_file = {}
    for vf in vital_files:
        try:
            stem = vf.stem.lstrip('0') or '0'
            caseid = int(stem)
            caseid_to_file[caseid] = vf
        except ValueError:
            continue
    
    logging.info(f"   Mapped case IDs: {len(caseid_to_file)}")
    
    # 3. ëª¨ë“  ì¼€ì´ìŠ¤ë¥¼ í”Œë« ë¦¬ìŠ¤íŠ¸ë¡œ ì¤€ë¹„ (ë³‘ë ¬ ì²˜ë¦¬ìš©) - ì „ì²´ ì‹ í˜¸ ì‚¬ìš© (ì‹œê°„ í•„í„°ë§ ì—†ìŒ)
    all_cases = []
    for _, row in cohort.iterrows():
        subj_id = str(row['subjectid'])
        case_id = int(row['caseid'])
        
        # case_ids í•„í„° ì ìš© (ì§€ì •ëœ ê²½ìš°)
        if case_ids_filter and case_id not in case_ids_filter:
            continue
        
        all_cases.append({
            'subjectid': subj_id,
            'caseid': case_id,
        })
    
    logging.info(f"   Valid cases to process: {len(all_cases)}")
    if case_ids_filter:
        logging.info(f"   (filtered from {len(case_ids_filter)} requested)")
    
    # 4. ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    logging.info(f"\nğŸš€ Starting parallel processing ({max_workers} workers)...")
    
    start_time = time.time()
    results = []
    
    # ì§„í–‰ ìƒí™© ì¶”ì 
    total_cases = len(all_cases)
    completed = 0
    log_interval = max(1, total_cases // 20)  # 5% ë‹¨ìœ„
    
    # ì²« ì¼€ì´ìŠ¤ ë””ë²„ê·¸ í”Œë˜ê·¸
    first_success_logged = False
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì¼€ì´ìŠ¤ì— ëŒ€í•´ Future ìƒì„±
        futures = {
            executor.submit(
                _process_single_case_ground_truth,
                (case, caseid_to_file, signal_column, segment_duration_seconds)
            ): case
            for case in all_cases
        }
        
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            completed += 1
            
            # ì²« ë²ˆì§¸ ì„±ê³µ ì¼€ì´ìŠ¤ ë””ë²„ê·¸ ì¶œë ¥
            if not first_success_logged and result['status'] == 'success' and result['segment_means']:
                logging.info(f"   ğŸ” First successful case: {result['case_id']}")
                logging.info(f"      Subject: {result['subj_id']}")
                logging.info(f"      Segments: {len(result['segment_means'])}")
                logging.info(f"      Mean: {np.mean(result['segment_means']):.2f}")
                first_success_logged = True
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸
            if completed % log_interval == 0 or completed == total_cases:
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                eta = (total_cases - completed) / rate if rate > 0 else 0
                logging.info(
                    f"   Progress: {completed}/{total_cases} cases "
                    f"({completed/total_cases*100:.1f}%, {elapsed:.1f}s, ETA: {eta:.0f}s)"
                )
    
    # 5. ê²°ê³¼ ì§‘ê³„ (Subjectë³„)
    logging.info("\nğŸ“Š Aggregating results by subject...")
    
    subject_segment_means = defaultdict(list)
    status_counts = defaultdict(int)
    
    for r in results:
        status_counts[r['status']] += 1
        if r['status'] == 'success' and r['segment_means']:
            subject_segment_means[r['subj_id']].extend(r['segment_means'])
    
    # Subjectë³„ í‰ê·  ê³„ì‚°
    subject_means = {}
    for subj_id, means in subject_segment_means.items():
        if means:
            subject_means[subj_id] = float(np.mean(means))
    
    elapsed = time.time() - start_time
    
    # 6. ì „ì²´ í‰ê·  ê³„ì‚°
    overall_mean = float(np.mean(list(subject_means.values()))) if subject_means else float('nan')
    
    # ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤ IDì™€ Subject ID ìˆ˜ì§‘
    processed_case_ids = [r['case_id'] for r in results if r['status'] == 'success']
    processed_subject_ids = list(subject_means.keys())
    
    # 7. í†µê³„ ì •ë³´
    stats = {
        'total_cases_in_cohort': len(all_cases),
        'processed_subjects': len(subject_means),
        'status_counts': dict(status_counts),
        'computation_time_seconds': elapsed,
        'cases_per_second': len(all_cases) / elapsed if elapsed > 0 else 0,
        # íŒŒì´í”„ë¼ì¸ì—ì„œ ë™ì¼í•œ ì¼€ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì •ë³´
        'processed_case_ids': processed_case_ids,
        'processed_subject_ids': processed_subject_ids,
    }
    
    logging.info("\n" + "=" * 60)
    logging.info("âœ… Ground Truth Computation Complete")
    logging.info("=" * 60)
    logging.info(f"   Processed subjects: {len(subject_means)}")
    logging.info(f"   Total cases processed: {status_counts.get('success', 0)}")
    logging.info(f"   Status breakdown:")
    for status, count in sorted(status_counts.items()):
        logging.info(f"      - {status}: {count}")
    logging.info(f"   Overall mean: {overall_mean:.4f}" if not np.isnan(overall_mean) else "   Overall mean: NaN")
    logging.info(f"   Computation time: {elapsed:.2f}s ({stats['cases_per_second']:.1f} cases/s)")
    logging.info("=" * 60)
    
    return subject_means, overall_mean, stats


# =============================================================================
# Ground Truth ë¹„êµ í•¨ìˆ˜
# =============================================================================

def compare_numeric(result: Any, expected: float, tolerance: float = 0.05) -> tuple:
    """
    ìˆ«ì ê²°ê³¼ ë¹„êµ (ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›)
    
    Args:
        result: ë¹„êµí•  ê²°ê³¼ ê°’
        expected: ê¸°ëŒ€ê°’
        tolerance: í—ˆìš© ì˜¤ì°¨ (ìƒëŒ€ ì˜¤ì°¨, ê¸°ë³¸ 5%)
    
    Returns:
        (is_match, message)
    """
    # DataFrame/Seriesì—ì„œ ê°’ ì¶”ì¶œ ì‹œë„
    if isinstance(result, pd.DataFrame):
        if 'mean' in result.columns:
            result = result['mean'].mean()
        elif len(result.columns) == 1:
            result = result.iloc[:, 0].mean()
        else:
            return False, f"Cannot extract numeric from DataFrame: {result.columns.tolist()}"
    elif isinstance(result, pd.Series):
        result = result.mean()
    elif isinstance(result, dict):
        if len(result) == 1:
            result = list(result.values())[0]
        elif 'mean' in result:
            result = result['mean']
        elif 'overall_mean' in result:
            result = result['overall_mean']
    
    if result is None or (isinstance(result, float) and np.isnan(result)):
        return False, f"Result is None/NaN, expected {expected:.4f}"
    
    try:
        result_val = float(result)
    except (TypeError, ValueError):
        return False, f"Cannot convert result to float: {result}"
    
    diff = abs(result_val - expected)
    rel_diff = diff / abs(expected) if expected != 0 else diff
    
    if rel_diff <= tolerance:
        return True, f"âœ… {result_val:.4f} â‰ˆ {expected:.4f} (diff: {rel_diff*100:.2f}%)"
    else:
        return False, f"âŒ {result_val:.4f} â‰  {expected:.4f} (diff: {rel_diff*100:.2f}%)"


def compare_dict_means(result: Any, expected: Dict[str, float], tolerance: float = 0.05) -> tuple:
    """
    í™˜ìë³„ í‰ê·  ë”•ì…”ë„ˆë¦¬ ë¹„êµ
    
    Returns:
        (is_match, message)
    """
    # DataFrameì„ dictë¡œ ë³€í™˜
    if isinstance(result, pd.DataFrame):
        if 'subjectid' in result.columns and 'mean' in result.columns:
            result = dict(zip(result['subjectid'].astype(str), result['mean']))
        elif 'subject_id' in result.columns and 'mean' in result.columns:
            result = dict(zip(result['subject_id'].astype(str), result['mean']))
        elif 'mean' in result.columns:
            result = result['mean'].to_dict()
            result = {str(k): v for k, v in result.items()}
        else:
            return False, f"Cannot extract subject means from DataFrame: {result.columns.tolist()}"
    
    if not isinstance(result, dict):
        return False, f"Result is not a dict: {type(result)}"
    
    messages = []
    all_match = True
    matched = 0
    total = len(expected)
    mismatched = 0
    missing = 0
    
    for subj_id, exp_val in expected.items():
        # subjectid íƒ€ì… ë§ì¶”ê¸°
        subj_key = None
        for key in [subj_id, str(subj_id), int(subj_id) if subj_id.isdigit() else subj_id]:
            if key in result:
                subj_key = key
                break
        
        if subj_key is None:
            missing += 1
            if missing <= 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
                messages.append(f"  âŒ Subject {subj_id}: MISSING")
            all_match = False
            continue
        
        res_val = result[subj_key]
        if isinstance(res_val, float) and np.isnan(res_val):
            messages.append(f"  âŒ Subject {subj_id}: NaN (expected {exp_val:.4f})")
            all_match = False
            continue
        
        try:
            res_val = float(res_val)
        except (TypeError, ValueError):
            messages.append(f"  âŒ Subject {subj_id}: Cannot convert {res_val}")
            all_match = False
            continue
        
        diff = abs(res_val - exp_val)
        rel_diff = diff / abs(exp_val) if exp_val != 0 else diff
        
        if rel_diff <= tolerance:
            matched += 1
            # ì „ì²´ ëª¨ë“œì—ì„œëŠ” ìƒì„¸ ë¡œê·¸ ìƒëµ
            if total <= 20:
                messages.append(f"  âœ… Subject {subj_id}: {res_val:.4f} â‰ˆ {exp_val:.4f}")
        else:
            mismatched += 1
            if mismatched <= 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
                messages.append(f"  âŒ Subject {subj_id}: {res_val:.4f} â‰  {exp_val:.4f} (diff: {rel_diff*100:.2f}%)")
            all_match = False
    
    # ìš”ì•½
    summary_parts = [f"Matched: {matched}/{total}"]
    if missing > 0:
        summary_parts.append(f"Missing: {missing}")
    if mismatched > 0:
        summary_parts.append(f"Mismatched: {mismatched}")
    
    summary = ", ".join(summary_parts)
    
    if total > 20:
        messages.insert(0, f"  (Showing first few mismatches, {matched} subjects matched)")
    
    return all_match, summary + "\n" + "\n".join(messages)


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# =============================================================================

def run_full_pipeline_test(
    full_mode: bool = False,
    verbose: bool = False,
    validate: bool = True,
    max_signal_cases: int = None,  # Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    force_mapreduce: bool = False,  # Map-Reduce ê°•ì œ ì‚¬ìš©
    batch_size: int = 100,
    sample_size: int = 0,  # ìƒ˜í”Œ ì¼€ì´ìŠ¤ ìˆ˜ (0ì´ë©´ ì „ì²´)
) -> bool:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ExtractionAgent â†’ DataContext â†’ AnalysisAgent
    
    Args:
        full_mode: Trueë©´ ì „ì²´/ìƒ˜í”Œ ì‹œê·¸ë„ ëŒ€ìƒ, Falseë©´ íŠ¹ì • subjectid ëŒ€ìƒ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        validate: Ground Truthì™€ ë¹„êµ ì—¬ë¶€
        max_signal_cases: Signal ë¡œë“œ ì‹œ ìµœëŒ€ ì¼€ì´ìŠ¤ ìˆ˜ (0: ë¬´ì œí•œ)
        sample_size: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜ (0ì´ë©´ ì „ì²´, >0ì´ë©´ ëœë¤ ìƒ˜í”Œ)
    
    Returns:
        ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
    """
    from OrchestrationAgent.src.orchestrator import Orchestrator
    from OrchestrationAgent.src.config import OrchestratorConfig
    from shared.data.context import DataContext
    
    # ìºì‹œ í´ë¦¬ì–´ (ì¬í˜„ì„± ë³´ì¥)
    DataContext.clear_cache()
    logging.info("ğŸ—‘ï¸ Cache cleared for reproducibility")
    
    # ==================================================
    # Ground Truth ë° ì¿¼ë¦¬ ì„¤ì •
    # ==================================================
    if full_mode:
        # ìƒ˜í”Œ ëª¨ë“œ vs ì „ì²´ ëª¨ë“œ
        if sample_size > 0:
            # ìƒ˜í”Œ ëª¨ë“œ: ì§€ì •ëœ ìˆ˜ì˜ ì¼€ì´ìŠ¤ë§Œ ì„ íƒ
            logging.info(f"\nğŸ¯ SAMPLE MODE: Selecting {sample_size} cases...")
            selected_case_ids = sample_valid_case_ids(sample_size=sample_size)
            
            logging.info(f"\nğŸ”¬ Computing Ground Truth for {len(selected_case_ids)} sampled cases...")
            subject_means, overall_mean, gt_stats = compute_full_ground_truth(
                case_ids=selected_case_ids
            )
            
            # ì¿¼ë¦¬ ìƒì„± (ì„ íƒëœ ì¼€ì´ìŠ¤ë§Œ ëŒ€ìƒ)
            # ì¼€ì´ìŠ¤ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì¿¼ë¦¬ì— í¬í•¨
            case_ids_str = ", ".join(str(c) for c in selected_case_ids)
            
            queries = [
                # Query 1: í™˜ìë³„ SBP í‰ê·  (ì„ íƒëœ ì¼€ì´ìŠ¤)
                (
                    f"caseidê°€ [{case_ids_str}] ì¤‘ í•˜ë‚˜ì¸ ì¼€ì´ìŠ¤ë“¤ì— ëŒ€í•´ì„œë§Œ: "
                    "NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                    "ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                    "í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                    "ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "ê²°ê³¼ëŠ” {subjectid: mean} í˜•íƒœì˜ dictionaryë¡œ ë°˜í™˜í•´ì¤˜.",
                    lambda result, sm=subject_means: compare_dict_means(result, sm)
                ),
                # Query 2: ì „ì²´ í‰ê·  (ì„ íƒëœ ì¼€ì´ìŠ¤ì˜ í™˜ì í‰ê· ì˜ í‰ê· )
                (
                    f"caseidê°€ [{case_ids_str}] ì¤‘ í•˜ë‚˜ì¸ ì¼€ì´ìŠ¤ë“¤ì— ëŒ€í•´ì„œë§Œ: "
                    "NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                    "ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                    "í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•œ í›„, "
                    "ëª¨ë“  í™˜ìì˜ í‰ê· ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ ì „ì²´ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                    "ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "ê²°ê³¼ëŠ” ë‹¨ì¼ float ê°’ìœ¼ë¡œ ë°˜í™˜í•´ì¤˜.",
                    lambda result, om=overall_mean: compare_numeric(result, om)
                ),
            ]
            
            test_info = {
                'mode': f'SAMPLE ({sample_size})',
                'description': f'Sampled {len(selected_case_ids)} cases',
                'subjects_count': gt_stats['processed_subjects'],
                'cases_count': len(selected_case_ids),
                'selected_case_ids': selected_case_ids,
            }
        else:
            # ì „ì²´ ëª¨ë“œ: Ground Truth ë™ì  ê³„ì‚° (ëª¨ë“  ì¼€ì´ìŠ¤)
            logging.info("\nğŸ”¬ FULL MODE: Computing Ground Truth from all signals...")
            subject_means, overall_mean, gt_stats = compute_full_ground_truth()
            
            # ì¿¼ë¦¬ ìƒì„± (ì „ì²´ í™˜ì ëŒ€ìƒ)
            queries = [
                # Query 1: í™˜ìë³„ SBP í‰ê·  (ì „ì²´ í™˜ì)
                (
                    "ëª¨ë“  í™˜ìë“¤ì˜ "
                    "NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                    "ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                    "í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                    "ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "ê²°ê³¼ëŠ” {subjectid: mean} í˜•íƒœì˜ dictionaryë¡œ ë°˜í™˜í•´ì¤˜.",
                    lambda result: compare_dict_means(result, subject_means)
                ),
                # Query 2: ì „ì²´ í‰ê·  (í™˜ì í‰ê· ì˜ í‰ê· )
                (
                    "ëª¨ë“  í™˜ìë“¤ì˜ "
                    "NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                    "ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                    "í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•œ í›„, "
                    "ëª¨ë“  í™˜ìì˜ í‰ê· ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ ì „ì²´ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                    "ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                    "ê²°ê³¼ëŠ” ë‹¨ì¼ float ê°’ìœ¼ë¡œ ë°˜í™˜í•´ì¤˜.",
                    lambda result: compare_numeric(result, overall_mean)
                ),
            ]
            
            test_info = {
                'mode': 'FULL',
                'description': 'All patients (no filter)',
                'subjects_count': gt_stats['processed_subjects'],
                'cases_count': gt_stats.get('status_counts', {}).get('success', 0),
            }
        
        # Config ì„¤ì •: ì „ì²´/ìƒ˜í”Œ ì¼€ì´ìŠ¤ ë¡œë“œ (max_signal_cases=0)
        if max_signal_cases is None:
            max_signal_cases = 0  # ë¬´ì œí•œ
        
        # sample_size > 0ì´ë©´ í•´ë‹¹ ì¼€ì´ìŠ¤ ìˆ˜ë§Œí¼ë§Œ ë¡œë“œí•˜ë„ë¡ ì„¤ì •
        if sample_size > 0 and max_signal_cases == 0:
            max_signal_cases = sample_size
    else:
        # ê¸°ë³¸ ëª¨ë“œ: íŠ¹ì • ì¼€ì´ìŠ¤ë§Œ ëŒ€ìƒìœ¼ë¡œ ë™ì  Ground Truth ê³„ì‚°
        # SUBJECT_CASESì—ì„œ ì¼€ì´ìŠ¤ ID ì¶”ì¶œ
        default_case_ids = []
        for case_list in SUBJECT_CASES.values():
            default_case_ids.extend(case_list)
        
        logging.info(f"\nğŸ”¬ Computing Ground Truth for default test cases ({len(default_case_ids)} cases)...")
        subject_means, overall_mean, gt_stats = compute_full_ground_truth(
            case_ids=default_case_ids
        )
        
        # ì¿¼ë¦¬ ìƒì„± (íŠ¹ì • subjectid ëŒ€ìƒ)
        queries = [
            # Query 1: í™˜ìë³„ SBP í‰ê·  (íŠ¹ì • í™˜ì)
            (
                f"subjectidê°€ {DEFAULT_TEST_SUBJECT_IDS_STR}ì¸ í™˜ìë“¤ì˜ "
                f"NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                f"ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                f"í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•´ì¤˜. "
                f"í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                f"ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                f"ê²°ê³¼ëŠ” {{subjectid: mean}} í˜•íƒœì˜ dictionaryë¡œ ë°˜í™˜í•´ì¤˜.",
                lambda result, sm=subject_means: compare_dict_means(result, sm)
            ),
            # Query 2: ì „ì²´ í‰ê·  (í™˜ì í‰ê· ì˜ í‰ê· )
            (
                f"subjectidê°€ {DEFAULT_TEST_SUBJECT_IDS_STR}ì¸ í™˜ìë“¤ì˜ "
                f"NIBP_SBP(Solar8000/NIBP_SBP) ê°’ì„ 10ë¶„(600ì´ˆ) ë‹¨ìœ„ë¡œ segmentation í•´ì„œ "
                f"ê° segmentì˜ í‰ê· ì„ êµ¬í•˜ê³ , í™˜ìë³„ë¡œ ëª¨ë“  segment í‰ê· ë“¤ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ "
                f"í™˜ìë‹¹ SBP í‰ê· ì„ êµ¬í•œ í›„, "
                f"ëª¨ë“  í™˜ìì˜ í‰ê· ì„ ë‹¤ì‹œ í‰ê· ë‚´ì„œ ì „ì²´ í‰ê· ì„ êµ¬í•´ì¤˜. "
                f"í•œ í™˜ìê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ìˆ í•œ ê²½ìš°(ê°™ì€ subjectidì˜ ì—¬ëŸ¬ caseid)ëŠ” "
                f"ëª¨ë“  ìˆ˜ìˆ ì˜ segmentë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ í‰ê· ì„ êµ¬í•´ì¤˜. "
                f"ê²°ê³¼ëŠ” ë‹¨ì¼ float ê°’ìœ¼ë¡œ ë°˜í™˜í•´ì¤˜.",
                lambda result, om=overall_mean: compare_numeric(result, om)
            ),
        ]
        
        # Config ì„¤ì •: ê¸°ë³¸ê°’ ìœ ì§€ (max_signal_cases=10)
        if max_signal_cases is None:
            max_signal_cases = 10
        
        test_info = {
            'mode': 'DEFAULT',
            'description': f'Selected subjects: {DEFAULT_TEST_SUBJECT_IDS}',
            'subjects_count': gt_stats['processed_subjects'],
            'cases_count': len(default_case_ids),
        }
    
    # ==================================================
    # í…ŒìŠ¤íŠ¸ ì‹œì‘
    # ==================================================
    logging.info("\n" + "=" * 70)
    logging.info("  Signal Segmentation Mean - End-to-End Test")
    logging.info("=" * 70)
    logging.info("  Pipeline: ExtractionAgent â†’ DataContext â†’ AnalysisAgent")
    logging.info("  Feature: 10-minute segmentation, multi-surgery patient aggregation")
    logging.info(f"  Mode: {test_info['mode']} ({test_info['description']})")
    logging.info(f"  Max Signal Cases: {max_signal_cases if max_signal_cases > 0 else 'UNLIMITED'}")
    if validate:
        logging.info("  Validation: Ground Truth âœ“")
        logging.info(f"  Test Subjects: {test_info['subjects_count']}")
        logging.info(f"  Test Cases: {test_info['cases_count']}")
    logging.info("=" * 70)
    
    # Orchestrator ì„¤ì •
    if force_mapreduce:
        # Map-Reduce ê°•ì œ ëª¨ë“œ
        execution_mode = "mapreduce"
        logging.info(f"   ğŸ—ºï¸ Execution Mode: MAPREDUCE (forced)")
    elif full_mode:
        # ì „ì²´ ëª¨ë“œ: Auto ëª¨ë“œ (ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ ìë™ ì„ íƒ)
        execution_mode = "auto"
        logging.info(f"   ğŸ”„ Execution Mode: AUTO (threshold: 100 cases)")
    else:
        # ê¸°ë³¸ ëª¨ë“œ: í‘œì¤€ ì‹¤í–‰
        execution_mode = "standard"
        logging.info(f"   âš¡ Execution Mode: STANDARD")
    
    if full_mode or force_mapreduce:
        config = OrchestratorConfig(
            max_signal_cases=max_signal_cases,
            max_retries=2,
            execution_mode=execution_mode,
            mapreduce_threshold=100,  # 100ê°œ ì´ìƒì´ë©´ Map-Reduce
            batch_size=batch_size,
            mapreduce_max_workers=4,
        )
    else:
        # ê¸°ë³¸ ëª¨ë“œ: í‘œì¤€ ì‹¤í–‰
        config = OrchestratorConfig(
            max_signal_cases=max_signal_cases,
            max_retries=2,
        )
    orchestrator = Orchestrator(config=config)
    
    test_results = []
    
    for i, query_item in enumerate(queries, 1):
        # ì¿¼ë¦¬ì™€ validator ë¶„ë¦¬
        if isinstance(query_item, tuple):
            query, validator = query_item
        else:
            query, validator = query_item, None
        
        logging.info(f"\n{'='*60}")
        logging.info(f"[Test {i}/{len(queries)}]")
        logging.info(f"Query: {query[:100]}..." if len(query) > 100 else f"Query: {query}")
        logging.info("=" * 60)
        
        start_time = time.time()
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë“œì— ë”°ë¼ ì„ íƒ)
        if force_mapreduce:
            # Map-Reduce ê°•ì œ ì‹¤í–‰
            def progress_callback(batch_idx, total_batches, processed):
                logging.info(f"   ğŸ“¦ Batch {batch_idx+1}/{total_batches}: {processed} cases processed")
            
            result = orchestrator.run_mapreduce(
                query,
                batch_size=batch_size,
                progress_callback=progress_callback,
            )
        elif full_mode:
            # Auto ëª¨ë“œ ì‹¤í–‰ (ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ standard/mapreduce ìë™ ì„ íƒ)
            def progress_callback(batch_idx, total_batches, processed):
                logging.info(f"   ğŸ“¦ Batch {batch_idx+1}/{total_batches}: {processed} cases processed")
            
            result = orchestrator.run_auto(
                query,
                progress_callback=progress_callback,
            )
        else:
            # í‘œì¤€ ëª¨ë“œ
            result = orchestrator.run(query)
        
        elapsed = time.time() - start_time
        
        if result.status == "success":
            logging.info(f"âœ… Execution SUCCESS ({elapsed:.2f}s)")
            logging.info(f"   Result Type: {type(result.result).__name__}")
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            if isinstance(result.result, dict):
                logging.info(f"   Result (dict): {len(result.result)} items")
                for k, v in list(result.result.items())[:3]:
                    logging.info(f"      {k}: {v}")
                if len(result.result) > 3:
                    logging.info(f"      ... ({len(result.result) - 3} more)")
            else:
                logging.info(f"   Result: {result.result}")
            
            # Ground Truth ê²€ì¦
            validation_passed = True
            validation_msg = ""
            
            if validate and validator:
                logging.info("\n   ğŸ“Š Ground Truth Validation:")
                validation_passed, validation_msg = validator(result.result)
                
                for line in validation_msg.split("\n"):
                    logging.info(f"   {line}")
                
                if validation_passed:
                    logging.info("   âœ… VALIDATION PASSED")
                else:
                    logging.error("   âŒ VALIDATION FAILED")
            
            if verbose and result.generated_code:
                logging.info(f"\n   Generated Code:")
                logging.info("-" * 40)
                for line in result.generated_code.split("\n"):
                    logging.info(f"   {line}")
            
            test_results.append((query, validation_passed, result, validation_msg))
        else:
            logging.error(f"âŒ Execution FAILED ({elapsed:.2f}s)")
            logging.error(f"   Error Stage: {result.error_stage}")
            logging.error(f"   Error: {result.error_message}")
            test_results.append((query, False, result, "Execution failed"))
    
    # ê²°ê³¼ ìš”ì•½
    logging.info("\n" + "=" * 70)
    logging.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logging.info("=" * 70)
    
    passed_count = sum(1 for _, passed, _, _ in test_results if passed)
    total_count = len(test_results)
    
    for query, passed, result, val_msg in test_results:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        short_query = query[:50] + "..." if len(query) > 50 else query
        logging.info(f"  {status} | {short_query}")
    
    logging.info("-" * 70)
    logging.info(f"  Total: {passed_count}/{total_count} passed")
    
    all_passed = passed_count == total_count
    
    logging.info("\n" + "=" * 70)
    if all_passed:
        logging.info("  âœ… ALL TESTS PASSED (with Ground Truth validation)!")
    else:
        logging.info("  âŒ SOME TESTS FAILED")
    logging.info("=" * 70)
    
    return all_passed


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Signal Segmentation Mean - End-to-End Test",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # ê¸°ë³¸ ëª¨ë“œ (íŠ¹ì • í™˜ì, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    python test_e2e_signal_segmentation_mean.py
    
    # ì „ì²´ ëª¨ë“œ (ëª¨ë“  í™˜ì, Auto ëª¨ë“œë¡œ ì‹¤í–‰)
    python test_e2e_signal_segmentation_mean.py --full
    
    # ì „ì²´ ëª¨ë“œ + Map-Reduce ê°•ì œ ì‚¬ìš©
    python test_e2e_signal_segmentation_mean.py --full --mapreduce
    
    # Map-Reduce ë°°ì¹˜ í¬ê¸° ì„¤ì •
    python test_e2e_signal_segmentation_mean.py --full --mapreduce --batch-size 50
    
    # ì „ì²´ ëª¨ë“œ + ìƒì„¸ ë¡œê·¸
    python test_e2e_signal_segmentation_mean.py --full --verbose
    
    # ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ ì„¤ì •
    python test_e2e_signal_segmentation_mean.py --full --max-cases 100
    
    # Ground Truth ê²€ì¦ ë¹„í™œì„±í™”
    python test_e2e_signal_segmentation_mean.py --no-validate

Execution Modes:
    STANDARD (ê¸°ë³¸):
        - ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ í›„ ë‹¨ì¼ ì½”ë“œ ì‹¤í–‰
        - ì†ŒëŸ‰ ë°ì´í„°ì— ì í•© (< 100 ì¼€ì´ìŠ¤)
    
    AUTO (--full):
        - ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ STANDARD/MAPREDUCE ìë™ ì„ íƒ
        - 100ê°œ ì´ìƒì´ë©´ ìë™ìœ¼ë¡œ Map-Reduce ì „í™˜
    
    MAPREDUCE (--mapreduce):
        - ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•© (1000+ ì¼€ì´ìŠ¤)
        - map_func: ì¼€ì´ìŠ¤ë³„ ì²˜ë¦¬, reduce_func: ìµœì¢… ì§‘ê³„

Test Modes:
    DEFAULT (ê¸°ë³¸):
        - íŠ¹ì • í™˜ì ëŒ€ìƒ: subjectid 1, 2, 4, 5, 6, 7, 32, 150
        - ì‚¬ì „ ê³„ì‚°ëœ Ground Truth ì‚¬ìš©
        - ë¹ ë¥¸ ì‹¤í–‰ (ì•½ 30ì´ˆ)
    
    SAMPLE (--full --sample-size N):
        - ëœë¤í•˜ê²Œ Nê°œì˜ ì¼€ì´ìŠ¤ë¥¼ ì„ íƒ
        - Ground Truthì™€ íŒŒì´í”„ë¼ì¸ ëª¨ë‘ ë™ì¼í•œ ì¼€ì´ìŠ¤ ì‚¬ìš©
        - ë¹ ë¥¸ ê²€ì¦ì— ì í•© (ê¸°ë³¸: 100ê°œ)
    
    FULL (--full --sample-size 0):
        - ëª¨ë“  í™˜ì ëŒ€ìƒ (í•„í„° ì—†ìŒ)
        - Ground Truth ë™ì  ê³„ì‚° (ì „ì²´ .vital íŒŒì¼ ìŠ¤ìº”)
        - Auto ëª¨ë“œ ì‚¬ìš© (ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ ìë™ ì„ íƒ)

Test Details:
    - Signal: NIBP_SBP (Solar8000/NIBP_SBP)
    - Segmentation: 10 minutes (600 seconds)
    - Time Window: Full signal data (no temporal filtering)
    - Multi-Case: Aggregate all segments across cases per patient
        """
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="ì „ì²´ ëª¨ë“œ: ëª¨ë“  í™˜ì ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (Ground Truth ë™ì  ê³„ì‚°)"
    )
    parser.add_argument(
        "--sample-size", "-s",
        type=int,
        default=100,
        help="ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜ (ê¸°ë³¸: 100, 0ì´ë©´ ì „ì²´ ì¼€ì´ìŠ¤)"
    )
    parser.add_argument(
        "--mapreduce", "-m",
        action="store_true",
        help="Map-Reduce ëª¨ë“œ ê°•ì œ ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬)"
    )
    parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=100,
        help="Map-Reduce ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ë° ìƒì„±ëœ ì½”ë“œ ì¶œë ¥"
    )
    parser.add_argument(
        "--no-validate",
        action="store_true",
        help="Ground Truth ê²€ì¦ ë¹„í™œì„±í™”"
    )
    parser.add_argument(
        "--max-cases",
        type=int,
        default=None,
        help="Signal ë¡œë“œ ì‹œ ìµœëŒ€ ì¼€ì´ìŠ¤ ìˆ˜ (0: ë¬´ì œí•œ, ê¸°ë³¸: --fullì´ë©´ 0, ì•„ë‹ˆë©´ 10)"
    )
    parser.add_argument(
        "--query", "-q",
        type=int,
        choices=[1, 2],
        help="íŠ¹ì • ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸ (1: í™˜ìë³„ í‰ê· , 2: ì „ì²´ í‰ê· ) - ê¸°ë³¸ ëª¨ë“œì—ì„œë§Œ ì§€ì›"
    )
    args = parser.parse_args()
    
    setup_logging(args.verbose)
    
    # LLM ë¡œê¹… í™œì„±í™” (ìƒì„±ëœ ì½”ë“œ, í”„ë¡¬í”„íŠ¸, ì‘ë‹µ ì €ì¥)
    from shared.llm import enable_llm_logging
    log_session_dir = enable_llm_logging("./data/llm_logs")
    logging.info(f"ğŸ“ LLM Logs: {log_session_dir}")
    
    validate = not args.no_validate
    
    try:
        success = run_full_pipeline_test(
            full_mode=args.full,
            verbose=args.verbose,
            validate=validate,
            max_signal_cases=args.max_cases,
            force_mapreduce=args.mapreduce,
            batch_size=args.batch_size,
            sample_size=args.sample_size if args.full else 0,  # full ëª¨ë“œì—ì„œë§Œ ìƒ˜í”Œë§
        )
        sys.exit(0 if success else 1)
    
    except Exception as e:
        logging.exception(f"Test failed with exception: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
