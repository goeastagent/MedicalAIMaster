#!/usr/bin/env python
"""
End-to-End Test: Multi-Parameter Correlation Analysis
======================================================

í…ŒìŠ¤íŠ¸ ëª©í‘œ:
  - HRê³¼ SpO2ì˜ Pearson ìƒê´€ê³„ìˆ˜ ê³„ì‚°
  - ê° ì¼€ì´ìŠ¤ë³„ë¡œ ìƒê´€ê³„ìˆ˜ì™€ p-value ê³„ì‚°
  - í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì¼€ì´ìŠ¤ ë¹„ìœ¨ ë°˜í™˜

í…ŒìŠ¤íŠ¸í•˜ëŠ” ê¸°ëŠ¥:
  - scipy.stats ì‚¬ìš© (pearsonr)
  - ë‹¤ì¤‘ ì»¬ëŸ¼ ë™ì‹œ ì ‘ê·¼
  - í†µê³„ì  ìœ ì˜ì„± í•„í„°ë§ (p < 0.05)
  - ë³µí•© ê²°ê³¼ ë°˜í™˜

ì‚¬ìš©ë²•:
    python test_e2e_correlation.py
    python test_e2e_correlation.py --verbose
"""

import sys
import os
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import time
import numpy as np
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%H:%M:%S"
    )
    for logger_name in ["httpx", "httpcore", "openai", "urllib3"]:
        logging.getLogger(logger_name).setLevel(logging.WARNING)


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì„¤ì •
# =============================================================================

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ìƒ˜í”Œ íŒŒì¼ ê¸°ì¤€: 0001~0020.vital)
TEST_CASE_IDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Signal ì»¬ëŸ¼
HR_COLUMN = "Solar8000/HR"
SPO2_COLUMN = "Solar8000/PLETH_SPO2"

# ìœ ì˜ìˆ˜ì¤€
SIGNIFICANCE_LEVEL = 0.05


# =============================================================================
# Ground Truth ê³„ì‚°
# =============================================================================

def calculate_ground_truth(case_ids: List[int]) -> Dict[str, Any]:
    """
    Ground Truth ê³„ì‚°: ê° ì¼€ì´ìŠ¤ë³„ HR-SpO2 ìƒê´€ê³„ìˆ˜
    
    Returns:
        {
            "case_correlations": {caseid: {"r": float, "p": float, "significant": bool}, ...},
            "significant_ratio": float,
            "mean_correlation": float,
            "valid_cases": int
        }
    """
    import vitaldb
    from scipy import stats
    
    data_dir = Path(__file__).parent.parent / "IndexingAgent" / "data" / "test" / "vitaldb_sample" / "vital_files"
    case_correlations = {}
    
    for case_id in case_ids:
        vital_path = data_dir / f"{case_id:04d}.vital"
        if not vital_path.exists():
            continue
        
        try:
            vf = vitaldb.read_vital(str(vital_path), [HR_COLUMN, SPO2_COLUMN])
            hr_vals = vf.to_numpy([HR_COLUMN], 1)
            spo2_vals = vf.to_numpy([SPO2_COLUMN], 1)
            
            if hr_vals is not None and hasattr(hr_vals, 'ndim') and hr_vals.ndim == 2:
                hr_vals = hr_vals.flatten()
            if spo2_vals is not None and hasattr(spo2_vals, 'ndim') and spo2_vals.ndim == 2:
                spo2_vals = spo2_vals.flatten()
            
            if hr_vals is None or spo2_vals is None:
                continue
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            min_len = min(len(hr_vals), len(spo2_vals))
            hr_vals = hr_vals[:min_len]
            spo2_vals = spo2_vals[:min_len]
            
            # NaN ë™ì‹œ ì œê±°
            mask = ~(np.isnan(hr_vals) | np.isnan(spo2_vals))
            hr_clean = hr_vals[mask]
            spo2_clean = spo2_vals[mask]
            
            if len(hr_clean) < 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
                continue
            
            # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            r, p = stats.pearsonr(hr_clean, spo2_clean)
            
            case_correlations[str(case_id)] = {
                "r": r,
                "p": p,
                "significant": p < SIGNIFICANCE_LEVEL
            }
            
        except Exception as e:
            logging.warning(f"Error processing case {case_id}: {e}")
            continue
    
    valid_cases = len(case_correlations)
    significant_count = sum(1 for c in case_correlations.values() if c["significant"])
    significant_ratio = significant_count / valid_cases if valid_cases > 0 else 0.0
    
    correlations = [c["r"] for c in case_correlations.values()]
    mean_correlation = np.mean(correlations) if correlations else 0.0
    
    return {
        "case_correlations": case_correlations,
        "significant_ratio": significant_ratio,
        "mean_correlation": mean_correlation,
        "valid_cases": valid_cases,
        "significant_count": significant_count
    }


# =============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# =============================================================================

def run_test(verbose: bool = False) -> bool:
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    from OrchestrationAgent.src.orchestrator import Orchestrator
    from shared.llm import enable_llm_logging
    
    # LLM ë¡œê¹… í™œì„±í™”
    log_session_dir = enable_llm_logging("./data/llm_logs")
    logging.info(f"ğŸ“ LLM Logs: {log_session_dir}")
    
    # Ground Truth ê³„ì‚°
    logging.info("ğŸ“Š Calculating Ground Truth...")
    ground_truth = calculate_ground_truth(TEST_CASE_IDS)
    logging.info(f"   Valid cases: {ground_truth['valid_cases']}")
    logging.info(f"   Significant cases: {ground_truth['significant_count']}")
    logging.info(f"   Significant ratio: {ground_truth['significant_ratio']:.4f}")
    logging.info(f"   Mean correlation: {ground_truth['mean_correlation']:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    case_ids_str = str(TEST_CASE_IDS)
    query = f"""caseidê°€ {case_ids_str} ì¤‘ í•˜ë‚˜ì¸ ì¼€ì´ìŠ¤ë“¤ì— ëŒ€í•´ì„œ:
ê° ì¼€ì´ìŠ¤ë³„ë¡œ HR(Solar8000/HR)ê³¼ SpO2(Solar8000/PLETH_SPO2)ì˜ Pearson ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•´ì¤˜.
NaN ê°’ì€ ì œì™¸í•˜ê³  ê³„ì‚°í•˜ê³ , ë‘ ì‹ í˜¸ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì§§ì€ ìª½ì— ë§ì¶°ì¤˜.
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„(p-value < 0.05)ë¥¼ ê°€ì§„ ì¼€ì´ìŠ¤ì˜ ë¹„ìœ¨ì„ êµ¬í•´ì¤˜.
ê²°ê³¼ëŠ” ë‹¨ì¼ float ê°’(0~1 ì‚¬ì´ì˜ ë¹„ìœ¨)ìœ¼ë¡œ ë°˜í™˜í•´ì¤˜."""

    logging.info(f"ğŸ” Query: {query[:100]}...")
    
    # Orchestrator ì‹¤í–‰
    orchestrator = Orchestrator()
    start_time = time.time()
    
    try:
        result = orchestrator.run(query)
        elapsed = time.time() - start_time
        
        if result.status == "success":
            analysis_result = result.result
            logging.info(f"âœ… Execution SUCCESS ({elapsed:.2f}s)")
            logging.info(f"   Result: {analysis_result}")
            
            # ê²€ì¦
            if isinstance(analysis_result, (int, float)):
                expected = ground_truth["significant_ratio"]
                
                # ë¹„ìœ¨ì€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨ (ì´ì‚°ê°’)
                diff = abs(analysis_result - expected)
                
                logging.info(f"\nğŸ“Š Ground Truth Validation:")
                logging.info(f"   Result: {analysis_result:.4f}")
                logging.info(f"   Expected: {expected:.4f}")
                logging.info(f"   Diff: {diff:.4f}")
                
                # 10% ì´ë‚´ ë˜ëŠ” 0.1 ì´ë‚´ë©´ í†µê³¼ (ì¼€ì´ìŠ¤ ìˆ˜ê°€ ì ì–´ ì´ì‚° ì˜¤ì°¨ í—ˆìš©)
                if diff <= 0.15:
                    logging.info("   âœ… VALIDATION PASSED")
                    return True
                else:
                    logging.error("   âŒ VALIDATION FAILED")
                    return False
            else:
                logging.warning(f"   âš ï¸ Unexpected result type: {type(analysis_result)}")
                # dict í˜•íƒœë¡œ ë°˜í™˜í–ˆì„ ìˆ˜ë„ ìˆìŒ
                if isinstance(analysis_result, dict):
                    logging.info(f"   Result dict: {analysis_result}")
                return False
        else:
            logging.error(f"âŒ Execution FAILED: {result.error_message}")
            return False
            
    except Exception as e:
        logging.error(f"âŒ Exception: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    import argparse
    parser = argparse.ArgumentParser(description="E2E Test: Correlation Analysis")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose logging")
    args = parser.parse_args()
    
    setup_logging(args.verbose)
    
    logging.info("=" * 60)
    logging.info("E2E Test: Multi-Parameter Correlation (HR vs SpO2)")
    logging.info("=" * 60)
    
    success = run_test(args.verbose)
    
    logging.info("=" * 60)
    if success:
        logging.info("âœ… TEST PASSED")
    else:
        logging.info("âŒ TEST FAILED")
    logging.info("=" * 60)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
