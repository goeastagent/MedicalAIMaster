"""ê²½ëŸ‰ Orchestrator - ExtractionAgent + DataContext + CodeGen ì—°ê²°

ì±…ì„:
- ExtractionAgent í˜¸ì¶œí•˜ì—¬ Execution Plan íšë“
- DataContextë¡œ ë°ì´í„° ë¡œë“œ
- AnalysisAgent(CodeGen)ë¡œ ë¶„ì„ ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
- ê²°ê³¼ í†µí•© ë° ë°˜í™˜
"""

import time
import gc
import logging
from typing import Dict, Any, Optional, Tuple, List, Callable, Iterator
from concurrent.futures import ThreadPoolExecutor, as_completed

from .models import OrchestrationResult, DataSummary
from .config import OrchestratorConfig, DEFAULT_CONFIG

logger = logging.getLogger("OrchestrationAgent.orchestrator")


class Orchestrator:
    """
    ExtractionAgentì™€ AnalysisAgent(CodeGen)ë¥¼ ì—°ê²°í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    
    ì‚¬ìš©ë²•:
        orchestrator = Orchestrator()
        result = orchestrator.run("ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· ì„ ë‹¨ì¼ float ê°’ìœ¼ë¡œ êµ¬í•´ì¤˜")
        
        if result.status == "success":
            print(result.result)
            print(result.generated_code)
    
    ì‹¤í–‰ ëª¨ë“œ:
        1. run(query) - ì „ì²´ íŒŒì´í”„ë¼ì¸ (Extraction â†’ DataLoad â†’ Analysis)
        2. run_with_plan(query, plan) - Plan ìˆì„ ë•Œ (DataLoad â†’ Analysis)
        3. run_analysis_only(query, data) - ë°ì´í„° ìˆì„ ë•Œ (Analysisë§Œ)
    """
    
    def __init__(self, config: Optional[OrchestratorConfig] = None):
        """
        Args:
            config: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.config = config or DEFAULT_CONFIG
        
        # Lazy initialization
        self._extraction_agent = None
        self._data_context = None
        self._code_generator = None
        self._sandbox = None
        self._code_execution_engine = None  # í†µí•© ì‹¤í–‰ ì—”ì§„
        self._llm_client = None
    
    # =========================================================================
    # Public API
    # =========================================================================
    
    def run(
        self, 
        query: str,
        max_retries: Optional[int] = None,
        timeout_seconds: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ì§ˆì˜ ì‹¤í–‰ - ì „ì²´ íŒŒì´í”„ë¼ì¸
        
        Args:
            query: ìì—°ì–´ ì§ˆì˜
            max_retries: ì½”ë“œ ìƒì„± ì¬ì‹œë„ íšŸìˆ˜ (Noneì´ë©´ config ê°’)
            timeout_seconds: ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (Noneì´ë©´ config ê°’)
        
        Returns:
            OrchestrationResult
        
        Example:
            result = orchestrator.run("ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· ì„ ì„±ë³„ë¡œ ë¹„êµí•´ì¤˜")
        """
        start_time = time.time()
        
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        timeout = timeout_seconds if timeout_seconds is not None else self.config.timeout_seconds
        
        logger.info(f"ğŸš€ Starting pipeline for query: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        try:
            # Step 1: Extraction - ì‹¤í–‰ ê³„íš ìƒì„±
            logger.info("ğŸ“ Step 1/3: Running ExtractionAgent...")
            extraction_result = self._run_extraction(query)
            
            if not extraction_result.get("execution_plan"):
                logger.error("âŒ Extraction failed: No execution plan generated")
                return OrchestrationResult(
                    status="error",
                    error_message="Extraction failed: No execution plan generated",
                    error_stage="extraction",
                    extraction_plan=extraction_result,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            execution_plan = extraction_result["execution_plan"]
            extraction_confidence = extraction_result.get("confidence", 0.0)
            ambiguities = extraction_result.get("ambiguities", [])
            logger.info(f"âœ… Extraction complete (confidence: {extraction_confidence:.2f})")
            logger.debug(f"   Plan: {execution_plan}")
            
            # Step 2: Data Load - ë°ì´í„° ë¡œë“œ
            logger.info("ğŸ“¦ Step 2/3: Loading data via DataContext...")
            runtime_data, data_summary = self._load_data(execution_plan)
            
            if not runtime_data:
                logger.error("âŒ Data loading failed: No data available")
                return OrchestrationResult(
                    status="error",
                    error_message="Data loading failed: No data available",
                    error_stage="data_load",
                    extraction_plan=execution_plan,
                    extraction_confidence=extraction_confidence,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            signals_count = len(runtime_data.get("signals", {}))
            total_rows = sum(len(df) for df in runtime_data.get("signals", {}).values())
            cohort_shape = runtime_data.get("cohort", {}).shape if hasattr(runtime_data.get("cohort", {}), "shape") else "N/A"
            logger.info(f"âœ… Data loaded (signals: {signals_count} cases, {total_rows} rows, cohort: {cohort_shape})")
            
            # Step 3: Analysis - ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
            logger.info("ğŸ§® Step 3/3: Running AnalysisAgent (CodeGen)...")
            analysis_result = self._run_analysis(
                query=query,
                runtime_data=runtime_data,
                data_summary=data_summary,
                max_retries=max_retries,
                timeout=timeout
            )
            
            execution_time = (time.time() - start_time) * 1000
            
            if analysis_result["success"]:
                logger.info(f"âœ… Analysis complete ({execution_time:.1f}ms, retries: {analysis_result.get('retry_count', 0)})")
            else:
                logger.error(f"âŒ Analysis failed: {analysis_result.get('error')}")
            
            return OrchestrationResult(
                status="success" if analysis_result["success"] else "error",
                result=analysis_result.get("result"),
                generated_code=analysis_result.get("code"),
                error_message=analysis_result.get("error"),
                error_stage="analysis" if not analysis_result["success"] else None,
                execution_time_ms=execution_time,
                data_summary=data_summary,
                extraction_plan=execution_plan,
                extraction_confidence=extraction_confidence,
                ambiguities=ambiguities,
                retry_count=analysis_result.get("retry_count", 0)
            )
        
        except Exception as e:
            logger.exception(f"âŒ Unexpected error: {e}")
            return OrchestrationResult(
                status="error",
                error_message=f"Unexpected error: {str(e)}",
                execution_time_ms=(time.time() - start_time) * 1000
            )
    
    def run_with_plan(
        self,
        query: str,
        execution_plan: Dict[str, Any],
        max_retries: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ì´ë¯¸ ìˆëŠ” Execution Planìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰ (ExtractionAgent ìŠ¤í‚µ)
        
        Args:
            query: ë¶„ì„ ì§ˆì˜
            execution_plan: ë¯¸ë¦¬ ìƒì„±ëœ ì‹¤í–‰ ê³„íš
            max_retries: ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            OrchestrationResult
        """
        start_time = time.time()
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        
        try:
            # Data Load
            runtime_data, data_summary = self._load_data(execution_plan)
            
            if not runtime_data:
                return OrchestrationResult(
                    status="error",
                    error_message="Data loading failed",
                    error_stage="data_load",
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            # Analysis
            analysis_result = self._run_analysis(
                query=query,
                runtime_data=runtime_data,
                data_summary=data_summary,
                max_retries=max_retries
            )
            
            return OrchestrationResult(
                status="success" if analysis_result["success"] else "error",
                result=analysis_result.get("result"),
                generated_code=analysis_result.get("code"),
                error_message=analysis_result.get("error"),
                error_stage="analysis" if not analysis_result["success"] else None,
                execution_time_ms=(time.time() - start_time) * 1000,
                data_summary=data_summary,
                extraction_plan=execution_plan,
                retry_count=analysis_result.get("retry_count", 0)
            )
        
        except Exception as e:
            return OrchestrationResult(
                status="error",
                error_message=f"Unexpected error: {str(e)}",
                execution_time_ms=(time.time() - start_time) * 1000
            )
    
    def run_analysis_only(
        self,
        query: str,
        runtime_data: Dict[str, Any],
        max_retries: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ë°ì´í„°ê°€ ì´ë¯¸ ìˆì„ ë•Œ ë¶„ì„ë§Œ ì‹¤í–‰ (Extraction + DataLoad ìŠ¤í‚µ)
        
        Args:
            query: ë¶„ì„ ì§ˆì˜
            runtime_data: ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°
                - ìƒˆ í˜•ì‹: {"signals": Dict[caseid, DataFrame], "cohort": DataFrame}
                - ê¸°ì¡´ í˜•ì‹: {"df": DataFrame, "cohort": DataFrame} (í•˜ìœ„í˜¸í™˜)
            max_retries: ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            OrchestrationResult
        
        Example (ìƒˆ í˜•ì‹):
            runtime_data = {
                "signals": {"case1": df1, "case2": df2},
                "cohort": cohort_df,
            }
            
        Example (ê¸°ì¡´ í˜•ì‹ - í•˜ìœ„í˜¸í™˜):
            runtime_data = {
                "df": signals_df,
                "cohort": cohort_df,
            }
        """
        start_time = time.time()
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        
        logger.info(f"ğŸ§® Running analysis only for: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ìƒì„± (ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ)
        runtime_data = self._prepare_runtime_data_with_guide(runtime_data)
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = self._create_data_summary(runtime_data)
        logger.debug(f"   Data summary: {data_summary}")
        
        # Analysis
        analysis_result = self._run_analysis(
            query=query,
            runtime_data=runtime_data,
            data_summary=data_summary,
            max_retries=max_retries
        )
        
        execution_time = (time.time() - start_time) * 1000
        
        if analysis_result["success"]:
            logger.info(f"âœ… Analysis complete ({execution_time:.1f}ms, retries: {analysis_result.get('retry_count', 0)})")
        else:
            logger.error(f"âŒ Analysis failed: {analysis_result.get('error')}")
        
        return OrchestrationResult(
            status="success" if analysis_result["success"] else "error",
            result=analysis_result.get("result"),
            generated_code=analysis_result.get("code"),
            error_message=analysis_result.get("error"),
            error_stage="analysis" if not analysis_result["success"] else None,
            execution_time_ms=execution_time,
            data_summary=data_summary,
            retry_count=analysis_result.get("retry_count", 0)
        )
    
    # =========================================================================
    # Step 1: Extraction
    # =========================================================================
    
    def _run_extraction(self, query: str) -> Dict[str, Any]:
        """ExtractionAgent í˜¸ì¶œí•˜ì—¬ Execution Plan ìƒì„±"""
        
        if self._extraction_agent is None:
            self._extraction_agent = self._create_extraction_agent()
        
        # ExtractionAgent ì‹¤í–‰
        result = self._extraction_agent.invoke({"user_query": query})
        
        # ê²°ê³¼ì—ì„œ plan ì¶”ì¶œ
        validation = result.get("validation", {}) or {}
        return {
            "execution_plan": result.get("validated_plan") or result.get("execution_plan"),
            "confidence": validation.get("confidence", 0.0),
            "ambiguities": result.get("ambiguities", []),
            "intent": result.get("intent")
        }
    
    def _create_extraction_agent(self):
        """ExtractionAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        import sys
        from pathlib import Path
        
        # ExtractionAgentë¥¼ sys.path ì•ì— ì¶”ê°€ (src.agents import ìœ„í•´)
        extraction_path = str(Path(__file__).parent.parent.parent / "ExtractionAgent")
        if extraction_path not in sys.path:
            sys.path.insert(0, extraction_path)
        
        # src.agentsë¡œ import (ExtractionAgent ë‚´ë¶€ import ê²½ë¡œì™€ ì¼ì¹˜)
        from src.agents.graph import build_agent
        return build_agent()
    
    # =========================================================================
    # Step 2: Data Load
    # =========================================================================
    
    def _load_data(
        self, 
        execution_plan: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """DataContextë¡œ ë°ì´í„° ë¡œë“œ (ì¼€ì´ìŠ¤ë³„ Dict í˜•íƒœ)
        
        Returns:
            (runtime_data, data_summary)
            
        runtime_data êµ¬ì¡°:
            - signals: Dict[caseid, DataFrame] - ì¼€ì´ìŠ¤ë³„ ì‹œê³„ì—´ ë°ì´í„°
            - cohort: DataFrame - ì „ì²´ ë©”íƒ€ë°ì´í„°
            - case_ids: List[str] - ë¡œë“œëœ ì¼€ì´ìŠ¤ ID
            - total_cases: int - ì „ì²´ ì¼€ì´ìŠ¤ ìˆ˜
            - _access_guide: str - LLMìš© ë™ì  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ
        """
        from shared.data.context import DataContext
        
        ctx = DataContext()
        ctx.load_from_plan(execution_plan, preload_cohort=self.config.preload_cohort)
        
        # runtime_data êµ¬ì„±
        runtime_data = {}
        
        # Cohort (ì „ì²´)
        cohort = ctx.get_cohort()
        if cohort is not None and not cohort.empty:
            runtime_data["cohort"] = cohort
        
        # Signals - Dict[caseid, DataFrame] í˜•íƒœë¡œ!
        max_cases = self.config.max_signal_cases if self.config.max_signal_cases > 0 else None
        signals_dict = ctx.get_signals_dict(max_cases=max_cases)
        if signals_dict:
            runtime_data["signals"] = signals_dict
        
        # ë©”íƒ€ë°ì´í„°
        runtime_data["case_ids"] = list(signals_dict.keys()) if signals_dict else []
        runtime_data["total_cases"] = len(ctx.get_case_ids())
        runtime_data["param_keys"] = ctx.get_available_parameters()
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ìƒì„± (LLM í”„ë¡¬í”„íŠ¸ìš©)
        access_guide = ctx.generate_access_guide(signals_dict, cohort)
        runtime_data["_access_guide"] = access_guide
        
        # ìš”ì•½ ìƒì„±
        data_summary = {
            "signals_count": len(signals_dict) if signals_dict else 0,
            "total_cases": runtime_data["total_cases"],
            "cohort_shape": cohort.shape if cohort is not None and not cohort.empty else None,
            "param_keys": runtime_data["param_keys"],
            "loaded_case_ids": runtime_data["case_ids"][:10],  # ìƒ˜í”Œ
        }
        
        # DataContext ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
        self._data_context = ctx
        
        return runtime_data, data_summary
    
    def _create_data_summary(self, runtime_data: Dict[str, Any]) -> Dict[str, Any]:
        """runtime_dataì—ì„œ ìš”ì•½ ìƒì„± (ìƒˆ í˜•ì‹ + ê¸°ì¡´ í˜•ì‹ ì§€ì›)"""
        summary = {}
        
        # ìƒˆ í˜•ì‹: signals Dict
        if "signals" in runtime_data and runtime_data["signals"]:
            signals_dict = runtime_data["signals"]
            sample_cid = list(signals_dict.keys())[0]
            sample_df = signals_dict[sample_cid]
            summary["signals"] = {
                "case_count": len(signals_dict),
                "total_rows": sum(len(df) for df in signals_dict.values()),
                "sample_shape": sample_df.shape,
                "columns": list(sample_df.columns)
            }
        
        # ê¸°ì¡´ í˜•ì‹: df (í•˜ìœ„í˜¸í™˜)
        elif "df" in runtime_data and hasattr(runtime_data["df"], "shape"):
            df = runtime_data["df"]
            summary["signals"] = {
                "shape": df.shape,
                "columns": list(df.columns)
            }
        
        if "cohort" in runtime_data and hasattr(runtime_data["cohort"], "shape"):
            cohort = runtime_data["cohort"]
            summary["cohort"] = {
                "shape": cohort.shape,
                "columns": list(cohort.columns)
            }
        
        summary["case_count"] = len(runtime_data.get("case_ids", []))
        summary["total_cases"] = runtime_data.get("total_cases", 0)
        summary["param_keys"] = runtime_data.get("param_keys", [])
        
        return summary
    
    def _prepare_runtime_data_with_guide(self, runtime_data: Dict[str, Any]) -> Dict[str, Any]:
        """runtime_dataì— ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ì¶”ê°€ (ê¸°ì¡´ df í˜•íƒœë„ ì§€ì›)"""
        # ì´ë¯¸ ê°€ì´ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if "_access_guide" in runtime_data:
            return runtime_data
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë™ì ìœ¼ë¡œ entity_id ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸°
        entity_col = "id"  # ê¸°ë³¸ê°’
        if self._data_context and self._data_context.entity_id_column:
            entity_col = self._data_context.entity_id_column
        elif "_plan_metadata" in runtime_data:
            entity_col = runtime_data["_plan_metadata"].get("entity_id_column") or "id"
        
        guide_parts = ["## Available Data\n"]
        
        # ìƒˆ í˜•ì‹: signals Dict
        if "signals" in runtime_data and runtime_data["signals"]:
            signals_dict = runtime_data["signals"]
            sample_cid = list(signals_dict.keys())[0]
            sample_df = signals_dict[sample_cid]
            columns = list(sample_df.columns)
            # Time ì»¬ëŸ¼ ì œì™¸í•œ ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼
            data_columns = [c for c in columns if c != "Time"]
            first_col = data_columns[0] if data_columns else "col"
            
            guide_parts.append(f"""### signals: Dict[{entity_col} â†’ DataFrame]
- Type: Case-level independent time series data
- Entity identifier: `{entity_col}`
- Loaded cases: {list(signals_dict.keys())[:5]}{'...' if len(signals_dict) > 5 else ''} (total: {len(signals_dict)})
- **EXACT DataFrame columns: {columns}**
- Sample shape: {sample_df.shape}

âš ï¸ **CRITICAL: Use EXACT column names from the list above!**
Column names contain device prefixes like 'Solar8000/HR', NOT just 'HR'.

**Access Patterns (using actual column name):**
```python
# Single case - USE EXACT COLUMN NAME
signals['{sample_cid}']['{first_col}'].dropna().mean()

# All cases (RECOMMENDED for statistics)
case_stats = {{cid: df['{first_col}'].dropna().mean() for cid, df in signals.items()}}
overall_mean = np.nanmean(list(case_stats.values()))
```
""")
        
        # ê¸°ì¡´ í˜•ì‹: df (í•˜ìœ„í˜¸í™˜)
        elif "df" in runtime_data and hasattr(runtime_data["df"], "columns"):
            df = runtime_data["df"]
            columns = list(df.columns)
            
            # dfì—ì„œ ê°€ëŠ¥í•œ entity ì»¬ëŸ¼ ì°¾ê¸°
            possible_entity_cols = [c for c in columns if 'id' in c.lower() or 'case' in c.lower()]
            detected_entity = possible_entity_cols[0] if possible_entity_cols else entity_col
            
            guide_parts.append(f"""### df: pandas DataFrame
- Type: Signal data (all cases concatenated)
- Shape: {df.shape}
- Columns: {columns}

**Access Patterns:**
```python
# Direct access
df['ColumnName'].mean()

# Group by entity (if available)
df.groupby('{detected_entity}')['ColumnName'].mean()
```
""")
        
        # Cohort
        if "cohort" in runtime_data and hasattr(runtime_data["cohort"], "columns"):
            cohort = runtime_data["cohort"]
            columns = list(cohort.columns)[:15]
            
            guide_parts.append(f"""
### cohort: pandas DataFrame
- Shape: {cohort.shape}
- Entity identifier: `{entity_col}`
- Columns: {columns}{'...' if len(cohort.columns) > 15 else ''}

**Access:**
```python
cohort[cohort['column'] == 'value']
case_list = cohort[cohort['filter'] == 'value']['{entity_col}'].astype(str).tolist()
```
""")
        
        guide_parts.append("""
## Analysis Guidelines
1. Assign final result to `result` variable
2. Handle NaN with dropna() or fillna()
""")
        
        runtime_data["_access_guide"] = "\n".join(guide_parts)
        return runtime_data
    
    # =========================================================================
    # Step 3: Analysis (CodeGen)
    # =========================================================================
    
    def _run_analysis(
        self,
        query: str,
        runtime_data: Dict[str, Any],
        data_summary: Dict[str, Any],
        max_retries: int = 2,
        timeout: int = 30
    ) -> Dict[str, Any]:
        """CodeExecutionEngineìœ¼ë¡œ ë¶„ì„ ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
        
        Returns:
            {"success": bool, "result": Any, "code": str, "error": str, "retry_count": int}
        """
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if self._code_execution_engine is None:
            self._init_code_gen_components(timeout, max_retries)
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ì¶”ì¶œ (LLM í”„ë¡¬í”„íŠ¸ìš©)
        access_guide = runtime_data.pop("_access_guide", None)
        
        # ExecutionContext ìƒì„±
        exec_context = self._build_execution_context(runtime_data, data_summary)
        
        # CodeRequest ìƒì„± (ë™ì  ê°€ì´ë“œ + ë©”íƒ€ë°ì´í„° ê¸°ë°˜ íŒíŠ¸ í¬í•¨)
        request = self._build_code_request(
            query, exec_context, data_summary, 
            runtime_data=runtime_data, 
            access_guide=access_guide
        )
        
        # CodeExecutionEngineìœ¼ë¡œ ìƒì„± + ì‹¤í–‰ (with retry)
        result = self._code_execution_engine.execute(
            request, 
            runtime_data,
            log_prefix="[Orchestrator] "
        )
        
        return result.to_dict()
    
    def _init_code_gen_components(self, timeout: int = 30, max_retries: int = 2):
        """CodeExecutionEngine ì´ˆê¸°í™”"""
        from AnalysisAgent.src.code_gen import (
            CodeGenerator, 
            SandboxExecutor, 
            CodeExecutionEngine
        )
        from shared.llm import get_llm_client
        
        if self._llm_client is None:
            self._llm_client = get_llm_client()
        
        self._code_generator = CodeGenerator(llm_client=self._llm_client)
        self._sandbox = SandboxExecutor(timeout_seconds=timeout)
        self._code_execution_engine = CodeExecutionEngine(
            generator=self._code_generator,
            sandbox=self._sandbox,
            max_retries=max_retries
        )
    
    def _get_entity_column(self, runtime_data: Optional[Dict[str, Any]] = None) -> str:
        """
        entity_id ì»¬ëŸ¼ëª… ì¶”ì¶œ (ì¤‘ë³µ ì œê±°ìš© í—¬í¼)
        
        Args:
            runtime_data: ëŸ°íƒ€ì„ ë°ì´í„° (ë©”íƒ€ë°ì´í„° í¬í•¨ ê°€ëŠ¥)
        
        Returns:
            entity_id ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: "id")
        """
        if self._data_context and self._data_context.entity_id_column:
            return self._data_context.entity_id_column
        if runtime_data and "_plan_metadata" in runtime_data:
            return runtime_data["_plan_metadata"].get("entity_id_column") or "id"
        return "id"
    
    def _build_execution_context(
        self, 
        runtime_data: Dict[str, Any],
        data_summary: Dict[str, Any]
    ):
        """CodeGenìš© ExecutionContext ìƒì„± (ì»¬ëŸ¼ ì„¤ëª… í¬í•¨)"""
        from AnalysisAgent.src.models import ExecutionContext, DataSchema, ColumnDescription
        
        entity_col = self._get_entity_column(runtime_data)
        
        # ParameterRegistryì—ì„œ ì»¬ëŸ¼ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
        column_descriptions = {}
        if self._data_context:
            column_descriptions = self._data_context.get_column_descriptions()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜ ì„¤ëª…
        available_variables = {}
        data_schemas = {}
        
        # signals: Dict[entity_id, DataFrame]
        if "signals" in runtime_data and runtime_data["signals"]:
            signals_dict = runtime_data["signals"]
            case_count = len(signals_dict)
            sample_cid = list(signals_dict.keys())[0]
            sample_df = signals_dict[sample_cid]
            cols = list(sample_df.columns)
            
            available_variables["signals"] = (
                f"Dict[{entity_col}, DataFrame] - ì¼€ì´ìŠ¤ë³„ ì‹œê³„ì—´ ë°ì´í„°, {case_count} cases"
            )
            
            # DataSchema with column descriptions ìƒì„±
            schema_col_descs = {}
            for col in cols:
                if col in column_descriptions:
                    cd = column_descriptions[col]
                    schema_col_descs[col] = ColumnDescription(
                        name=cd.get("name", col),
                        dtype=cd.get("dtype") or str(sample_df[col].dtype),
                        semantic_name=cd.get("semantic_name"),
                        unit=cd.get("unit"),
                        description=cd.get("description"),
                    )
                else:
                    # DBì— ì—†ëŠ” ì»¬ëŸ¼ì€ ê¸°ë³¸ ì •ë³´
                    schema_col_descs[col] = ColumnDescription(
                        name=col,
                        dtype=str(sample_df[col].dtype)
                    )
            
            # ParameterRegistryì— ì‹¤ì œ ë°ì´í„° ì •ë³´ ë³´ê°•
            if self._data_context:
                self._data_context.enrich_param_registry_from_data(sample_df)
            
            data_schemas["signals_sample"] = DataSchema(
                name=f"signals[{entity_col}]",
                description="ì¼€ì´ìŠ¤ë³„ ì‹œê³„ì—´ DataFrame",
                columns=cols,
                dtypes={col: str(sample_df[col].dtype) for col in cols},
                shape=sample_df.shape,
                sample_rows=sample_df.head(2).to_dict(orient="records"),
                column_descriptions=schema_col_descs
            )
        
        if "cohort" in runtime_data and not runtime_data["cohort"].empty:
            cohort = runtime_data["cohort"]
            cols = list(cohort.columns)
            available_variables["cohort"] = (
                f"pandas DataFrame - Cohort ë©”íƒ€ë°ì´í„°, shape: {cohort.shape}"
            )
            
            # Cohortë„ DataSchema ìƒì„±
            cohort_col_descs = {}
            for col in cols[:20]:  # cohortëŠ” ì»¬ëŸ¼ì´ ë§ì„ ìˆ˜ ìˆì–´ ì œí•œ
                cohort_col_descs[col] = ColumnDescription(
                    name=col,
                    dtype=str(cohort[col].dtype)
                )
            
            data_schemas["cohort"] = DataSchema(
                name="cohort",
                description="Cohort ë©”íƒ€ë°ì´í„° DataFrame",
                columns=cols[:20],  # ì²˜ìŒ 20ê°œë§Œ
                dtypes={col: str(cohort[col].dtype) for col in cols[:20]},
                shape=cohort.shape,
                sample_rows=cohort.head(2).to_dict(orient="records"),
                column_descriptions=cohort_col_descs
            )
        
        case_ids = runtime_data.get("case_ids", [])
        available_variables["case_ids"] = f"List[str] - {len(case_ids)} loaded entity IDs"
        
        total_cases = runtime_data.get("total_cases", len(case_ids))
        available_variables["total_cases"] = f"int - total entities: {total_cases}"
        
        param_keys = runtime_data.get("param_keys", [])
        available_variables["param_keys"] = f"List[str] - parameter keys: {param_keys}"
        
        return ExecutionContext(
            available_variables=available_variables,
            data_schemas=data_schemas,
            sample_data=None  # data_schemasê°€ ìˆìœ¼ë©´ sample_data ë¶ˆí•„ìš”
        )
    
    def _build_code_request(
        self, 
        query: str,
        exec_context,
        data_summary: Dict[str, Any],
        runtime_data: Optional[Dict[str, Any]] = None,
        access_guide: Optional[str] = None
    ):
        """CodeRequest ìƒì„±"""
        from AnalysisAgent.src.models import CodeRequest
        
        entity_col = self._get_entity_column(runtime_data)
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ + ê¸°ì¡´ íŒíŠ¸ ê²°í•©
        hints_parts = []
        
        # 1. ë™ì  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ (ìš°ì„ )
        if access_guide:
            hints_parts.append(access_guide)
        
        # 2. ì§ˆì˜ ê¸°ë°˜ ì¶”ê°€ íŒíŠ¸
        if self.config.generate_hints:
            additional_hints = self._generate_hints(query, data_summary, runtime_data)
            if additional_hints:
                hints_parts.append("\n## Additional Hints\n" + additional_hints)
        
        hints = "\n".join(hints_parts) if hints_parts else None
        
        return CodeRequest(
            task_description=query,
            expected_output="Assign final result to `result` variable. Can be number, dict, or list.",
            execution_context=exec_context,
            hints=hints,
            constraints=[
                "Handle NaN with dropna() or fillna()",
                "Must assign final result to `result` variable",
                "For case-level statistics: compute per-case first, then aggregate",
                f"Use signals[{entity_col}] to access individual case DataFrame"
            ]
        )
    
    def _generate_hints(
        self, 
        query: str, 
        data_summary: Dict[str, Any],
        runtime_data: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """
        ì§ˆì˜ ê¸°ë°˜ ì¶”ê°€ íŒíŠ¸ ìƒì„± (ë™ì  ê°€ì´ë“œ ë³´ì™„ìš©)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            data_summary: ë°ì´í„° ìš”ì•½
            runtime_data: ëŸ°íƒ€ì„ ë°ì´í„° (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        hints = []
        query_lower = query.lower()
        entity_col = self._get_entity_column(runtime_data)
        
        # param_keys ì¶”ì¶œ (ì‹¤ì œ ì»¬ëŸ¼ëª…)
        param_keys = data_summary.get("param_keys", [])
        first_param = param_keys[0] if param_keys else "col"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ íŒíŠ¸ (signals Dict ê¸°ë°˜ - ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
        if "í‰ê· " in query_lower or "mean" in query_lower:
            hints.append("Mean calculation (per-case recommended):")
            hints.append("  case_means = {cid: df['col'].mean() for cid, df in signals.items()}")
            hints.append("  result = np.mean(list(case_means.values()))")
        
        if "ë¹„êµ" in query_lower or "ê·¸ë£¹" in query_lower or "ì„±ë³„" in query_lower:
            hints.append("Group comparison: use cohort to filter cases by group")
            # ë™ì  entity_col ì‚¬ìš© (í•˜ë“œì½”ë”© ì œê±°)
            hints.append(f"  target_cases = cohort[cohort['column'] == 'value']['{entity_col}'].astype(str).tolist()")
            hints.append("  filtered_signals = {cid: signals[cid] for cid in target_cases if cid in signals}")
        
        if "ìƒê´€" in query_lower or "correlation" in query_lower:
            hints.append("Correlation (per-case, then aggregate):")
            hints.append("  from scipy import stats")
            hints.append("  def case_corr(df):")
            hints.append("      clean = df[['col1', 'col2']].dropna()")
            hints.append("      if len(clean) < 3: return np.nan")
            hints.append("      r = stats.pearsonr(clean['col1'], clean['col2'])")
            hints.append("      return r.statistic  # Use .statistic, NOT tuple unpacking!")
            hints.append("  case_corrs = {cid: case_corr(df) for cid, df in signals.items()}")
            hints.append("  result = np.nanmean(list(case_corrs.values()))")
        
        if "ë¶„í¬" in query_lower or "distribution" in query_lower:
            hints.append("Distribution: compute per-case, then combine")
        
        # ë°ì´í„° êµ¬ì¡° íŒíŠ¸
        param_keys = data_summary.get("param_keys", [])
        if param_keys:
            hints.append(f"Available signal parameters: {param_keys[:5]}")
        
        return "\n".join(hints) if hints else None
    
    # =========================================================================
    # Utility
    # =========================================================================
    
    def get_data_context(self):
        """í˜„ì¬ DataContext ë°˜í™˜ (ë°ì´í„° ì¬ì‚¬ìš©ìš©)"""
        return self._data_context
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        from shared.data.context import DataContext
        DataContext.clear_cache()
        self._data_context = None
    
    def reset(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¦¬ì…‹"""
        self._extraction_agent = None
        self._data_context = None
        self._code_generator = None
        self._sandbox = None
        self._llm_client = None
    
    # =========================================================================
    # Map-Reduce Execution Mode
    # =========================================================================
    
    def run_mapreduce(
        self,
        query: str,
        batch_size: Optional[int] = None,
        max_workers: Optional[int] = None,
        max_retries: Optional[int] = None,
        timeout_seconds: Optional[int] = None,
        progress_callback: Optional[Callable[[int, int, int], None]] = None,
    ) -> OrchestrationResult:
        """
        Map-Reduce ëª¨ë“œë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
        
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ëŸ‰ì˜ ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        LLMì´ map_funcì™€ reduce_funcë¥¼ ìƒì„±í•˜ê³ ,
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ map_funcë¥¼ ì‹¤í–‰í•œ í›„ ìµœì¢… reduce_funcë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.
        
        Args:
            query: ìì—°ì–´ ì§ˆì˜
            batch_size: ë°°ì¹˜ë‹¹ ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ config.batch_size)
            max_workers: ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ config.mapreduce_max_workers)
            max_retries: ì½”ë“œ ìƒì„± ì¬ì‹œë„ íšŸìˆ˜
            timeout_seconds: ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ
            progress_callback: ì§„í–‰ ì½œë°± fn(batch_idx, total_batches, processed_count)
        
        Returns:
            OrchestrationResult
        
        Example:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
            config = OrchestratorConfig(
                execution_mode="mapreduce",
                max_signal_cases=0,  # ë¬´ì œí•œ
                batch_size=100,
            )
            orchestrator = Orchestrator(config=config)
            
            def on_progress(batch_idx, total, processed):
                print(f"Batch {batch_idx+1}/{total}: {processed} cases processed")
            
            result = orchestrator.run_mapreduce(
                "ëª¨ë“  í™˜ìì˜ SBP í‰ê· ì„ ë‹¨ì¼ float ê°’ìœ¼ë¡œ êµ¬í•´ì¤˜",
                progress_callback=on_progress
            )
        """
        start_time = time.time()
        
        batch_size = batch_size or self.config.batch_size
        max_workers = max_workers or self.config.mapreduce_max_workers
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        timeout = timeout_seconds if timeout_seconds is not None else self.config.timeout_seconds
        
        logger.info(f"ğŸš€ Starting Map-Reduce pipeline for query: '{query[:50]}...'")
        logger.info(f"   Settings: batch_size={batch_size}, max_workers={max_workers}")
        
        try:
            # Step 1: Extraction - ì‹¤í–‰ ê³„íš ìƒì„±
            logger.info("ğŸ“ Step 1/4: Running ExtractionAgent...")
            extraction_result = self._run_extraction(query)
            
            if not extraction_result.get("execution_plan"):
                logger.error("âŒ Extraction failed: No execution plan generated")
                return OrchestrationResult(
                    status="error",
                    error_message="Extraction failed: No execution plan generated",
                    error_stage="extraction",
                    extraction_plan=extraction_result,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            execution_plan = extraction_result["execution_plan"]
            extraction_confidence = extraction_result.get("confidence", 0.0)
            logger.info(f"âœ… Extraction complete (confidence: {extraction_confidence:.2f})")
            
            # Step 2: DataContext ì´ˆê¸°í™” ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
            logger.info("ğŸ“¦ Step 2/4: Initializing DataContext...")
            from shared.data.context import DataContext
            
            ctx = DataContext()
            ctx.load_from_plan(execution_plan, preload_cohort=True)
            self._data_context = ctx
            
            # ì¼€ì´ìŠ¤ ìˆ˜ í™•ì¸
            total_cases = len(ctx.get_available_case_ids())
            cohort = ctx.get_cohort()
            param_keys = ctx.get_available_parameters()
            
            logger.info(f"âœ… DataContext ready: {total_cases} cases, params: {param_keys[:5]}...")
            
            if total_cases == 0:
                logger.error("âŒ No cases available")
                return OrchestrationResult(
                    status="error",
                    error_message="No cases available for processing",
                    error_stage="data_load",
                    extraction_plan=execution_plan,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            # Step 3: Map-Reduce ì½”ë“œ ìƒì„±
            logger.info("ğŸ§¬ Step 3/4: Generating Map-Reduce code...")
            mapreduce_result = self._generate_mapreduce_code(
                query=query,
                ctx=ctx,
                cohort=cohort,
                param_keys=param_keys,
                total_cases=total_cases,
                max_retries=max_retries,
            )
            
            if not mapreduce_result["success"]:
                logger.error(f"âŒ Map-Reduce code generation failed: {mapreduce_result.get('error')}")
                return OrchestrationResult(
                    status="error",
                    error_message=f"Map-Reduce code generation failed: {mapreduce_result.get('error')}",
                    error_stage="code_generation",
                    extraction_plan=execution_plan,
                    generated_code=mapreduce_result.get("full_code"),
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            map_code = mapreduce_result["map_code"]
            reduce_code = mapreduce_result["reduce_code"]
            full_code = mapreduce_result["full_code"]
            
            logger.info("âœ… Map-Reduce code generated")
            
            # Step 4: Map-Reduce ì‹¤í–‰
            logger.info("âš¡ Step 4/4: Executing Map-Reduce...")
            execution_result = self._execute_mapreduce(
                ctx=ctx,
                cohort=cohort,
                map_code=map_code,
                reduce_code=reduce_code,
                param_keys=param_keys,
                batch_size=batch_size,
                max_workers=max_workers,
                timeout=timeout,
                progress_callback=progress_callback,
            )
            
            execution_time = (time.time() - start_time) * 1000
            
            if execution_result["success"]:
                logger.info(f"âœ… Map-Reduce completed successfully in {execution_time:.0f}ms")
                return OrchestrationResult(
                    status="success",
                    result=execution_result["result"],
                    generated_code=full_code,
                    extraction_plan=execution_plan,
                    extraction_confidence=extraction_confidence,
                    data_summary=DataSummary(
                        signals_count=total_cases,
                        total_rows=execution_result.get("total_rows", 0),
                        cohort_shape=cohort.shape if cohort is not None else None,
                        param_keys=param_keys,
                    ).model_dump(),
                    execution_time_ms=execution_time,
                    metadata={
                        "mode": "mapreduce",
                        "batch_size": batch_size,
                        "total_batches": execution_result.get("total_batches", 0),
                        "map_success_count": execution_result.get("map_success_count", 0),
                        "map_error_count": execution_result.get("map_error_count", 0),
                    }
                )
            else:
                logger.error(f"âŒ Map-Reduce execution failed: {execution_result.get('error')}")
                return OrchestrationResult(
                    status="error",
                    error_message=f"Map-Reduce execution failed: {execution_result.get('error')}",
                    error_stage="execution",
                    generated_code=full_code,
                    extraction_plan=execution_plan,
                    execution_time_ms=execution_time,
                    metadata={
                        "mode": "mapreduce",
                        "map_errors": execution_result.get("map_errors", []),
                    }
                )
                
        except Exception as e:
            logger.exception(f"âŒ Map-Reduce pipeline error: {e}")
            return OrchestrationResult(
                status="error",
                error_message=str(e),
                error_stage="unknown",
                execution_time_ms=(time.time() - start_time) * 1000
            )
    
    def _generate_mapreduce_code(
        self,
        query: str,
        ctx: Any,
        cohort: Any,
        param_keys: List[str],
        total_cases: int,
        max_retries: int = 2,
    ) -> Dict[str, Any]:
        """
        Map-Reduce ì½”ë“œ ìƒì„±
        
        LLMì„ í˜¸ì¶œí•˜ì—¬ map_funcì™€ reduce_func ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            {
                "success": bool,
                "map_code": str,
                "reduce_code": str,
                "full_code": str,
                "error": Optional[str]
            }
        """
        from AnalysisAgent.src.models.code_gen import MapReduceRequest
        from AnalysisAgent.src.code_gen.prompts import build_mapreduce_prompt
        
        # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ (ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤)
        entity_data_sample = None
        entity_data_dtypes = {}
        entity_data_columns = []
        
        sample_batch = None
        for batch in ctx.iter_cases_batch(batch_size=1, param_keys=param_keys):
            sample_batch = batch
            break
        
        # entity_id ì»¬ëŸ¼ ê²°ì •
        entity_id_column = ctx.entity_id_column or "id"
        
        # ìƒ˜í”Œ DataFrame ì¶”ì¶œ
        sample_df = None
        if sample_batch and sample_batch["signals"]:
            sample_entity_id = sample_batch["entity_ids"][0]
            sample_df = sample_batch["signals"][sample_entity_id]
        
        # AnalysisContextBuilderë¥¼ ì‚¬ìš©í•˜ì—¬ í’ë¶€í•œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        from shared.data.analysis_context import AnalysisContextBuilder
        context_builder = AnalysisContextBuilder(ctx)
        rich_context = context_builder.build_mapreduce_context(
            entity_sample=sample_df,
            cohort=cohort,
            total_cases=total_cases,
        )
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        entity_data_columns = rich_context["entity_data_columns"]
        entity_data_dtypes = rich_context["entity_data_dtypes"]
        entity_data_sample = rich_context["entity_data_sample"]
        metadata_columns = rich_context["metadata_columns"]
        metadata_dtypes = rich_context["metadata_dtypes"]
        metadata_sample = rich_context["metadata_sample"]
        dataset_description = rich_context["dataset_description"]
        
        # MapReduceRequest ìƒì„±
        request = MapReduceRequest(
            task_description=query,
            expected_output="The result format depends on the query",
            hints=self._generate_mapreduce_hints(query, param_keys, entity_data_columns),
            dataset_description=dataset_description,
            entity_id_column=entity_id_column,
            total_entities=total_cases,
            entity_data_columns=entity_data_columns,
            entity_data_dtypes=entity_data_dtypes,
            entity_data_sample=entity_data_sample,
            metadata_columns=metadata_columns,
            metadata_dtypes=metadata_dtypes,
            metadata_sample=metadata_sample,
        )
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt, user_prompt = build_mapreduce_prompt(request)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìœ ì € í”„ë¡¬í”„íŠ¸ ê²°í•©
        full_prompt = f"""[SYSTEM INSTRUCTIONS]
{system_prompt}

[USER REQUEST]
{user_prompt}"""
        
        # LLM í˜¸ì¶œ
        llm_client = self._get_llm_client()
        
        for attempt in range(max_retries + 1):
            try:
                logger.debug(f"Map-Reduce code generation attempt {attempt + 1}/{max_retries + 1}")
                
                response = llm_client.ask_text(full_prompt)
                
                # ì‘ë‹µ íŒŒì‹±
                parsed = self._parse_mapreduce_response(response)
                
                if parsed["success"]:
                    # ì½”ë“œ ê²€ì¦ (ì»¬ëŸ¼ ê²€ì¦ í¬í•¨)
                    validation_result = self._validate_mapreduce_code(
                        parsed["map_code"],
                        parsed["reduce_code"],
                        entity_data_columns=entity_data_columns,
                    )
                    
                    if validation_result["valid"]:
                        return {
                            "success": True,
                            "map_code": parsed["map_code"],
                            "reduce_code": parsed["reduce_code"],
                            "full_code": parsed["full_code"],
                        }
                    else:
                        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìˆ˜ì • ìš”ì²­
                        logger.warning(f"Map-Reduce code validation failed: {validation_result['errors']}")
                        if attempt < max_retries:
                            # ì—ëŸ¬ ìˆ˜ì • í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                            from AnalysisAgent.src.code_gen.prompts import build_mapreduce_error_fix_prompt
                            error_str = "\n".join(validation_result["errors"])
                            fix_prompt = build_mapreduce_error_fix_prompt(
                                previous_code=parsed["full_code"],
                                error_message=error_str,
                                request=request,
                                error_phase="validation",
                            )
                            full_prompt = f"{system_prompt}\n\n{fix_prompt}"
                            continue
                else:
                    logger.warning(f"Map-Reduce response parsing failed")
                    
            except Exception as e:
                logger.warning(f"Map-Reduce generation attempt {attempt + 1} failed: {e}")
                if attempt == max_retries:
                    return {"success": False, "error": str(e)}
        
        return {"success": False, "error": "Max retries exceeded"}
    
    def _parse_mapreduce_response(self, response: str) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µì—ì„œ map_funcì™€ reduce_func ì½”ë“œ ì¶”ì¶œ
        
        Expected format:
        ```python
        # MAP FUNCTION
        def map_func(entity_id, entity_data, metadata_row):
            ...
            return result
        
        # REDUCE FUNCTION
        def reduce_func(intermediate_results, full_metadata):
            ...
            return final_result
        ```
        """
        import re
        
        # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        code_blocks = re.findall(r'```(?:python)?\s*([\s\S]*?)```', response)
        
        if not code_blocks:
            return {"success": False, "error": "No code blocks found"}
        
        full_code = "\n\n".join(code_blocks)
        
        # map_func ì¶”ì¶œ
        map_match = re.search(
            r'def\s+map_func\s*\([^)]*\)\s*(?:->.*?)?:\s*([\s\S]*?)(?=\ndef\s+|\Z)',
            full_code
        )
        
        # reduce_func ì¶”ì¶œ
        reduce_match = re.search(
            r'def\s+reduce_func\s*\([^)]*\)\s*(?:->.*?)?:\s*([\s\S]*?)(?=\ndef\s+|\Z)',
            full_code
        )
        
        if not map_match or not reduce_match:
            return {"success": False, "error": "Could not find map_func or reduce_func"}
        
        # í•¨ìˆ˜ ì „ì²´ ì¶”ì¶œ
        map_func_match = re.search(
            r'(def\s+map_func\s*\([^)]*\)\s*(?:->.*?)?:[\s\S]*?)(?=\ndef\s+|\Z)',
            full_code
        )
        reduce_func_match = re.search(
            r'(def\s+reduce_func\s*\([^)]*\)\s*(?:->.*?)?:[\s\S]*?)(?=\ndef\s+|\Z)',
            full_code
        )
        
        map_code = map_func_match.group(1).strip() if map_func_match else ""
        reduce_code = reduce_func_match.group(1).strip() if reduce_func_match else ""
        
        return {
            "success": True,
            "map_code": map_code,
            "reduce_code": reduce_code,
            "full_code": full_code,
        }
    
    def _validate_mapreduce_code(
        self,
        map_code: str,
        reduce_code: str,
        entity_data_columns: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Map-Reduce ì½”ë“œ ê²€ì¦
        
        Args:
            map_code: map_func ì½”ë“œ
            reduce_code: reduce_func ì½”ë“œ
            entity_data_columns: ì‹¤ì œ entity_data DataFrame ì»¬ëŸ¼ ëª©ë¡ (ê²€ì¦ìš©)
        
        Returns:
            {"valid": bool, "errors": List[str], "warnings": List[str]}
        """
        import re
        
        errors = []
        warnings = []
        
        # ê¸°ë³¸ êµ¬ë¬¸ ê²€ì‚¬
        try:
            compile(map_code, "<map_func>", "exec")
        except SyntaxError as e:
            errors.append(f"map_func syntax error: {e}")
        
        try:
            compile(reduce_code, "<reduce_func>", "exec")
        except SyntaxError as e:
            errors.append(f"reduce_func syntax error: {e}")
        
        # í•„ìˆ˜ ìš”ì†Œ ê²€ì‚¬
        if "def map_func" not in map_code:
            errors.append("map_func definition not found")
        
        if "def reduce_func" not in reduce_code:
            errors.append("reduce_func definition not found")
        
        if "return" not in map_code:
            errors.append("map_func must have a return statement")
        
        if "return" not in reduce_code:
            errors.append("reduce_func must have a return statement")
        
        # ì»¬ëŸ¼ ì°¸ì¡° ê²€ì¦ (ì˜ëª»ëœ ì»¬ëŸ¼ ì ‘ê·¼ íŒ¨í„´ ê°ì§€)
        full_code = map_code + "\n" + reduce_code
        
        # í”íˆ ì˜ëª» ê°€ì •ë˜ëŠ” ì»¬ëŸ¼ëª… íŒ¨í„´ (ì¡´ì¬í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì»¬ëŸ¼)
        common_assumed_columns = ['Time', 'Timestamp', 'DateTime', 'time', 'timestamp', 'datetime', 'Value', 'value']
        
        for col in common_assumed_columns:
            # ë‹¤ì–‘í•œ ì»¬ëŸ¼ ì ‘ê·¼ íŒ¨í„´ ê°ì§€
            patterns = [
                rf"entity_data\[[\'\"]({col})[\'\"]\]",      # entity_data['Time']
                rf"entity_data\[\[.*[\'\"]({col})[\'\"]",    # entity_data[['Time']] ë˜ëŠ” entity_data[['A', 'Time']]
                rf"entity_data\.{col}\b",                    # entity_data.Time
                rf"\[[\'\"]({col})[\'\"]\]",                 # ['Time'] ì¼ë°˜ ìŠ¬ë¼ì´ì‹±
                rf"\.loc\[.*[\'\"]({col})[\'\"]",            # .loc[..., 'Time']
                rf"\.iloc\[.*[\'\"]({col})[\'\"]",           # .iloc with Time (shouldn't happen but check)
            ]
            for pattern in patterns:
                if re.search(pattern, full_code, re.IGNORECASE):
                    # entity_data_columnsê°€ ì œê³µë˜ì—ˆê³  í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ERROR
                    if entity_data_columns and col not in entity_data_columns and col.lower() not in [c.lower() for c in entity_data_columns]:
                        errors.append(
                            f"ğŸš¨ Code references '{col}' column which does NOT exist! "
                            f"Available columns: {entity_data_columns}. "
                            f"Use entity_data.index for row position or iloc[] for slicing."
                        )
                        break
        
        # set_index('Time') íŒ¨í„´ ê°ì§€
        if re.search(r"set_index\s*\(\s*['\"]Time['\"]", full_code, re.IGNORECASE):
            if entity_data_columns and 'Time' not in entity_data_columns:
                errors.append(
                    "ğŸš¨ Code uses set_index('Time') but 'Time' column does NOT exist! "
                    f"Available columns: {entity_data_columns}. "
                    "Use entity_data.index instead."
                )
        
        # resample() íŒ¨í„´ ê°ì§€ (Time indexê°€ í•„ìš”)
        if re.search(r"\.resample\s*\(", full_code):
            if entity_data_columns and 'Time' not in entity_data_columns:
                errors.append(
                    "ğŸš¨ Code uses .resample() which requires DatetimeIndex. "
                    f"Available columns: {entity_data_columns}. "
                    "Use iloc[] slicing for segmentation instead: entity_data.iloc[start:end]"
                )
        
        # ê²½ê³  ë¡œê¹…
        for w in warnings:
            logger.warning(f"âš ï¸ Code validation warning: {w}")
        
        # ì—ëŸ¬ ë¡œê¹…
        for e in errors:
            logger.error(f"âŒ Code validation error: {e}")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
        }
    
    def _execute_mapreduce(
        self,
        ctx: Any,
        cohort: Any,
        map_code: str,
        reduce_code: str,
        param_keys: List[str],
        batch_size: int,
        max_workers: int,
        timeout: int,
        progress_callback: Optional[Callable[[int, int, int], None]] = None,
    ) -> Dict[str, Any]:
        """
        Map-Reduce ì‹¤í–‰
        
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , map_funcë¥¼ ì‹¤í–‰í•œ í›„,
        ìµœì¢… reduce_funcë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.
        """
        import pandas as pd
        import numpy as np
        from typing import Any, List, Dict, Optional, Tuple
        
        # í•¨ìˆ˜ ì»´íŒŒì¼
        try:
            # ì•ˆì „í•œ ì‹¤í–‰ í™˜ê²½ êµ¬ì„±
            safe_globals = {
                "pd": pd,
                "np": np,
                "pandas": pd,
                "numpy": np,
                "math": __import__("math"),
                "datetime": __import__("datetime"),
                "statistics": __import__("statistics"),
                "collections": __import__("collections"),
                # typing ëª¨ë“ˆ ì¶”ê°€ (LLMì´ íƒ€ì… íŒíŠ¸ ì‚¬ìš© ì‹œ í•„ìš”)
                "Any": Any,
                "List": List,
                "Dict": Dict,
                "Optional": Optional,
                "Tuple": Tuple,
            }
            
            # scipy ì¶”ê°€ (ìˆìœ¼ë©´)
            try:
                import scipy.stats as stats
                safe_globals["stats"] = stats
                safe_globals["scipy"] = __import__("scipy")
            except ImportError:
                pass
            
            # map_func ì»´íŒŒì¼
            map_globals = safe_globals.copy()
            exec(map_code, map_globals)
            map_func = map_globals.get("map_func")
            
            if not callable(map_func):
                return {"success": False, "error": "map_func is not callable"}
            
            # reduce_func ì»´íŒŒì¼
            reduce_globals = safe_globals.copy()
            exec(reduce_code, reduce_globals)
            reduce_func = reduce_globals.get("reduce_func")
            
            if not callable(reduce_func):
                return {"success": False, "error": "reduce_func is not callable"}
                
        except Exception as e:
            return {"success": False, "error": f"Code compilation error: {e}"}
        
        # Map Phase
        intermediate_results = []
        map_errors = []
        total_rows = 0
        processed_count = 0
        total_batches = 0
        map_success_count = 0
        map_error_count = 0
        skipped_empty_count = 0
        skipped_no_signal_count = 0
        first_error_logged = False
        
        logger.info(f"ğŸ—ºï¸ Starting Map Phase (batch_size={batch_size})...")
        
        try:
            for batch in ctx.iter_cases_batch(
                batch_size=batch_size,
                param_keys=param_keys,
                apply_temporal=True,
                parallel=self.config.mapreduce_parallel,
                max_workers=max_workers,
            ):
                batch_idx = batch["batch_index"]
                total_batches = batch["total_batches"]
                
                logger.debug(f"  Processing batch {batch_idx + 1}/{total_batches} ({batch['batch_size']} cases)")
                
                # ë°°ì¹˜ ë‚´ map_func ì‹¤í–‰
                for entity_id in batch["entity_ids"]:
                    try:
                        entity_data = batch["signals"].get(entity_id)
                        
                        # ì‹ í˜¸ ë°ì´í„°ê°€ ì—†ëŠ” ì¼€ì´ìŠ¤ ê±´ë„ˆë›°ê¸° (ì •ìƒ ë™ì‘)
                        if entity_data is None:
                            skipped_no_signal_count += 1
                            continue
                        
                        if entity_data.empty:
                            skipped_empty_count += 1
                            continue
                        
                        # ìš”ì²­í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                        available_cols = list(entity_data.columns)
                        required_cols = [p for p in param_keys if p not in ['caseid', 'subjectid', 'id']]
                        missing_cols = [c for c in required_cols if c not in available_cols]
                        
                        if missing_cols and len(available_cols) <= 1:  # Timeë§Œ ìˆê±°ë‚˜ ì—†ëŠ” ê²½ìš°
                            skipped_no_signal_count += 1
                            continue
                        
                        total_rows += len(entity_data)
                        
                        # ë©”íƒ€ë°ì´í„° í–‰ ì¶”ì¶œ
                        metadata_row = ctx.get_batch_metadata_row(
                            batch["metadata_rows"],
                            entity_id
                        )
                        
                        # map_func í˜¸ì¶œ
                        result = map_func(entity_id, entity_data, metadata_row)
                        
                        if result is not None:
                            intermediate_results.append(result)
                            map_success_count += 1
                        
                    except Exception as e:
                        map_error_count += 1
                        error_str = str(e)
                        
                        # ì²« ë²ˆì§¸ ì—ëŸ¬ì—ì„œ ìƒì„¸ ì •ë³´ ì¶œë ¥
                        if not first_error_logged:
                            logger.warning(f"  âš ï¸ First map error for {entity_id}: {error_str}")
                            if entity_data is not None:
                                logger.warning(f"     entity_data columns: {list(entity_data.columns)}")
                                logger.warning(f"     entity_data shape: {entity_data.shape}")
                            first_error_logged = True
                        
                        map_errors.append({
                            "entity_id": entity_id,
                            "error": error_str,
                        })
                        
                        # ë™ì¼ ì—ëŸ¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                        if map_error_count >= 100:
                            logger.error(f"âŒ Too many map errors ({map_error_count}), stopping early")
                            break
                
                processed_count += batch["batch_size"]
                
                # ì§„í–‰ ì½œë°± í˜¸ì¶œ
                if progress_callback:
                    progress_callback(batch_idx, total_batches, processed_count)
                
                # ì—ëŸ¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¤‘ë‹¨
                if map_error_count >= 100:
                    break
                
                # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch
                gc.collect()
            
            logger.info(f"âœ… Map Phase complete:")
            logger.info(f"   Successes: {map_success_count}")
            logger.info(f"   Errors: {map_error_count}")
            logger.info(f"   Skipped (no signal): {skipped_no_signal_count}")
            logger.info(f"   Skipped (empty): {skipped_empty_count}")
            
        except Exception as e:
            return {"success": False, "error": f"Map Phase error: {e}", "map_errors": map_errors}
        
        # Reduce Phase
        logger.info(f"ğŸ“Š Starting Reduce Phase ({len(intermediate_results)} intermediate results)...")
        
        try:
            final_result = reduce_func(intermediate_results, cohort)
            
            logger.info("âœ… Reduce Phase complete")
            
            return {
                "success": True,
                "result": final_result,
                "total_rows": total_rows,
                "total_batches": total_batches,
                "map_success_count": map_success_count,
                "map_error_count": map_error_count,
                "map_errors": map_errors[:10],  # ì²˜ìŒ 10ê°œë§Œ
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Reduce Phase error: {e}",
                "map_errors": map_errors[:10],
            }
    
    def _generate_mapreduce_hints(
        self,
        query: str,
        param_keys: List[str],
        entity_data_columns: Optional[List[str]] = None,
    ) -> str:
        """
        Map-Reduceìš© íŒíŠ¸ ìƒì„± (ìµœì†Œí™”)
        
        ì² í•™: ì»¨í…ìŠ¤íŠ¸(ìŠ¤í‚¤ë§ˆ + ìƒ˜í”Œ ë°ì´í„°)ê°€ ì¶©ë¶„í•˜ë©´ LLMì´ ìŠ¤ìŠ¤ë¡œ ì¶”ë¡ .
        íŒíŠ¸ëŠ” ìµœì†Œí•œë§Œ ì œê³µí•˜ê³ , ì—ëŸ¬ ë°œìƒ ì‹œ ìˆ˜ì • í”„ë¡¬í”„íŠ¸ì—ì„œ êµ¬ì²´ì  ê°€ì´ë“œ ì œê³µ.
        """
        # íŒíŠ¸ ì—†ìŒ - ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ì— ì˜ì¡´
        # LLMì€ entity_data_sample, metadata_sampleì„ ë³´ê³  ìŠ¤ìŠ¤ë¡œ ì¶”ë¡ 
        return ""
    
    def _get_llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (Lazy init)"""
        if self._llm_client is None:
            from shared.llm.client import get_llm_client
            self._llm_client = get_llm_client()
        return self._llm_client
    
    # =========================================================================
    # Auto Mode Selection
    # =========================================================================
    
    def run_auto(
        self,
        query: str,
        max_retries: Optional[int] = None,
        timeout_seconds: Optional[int] = None,
        **kwargs,
    ) -> OrchestrationResult:
        """
        ìë™ ëª¨ë“œ ì„ íƒ ì‹¤í–‰
        
        ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ standard ë˜ëŠ” mapreduce ëª¨ë“œë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            query: ìì—°ì–´ ì§ˆì˜
            max_retries: ì½”ë“œ ìƒì„± ì¬ì‹œë„ íšŸìˆ˜
            timeout_seconds: ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ
            **kwargs: ëª¨ë“œë³„ ì¶”ê°€ ì¸ì (batch_size, max_workers ë“±)
        
        Returns:
            OrchestrationResult
        
        Example:
            config = OrchestratorConfig(
                execution_mode="auto",
                mapreduce_threshold=100,
            )
            orchestrator = Orchestrator(config=config)
            result = orchestrator.run_auto("ëª¨ë“  í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· ì„ ë‹¨ì¼ float ê°’ìœ¼ë¡œ êµ¬í•´ì¤˜")
        """
        # ë¨¼ì € Extractionìœ¼ë¡œ ì¼€ì´ìŠ¤ ìˆ˜ íŒŒì•…
        extraction_result = self._run_extraction(query)
        
        if not extraction_result.get("execution_plan"):
            return OrchestrationResult(
                status="error",
                error_message="Extraction failed",
                error_stage="extraction",
            )
        
        execution_plan = extraction_result["execution_plan"]
        
        # DataContextë¡œ ì¼€ì´ìŠ¤ ìˆ˜ í™•ì¸
        from shared.data.context import DataContext
        ctx = DataContext()
        ctx.load_from_plan(execution_plan, preload_cohort=True)
        
        total_cases = len(ctx.get_available_case_ids())
        threshold = self.config.mapreduce_threshold
        
        logger.info(f"ğŸ” Auto mode: {total_cases} cases (threshold: {threshold})")
        
        # ëª¨ë“œ ì„ íƒ
        if total_cases > threshold:
            logger.info(f"ğŸ“Š Selecting Map-Reduce mode (cases > threshold)")
            return self.run_mapreduce(
                query=query,
                max_retries=max_retries,
                timeout_seconds=timeout_seconds,
                **kwargs,
            )
        else:
            logger.info(f"âš¡ Selecting Standard mode (cases <= threshold)")
            return self.run(
                query=query,
                max_retries=max_retries,
                timeout_seconds=timeout_seconds,
            )

