"""ê²½ëŸ‰ Orchestrator - ExtractionAgent + DataContext + CodeGen ì—°ê²°

ì±…ì„:
- ExtractionAgent í˜¸ì¶œí•˜ì—¬ Execution Plan íšë“
- DataContextë¡œ ë°ì´í„° ë¡œë“œ
- AnalysisAgent(CodeGen)ë¡œ ë¶„ì„ ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
- ê²°ê³¼ í†µí•© ë° ë°˜í™˜
"""

import time
import logging
from typing import Dict, Any, Optional, Tuple

from .models import OrchestrationResult, DataSummary
from .config import OrchestratorConfig, DEFAULT_CONFIG

logger = logging.getLogger("OrchestrationAgent.orchestrator")


class Orchestrator:
    """
    ExtractionAgentì™€ AnalysisAgent(CodeGen)ë¥¼ ì—°ê²°í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    
    ì‚¬ìš©ë²•:
        orchestrator = Orchestrator()
        result = orchestrator.run("ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· ì„ êµ¬í•´ì¤˜")
        
        if result.status == "success":
            print(result.result)
            print(result.generated_code)
    
    ì‹¤í–‰ ëª¨ë“œ:
        1. run(query) - ì „ì²´ íŒŒì´í”„ë¼ì¸ (Extraction â†’ DataLoad â†’ Analysis)
        2. run_with_plan(query, plan) - Plan ìˆì„ ë•Œ (DataLoad â†’ Analysis)
        3. run_analysis_only(query, data) - ë°ì´í„° ìˆì„ ë•Œ (Analysisë§Œ)
    """
    
    def __init__(self, config: Optional[OrchestratorConfig] = None):
        """
        Args:
            config: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.config = config or DEFAULT_CONFIG
        
        # Lazy initialization
        self._extraction_agent = None
        self._data_context = None
        self._code_generator = None
        self._sandbox = None
        self._llm_client = None
    
    # =========================================================================
    # Public API
    # =========================================================================
    
    def run(
        self, 
        query: str,
        max_retries: Optional[int] = None,
        timeout_seconds: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ì§ˆì˜ ì‹¤í–‰ - ì „ì²´ íŒŒì´í”„ë¼ì¸
        
        Args:
            query: ìì—°ì–´ ì§ˆì˜
            max_retries: ì½”ë“œ ìƒì„± ì¬ì‹œë„ íšŸìˆ˜ (Noneì´ë©´ config ê°’)
            timeout_seconds: ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (Noneì´ë©´ config ê°’)
        
        Returns:
            OrchestrationResult
        
        Example:
            result = orchestrator.run("ìœ„ì•” í™˜ìì˜ ì‹¬ë°•ìˆ˜ í‰ê· ì„ ì„±ë³„ë¡œ ë¹„êµí•´ì¤˜")
        """
        start_time = time.time()
        
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        timeout = timeout_seconds if timeout_seconds is not None else self.config.timeout_seconds
        
        logger.info(f"ğŸš€ Starting pipeline for query: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        try:
            # Step 1: Extraction - ì‹¤í–‰ ê³„íš ìƒì„±
            logger.info("ğŸ“ Step 1/3: Running ExtractionAgent...")
            extraction_result = self._run_extraction(query)
            
            if not extraction_result.get("execution_plan"):
                logger.error("âŒ Extraction failed: No execution plan generated")
                return OrchestrationResult(
                    status="error",
                    error_message="Extraction failed: No execution plan generated",
                    error_stage="extraction",
                    extraction_plan=extraction_result,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            execution_plan = extraction_result["execution_plan"]
            extraction_confidence = extraction_result.get("confidence", 0.0)
            ambiguities = extraction_result.get("ambiguities", [])
            logger.info(f"âœ… Extraction complete (confidence: {extraction_confidence:.2f})")
            logger.debug(f"   Plan: {execution_plan}")
            
            # Step 2: Data Load - ë°ì´í„° ë¡œë“œ
            logger.info("ğŸ“¦ Step 2/3: Loading data via DataContext...")
            runtime_data, data_summary = self._load_data(execution_plan)
            
            if not runtime_data:
                logger.error("âŒ Data loading failed: No data available")
                return OrchestrationResult(
                    status="error",
                    error_message="Data loading failed: No data available",
                    error_stage="data_load",
                    extraction_plan=execution_plan,
                    extraction_confidence=extraction_confidence,
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            signals_count = len(runtime_data.get("signals", {}))
            total_rows = sum(len(df) for df in runtime_data.get("signals", {}).values())
            cohort_shape = runtime_data.get("cohort", {}).shape if hasattr(runtime_data.get("cohort", {}), "shape") else "N/A"
            logger.info(f"âœ… Data loaded (signals: {signals_count} cases, {total_rows} rows, cohort: {cohort_shape})")
            
            # Step 3: Analysis - ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
            logger.info("ğŸ§® Step 3/3: Running AnalysisAgent (CodeGen)...")
            analysis_result = self._run_analysis(
                query=query,
                runtime_data=runtime_data,
                data_summary=data_summary,
                max_retries=max_retries,
                timeout=timeout
            )
            
            execution_time = (time.time() - start_time) * 1000
            
            if analysis_result["success"]:
                logger.info(f"âœ… Analysis complete ({execution_time:.1f}ms, retries: {analysis_result.get('retry_count', 0)})")
            else:
                logger.error(f"âŒ Analysis failed: {analysis_result.get('error')}")
            
            return OrchestrationResult(
                status="success" if analysis_result["success"] else "error",
                result=analysis_result.get("result"),
                generated_code=analysis_result.get("code"),
                error_message=analysis_result.get("error"),
                error_stage="analysis" if not analysis_result["success"] else None,
                execution_time_ms=execution_time,
                data_summary=data_summary,
                extraction_plan=execution_plan,
                extraction_confidence=extraction_confidence,
                ambiguities=ambiguities,
                retry_count=analysis_result.get("retry_count", 0)
            )
        
        except Exception as e:
            logger.exception(f"âŒ Unexpected error: {e}")
            return OrchestrationResult(
                status="error",
                error_message=f"Unexpected error: {str(e)}",
                execution_time_ms=(time.time() - start_time) * 1000
            )
    
    def run_with_plan(
        self,
        query: str,
        execution_plan: Dict[str, Any],
        max_retries: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ì´ë¯¸ ìˆëŠ” Execution Planìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰ (ExtractionAgent ìŠ¤í‚µ)
        
        Args:
            query: ë¶„ì„ ì§ˆì˜
            execution_plan: ë¯¸ë¦¬ ìƒì„±ëœ ì‹¤í–‰ ê³„íš
            max_retries: ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            OrchestrationResult
        """
        start_time = time.time()
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        
        try:
            # Data Load
            runtime_data, data_summary = self._load_data(execution_plan)
            
            if not runtime_data:
                return OrchestrationResult(
                    status="error",
                    error_message="Data loading failed",
                    error_stage="data_load",
                    execution_time_ms=(time.time() - start_time) * 1000
                )
            
            # Analysis
            analysis_result = self._run_analysis(
                query=query,
                runtime_data=runtime_data,
                data_summary=data_summary,
                max_retries=max_retries
            )
            
            return OrchestrationResult(
                status="success" if analysis_result["success"] else "error",
                result=analysis_result.get("result"),
                generated_code=analysis_result.get("code"),
                error_message=analysis_result.get("error"),
                error_stage="analysis" if not analysis_result["success"] else None,
                execution_time_ms=(time.time() - start_time) * 1000,
                data_summary=data_summary,
                extraction_plan=execution_plan,
                retry_count=analysis_result.get("retry_count", 0)
            )
        
        except Exception as e:
            return OrchestrationResult(
                status="error",
                error_message=f"Unexpected error: {str(e)}",
                execution_time_ms=(time.time() - start_time) * 1000
            )
    
    def run_analysis_only(
        self,
        query: str,
        runtime_data: Dict[str, Any],
        max_retries: Optional[int] = None
    ) -> OrchestrationResult:
        """
        ë°ì´í„°ê°€ ì´ë¯¸ ìˆì„ ë•Œ ë¶„ì„ë§Œ ì‹¤í–‰ (Extraction + DataLoad ìŠ¤í‚µ)
        
        Args:
            query: ë¶„ì„ ì§ˆì˜
            runtime_data: ì´ë¯¸ ë¡œë“œëœ ë°ì´í„° {"df": ..., "cohort": ...}
            max_retries: ì¬ì‹œë„ íšŸìˆ˜
        
        Returns:
            OrchestrationResult
        
        Example:
            runtime_data = {
                "df": signals_df,
                "cohort": cohort_df,
                "case_ids": ["1", "2", "3"],
                "param_keys": ["HR", "SpO2"]
            }
            result = orchestrator.run_analysis_only("HR í‰ê·  êµ¬í•´ì¤˜", runtime_data)
        """
        start_time = time.time()
        max_retries = max_retries if max_retries is not None else self.config.max_retries
        
        logger.info(f"ğŸ§® Running analysis only for: '{query[:50]}{'...' if len(query) > 50 else ''}'")
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = self._create_data_summary(runtime_data)
        logger.debug(f"   Data summary: {data_summary}")
        
        # Analysis
        analysis_result = self._run_analysis(
            query=query,
            runtime_data=runtime_data,
            data_summary=data_summary,
            max_retries=max_retries
        )
        
        execution_time = (time.time() - start_time) * 1000
        
        if analysis_result["success"]:
            logger.info(f"âœ… Analysis complete ({execution_time:.1f}ms, retries: {analysis_result.get('retry_count', 0)})")
        else:
            logger.error(f"âŒ Analysis failed: {analysis_result.get('error')}")
        
        return OrchestrationResult(
            status="success" if analysis_result["success"] else "error",
            result=analysis_result.get("result"),
            generated_code=analysis_result.get("code"),
            error_message=analysis_result.get("error"),
            error_stage="analysis" if not analysis_result["success"] else None,
            execution_time_ms=execution_time,
            data_summary=data_summary,
            retry_count=analysis_result.get("retry_count", 0)
        )
    
    # =========================================================================
    # Step 1: Extraction
    # =========================================================================
    
    def _run_extraction(self, query: str) -> Dict[str, Any]:
        """ExtractionAgent í˜¸ì¶œí•˜ì—¬ Execution Plan ìƒì„±"""
        
        if self._extraction_agent is None:
            self._extraction_agent = self._create_extraction_agent()
        
        # ExtractionAgent ì‹¤í–‰
        result = self._extraction_agent.invoke({"user_query": query})
        
        # ê²°ê³¼ì—ì„œ plan ì¶”ì¶œ
        return {
            "execution_plan": result.get("validated_plan") or result.get("execution_plan"),
            "confidence": result.get("overall_confidence", 0.0),
            "ambiguities": result.get("ambiguities", []),
            "intent": result.get("intent")
        }
    
    def _create_extraction_agent(self):
        """ExtractionAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        import sys
        from pathlib import Path
        
        # ExtractionAgentë¥¼ sys.path ì•ì— ì¶”ê°€ (src.agents import ìœ„í•´)
        extraction_path = str(Path(__file__).parent.parent.parent / "ExtractionAgent")
        if extraction_path not in sys.path:
            sys.path.insert(0, extraction_path)
        
        # src.agentsë¡œ import (ExtractionAgent ë‚´ë¶€ import ê²½ë¡œì™€ ì¼ì¹˜)
        from src.agents.graph import build_agent
        return build_agent()
    
    # =========================================================================
    # Step 2: Data Load
    # =========================================================================
    
    def _load_data(
        self, 
        execution_plan: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """DataContextë¡œ ë°ì´í„° ë¡œë“œ (ì¼€ì´ìŠ¤ë³„ Dict í˜•íƒœ)
        
        Returns:
            (runtime_data, data_summary)
            
        runtime_data êµ¬ì¡°:
            - signals: Dict[caseid, DataFrame] - ì¼€ì´ìŠ¤ë³„ ì‹œê³„ì—´ ë°ì´í„°
            - cohort: DataFrame - ì „ì²´ ë©”íƒ€ë°ì´í„°
            - case_ids: List[str] - ë¡œë“œëœ ì¼€ì´ìŠ¤ ID
            - total_cases: int - ì „ì²´ ì¼€ì´ìŠ¤ ìˆ˜
            - _access_guide: str - LLMìš© ë™ì  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ
        """
        from shared.data.context import DataContext
        
        ctx = DataContext()
        ctx.load_from_plan(execution_plan, preload_cohort=self.config.preload_cohort)
        
        # runtime_data êµ¬ì„±
        runtime_data = {}
        
        # Cohort (ì „ì²´)
        cohort = ctx.get_cohort()
        if cohort is not None and not cohort.empty:
            runtime_data["cohort"] = cohort
        
        # Signals - Dict[caseid, DataFrame] í˜•íƒœë¡œ!
        max_cases = self.config.max_signal_cases if self.config.max_signal_cases > 0 else None
        signals_dict = ctx.get_signals_dict(max_cases=max_cases)
        if signals_dict:
            runtime_data["signals"] = signals_dict
        
        # ë©”íƒ€ë°ì´í„°
        runtime_data["case_ids"] = list(signals_dict.keys()) if signals_dict else []
        runtime_data["total_cases"] = len(ctx.get_case_ids())
        runtime_data["param_keys"] = ctx.get_available_parameters()
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ìƒì„± (LLM í”„ë¡¬í”„íŠ¸ìš©)
        access_guide = ctx.generate_access_guide(signals_dict, cohort)
        runtime_data["_access_guide"] = access_guide
        
        # ìš”ì•½ ìƒì„±
        data_summary = {
            "signals_count": len(signals_dict) if signals_dict else 0,
            "total_cases": runtime_data["total_cases"],
            "cohort_shape": cohort.shape if cohort is not None and not cohort.empty else None,
            "param_keys": runtime_data["param_keys"],
            "loaded_case_ids": runtime_data["case_ids"][:10],  # ìƒ˜í”Œ
        }
        
        # DataContext ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
        self._data_context = ctx
        
        return runtime_data, data_summary
    
    def _create_data_summary(self, runtime_data: Dict[str, Any]) -> Dict[str, Any]:
        """runtime_dataì—ì„œ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        if "signals" in runtime_data:
            signals_dict = runtime_data["signals"]
            if signals_dict:
                sample_cid = list(signals_dict.keys())[0]
                sample_df = signals_dict[sample_cid]
                summary["signals"] = {
                    "case_count": len(signals_dict),
                    "total_rows": sum(len(df) for df in signals_dict.values()),
                    "sample_shape": sample_df.shape,
                    "columns": list(sample_df.columns)
                }
        
        if "cohort" in runtime_data:
            cohort = runtime_data["cohort"]
            summary["cohort"] = {
                "shape": cohort.shape,
                "columns": list(cohort.columns)
            }
        
        summary["case_count"] = len(runtime_data.get("case_ids", []))
        summary["total_cases"] = runtime_data.get("total_cases", 0)
        summary["param_keys"] = runtime_data.get("param_keys", [])
        
        return summary
    
    # =========================================================================
    # Step 3: Analysis (CodeGen)
    # =========================================================================
    
    def _run_analysis(
        self,
        query: str,
        runtime_data: Dict[str, Any],
        data_summary: Dict[str, Any],
        max_retries: int = 2,
        timeout: int = 30
    ) -> Dict[str, Any]:
        """CodeGeneratorë¡œ ë¶„ì„ ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
        
        Returns:
            {"success": bool, "result": Any, "code": str, "error": str, "retry_count": int}
        """
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if self._code_generator is None:
            self._init_code_gen_components(timeout)
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ì¶”ì¶œ (LLM í”„ë¡¬í”„íŠ¸ìš©)
        access_guide = runtime_data.pop("_access_guide", None)
        
        # ExecutionContext ìƒì„±
        exec_context = self._build_execution_context(runtime_data, data_summary)
        
        # CodeRequest ìƒì„± (ë™ì  ê°€ì´ë“œ í¬í•¨)
        request = self._build_code_request(query, exec_context, data_summary, access_guide)
        
        # ìƒì„± + ì‹¤í–‰ (with retry)
        last_error = None
        generated_code = None
        
        for attempt in range(max_retries + 1):
            # ì²« ì‹œë„ ë˜ëŠ” ì¬ì‹œë„
            if attempt == 0:
                gen_result = self._code_generator.generate(request)
            else:
                gen_result = self._code_generator.generate_with_fix(
                    request, 
                    generated_code, 
                    last_error
                )
            
            generated_code = gen_result.code
            
            # ê²€ì¦ ì‹¤íŒ¨
            if not gen_result.is_valid:
                last_error = f"Validation failed: {gen_result.validation_errors}"
                continue
            
            # ì‹¤í–‰
            exec_result = self._sandbox.execute(gen_result.code, runtime_data)
            
            if exec_result.success:
                return {
                    "success": True,
                    "result": exec_result.result,
                    "code": gen_result.code,
                    "retry_count": attempt
                }
            
            last_error = exec_result.error
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {
            "success": False,
            "error": last_error,
            "code": generated_code,
            "retry_count": max_retries + 1
        }
    
    def _init_code_gen_components(self, timeout: int = 30):
        """CodeGeneratorì™€ Sandbox ì´ˆê¸°í™”"""
        from AnalysisAgent.src.code_gen import CodeGenerator, SandboxExecutor
        from shared.llm import get_llm_client
        
        if self._llm_client is None:
            self._llm_client = get_llm_client()
        
        self._code_generator = CodeGenerator(llm_client=self._llm_client)
        self._sandbox = SandboxExecutor(timeout_seconds=timeout)
    
    def _build_execution_context(
        self, 
        runtime_data: Dict[str, Any],
        data_summary: Dict[str, Any]
    ):
        """CodeGenìš© ExecutionContext ìƒì„±"""
        from AnalysisAgent.src.models import ExecutionContext
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜ ì„¤ëª…
        available_variables = {}
        
        # signals: Dict[caseid, DataFrame]
        if "signals" in runtime_data and runtime_data["signals"]:
            signals_dict = runtime_data["signals"]
            case_count = len(signals_dict)
            sample_cid = list(signals_dict.keys())[0]
            sample_df = signals_dict[sample_cid]
            cols = list(sample_df.columns)[:10]
            cols_str = str(cols) + ("..." if len(sample_df.columns) > 10 else "")
            available_variables["signals"] = (
                f"Dict[caseid, DataFrame] - ì¼€ì´ìŠ¤ë³„ ì‹œê³„ì—´ ë°ì´í„°, "
                f"{case_count} cases, columns: {cols_str}"
            )
        
        if "cohort" in runtime_data:
            cohort = runtime_data["cohort"]
            cols = list(cohort.columns)[:10]
            cols_str = str(cols) + ("..." if len(cohort.columns) > 10 else "")
            available_variables["cohort"] = (
                f"pandas DataFrame - Cohort ë©”íƒ€ë°ì´í„°, "
                f"shape: {cohort.shape}, columns: {cols_str}"
            )
        
        case_ids = runtime_data.get("case_ids", [])
        available_variables["case_ids"] = f"List[str] - {len(case_ids)}ê°œ ë¡œë“œëœ ì¼€ì´ìŠ¤ ID"
        
        total_cases = runtime_data.get("total_cases", len(case_ids))
        available_variables["total_cases"] = f"int - ì „ì²´ ì¼€ì´ìŠ¤ ìˆ˜: {total_cases}"
        
        param_keys = runtime_data.get("param_keys", [])
        available_variables["param_keys"] = f"List[str] - íŒŒë¼ë¯¸í„° í‚¤: {param_keys}"
        
        # ìƒ˜í”Œ ë°ì´í„° (LLM ì°¸ê³ ìš©)
        sample_data = {}
        if "signals" in runtime_data and runtime_data["signals"]:
            signals_dict = runtime_data["signals"]
            sample_cid = list(signals_dict.keys())[0]
            sample_df = signals_dict[sample_cid].head(3)
            sample_data["signals_sample"] = {
                "caseid": sample_cid,
                "data": sample_df.round(4).to_dict(orient="records")
            }
        
        if "cohort" in runtime_data and not runtime_data["cohort"].empty:
            sample_cohort = runtime_data["cohort"].head(3)
            sample_data["cohort_head"] = sample_cohort.to_dict(orient="records")
        
        return ExecutionContext(
            available_variables=available_variables,
            sample_data=sample_data if sample_data else None
        )
    
    def _build_code_request(
        self, 
        query: str,
        exec_context,
        data_summary: Dict[str, Any],
        access_guide: Optional[str] = None
    ):
        """CodeRequest ìƒì„±"""
        from AnalysisAgent.src.models import CodeRequest
        
        # ë™ì  ì ‘ê·¼ ê°€ì´ë“œ + ê¸°ì¡´ íŒíŠ¸ ê²°í•©
        hints_parts = []
        
        # 1. ë™ì  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ (ìš°ì„ )
        if access_guide:
            hints_parts.append(access_guide)
        
        # 2. ì§ˆì˜ ê¸°ë°˜ ì¶”ê°€ íŒíŠ¸
        if self.config.generate_hints:
            additional_hints = self._generate_hints(query, data_summary)
            if additional_hints:
                hints_parts.append("\n## Additional Hints\n" + additional_hints)
        
        hints = "\n".join(hints_parts) if hints_parts else None
        
        return CodeRequest(
            task_description=query,
            expected_output="Assign final result to `result` variable. Can be number, dict, or list.",
            execution_context=exec_context,
            hints=hints,
            constraints=[
                "Handle NaN with dropna() or fillna()",
                "Must assign final result to `result` variable",
                "For case-level statistics: compute per-case first, then aggregate",
                "Use signals[caseid] to access individual case DataFrame"
            ]
        )
    
    def _generate_hints(self, query: str, data_summary: Dict[str, Any]) -> Optional[str]:
        """ì§ˆì˜ ê¸°ë°˜ ì¶”ê°€ íŒíŠ¸ ìƒì„± (ë™ì  ê°€ì´ë“œ ë³´ì™„ìš©)"""
        hints = []
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ íŒíŠ¸ (signals Dict ê¸°ë°˜)
        if "í‰ê· " in query_lower or "mean" in query_lower:
            hints.append("Mean calculation (per-case recommended):")
            hints.append("  case_means = {cid: df['col'].mean() for cid, df in signals.items()}")
            hints.append("  result = np.mean(list(case_means.values()))")
        
        if "ë¹„êµ" in query_lower or "ê·¸ë£¹" in query_lower or "ì„±ë³„" in query_lower:
            hints.append("Group comparison: use cohort to filter cases by group")
            hints.append("  male_cases = cohort[cohort['sex'] == 'M']['caseid'].astype(str).tolist()")
            hints.append("  male_signals = {cid: signals[cid] for cid in male_cases if cid in signals}")
        
        if "ìƒê´€" in query_lower or "correlation" in query_lower:
            hints.append("Correlation (per-case, then aggregate):")
            hints.append("  from scipy import stats")
            hints.append("  def case_corr(df):")
            hints.append("      clean = df[['col1', 'col2']].dropna()")
            hints.append("      if len(clean) < 3: return np.nan")
            hints.append("      r = stats.pearsonr(clean['col1'], clean['col2'])")
            hints.append("      return r.statistic  # Use .statistic, NOT tuple unpacking!")
            hints.append("  case_corrs = {cid: case_corr(df) for cid, df in signals.items()}")
            hints.append("  result = np.nanmean(list(case_corrs.values()))")
        
        if "ë¶„í¬" in query_lower or "distribution" in query_lower:
            hints.append("Distribution: compute per-case, then combine")
        
        # ë°ì´í„° êµ¬ì¡° íŒíŠ¸
        param_keys = data_summary.get("param_keys", [])
        if param_keys:
            hints.append(f"Available signal parameters: {param_keys[:5]}")
        
        return "\n".join(hints) if hints else None
    
    # =========================================================================
    # Utility
    # =========================================================================
    
    def get_data_context(self):
        """í˜„ì¬ DataContext ë°˜í™˜ (ë°ì´í„° ì¬ì‚¬ìš©ìš©)"""
        return self._data_context
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        from shared.data.context import DataContext
        DataContext.clear_cache()
        self._data_context = None
    
    def reset(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¦¬ì…‹"""
        self._extraction_agent = None
        self._data_context = None
        self._code_generator = None
        self._sandbox = None
        self._llm_client = None

