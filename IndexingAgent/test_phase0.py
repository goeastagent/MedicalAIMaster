#!/usr/bin/env python
"""
Phase 0 í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (LangGraph ê¸°ë°˜)

Data Catalog ìŠ¤í‚¤ë§ˆ ìƒì„± ë° íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ/ì €ì¥ í…ŒìŠ¤íŠ¸
LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ phase0_catalog ë…¸ë“œë§Œ ì‹¤í–‰
"""

import sys
import os
import glob
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from agents.state import AgentState
from agents.nodes.catalog import phase0_catalog_node, get_catalog_stats
from database.schema_catalog import CatalogSchemaManager, init_catalog_schema


def build_phase0_only_agent(checkpointer=None):
    """
    Phase 0ë§Œ ì‹¤í–‰í•˜ëŠ” LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ
    
    Graph:
        START â†’ phase0_catalog â†’ END
    
    Returns:
        ì»´íŒŒì¼ëœ LangGraph ì›Œí¬í”Œë¡œìš°
    """
    workflow = StateGraph(AgentState)
    
    # Phase 0 ë…¸ë“œë§Œ ì¶”ê°€
    workflow.add_node("phase0_catalog", phase0_catalog_node)
    
    # Entry Point & Exit
    workflow.set_entry_point("phase0_catalog")
    workflow.add_edge("phase0_catalog", END)
    
    # Compile
    compile_config = {}
    if checkpointer:
        compile_config["checkpointer"] = checkpointer
    
    return workflow.compile(**compile_config)


def get_input_files():
    """
    test_agent_with_interrupt.pyì™€ ë™ì¼í•œ íŒŒì¼ ëª©ë¡ ë°˜í™˜
    
    Returns:
        tuple: (all_files, vital_csv_files, vital_signal_files, inspire_files)
    """
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼
    inspire_files = sorted(glob.glob(str(data_dir / "INSPIRE_130K_1.3/*.csv")))
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # VitalDB ë°ì´í„°ë§Œ ì²˜ë¦¬ (CSV + Signal ì²˜ìŒ 2ê°œ)
    all_files = vital_csv_files + vital_signal_files[:2]
    
    return all_files, vital_csv_files, vital_signal_files, inspire_files


def test_schema_creation():
    """ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("1. Testing Schema Creation")
    print("=" * 60)
    
    schema_manager = init_catalog_schema(reset=True)
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    assert schema_manager.table_exists('file_catalog'), "file_catalog table not created"
    assert schema_manager.table_exists('column_metadata'), "column_metadata table not created"
    
    print("âœ“ Tables created successfully")
    print()


def test_show_table_schema():
    """DB í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì „ì²´ ì¶œë ¥"""
    print("=" * 60)
    print("2. Database Table Schema")
    print("=" * 60)
    
    from database.connection import get_db_manager
    
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    tables = ['file_catalog', 'column_metadata']
    
    for table_name in tables:
        print(f"\nğŸ“‹ Table: {table_name}")
        print("-" * 50)
        
        # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT 
                column_name,
                data_type,
                character_maximum_length,
                is_nullable,
                column_default
            FROM information_schema.columns
            WHERE table_name = %s
            ORDER BY ordinal_position
        """, (table_name,))
        
        columns = cursor.fetchall()
        
        # í—¤ë”
        print(f"{'Column':<25} {'Type':<20} {'Nullable':<10} {'Default'}")
        print("-" * 80)
        
        for col in columns:
            col_name = col[0]
            data_type = col[1]
            max_len = col[2]
            nullable = col[3]
            default = col[4]
            
            # íƒ€ì… í¬ë§·íŒ…
            if max_len:
                type_str = f"{data_type}({max_len})"
            else:
                type_str = data_type
            
            # Default ê°’ ê°„ì†Œí™”
            if default:
                if 'nextval' in str(default):
                    default_str = 'SERIAL'
                elif len(str(default)) > 20:
                    default_str = str(default)[:17] + "..."
                else:
                    default_str = str(default)
            else:
                default_str = ""
            
            print(f"{col_name:<25} {type_str:<20} {nullable:<10} {default_str}")
        
        # ì¸ë±ìŠ¤ ì¡°íšŒ
        cursor.execute("""
            SELECT indexname, indexdef
            FROM pg_indexes
            WHERE tablename = %s
        """, (table_name,))
        
        indexes = cursor.fetchall()
        if indexes:
            print(f"\n  Indexes:")
            for idx in indexes:
                print(f"    - {idx[0]}")
    
    print()


def test_phase0_workflow(all_files: list):
    """
    LangGraphë¥¼ ì‚¬ìš©í•œ Phase 0 ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    """
    print("=" * 60)
    print("3. Running Phase 0 via LangGraph Workflow")
    print("=" * 60)
    
    print(f"\nğŸ“ Input Files: {len(all_files)}ê°œ")
    for f in all_files:
        print(f"   - {os.path.basename(f)}")
    print()
    
    # LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ
    memory = MemorySaver()
    agent = build_phase0_only_agent(checkpointer=memory)
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        # Phase 0 í•„ìˆ˜ í•„ë“œ
        "input_files": all_files,
        "phase0_result": None,
        "phase0_file_ids": [],
        
        # ê¸°íƒ€ í•„ë“œ (AgentState í˜¸í™˜)
        "current_dataset_id": "test_dataset",
        "current_table_name": None,
        "data_catalog": {},
        "classification_result": None,
        "processing_progress": {
            "phase": "phase0",
            "metadata_processed": [],
            "data_processed": [],
            "current_file": None,
            "current_file_index": 0,
            "total_files": len(all_files)
        },
        "file_path": "",
        "file_type": None,
        "raw_metadata": {},
        "entity_identification": None,
        "finalized_schema": [],
        "entity_understanding": None,
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "review_type": None,
        "conversation_history": {},
        "logs": [],
        "ontology_context": {},
        "skip_indexing": False,
        "retry_count": 0,
        "error_message": None,
        "project_context": {}
    }
    
    # Thread ì„¤ì •
    thread_config = {"configurable": {"thread_id": "phase0-test-1"}}
    
    print("â–¶ï¸  LangGraph Phase 0 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...\n")
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    final_state = None
    for event in agent.stream(initial_state, thread_config, stream_mode="values"):
        # ë¡œê·¸ ì¶œë ¥
        if "logs" in event and event["logs"]:
            for log in event["logs"]:
                if not final_state or log not in final_state.get("logs", []):
                    print(f"ğŸ“ {log}")
        final_state = event
    
    # ê²°ê³¼ ì¶œë ¥
    phase0_result = final_state.get("phase0_result", {}) if final_state else {}
    file_ids = final_state.get("phase0_file_ids", []) if final_state else []
    
    print()
    print(f"ğŸ“Š Phase 0 Result (via LangGraph):")
    print(f"   Total: {phase0_result.get('total_files', 0)}")
    print(f"   Processed: {phase0_result.get('processed_files', 0)}")
    print(f"   Skipped: {phase0_result.get('skipped_files', 0)}")
    print(f"   Failed: {phase0_result.get('failed_files', 0)}")
    print(f"   Success Rate: {phase0_result.get('success_rate', 'N/A')}")
    
    # File IDs (stateì— ì €ì¥ëœ ê°’)
    if file_ids:
        print(f"\nğŸ“‹ File IDs in State ({len(file_ids)}ê°œ):")
        for fid in file_ids:
            print(f"   - {fid}")
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ìƒì„¸
    results = phase0_result.get('results', [])
    failed = [r for r in results if not r.get('success', False)]
    if failed:
        print(f"\nâŒ Failed Files:")
        for r in failed:
            print(f"   - {os.path.basename(r.get('file_path', 'unknown'))}: {r.get('error', 'unknown')}")
    
    print()
    return phase0_result


def test_catalog_stats():
    """ì¹´íƒˆë¡œê·¸ í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("4. Catalog Statistics")
    print("=" * 60)
    
    stats = get_catalog_stats()
    
    print(f"Total Files: {stats.get('total_files', 0)}")
    print(f"Files by Type: {stats.get('files_by_type', {})}")
    print(f"Total Columns: {stats.get('total_columns', 0)}")
    print(f"Columns by Type: {stats.get('columns_by_type', {})}")
    print()


def test_query_catalog():
    """ì¹´íƒˆë¡œê·¸ ì „ì²´ ë°ì´í„° ì¶œë ¥"""
    print("=" * 60)
    print("5. Full Catalog Data")
    print("=" * 60)
    
    from database.connection import get_db_manager
    
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    # =========================================================================
    # file_catalog ì „ì²´ ì¶œë ¥ (ëª¨ë“  ì»¬ëŸ¼)
    # =========================================================================
    cursor.execute("""
        SELECT 
            file_id,
            file_path,
            file_name,
            file_extension,
            file_size_bytes,
            file_size_mb,
            file_modified_at,
            processor_type,
            is_text_readable,
            semantic_type,
            file_metadata,
            LENGTH(raw_stats::text) as raw_stats_size,
            created_at
        FROM file_catalog
        ORDER BY file_id
    """)
    
    files = cursor.fetchall()
    print(f"\nğŸ“‚ file_catalog ({len(files)} rows) - ALL COLUMNS:")
    print("=" * 140)
    
    for row in files:
        (file_id, file_path, file_name, file_extension, file_size_bytes, file_size_mb,
         file_modified_at, processor_type, is_text_readable, semantic_type, file_metadata, 
         raw_stats_size, created_at) = row
        
        short_id = str(file_id)[:8]  # UUID ì• 8ìë¦¬
        print(f"\nâ”Œâ”€ [{short_id}] {file_name}")
        print(f"â”‚  file_path:        {file_path}")
        print(f"â”‚  file_extension:   {file_extension}")
        print(f"â”‚  file_size_bytes:  {file_size_bytes:,}")
        print(f"â”‚  file_size_mb:     {file_size_mb}")
        print(f"â”‚  file_modified_at: {file_modified_at}")
        print(f"â”‚  processor_type:   {processor_type}")
        print(f"â”‚  is_text_readable: {is_text_readable}")
        print(f"â”‚  semantic_type:    {semantic_type or '(null)'}")
        print(f"â”‚  created_at:       {created_at}")
        print(f"â”‚  raw_stats_size:   {raw_stats_size:,} bytes")
        print(f"â”‚  file_metadata:")
        if file_metadata:
            import json
            for key, value in file_metadata.items():
                # ê¸´ ë¦¬ìŠ¤íŠ¸ëŠ” ì¶•ì•½
                if isinstance(value, list) and len(value) > 5:
                    value_str = f"[{', '.join(map(str, value[:3]))} ... ({len(value)} items)]"
                elif isinstance(value, dict) and len(value) > 3:
                    value_str = f"{{...}} ({len(value)} keys)"
                else:
                    value_str = str(value)
                print(f"â”‚      {key}: {value_str}")
        print(f"â””{'â”€' * 80}")
    print()
    
    # =========================================================================
    # column_metadata ì „ì²´ ì¶œë ¥ (íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”)
    # =========================================================================
    cursor.execute("""
        SELECT fc.file_id, fc.file_name, cm.col_id, cm.original_name, cm.column_type, 
               cm.data_type, cm.column_info->>'null_ratio' as null_ratio,
               cm.column_info->>'unit' as unit,
               cm.column_info->>'sample_rate' as sample_rate
        FROM column_metadata cm
        JOIN file_catalog fc ON cm.file_id = fc.file_id
        ORDER BY fc.file_id, cm.col_id
    """)
    
    columns = cursor.fetchall()
    print(f"ğŸ“Š column_metadata ({len(columns)} rows):")
    print("=" * 120)
    
    current_file = None
    for row in columns:
        file_id, file_name, col_id, col_name, col_type, dtype, null_ratio, unit, sample_rate = row
        
        # íŒŒì¼ì´ ë°”ë€Œë©´ í—¤ë” ì¶œë ¥
        if current_file != file_id:
            if current_file is not None:
                print()  # ì´ì „ íŒŒì¼ê³¼ êµ¬ë¶„
            print(f"\nğŸ“ [{file_id}] {file_name}")
            print("-" * 100)
            print(f"  {'ID':<4} {'Column Name':<30} {'Type':<15} {'Dtype':<15} {'Null%':<8} {'Unit':<10} {'SRate'}")
            print("  " + "-" * 95)
            current_file = file_id
        
        # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥
        null_str = f"{float(null_ratio):.1%}" if null_ratio else "-"
        unit_str = unit if unit else "-"
        srate_str = sample_rate if sample_rate else "-"
        print(f"  {col_id:<4} {col_name:<30} {col_type:<15} {dtype:<15} {null_str:<8} {unit_str:<10} {srate_str}")
    
    print()


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("Phase 0 Data Catalog Test (LangGraph ê¸°ë°˜)")
    print("(Using same input files as test_agent_with_interrupt.py)")
    print("=" * 60 + "\n")
    
    # ì…ë ¥ íŒŒì¼ ë¡œë“œ (test_agent_with_interrupt.pyì™€ ë™ì¼)
    all_files, vital_csv_files, vital_signal_files, inspire_files = get_input_files()
    
    print(f"ğŸ“ Found files:")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ (using first 2)")
    print(f"   ğŸ“‹ INSPIRE CSV: {len(inspire_files)}ê°œ (not used)")
    print(f"   â¡ï¸  Total to process: {len(all_files)}ê°œ")
    print()
    
    if not all_files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return 1
    
    try:
        # 1. ìŠ¤í‚¤ë§ˆ ìƒì„±
        test_schema_creation()
        
        # 2. ìŠ¤í‚¤ë§ˆ í™•ì¸
        test_show_table_schema()
        
        # 3. LangGraph ì›Œí¬í”Œë¡œìš°ë¡œ Phase 0 ì‹¤í–‰
        test_phase0_workflow(all_files)
        
        # 4. í†µê³„ ì¡°íšŒ
        test_catalog_stats()
        
        # 5. ì „ì²´ ë°ì´í„° ì¶œë ¥
        test_query_catalog()
        
        print("=" * 60)
        print("âœ… All tests completed!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
