#!/usr/bin/env python3
# build_vector_db.py
"""
VectorDB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì˜¨í†¨ë¡œì§€ íŒŒì¼ì„ ì½ì–´ì„œ ChromaDBì— ì„ë² ë”© ìƒì„±
"""

import sys
import os

# â­ .env íŒŒì¼ ë¡œë“œ (OPENAI_API_KEY ë“±)
from dotenv import load_dotenv
load_dotenv()

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from utils.ontology_manager import get_ontology_manager
from knowledge.vector_store import VectorStore
from config import EmbeddingConfig, LLMConfig


def main():
    """VectorDB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("ğŸš€ VectorDB êµ¬ì¶• ì‹œì‘")
    print("="*80)
    
    # 1. ì˜¨í†¨ë¡œì§€ ë¡œë“œ
    print("\nğŸ“š [Step 1] ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì¤‘...")
    ontology_mgr = get_ontology_manager()
    ontology = ontology_mgr.load()
    
    if not ontology or not ontology.get("definitions"):
        print("âŒ ì˜¨í†¨ë¡œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € test_agent_with_interrupt.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print(f"âœ… ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì™„ë£Œ")
    print(f"   - ìš©ì–´: {len(ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(ontology.get('hierarchy', []))}ê°œ")
    print(f"   - íŒŒì¼: {len(ontology.get('file_tags', {}))}ê°œ")
    
    # 2. VectorDB ì´ˆê¸°í™”
    print("\nğŸ”§ [Step 2] VectorDB ì´ˆê¸°í™” ì¤‘...")
    
    vector_store = VectorStore()
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ (configì—ì„œ ê¸°ë³¸ê°’)
    print(f"\nğŸ“‹ [Config] í˜„ì¬ ì„¤ì •:")
    print(f"   - Provider: {EmbeddingConfig.PROVIDER}")
    print(f"   - OpenAI Model: {EmbeddingConfig.OPENAI_MODEL}")
    print(f"   - Local Model: {EmbeddingConfig.LOCAL_MODEL}")
    
    print("\nì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
    print(f"  1. OpenAI ({EmbeddingConfig.OPENAI_MODEL})")
    print(f"  2. Local ({EmbeddingConfig.LOCAL_MODEL})")
    print(f"  Enter. Config ê¸°ë³¸ê°’ ì‚¬ìš© ({EmbeddingConfig.PROVIDER})")
    
    choice = input("\nì„ íƒ (1, 2, Enter): ").strip()
    
    if choice == "2":
        embedding_model = "local"
        print(f"âœ… Local ëª¨ë¸ ì‚¬ìš© ({EmbeddingConfig.LOCAL_MODEL})")
    elif choice == "1":
        embedding_model = "openai"
        print(f"âœ… OpenAI ëª¨ë¸ ì‚¬ìš© ({EmbeddingConfig.OPENAI_MODEL})")
        
        # API í‚¤ í™•ì¸
        if not LLMConfig.OPENAI_API_KEY:
            print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            return
    else:
        # Enter = config ê¸°ë³¸ê°’ ì‚¬ìš©
        embedding_model = EmbeddingConfig.PROVIDER
        if embedding_model == "openai":
            print(f"âœ… Config ê¸°ë³¸ê°’: OpenAI ({EmbeddingConfig.OPENAI_MODEL})")
            if not LLMConfig.OPENAI_API_KEY:
                print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
                return
        else:
            print(f"âœ… Config ê¸°ë³¸ê°’: Local ({EmbeddingConfig.LOCAL_MODEL})")
    
    try:
        vector_store.initialize(embedding_model=embedding_model)
    except Exception as e:
        print(f"âŒ VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # 3. ì„ë² ë”© ìƒì„±
    print("\nğŸ“ [Step 3] ì„ë² ë”© ìƒì„± ì¤‘...")
    
    try:
        vector_store.build_index(ontology)
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("âœ… VectorDB êµ¬ì¶• ì™„ë£Œ!")
    print("="*80)
    
    print("\nğŸ§ª [í…ŒìŠ¤íŠ¸] ì‹œë§¨í‹± ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        ("í˜ˆì•• ê´€ë ¨ ë°ì´í„°", None),
        ("í™˜ì ì‹ë³„ì", "column"),
        ("í™˜ì ì •ë³´ í…Œì´ë¸”", "table"),
        ("lab ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì—°ê²°ë˜ë‚˜", "relationship")
    ]
    
    for query, filter_type in test_queries:
        print(f"\nğŸ“ Query: '{query}' (filter: {filter_type or 'all'})")
        
        try:
            results = vector_store.semantic_search(query, n_results=3, filter_type=filter_type)
            
            if results:
                for i, result in enumerate(results, 1):
                    meta = result["metadata"]
                    doc_preview = result["document"][:100].replace('\n', ' ')
                    print(f"   {i}. [{meta.get('type', '?')}] {doc_preview}...")
            else:
                print("   ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*80)
    
    print("\nì‚¬ìš© ë°©ë²•:")
    print("  python test_vector_search.py  # ëŒ€í™”í˜• ê²€ìƒ‰")


if __name__ == "__main__":
    main()

