#!/usr/bin/env python3
# test_agent_with_interrupt.py
"""
LangGraph 3-Phase Workflow í…ŒìŠ¤íŠ¸

â­ 3-Phase Architecture:
   Phase 0: ë°ì´í„° ì¹´íƒˆë¡œê·¸ (Data Catalog)
            - phase0_catalog: ê·œì¹™ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° DB ì €ì¥ (LLM ì—†ìŒ)
   
   Phase 1: ì „ì²´ íŒŒì¼ ë¶„ë¥˜ (Classification)
            - batch_classifier: ëª¨ë“  íŒŒì¼ ë¶„ë¥˜
            - classification_review: ë¶ˆí™•ì‹¤í•œ íŒŒì¼ Human í™•ì¸ (interrupt() ì‚¬ìš©)
   
   Phase 2: ìˆœì°¨ ì²˜ë¦¬ (Processing)
            - process_metadata: ë©”íƒ€ë°ì´í„° ë¨¼ì € ì²˜ë¦¬ (ì˜¨í†¨ë¡œì§€ êµ¬ì¶•)
            - process_data_batch: ë°ì´í„° íŒŒì¼ ì²˜ë¦¬
              â””â”€ loader â†’ analyzer â†’ human_review â†’ indexer â†’ advance

â­ Human-in-the-Loop:
   ê° ë…¸ë“œ ë‚´ë¶€ì—ì„œ interrupt()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
   - interrupt() í˜¸ì¶œ ì‹œ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬
   - Command(resume=...) ë¡œ ì‘ë‹µ ì „ë‹¬
   - ëŒ€í™” íˆìŠ¤í† ë¦¬ëŠ” ìë™ìœ¼ë¡œ íŒŒì¼ì— ì €ì¥ë¨
"""

import sys
import os
import glob
import json
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.agents.graph import build_agent, build_batch_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command


# ============================================================================
# Test 1: ìƒˆë¡œìš´ 2-Phase Batch Workflow
# ============================================================================

def test_batch_workflow(file_paths: list, dataset_id: str = None):
    """
    [NEW] 3-Phase Batch Workflow í…ŒìŠ¤íŠ¸
    
    Phase 0: ê·œì¹™ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° DB ì¹´íƒˆë¡œê·¸ ì €ì¥
    Phase 1: íŒŒì¼ ë¶„ë¥˜ (ë©”íƒ€ë°ì´í„°/ë°ì´í„°)
    Phase 2: ë©”íƒ€ë°ì´í„° â†’ ë°ì´í„° ìˆœì„œë¡œ ì²˜ë¦¬
    """
    print("\n" + "ğŸŒ"*40)
    print("ğŸŒ 3-Phase Batch Workflow Test")
    print("ğŸŒ"*40)
    
    # Dataset ID ê°ì§€
    from src.utils.dataset_detector import detect_dataset_from_path, get_dataset_source_path
    from src.utils.naming import extract_dataset_prefix
    
    if dataset_id is None and file_paths:
        dataset_id = detect_dataset_from_path(file_paths[0])
        if not dataset_id:
            dataset_id = "default_dataset"
    
    print(f"\nğŸ“ [Dataset-First] Dataset ID: {dataset_id}")
    print(f"   Prefix: {extract_dataset_prefix(dataset_id)}")
    print(f"   Total Files: {len(file_paths)}ê°œ")
    
    # Ontology Manager ì´ˆê¸°í™”
    from src.utils.ontology_manager import get_ontology_manager
    ontology_mgr = get_ontology_manager()
    
    print("\nğŸ“š [Ontology] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ í™•ì¸ ì¤‘...")
    shared_ontology = ontology_mgr.load(dataset_id=dataset_id)
    shared_ontology["dataset_id"] = dataset_id
    
    # DataCatalog ìƒì„±
    from src.utils.dataset_detector import create_empty_data_catalog, create_dataset_info
    data_catalog = create_empty_data_catalog()
    
    if file_paths:
        source_path = get_dataset_source_path(file_paths[0])
        data_catalog["datasets"][dataset_id] = create_dataset_info(
            dataset_id=dataset_id,
            source_path=source_path
        )
    
    # Checkpointer & Agent ìƒì„±
    memory = MemorySaver()
    agent = build_batch_agent(checkpointer=memory)
    
    # ê³µìœ  Project Context
    shared_context = {
        "master_anchor_name": None,
        "known_aliases": [],
        "example_id_values": []
    }
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    from datetime import datetime
    conversation_history = {
        "session_id": f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "dataset_id": dataset_id,
        "started_at": datetime.now().isoformat(),
        "turns": [],
        "classification_decisions": [],
        "anchor_decisions": [],
        "user_preferences": {}
    }
    
    # ì´ˆê¸° ìƒíƒœ (3-Phaseìš©)
    initial_state = {
        # 3-Phase Workflow í•„ë“œ
        "input_files": file_paths,
        "phase0_result": None,  # Phase 0ì—ì„œ ì±„ì›Œì§
        "phase0_file_ids": [],  # Phase 0ì—ì„œ ì±„ì›Œì§ (UUID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸)
        "classification_result": None,
        "processing_progress": {
            "phase": "phase0",  # Phase 0ë¶€í„° ì‹œì‘
            "metadata_processed": [],
            "data_processed": [],
            "current_file": None,
            "current_file_index": 0,
            "total_files": len(file_paths)
        },
        # Dataset-First Architecture
        "current_dataset_id": dataset_id,
        "current_table_name": None,
        "data_catalog": data_catalog,
        # ê¸°ì¡´ í•„ë“œë“¤
        "file_path": "",  # batch_classifierì—ì„œ ì„¤ì •ë¨
        "file_type": None,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "review_type": None,
        "conversation_history": conversation_history,  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬
        "logs": [],
        "retry_count": 0,
        "error_message": None,
        "project_context": shared_context.copy(),
        "ontology_context": shared_ontology.copy(),
        "skip_indexing": False
    }
    
    # Thread ID
    thread_config = {"configurable": {"thread_id": "batch-session-1"}}
    
    print(f"\nğŸ§µ Thread ID: {thread_config['configurable']['thread_id']}")
    print(f"\nâ–¶ï¸  2-Phase Workflow ì‹¤í–‰ ì¤‘...\n")
    
    try:
        final_state = None
        
        # =====================================================================
        # ìƒˆë¡œìš´ interrupt() ê¸°ë°˜ Human-in-the-Loop ì²˜ë¦¬
        # =====================================================================
        # ê° ë…¸ë“œê°€ ë‚´ë¶€ì—ì„œ interrupt()ë¥¼ í˜¸ì¶œí•˜ë©´:
        # 1. stream()ì´ interrupt ì´ë²¤íŠ¸ë¥¼ ë°˜í™˜
        # 2. ì™¸ë¶€ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŒ
        # 3. Command(resume=ì‘ë‹µ)ìœ¼ë¡œ ì¬ì‹¤í–‰
        # =====================================================================
        
        while True:
            # ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰
            events = list(agent.stream(initial_state, thread_config, stream_mode="values"))
            
            for event in events:
                # ë¡œê·¸ ì¶œë ¥
                if "logs" in event and event["logs"]:
                    last_log = event["logs"][-1]
                    if not final_state or last_log not in final_state.get("logs", []):
                        print(f"ğŸ“ {last_log}")
                final_state = event
            
            # Interrupt í™•ì¸ (agent.get_state()ë¡œ í™•ì¸)
            current_state = agent.get_state(thread_config)
            
            # interruptê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not current_state.tasks or not any(
                hasattr(task, 'interrupts') and task.interrupts 
                for task in current_state.tasks
            ):
                break
            
            # Interrupt ì²˜ë¦¬
            for task in current_state.tasks:
                if hasattr(task, 'interrupts') and task.interrupts:
                    for interrupt_data in task.interrupts:
                        # interrupt()ì—ì„œ ì „ë‹¬í•œ ë°ì´í„° ì¶”ì¶œ
                        interrupt_value = interrupt_data.value if hasattr(interrupt_data, 'value') else interrupt_data
                        
                        review_type = interrupt_value.get("type", "general") if isinstance(interrupt_value, dict) else "general"
                        question = interrupt_value.get("question", "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤") if isinstance(interrupt_value, dict) else str(interrupt_value)
                        instructions = interrupt_value.get("instructions", {}) if isinstance(interrupt_value, dict) else {}
                        
                        # UI í‘œì‹œ
                        print("\n")
                        print("â–ˆ" * 80)
                        print("â–ˆ" + " " * 30 + "âš ï¸  ì‚¬ìš©ì í™•ì¸ í•„ìš”" + " " * 29 + "â–ˆ")
                        print("â–ˆ" * 80)
                        print()
                        print(question)
                        
                        # ë¦¬ë·° íƒ€ì…ë³„ ì•ˆë‚´
                        print("\n" + "â”€" * 80)
                        if review_type == "classification_review":
                            print("ğŸ’¡ [íŒŒì¼ ë¶„ë¥˜ í™•ì¸]")
                            print("   - ëª¨ë‘ ë§ìœ¼ë©´: í™•ì¸ ë˜ëŠ” ok")
                            print("   - ìˆ˜ì •: 1:ë°ì´í„°, 2:ë©”íƒ€ë°ì´í„° (ë²ˆí˜¸:ë¶„ë¥˜)")
                            print("   - ì œì™¸: 1:ì œì™¸ ë˜ëŠ” 1:skip")
                        elif review_type == "anchor_review":
                            print("ğŸ’¡ [ë°ì´í„° ë¶„ì„ í™•ì¸]")
                            print("   - ì»¬ëŸ¼ëª… ì…ë ¥: í•´ë‹¹ ì»¬ëŸ¼ì„ Anchorë¡œ ì§€ì •")
                            print("   - 'skip' ì…ë ¥: ì´ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                            print("   - Enterë§Œ ì…ë ¥: AI ì¶”ì²œ ìŠ¹ì¸")
                        else:
                            print("ğŸ’¡ [ì¼ë°˜ í™•ì¸]")
                            if instructions:
                                for key, val in instructions.items():
                                    print(f"   - {key}: {val}")
                        print("â”€" * 80)
                        
                        # ì‚¬ìš©ì ì…ë ¥
                        user_input = input("\n>>> ì…ë ¥: ").strip()
                        
                        # ê¸°ë³¸ê°’ ì²˜ë¦¬
                        if not user_input:
                            if review_type == "classification_review":
                                user_input = "í™•ì¸"
                                print("   (ê¸°ë³¸ê°’ 'í™•ì¸' ì‚¬ìš©)")
                            else:
                                user_input = "ok"
                                print("   (ê¸°ë³¸ê°’ 'ok' ì‚¬ìš©)")
                        
                        print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_input}'")
                        print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                        
                        # Command(resume=...)ë¡œ ì‘ë‹µ ì „ë‹¬í•˜ì—¬ ì¬ì‹¤í–‰
                        # initial_stateë¥¼ Commandë¡œ êµì²´
                        initial_state = Command(resume=user_input)
        
        # ê²°ê³¼ ìš”ì•½
        _print_batch_summary(final_state, shared_ontology, ontology_mgr, dataset_id)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


def _print_batch_summary(final_state: dict, shared_ontology: dict, ontology_mgr, dataset_id: str):
    """Batch ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    print("\n\n" + "="*80)
    print("ğŸ“Š 3-Phase Workflow ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    phase0_result = final_state.get("phase0_result", {})
    classification_result = final_state.get("classification_result", {})
    processing_progress = final_state.get("processing_progress", {})
    
    # =========================================================================
    # Phase 0: Data Catalog ê²°ê³¼
    # =========================================================================
    file_ids = final_state.get("phase0_file_ids", [])
    print(f"\nğŸ“¦ [Phase 0] Data Catalog ê²°ê³¼:")
    print(f"   - ì „ì²´ íŒŒì¼: {phase0_result.get('total_files', 0)}ê°œ")
    print(f"   - ì²˜ë¦¬ ì™„ë£Œ: {phase0_result.get('processed_files', 0)}ê°œ")
    print(f"   - ìŠ¤í‚µ (ë³€ê²½ì—†ìŒ): {phase0_result.get('skipped_files', 0)}ê°œ")
    print(f"   - ì‹¤íŒ¨: {phase0_result.get('failed_files', 0)}ê°œ")
    print(f"   - ì„±ê³µë¥ : {phase0_result.get('success_rate', 'N/A')}")
    print(f"   - File IDs: {len(file_ids)}ê°œ")
    
    # =========================================================================
    # Phase 1: Classification ê²°ê³¼
    # =========================================================================
    print(f"\nğŸ“‹ [Phase 1] ë¶„ë¥˜ ê²°ê³¼:")
    print(f"   - ë©”íƒ€ë°ì´í„°: {len(classification_result.get('metadata_files', []))}ê°œ")
    for f in classification_result.get("metadata_files", []):
        clf = classification_result.get("classifications", {}).get(f, {})
        confirmed = "âœ“ Human" if clf.get("human_confirmed") else "AI"
        print(f"      ğŸ“– [{confirmed}] {os.path.basename(f)}")
    
    print(f"   - ë°ì´í„°: {len(classification_result.get('data_files', []))}ê°œ")
    for f in classification_result.get("data_files", []):
        clf = classification_result.get("classifications", {}).get(f, {})
        confirmed = "âœ“ Human" if clf.get("human_confirmed") else "AI"
        print(f"      ğŸ“Š [{confirmed}] {os.path.basename(f)}")
    
    # =========================================================================
    # Phase 2: Processing ê²°ê³¼
    # =========================================================================
    print(f"\nğŸ”„ [Phase 2] ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - Phase: {processing_progress.get('phase')}")
    
    # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
    metadata_processed = processing_progress.get('metadata_processed', [])
    skipped_metadata = processing_progress.get('skipped_metadata_files', [])
    print(f"   - ë©”íƒ€ë°ì´í„° ì²˜ë¦¬: {len(metadata_processed)}ê°œ")
    for f in metadata_processed:
        print(f"      âœ… {os.path.basename(f)}")
    if skipped_metadata:
        print(f"   - ë©”íƒ€ë°ì´í„° ìŠ¤í‚µ: {len(skipped_metadata)}ê°œ")
        for skip in skipped_metadata:
            print(f"      â­ï¸ {skip.get('filename', 'unknown')}: {skip.get('reason', '')}")
    
    # ë°ì´í„° ì²˜ë¦¬
    data_processed = processing_progress.get('data_processed', [])
    skipped_data = processing_progress.get('skipped_data_files', [])
    print(f"   - ë°ì´í„° ì²˜ë¦¬: {len(data_processed)}ê°œ")
    for f in data_processed:
        print(f"      âœ… {os.path.basename(f)}")
    if skipped_data:
        print(f"   - ë°ì´í„° ìŠ¤í‚µ: {len(skipped_data)}ê°œ")
        for skip in skipped_data:
            print(f"      â­ï¸ {skip.get('filename', 'unknown')}: {skip.get('reason', '')}")
    
    # =========================================================================
    # Ontology ì •ë³´
    # =========================================================================
    ontology = final_state.get("ontology_context", shared_ontology)
    print(f"\nğŸ“š [Ontology] ìµœì¢… ìƒíƒœ:")
    print(f"   - ìš©ì–´ ìˆ˜: {len(ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(ontology.get('hierarchy', []))}ê°œ")
    print(f"   - ì»¬ëŸ¼ ê³„ì¸µ: {len(ontology.get('column_hierarchy', []))}ê°œ")
    print(f"   - íƒœê·¸ëœ íŒŒì¼: {len(ontology.get('file_tags', {}))}ê°œ")
    print(f"   - ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°: {len(ontology.get('column_metadata', {}))}ê°œ")
    
    # ì˜¨í†¨ë¡œì§€ ìƒì„¸ ìš”ì•½
    print(ontology_mgr.export_summary())
    
    # =========================================================================
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½
    # =========================================================================
    conversation_history = final_state.get("conversation_history", {})
    turns = conversation_history.get("turns", [])
    if turns:
        print(f"\nğŸ’¬ [Conversation] ëŒ€í™” íˆìŠ¤í† ë¦¬:")
        print(f"   - Session ID: {conversation_history.get('session_id')}")
        print(f"   - Total Turns: {len(turns)}ê°œ")
        print(f"   - Classification Decisions: {len(conversation_history.get('classification_decisions', []))}ê°œ")
        print(f"   - Anchor Decisions: {len(conversation_history.get('anchor_decisions', []))}ê°œ")
    
    print("\n" + "="*80)
    print("âœ… 3-Phase Workflow ì™„ë£Œ!")
    print("="*80)


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ - 3-Phase Batch Workflow"""
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼
    inspire_files = sorted(glob.glob(str(data_dir / "INSPIRE_130K_1.3/*.csv")))
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # VitalDB ë°ì´í„°ë§Œ ì²˜ë¦¬ (CSV + Signal)
    all_files = vital_csv_files + vital_signal_files[:2]
    
    if not all_files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    print(f"\nğŸ“ Found {len(all_files)} files:")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    for f in vital_csv_files:
        print(f"      - {os.path.basename(f)}")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ")
    
    # Dataset ID ê°ì§€
    from src.utils.dataset_detector import detect_dataset_from_path
    dataset_id = None
    if all_files:
        dataset_id = detect_dataset_from_path(all_files[0])
    print(f"\nğŸ“ [Dataset-First] Detected Dataset: {dataset_id}")
    
    # â­ 2-Phase Batch Workflow ì‹¤í–‰
    test_batch_workflow(all_files, dataset_id=dataset_id)
    
    # ìºì‹œ í†µê³„ ì¶œë ¥
    from src.utils.llm_cache import get_llm_cache
    cache = get_llm_cache()
    cache.print_stats()
    
    # [DISABLED] VectorDB ì„ë² ë”© ìƒì„± - ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ ë¹„í™œì„±í™”
    # í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì„¸ìš”
    # print("\n" + "="*80)
    # print(f"ğŸ”¢ [VectorDB] ì„ë² ë”© ìƒì„± ì‹œì‘... (dataset: {dataset_id})")
    # print("="*80)
    # 
    # try:
    #     from src.knowledge.vector_store import VectorStore
    #     from src.utils.ontology_manager import get_ontology_manager
    #     
    #     ontology_mgr = get_ontology_manager()
    #     ontology = ontology_mgr.load(dataset_id=dataset_id)
    #     
    #     if ontology and (ontology.get("definitions") or ontology.get("column_metadata")):
    #         vector_store = VectorStore()
    #         vector_store.initialize()
    #         vector_store.build_index(ontology, dataset_id=dataset_id)
    #         
    #         stats = vector_store.get_stats()
    #         print(f"\nâœ… [VectorDB] ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    #         print(f"   - Dataset: {dataset_id}")
    #         print(f"   - Provider: {stats.get('provider')}")
    #         print(f"   - Dimensions: {stats.get('dimensions')}")
    #         print(f"   - Total Embeddings: {stats.get('total', 0)}ê°œ")
    #     else:
    #         print("âš ï¸  [VectorDB] ì„ë² ë”©í•  ë°ì´í„° ì—†ìŒ (ì˜¨í†¨ë¡œì§€ ë¹„ì–´ìˆìŒ)")
    # except Exception as e:
    #     print(f"âš ï¸  [VectorDB] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    #     print("   (pgvector ë¯¸ì„¤ì¹˜ ì‹œ: brew install pgvector)")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()
