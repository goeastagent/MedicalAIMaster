#!/usr/bin/env python3
# test_agent_with_interrupt.py
"""
LangGraphì˜ ê³µì‹ Interrupt ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œ Human-in-the-Loop í…ŒìŠ¤íŠ¸
ì—¬ëŸ¬ CSV íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©° Global Contextë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

â­ 2-Pass ì²˜ë¦¬ ë°©ì‹:
   Pass 1: ëª¨ë“  íŒŒì¼ì„ ì‚¬ì „ ë¶„ë¥˜ (ë©”íƒ€ë°ì´í„° vs ë°ì´í„°)
   Pass 2: ë©”íƒ€ë°ì´í„° ë¨¼ì € ì²˜ë¦¬ â†’ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬
"""

import sys
import os
import glob
import json

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from agents.graph import build_agent
from langgraph.checkpoint.memory import MemorySaver


# ============================================================================
# Pass 1: ì‚¬ì „ ë¶„ë¥˜ (Pre-classification)
# ============================================================================

def preclassify_files(file_paths: list) -> dict:
    """
    [Pass 1] ëª¨ë“  íŒŒì¼ì„ ë¹ ë¥´ê²Œ ìŠ¤ìº”í•˜ì—¬ ë©”íƒ€ë°ì´í„°/ë°ì´í„°ë¡œ ë¶„ë¥˜
    
    Returns:
        {
            "metadata_files": [...],  # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            "data_files": [...],      # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            "classification": {...}   # íŒŒì¼ë³„ ë¶„ë¥˜ ê²°ê³¼
        }
    """
    from src.processors.tabular import TabularProcessor
    from src.utils.llm_client import get_llm_client
    from src.utils.llm_cache import get_llm_cache
    
    print("\n" + "="*80)
    print("ğŸ” [Pass 1] íŒŒì¼ ì‚¬ì „ ë¶„ë¥˜ (Pre-classification)")
    print("="*80)
    print(f"   ì´ {len(file_paths)}ê°œ íŒŒì¼ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤...\n")
    
    llm_client = get_llm_client()
    llm_cache = get_llm_cache()
    processor = TabularProcessor(llm_client)
    
    metadata_files = []
    data_files = []
    classification = {}
    
    for idx, file_path in enumerate(file_paths, 1):
        filename = os.path.basename(file_path)
        print(f"   [{idx}/{len(file_paths)}] {filename}...", end=" ")
        
        try:
            # 1. ê¸°ì´ˆ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë¹ ë¥¸ ìŠ¤ìº”)
            if not processor.can_handle(file_path):
                print("âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹")
                data_files.append(file_path)
                classification[file_path] = {"is_metadata": False, "reason": "Unsupported format"}
                continue
            
            raw_metadata = processor.extract_metadata(file_path)
            
            # 2. ë¶„ë¥˜ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê°„ì†Œí™”)
            context = _build_classification_context(file_path, raw_metadata)
            
            # 3. LLMì—ê²Œ ë¶„ë¥˜ ìš”ì²­ (ìºì‹œ í™œìš©)
            result = _classify_file_with_llm(context, llm_cache, llm_client)
            
            is_metadata = result.get("is_metadata", False)
            confidence = result.get("confidence", 0.0)
            
            classification[file_path] = {
                "is_metadata": is_metadata,
                "confidence": confidence,
                "reason": result.get("reasoning", "N/A")
            }
            
            if is_metadata:
                metadata_files.append(file_path)
                print(f"ğŸ“– ë©”íƒ€ë°ì´í„° ({confidence:.0%})")
            else:
                data_files.append(file_path)
                print(f"ğŸ“Š ë°ì´í„° ({confidence:.0%})")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            data_files.append(file_path)  # ì‹¤íŒ¨ ì‹œ ë°ì´í„°ë¡œ ê°€ì •
            classification[file_path] = {"is_metadata": False, "reason": f"Error: {str(e)}"}
    
    print("\n" + "-"*80)
    print(f"ğŸ“– ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
    for f in metadata_files:
        print(f"   â€¢ {os.path.basename(f)}")
    print(f"ğŸ“Š ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    for f in data_files:
        print(f"   â€¢ {os.path.basename(f)}")
    print("="*80)
    
    return {
        "metadata_files": metadata_files,
        "data_files": data_files,
        "classification": classification
    }


def _build_classification_context(file_path: str, raw_metadata: dict) -> dict:
    """ë¶„ë¥˜ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê°„ì†Œí™” ë²„ì „)"""
    import pandas as pd
    
    filename = os.path.basename(file_path)
    name_parts = filename.replace(".csv", "").replace("_", " ").replace("-", " ").split()
    base_name = filename.rsplit(".", 1)[0]
    extension = filename.rsplit(".", 1)[-1] if "." in filename else ""
    
    columns = raw_metadata.get("columns", [])
    
    # ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5í–‰ë§Œ)
    sample_data = []
    try:
        df = pd.read_csv(file_path, nrows=5)
        for col in columns[:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
            if col in df.columns:
                samples = df[col].dropna().head(3).tolist()
                sample_data.append({
                    "column": col,
                    "samples": [str(s)[:100] for s in samples],
                    "avg_text_length": sum(len(str(s)) for s in samples) / max(len(samples), 1)
                })
    except:
        pass
    
    return {
        "filename": filename,
        "name_parts": name_parts,
        "base_name": base_name,
        "extension": extension,
        "num_columns": len(columns),
        "columns": columns,
        "sample_data": sample_data
    }


def _classify_file_with_llm(context: dict, llm_cache, llm_client) -> dict:
    """LLMìœ¼ë¡œ íŒŒì¼ ë¶„ë¥˜ (ìºì‹œ í™œìš©)"""
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("file_preclassification", context)
    if cached:
        return cached
    
    prompt = f"""
You are a Data Classification Expert. Quickly classify this file.

[FILE INFO]
Filename: {context['filename']}
Name Parts: {context['name_parts']}
Columns ({context['num_columns']}): {context['columns'][:10]}...
Sample Data: {json.dumps(context['sample_data'][:3], ensure_ascii=False)}

[CLASSIFICATION]
- METADATA: Describes other data (codebook, parameter list, dictionary)
  * Usually has columns like: [Name, Description, Unit, Type]
  * Content is explanatory text, not measurements

- DATA: Actual records/measurements
  * Contains patient records, lab results, events
  * Values are data points, not descriptions

[OUTPUT - JSON ONLY]
{{
    "is_metadata": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief reason"
}}
"""
    
    try:
        result = llm_client.ask_json(prompt)
        llm_cache.set("file_preclassification", context, result)
        return result
    except Exception as e:
        return {"is_metadata": False, "confidence": 0.5, "reasoning": f"LLM error: {str(e)}"}


def test_with_interrupt(file_path: str):
    """
    LangGraph Interrupt ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ Human Feedback í…ŒìŠ¤íŠ¸
    """
    print("="*80)
    print("ğŸš€ LangGraph Interrupt ë°©ì‹ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # 1. Checkpointer ìƒì„± (State ì €ì¥/ë³µì›ìš©)
    memory = MemorySaver()
    
    # 2. Agent ë¹Œë“œ (checkpointer ì „ë‹¬)
    agent = build_agent(checkpointer=memory)
    
    # 3. ì´ˆê¸° ìƒíƒœ ë° ì„¤ì •
    initial_state = {
        "file_path": file_path,
        "file_type": None,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "logs": [],
        "retry_count": 0,
        "error_message": None,
        "project_context": {
            "master_anchor_name": None,
            "known_aliases": [],
            "example_id_values": []
        }
    }
    
    # Thread ID (ê°™ì€ ì„¸ì…˜ì„ ì¶”ì í•˜ê¸° ìœ„í•¨)
    thread_config = {"configurable": {"thread_id": "test-session-1"}}
    
    print(f"\nğŸ“ íŒŒì¼: {os.path.basename(file_path)}")
    print(f"ğŸ§µ Thread ID: {thread_config['configurable']['thread_id']}\n")
    
    # 4. ì‹¤í–‰ (Interrupt ë°œìƒ ì‹œ ë©ˆì¶¤)
    try:
        print("â–¶ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n")
        
        # stream()ìœ¼ë¡œ ë‹¨ê³„ë³„ ì‹¤í–‰ í™•ì¸
        for event in agent.stream(initial_state, thread_config, stream_mode="values"):
            # ê° ë…¸ë“œ ì‹¤í–‰ í›„ state ì¶œë ¥
            if "logs" in event and event["logs"]:
                print(f"ğŸ“ {event['logs'][-1]}")
            
            # needs_human_review ì²´í¬
            if event.get("needs_human_review"):
                print("\n")
                print("â–ˆ" * 80)
                print("â–ˆ" + " " * 30 + "âš ï¸  ì‚¬ìš©ì í™•ì¸ í•„ìš”" + " " * 29 + "â–ˆ")
                print("â–ˆ" * 80)
                
                question = event.get("human_question", "í™•ì¸ í•„ìš”")
                print(question)
                
                print("\n" + "â”€" * 80)
                print("ğŸ’¡ ì…ë ¥ ì•ˆë‚´:")
                print("   - ì»¬ëŸ¼ëª… ì…ë ¥: í•´ë‹¹ ì»¬ëŸ¼ì„ Anchorë¡œ ì§€ì •")
                print("   - 'skip' ì…ë ¥: ì´ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                print("   - 'unknown' ì…ë ¥: AIê°€ ìë™ ê²°ì •í•˜ë„ë¡ ìœ„ì„")
                print("â”€" * 80)
                
                # ì‚¬ìš©ì ì…ë ¥
                user_feedback = input("\n>>> ì…ë ¥: ").strip()
                
                if not user_feedback:
                    print("âš ï¸  ì…ë ¥ ì—†ìŒ. 'unknown' ì‚¬ìš©")
                    user_feedback = "unknown"
                
                print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                
                # 5. State ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰ (ê°™ì€ thread_id ì‚¬ìš©)
                print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                
                # Noneìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ì „ state ìœ ì§€í•˜ë©´ì„œ íŠ¹ì • í•„ë“œë§Œ ë³€ê²½
                update_state = {
                    "human_feedback": user_feedback,
                    "needs_human_review": False
                }
                
                # ì¬ì‹¤í–‰ (update í›„)
                for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                    if "logs" in event2 and event2["logs"]:
                        last_log = event2["logs"][-1]
                        if last_log not in event.get("logs", []):
                            print(f"ğŸ“ {last_log}")
        
        print("\n" + "="*80)
        print("âœ… WORKFLOW COMPLETED")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


def test_multiple_files_with_interrupt(file_paths: list):
    """
    ì—¬ëŸ¬ CSV íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (Global Context ìœ ì§€)
    """
    print("\n" + "ğŸŒ"*40)
    print("ğŸŒ MULTI-FILE PROCESSING with Global Context")
    print("ğŸŒ"*40)
    
    # Ontology Manager ì´ˆê¸°í™” ë° ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ë¡œë“œ
    from src.utils.ontology_manager import get_ontology_manager
    ontology_mgr = get_ontology_manager()
    
    print("\nğŸ“š [Ontology] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ í™•ì¸ ì¤‘...")
    shared_ontology = ontology_mgr.load()
    
    # Checkpointer ìƒì„± (ëª¨ë“  íŒŒì¼ì´ ê³µìœ )
    memory = MemorySaver()
    agent = build_agent(checkpointer=memory)
    
    # ê³µìœ  Project Context
    shared_context = {
        "master_anchor_name": None,
        "known_aliases": [],
        "example_id_values": []
    }
    
    results = []
    
    for idx, file_path in enumerate(file_paths, 1):
        print(f"\n\n{'#'*80}")
        print(f"# File {idx}/{len(file_paths)}: {os.path.basename(file_path)}")
        print(f"{'#'*80}")
        
        # Thread ID (ê° íŒŒì¼ë§ˆë‹¤ ë‹¤ë¥¸ ì„¸ì…˜)
        thread_config = {"configurable": {"thread_id": f"file-{idx}"}}
        
        # ì´ˆê¸° ìƒíƒœ (ì´ì „ íŒŒì¼ì˜ Context ì „ë‹¬)
        initial_state = {
            "file_path": file_path,
            "file_type": None,
            "raw_metadata": {},
            "finalized_anchor": None,
            "finalized_schema": [],
            "needs_human_review": False,
            "human_question": "",
            "human_feedback": None,
            "logs": [],
            "retry_count": 0,
            "error_message": None,
            "project_context": shared_context.copy(),
            # [NEW] ê³µìœ  Ontology Context (ëˆ„ì ë¨)
            "ontology_context": shared_ontology.copy(),
            "skip_indexing": False
        }
        
        try:
            print(f"\nâ–¶ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n")
            
            # ì‹¤í–‰
            final_state = None
            for event in agent.stream(initial_state, thread_config, stream_mode="values"):
                # ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                if "logs" in event and event["logs"]:
                    last_log = event["logs"][-1]
                    if not final_state or last_log not in final_state.get("logs", []):
                        print(f"ğŸ“ {last_log}")
                
                final_state = event
                
                # Human Review í•„ìš” ì‹œ
                if event.get("needs_human_review"):
                    print("\n")
                    print("â–ˆ" * 80)
                    print("â–ˆ" + " " * 30 + "âš ï¸  ì‚¬ìš©ì í™•ì¸ í•„ìš”" + " " * 29 + "â–ˆ")
                    print("â–ˆ" * 80)
                    
                    question = event.get("human_question", "í™•ì¸ í•„ìš”")
                    print(question)
                    
                    print("\n" + "â”€" * 80)
                    print("ğŸ’¡ ì…ë ¥ ì•ˆë‚´:")
                    print("   - ì»¬ëŸ¼ëª… ì…ë ¥: í•´ë‹¹ ì»¬ëŸ¼ì„ Anchorë¡œ ì§€ì •")
                    print("   - 'skip' ì…ë ¥: ì´ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                    print("   - Enterë§Œ ì…ë ¥: ìë™ ì²˜ë¦¬ (3ë²ˆ ì¬ì‹œë„ í›„ ê°•ì œ ì§„í–‰)")
                    print("â”€" * 80)
                    
                    # ì‚¬ìš©ì ì…ë ¥
                    user_feedback = input("\n>>> ì…ë ¥: ").strip()
                    
                    if not user_feedback:
                        print("âš ï¸  ì…ë ¥ ì—†ìŒ. ìë™ ì²˜ë¦¬ (3ë²ˆ ì¬ì‹œë„ í›„ ê°•ì œ ì§„í–‰)")
                        continue  # ìë™ ëª¨ë“œ
                    
                    print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                    
                    # State ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰
                    print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                    update_state = {
                        "human_feedback": user_feedback,
                        "needs_human_review": False
                    }
                    
                    # ì¬ì‹¤í–‰
                    for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                        if "logs" in event2 and event2["logs"]:
                            last_log = event2["logs"][-1]
                            if last_log not in event.get("logs", []):
                                print(f"ğŸ“ {last_log}")
                        final_state = event2
            
            # ì„±ê³µ
            if final_state:
                # Context ì—…ë°ì´íŠ¸ (ë‹¤ìŒ íŒŒì¼ì„ ìœ„í•´)
                shared_context = final_state.get('project_context', shared_context)
                shared_ontology = final_state.get('ontology_context', shared_ontology)
                
                results.append({
                    'file': file_path,
                    'success': True,
                    'anchor': final_state.get('finalized_anchor'),
                    'was_metadata': final_state.get('skip_indexing', False)
                })
                
                print(f"\nâœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(file_path)}")
                print(f"ğŸ”„ Global Context ì—…ë°ì´íŠ¸:")
                print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
                print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
                print(f"ğŸ”„ Ontology Context:")
                print(f"   - ìš©ì–´ ìˆ˜: {len(shared_ontology.get('definitions', {}))}ê°œ")
                print(f"   - íŒŒì¼ íƒœê·¸: {len(shared_ontology.get('file_tags', {}))}ê°œ")
            else:
                results.append({'file': file_path, 'success': False})
                
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
            results.append({'file': file_path, 'success': False})
    
    # ìµœì¢… ìš”ì•½
    print("\n\n" + "="*80)
    print("ğŸ“Š FINAL SUMMARY - All Files")
    print("="*80)
    print(f"\nâœ… Successfully processed: {sum(1 for r in results if r['success'])}/{len(results)} files")
    
    # ë©”íƒ€ë°ì´í„° vs ë°ì´í„° ë¶„ë¦¬
    metadata_files = [r for r in results if r.get('was_metadata')]
    data_files = [r for r in results if not r.get('was_metadata')]
    
    print(f"\nğŸ“– Metadata Files: {len(metadata_files)}ê°œ")
    for r in metadata_files:
        print(f"   â€¢ {os.path.basename(r['file'])} â†’ ì˜¨í†¨ë¡œì§€ ì¶”ê°€ë¨")
    
    print(f"\nğŸ“Š Data Files: {len(data_files)}ê°œ")
    for r in data_files:
        print(f"   â€¢ {os.path.basename(r['file'])}")
        if r.get('anchor'):
            anchor = r['anchor']
            print(f"      â†’ Anchor: {anchor.get('column_name')} (mapped: {anchor.get('mapped_to_master', 'N/A')})")
    
    print(f"\nğŸŒ Final Global Context:")
    print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
    print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
    
    print(f"\nğŸ“š Ontology Context:")
    print(f"   - ì´ ìš©ì–´: {len(shared_ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(shared_ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(shared_ontology.get('hierarchy', []))}ê°œ")
    print(f"   - íƒœê·¸ëœ íŒŒì¼: {len(shared_ontology.get('file_tags', {}))}ê°œ")
    
    # ì˜¨í†¨ë¡œì§€ ìš”ì•½ ì¶œë ¥
    print(ontology_mgr.export_summary())


def main():
    """ë©”ì¸ í•¨ìˆ˜ - 2-Pass ì²˜ë¦¬ ë°©ì‹"""
    from pathlib import Path
    
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼ (INSPIRE ë°ì´í„°ì…‹)
    inspire_files = sorted(glob.glob(str(data_dir / "INSPIRE_130K_1.3/*.csv")))
    
    # VitalDB CSV íŒŒì¼
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    
    # VitalDB íŒŒì¼ (ì‹ í˜¸ ë°ì´í„°)
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # ìš”ì²­ì— ì˜í•´ Open_VitalDB_1.0.0 CSV íŒŒì¼ë§Œ indexingí•˜ë„ë¡ ì„¤ì •
    all_files = vital_csv_files

    if not all_files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    print(f"\nğŸ“ Found {len(all_files)} files:")
    print(f"   ğŸ“Š INSPIRE CSV: {len(inspire_files)}ê°œ")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    for f in vital_csv_files:
        print(f"      - {os.path.basename(f)}")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ")
    
    # â­ [Pass 1] ì‚¬ì „ ë¶„ë¥˜ - ë©”íƒ€ë°ì´í„° vs ë°ì´í„° êµ¬ë¶„
    classification_result = preclassify_files(all_files)
    
    metadata_files = classification_result["metadata_files"]
    data_files = classification_result["data_files"]
    
    # â­ [Pass 2] ë©”íƒ€ë°ì´í„° ë¨¼ì € â†’ ë°ì´í„° ë‚˜ì¤‘ì— ì²˜ë¦¬
    ordered_files = metadata_files + data_files
    
    print(f"\nğŸ“‹ ì²˜ë¦¬ ìˆœì„œ (ë©”íƒ€ë°ì´í„° ìš°ì„ ):")
    for i, f in enumerate(ordered_files, 1):
        file_type = "ğŸ“– ë©”íƒ€ë°ì´í„°" if f in metadata_files else "ğŸ“Š ë°ì´í„°"
        print(f"   {i}. {file_type}: {os.path.basename(f)}")
    
    # [TEST] ì†ë„ í–¥ìƒì„ ìœ„í•´ ë°ì´í„° ë¡œë“œ ì œí•œ ì„¤ì • (1000í–‰)
    # os.environ["TEST_ROW_LIMIT"] = "1000"
    # print("\nâš ï¸  [TEST MODE] ë°ì´í„° ë¡œë“œ ì œí•œ ì„¤ì •ë¨ (TEST_ROW_LIMIT=1000)")
    
    # Pass 2 ì‹¤í–‰
    test_multiple_files_with_interrupt(ordered_files)
    
    # ìºì‹œ í†µê³„ ì¶œë ¥ (ì „ì—­ ìºì‹œ import)
    from src.utils.llm_cache import get_llm_cache
    cache = get_llm_cache()
    cache.print_stats()
    
    # â­ [Pass 3] VectorDB ì„ë² ë”© ìë™ ìƒì„±
    print("\n" + "="*80)
    print("ğŸ”¢ [VectorDB] ì„ë² ë”© ìƒì„± ì‹œì‘...")
    print("="*80)
    
    try:
        from src.knowledge.vector_store import VectorStore
        from src.utils.ontology_manager import get_ontology_manager
        
        # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        ontology_mgr = get_ontology_manager()
        ontology = ontology_mgr.load()
        
        if ontology and (ontology.get("definitions") or ontology.get("column_metadata")):
            # VectorStore ì´ˆê¸°í™” ë° ì„ë² ë”© ìƒì„±
            vector_store = VectorStore()
            vector_store.initialize()
            vector_store.build_index(ontology)
            
            # í†µê³„ ì¶œë ¥
            stats = vector_store.get_stats()
            print(f"\nâœ… [VectorDB] ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            print(f"   - Provider: {stats.get('provider')}")
            print(f"   - Dimensions: {stats.get('dimensions')}")
            print(f"   - Total Embeddings: {stats.get('total', 0)}ê°œ")
        else:
            print("âš ï¸  [VectorDB] ì„ë² ë”©í•  ë°ì´í„° ì—†ìŒ (ì˜¨í†¨ë¡œì§€ ë¹„ì–´ìˆìŒ)")
    except Exception as e:
        print(f"âš ï¸  [VectorDB] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        print("   (pgvector ë¯¸ì„¤ì¹˜ ì‹œ: brew install pgvector)")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()

