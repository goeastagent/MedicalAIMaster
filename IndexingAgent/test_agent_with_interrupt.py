#!/usr/bin/env python3
# test_agent_with_interrupt.py
"""
LangGraph 2-Phase Workflow í…ŒìŠ¤íŠ¸

â­ 2-Phase Architecture:
   Phase 1: ì „ì²´ íŒŒì¼ ë¶„ë¥˜ (Classification)
            - batch_classifier: ëª¨ë“  íŒŒì¼ ë¶„ë¥˜
            - classification_review: ë¶ˆí™•ì‹¤í•œ íŒŒì¼ Human í™•ì¸
   
   Phase 2: ìˆœì°¨ ì²˜ë¦¬ (Processing)
            - process_metadata: ë©”íƒ€ë°ì´í„° ë¨¼ì € ì²˜ë¦¬ (ì˜¨í†¨ë¡œì§€ êµ¬ì¶•)
            - process_data_batch: ë°ì´í„° íŒŒì¼ ì²˜ë¦¬
              â””â”€ loader â†’ analyzer â†’ human_review â†’ indexer â†’ advance
"""

import sys
import os
import glob
import json

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from agents.graph import build_agent, build_batch_agent, build_single_agent
from langgraph.checkpoint.memory import MemorySaver


# ============================================================================
# Test 1: ìƒˆë¡œìš´ 2-Phase Batch Workflow
# ============================================================================

def test_batch_workflow(file_paths: list, dataset_id: str = None):
    """
    [NEW] 2-Phase Batch Workflow í…ŒìŠ¤íŠ¸
    
    ëª¨ë“  íŒŒì¼ì„ í•œ ë²ˆì— ë¶„ë¥˜í•˜ê³ , ë©”íƒ€ë°ì´í„° â†’ ë°ì´í„° ìˆœì„œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    print("\n" + "ğŸŒ"*40)
    print("ğŸŒ 2-Phase Batch Workflow Test")
    print("ğŸŒ"*40)
    
    # Dataset ID ê°ì§€
    from src.utils.dataset_detector import detect_dataset_from_path, get_dataset_source_path
    from src.utils.naming import extract_dataset_prefix
    
    if dataset_id is None and file_paths:
        dataset_id = detect_dataset_from_path(file_paths[0])
        if not dataset_id:
            dataset_id = "default_dataset"
    
    print(f"\nğŸ“ [Dataset-First] Dataset ID: {dataset_id}")
    print(f"   Prefix: {extract_dataset_prefix(dataset_id)}")
    print(f"   Total Files: {len(file_paths)}ê°œ")
    
    # Ontology Manager ì´ˆê¸°í™”
    from src.utils.ontology_manager import get_ontology_manager
    ontology_mgr = get_ontology_manager()
    
    print("\nğŸ“š [Ontology] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ í™•ì¸ ì¤‘...")
    shared_ontology = ontology_mgr.load(dataset_id=dataset_id)
    shared_ontology["dataset_id"] = dataset_id
    
    # DataCatalog ìƒì„±
    from src.utils.dataset_detector import create_empty_data_catalog, create_dataset_info
    data_catalog = create_empty_data_catalog()
    
    if file_paths:
        source_path = get_dataset_source_path(file_paths[0])
        data_catalog["datasets"][dataset_id] = create_dataset_info(
            dataset_id=dataset_id,
            source_path=source_path
        )
    
    # Checkpointer & Agent ìƒì„±
    memory = MemorySaver()
    agent = build_batch_agent(checkpointer=memory)
    
    # ê³µìœ  Project Context
    shared_context = {
        "master_anchor_name": None,
        "known_aliases": [],
        "example_id_values": []
    }
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    from datetime import datetime
    conversation_history = {
        "session_id": f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "dataset_id": dataset_id,
        "started_at": datetime.now().isoformat(),
        "turns": [],
        "classification_decisions": [],
        "anchor_decisions": [],
        "user_preferences": {}
    }
    
    # ì´ˆê¸° ìƒíƒœ (2-Phaseìš©)
    initial_state = {
        # 2-Phase Workflow í•„ë“œ
        "input_files": file_paths,
        "classification_result": None,
        "processing_progress": {
            "phase": "classification",
            "metadata_processed": [],
            "data_processed": [],
            "current_file": None,
            "current_file_index": 0,
            "total_files": len(file_paths)
        },
        # Dataset-First Architecture
        "current_dataset_id": dataset_id,
        "current_table_name": None,
        "data_catalog": data_catalog,
        # ê¸°ì¡´ í•„ë“œë“¤
        "file_path": "",  # batch_classifierì—ì„œ ì„¤ì •ë¨
        "file_type": None,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "review_type": None,
        "conversation_history": conversation_history,  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬
        "logs": [],
        "retry_count": 0,
        "error_message": None,
        "project_context": shared_context.copy(),
        "ontology_context": shared_ontology.copy(),
        "skip_indexing": False
    }
    
    # Thread ID
    thread_config = {"configurable": {"thread_id": "batch-session-1"}}
    
    print(f"\nğŸ§µ Thread ID: {thread_config['configurable']['thread_id']}")
    print(f"\nâ–¶ï¸  2-Phase Workflow ì‹¤í–‰ ì¤‘...\n")
    
    try:
        final_state = None
        
        for event in agent.stream(initial_state, thread_config, stream_mode="values"):
            # ë¡œê·¸ ì¶œë ¥
            if "logs" in event and event["logs"]:
                last_log = event["logs"][-1]
                if not final_state or last_log not in final_state.get("logs", []):
                    print(f"ğŸ“ {last_log}")
            
            final_state = event
            
            # Human Review í•„ìš” ì‹œ
            if event.get("needs_human_review"):
                review_type = event.get("review_type", "general")
                
                print("\n")
                print("â–ˆ" * 80)
                print("â–ˆ" + " " * 30 + "âš ï¸  ì‚¬ìš©ì í™•ì¸ í•„ìš”" + " " * 29 + "â–ˆ")
                print("â–ˆ" * 80)
                
                question = event.get("human_question", "í™•ì¸ í•„ìš”")
                print(question)
                
                # ë¦¬ë·° íƒ€ì…ë³„ ì•ˆë‚´
                print("\n" + "â”€" * 80)
                if review_type == "classification":
                    print("ğŸ’¡ [íŒŒì¼ ë¶„ë¥˜ í™•ì¸]")
                    print("   - ëª¨ë‘ ë§ìœ¼ë©´: í™•ì¸ ë˜ëŠ” ok")
                    print("   - ìˆ˜ì •: 1:ë°ì´í„°, 2:ë©”íƒ€ë°ì´í„° (ë²ˆí˜¸:ë¶„ë¥˜)")
                    print("   - ì œì™¸: 1:ì œì™¸ ë˜ëŠ” 1:skip")
                else:
                    print("ğŸ’¡ [ë°ì´í„° ë¶„ì„ í™•ì¸]")
                    print("   - ì»¬ëŸ¼ëª… ì…ë ¥: í•´ë‹¹ ì»¬ëŸ¼ì„ Anchorë¡œ ì§€ì •")
                    print("   - 'skip' ì…ë ¥: ì´ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                    print("   - Enterë§Œ ì…ë ¥: ìë™ ì²˜ë¦¬")
                print("â”€" * 80)
                
                # ì‚¬ìš©ì ì…ë ¥
                user_feedback = input("\n>>> ì…ë ¥: ").strip()
                
                if not user_feedback:
                    if review_type == "classification":
                        user_feedback = "í™•ì¸"  # ê¸°ë³¸ê°’: ìŠ¹ì¸
                    else:
                        print("âš ï¸  ì…ë ¥ ì—†ìŒ. ìë™ ì²˜ë¦¬...")
                        continue
                
                print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                
                # State ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰
                update_state = {
                    "human_feedback": user_feedback,
                    "needs_human_review": False
                }
                
                for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                    if "logs" in event2 and event2["logs"]:
                        last_log = event2["logs"][-1]
                        if last_log not in final_state.get("logs", []):
                            print(f"ğŸ“ {last_log}")
                    final_state = event2
        
        # ê²°ê³¼ ìš”ì•½
        _print_batch_summary(final_state, shared_ontology, ontology_mgr, dataset_id)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


def _print_batch_summary(final_state: dict, shared_ontology: dict, ontology_mgr, dataset_id: str):
    """Batch ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    print("\n\n" + "="*80)
    print("ğŸ“Š 2-Phase Workflow ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    classification_result = final_state.get("classification_result", {})
    processing_progress = final_state.get("processing_progress", {})
    
    print(f"\nğŸ“‹ [Phase 1] ë¶„ë¥˜ ê²°ê³¼:")
    print(f"   - ë©”íƒ€ë°ì´í„°: {len(classification_result.get('metadata_files', []))}ê°œ")
    for f in classification_result.get("metadata_files", []):
        print(f"      ğŸ“– {os.path.basename(f)}")
    print(f"   - ë°ì´í„°: {len(classification_result.get('data_files', []))}ê°œ")
    for f in classification_result.get("data_files", []):
        print(f"      ğŸ“Š {os.path.basename(f)}")
    
    print(f"\nğŸ”„ [Phase 2] ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - Phase: {processing_progress.get('phase')}")
    print(f"   - ë©”íƒ€ë°ì´í„° ì²˜ë¦¬: {len(processing_progress.get('metadata_processed', []))}ê°œ")
    print(f"   - ë°ì´í„° ì²˜ë¦¬: {len(processing_progress.get('data_processed', []))}ê°œ")
    
    # ì˜¨í†¨ë¡œì§€ ì •ë³´
    ontology = final_state.get("ontology_context", shared_ontology)
    print(f"\nğŸ“š [Ontology] ìµœì¢… ìƒíƒœ:")
    print(f"   - ìš©ì–´ ìˆ˜: {len(ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(ontology.get('hierarchy', []))}ê°œ")
    print(f"   - íƒœê·¸ëœ íŒŒì¼: {len(ontology.get('file_tags', {}))}ê°œ")
    
    # ì˜¨í†¨ë¡œì§€ ìƒì„¸ ìš”ì•½
    print(ontology_mgr.export_summary())
    
    print("="*80)
    print("âœ… 2-Phase Workflow ì™„ë£Œ!")
    print("="*80)


# ============================================================================
# Test 2: Legacy ë‹¨ì¼ íŒŒì¼ ì›Œí¬í”Œë¡œìš° (í˜¸í™˜ì„±)
# ============================================================================

def test_single_file_workflow(file_path: str, dataset_id: str = None):
    """
    [Legacy] ë‹¨ì¼ íŒŒì¼ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*80)
    print("ğŸš€ Single File Workflow Test")
    print("="*80)
    
    from src.utils.dataset_detector import detect_dataset_from_path
    
    if dataset_id is None:
        dataset_id = detect_dataset_from_path(file_path) or "default_dataset"
    
    memory = MemorySaver()
    agent = build_single_agent(checkpointer=memory)
    
    initial_state = {
        "current_dataset_id": dataset_id,
        "file_path": file_path,
        "file_type": None,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "logs": [],
        "retry_count": 0,
        "error_message": None,
        "project_context": {
            "master_anchor_name": None,
            "known_aliases": [],
            "example_id_values": []
        },
        "ontology_context": {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        },
        "skip_indexing": False
    }
    
    thread_config = {"configurable": {"thread_id": "single-file-1"}}
    
    print(f"\nğŸ“ íŒŒì¼: {os.path.basename(file_path)}")
    print(f"ğŸ“ Dataset: {dataset_id}")
    
    try:
        for event in agent.stream(initial_state, thread_config, stream_mode="values"):
            if "logs" in event and event["logs"]:
                print(f"ğŸ“ {event['logs'][-1]}")
            
            if event.get("needs_human_review"):
                question = event.get("human_question", "í™•ì¸ í•„ìš”")
                print(f"\nâš ï¸ Human Review: {question}")
                
                user_feedback = input(">>> ì…ë ¥: ").strip() or "unknown"
                
                update_state = {
                    "human_feedback": user_feedback,
                    "needs_human_review": False
                }
                
                for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                    if "logs" in event2 and event2["logs"]:
                        print(f"ğŸ“ {event2['logs'][-1]}")
        
        print("\nâœ… Single File Workflow ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


# ============================================================================
# Test 3: Legacy ë©€í‹° íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬ (í˜¸í™˜ì„±)
# ============================================================================

def test_multiple_files_with_interrupt(file_paths: list, dataset_id: str = None):
    """
    [Legacy] ì—¬ëŸ¬ CSV íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (Global Context ìœ ì§€)
    
    ì´ í•¨ìˆ˜ëŠ” ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤.
    ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ì—ì„œëŠ” test_batch_workflow()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    print("\n" + "ğŸŒ"*40)
    print("ğŸŒ LEGACY: Sequential Multi-File Processing")
    print("ğŸŒ (Use test_batch_workflow() for 2-Phase processing)")
    print("ğŸŒ"*40)
    
    from src.utils.dataset_detector import detect_dataset_from_path, get_dataset_source_path
    from src.utils.naming import extract_dataset_prefix
    
    if dataset_id is None and file_paths:
        dataset_id = detect_dataset_from_path(file_paths[0])
        if not dataset_id:
            dataset_id = "default_dataset"
    
    print(f"\nğŸ“ [Dataset-First] Dataset ID: {dataset_id}")
    print(f"   Prefix: {extract_dataset_prefix(dataset_id)}")
    
    from src.utils.ontology_manager import get_ontology_manager
    ontology_mgr = get_ontology_manager()
    
    print("\nğŸ“š [Ontology] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ í™•ì¸ ì¤‘...")
    shared_ontology = ontology_mgr.load(dataset_id=dataset_id)
    shared_ontology["dataset_id"] = dataset_id
    
    memory = MemorySaver()
    agent = build_single_agent(checkpointer=memory)  # ë‹¨ì¼ íŒŒì¼ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
    
    shared_context = {
        "master_anchor_name": None,
        "known_aliases": [],
        "example_id_values": []
    }
    
    from src.utils.dataset_detector import create_empty_data_catalog, create_dataset_info
    data_catalog = create_empty_data_catalog()
    
    if file_paths:
        source_path = get_dataset_source_path(file_paths[0])
        data_catalog["datasets"][dataset_id] = create_dataset_info(
            dataset_id=dataset_id,
            source_path=source_path
        )
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„¸ì…˜ ì „ì²´ì—ì„œ ê³µìœ )
    from datetime import datetime
    shared_conversation_history = {
        "session_id": f"legacy_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "dataset_id": dataset_id,
        "started_at": datetime.now().isoformat(),
        "turns": [],
        "classification_decisions": [],
        "anchor_decisions": [],
        "user_preferences": {}
    }
    
    results = []
    
    for idx, file_path in enumerate(file_paths, 1):
        print(f"\n\n{'#'*80}")
        print(f"# File {idx}/{len(file_paths)}: {os.path.basename(file_path)}")
        print(f"{'#'*80}")
        
        thread_config = {"configurable": {"thread_id": f"file-{idx}"}}
        
        initial_state = {
            "current_dataset_id": dataset_id,
            "current_table_name": None,
            "data_catalog": data_catalog,
            "file_path": file_path,
            "file_type": None,
            "raw_metadata": {},
            "finalized_anchor": None,
            "finalized_schema": [],
            "needs_human_review": False,
            "human_question": "",
            "human_feedback": None,
            "conversation_history": shared_conversation_history.copy(),  # [NEW]
            "logs": [],
            "retry_count": 0,
            "error_message": None,
            "project_context": shared_context.copy(),
            "ontology_context": shared_ontology.copy(),
            "skip_indexing": False
        }
        
        try:
            print(f"\nâ–¶ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n")
            
            final_state = None
            for event in agent.stream(initial_state, thread_config, stream_mode="values"):
                if "logs" in event and event["logs"]:
                    last_log = event["logs"][-1]
                    if not final_state or last_log not in final_state.get("logs", []):
                        print(f"ğŸ“ {last_log}")
                
                final_state = event
                
                if event.get("needs_human_review"):
                    print("\n")
                    print("â–ˆ" * 80)
                    print("â–ˆ" + " " * 30 + "âš ï¸  ì‚¬ìš©ì í™•ì¸ í•„ìš”" + " " * 29 + "â–ˆ")
                    print("â–ˆ" * 80)
                    
                    question = event.get("human_question", "í™•ì¸ í•„ìš”")
                    print(question)
                    
                    print("\n" + "â”€" * 80)
                    print("ğŸ’¡ ì…ë ¥ ì•ˆë‚´:")
                    print("   - ì»¬ëŸ¼ëª… ì…ë ¥: í•´ë‹¹ ì»¬ëŸ¼ì„ Anchorë¡œ ì§€ì •")
                    print("   - 'skip' ì…ë ¥: ì´ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                    print("   - Enterë§Œ ì…ë ¥: ìë™ ì²˜ë¦¬")
                    print("â”€" * 80)
                    
                    user_feedback = input("\n>>> ì…ë ¥: ").strip()
                    
                    if not user_feedback:
                        print("âš ï¸  ì…ë ¥ ì—†ìŒ. ìë™ ì²˜ë¦¬...")
                        continue
                    
                    print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                    print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                    
                    update_state = {
                        "human_feedback": user_feedback,
                        "needs_human_review": False
                    }
                    
                    for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                        if "logs" in event2 and event2["logs"]:
                            last_log = event2["logs"][-1]
                            if last_log not in event.get("logs", []):
                                print(f"ğŸ“ {last_log}")
                        final_state = event2
            
            if final_state:
                shared_context = final_state.get('project_context', shared_context)
                shared_ontology = final_state.get('ontology_context', shared_ontology)
                # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (íŒŒì¼ ê°„ ê³µìœ )
                if final_state.get('conversation_history'):
                    shared_conversation_history = final_state.get('conversation_history')
                
                results.append({
                    'file': file_path,
                    'success': True,
                    'anchor': final_state.get('finalized_anchor'),
                    'was_metadata': final_state.get('skip_indexing', False)
                })
                
                print(f"\nâœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(file_path)}")
                print(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(shared_conversation_history.get('turns', []))}ê°œ í„´")
                print(f"ğŸ”„ Global Context ì—…ë°ì´íŠ¸:")
                print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
                print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
            else:
                results.append({'file': file_path, 'success': False})
                
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
            results.append({'file': file_path, 'success': False})
    
    # ìµœì¢… ìš”ì•½
    print("\n\n" + "="*80)
    print("ğŸ“Š FINAL SUMMARY - All Files")
    print("="*80)
    print(f"\nâœ… Successfully processed: {sum(1 for r in results if r['success'])}/{len(results)} files")
    
    metadata_files = [r for r in results if r.get('was_metadata')]
    data_files = [r for r in results if not r.get('was_metadata')]
    
    print(f"\nğŸ“– Metadata Files: {len(metadata_files)}ê°œ")
    for r in metadata_files:
        print(f"   â€¢ {os.path.basename(r['file'])} â†’ ì˜¨í†¨ë¡œì§€ ì¶”ê°€ë¨")
    
    print(f"\nğŸ“Š Data Files: {len(data_files)}ê°œ")
    for r in data_files:
        print(f"   â€¢ {os.path.basename(r['file'])}")
        if r.get('anchor'):
            anchor = r['anchor']
            print(f"      â†’ Anchor: {anchor.get('column_name')} (mapped: {anchor.get('mapped_to_master', 'N/A')})")
    
    print(f"\nğŸŒ Final Global Context:")
    print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
    print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
    
    print(f"\nğŸ“š Ontology Context:")
    print(f"   - ì´ ìš©ì–´: {len(shared_ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(shared_ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(shared_ontology.get('hierarchy', []))}ê°œ")
    print(f"   - íƒœê·¸ëœ íŒŒì¼: {len(shared_ontology.get('file_tags', {}))}ê°œ")
    
    print(ontology_mgr.export_summary())


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜ - 2-Phase Batch Workflow"""
    from pathlib import Path
    
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼
    inspire_files = sorted(glob.glob(str(data_dir / "INSPIRE_130K_1.3/*.csv")))
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # VitalDB ë°ì´í„°ë§Œ ì²˜ë¦¬ (CSV + Signal)
    all_files = vital_csv_files + vital_signal_files[:2]
    
    if not all_files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    print(f"\nğŸ“ Found {len(all_files)} files:")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    for f in vital_csv_files:
        print(f"      - {os.path.basename(f)}")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ")
    
    # Dataset ID ê°ì§€
    from src.utils.dataset_detector import detect_dataset_from_path
    dataset_id = None
    if all_files:
        dataset_id = detect_dataset_from_path(all_files[0])
    print(f"\nğŸ“ [Dataset-First] Detected Dataset: {dataset_id}")
    
    # â­ 2-Phase Batch Workflow ì‹¤í–‰
    test_batch_workflow(all_files, dataset_id=dataset_id)
    
    # ìºì‹œ í†µê³„ ì¶œë ¥
    from src.utils.llm_cache import get_llm_cache
    cache = get_llm_cache()
    cache.print_stats()
    
    # [DISABLED] VectorDB ì„ë² ë”© ìƒì„± - ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ ë¹„í™œì„±í™”
    # í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì„¸ìš”
    # print("\n" + "="*80)
    # print(f"ğŸ”¢ [VectorDB] ì„ë² ë”© ìƒì„± ì‹œì‘... (dataset: {dataset_id})")
    # print("="*80)
    # 
    # try:
    #     from src.knowledge.vector_store import VectorStore
    #     from src.utils.ontology_manager import get_ontology_manager
    #     
    #     ontology_mgr = get_ontology_manager()
    #     ontology = ontology_mgr.load(dataset_id=dataset_id)
    #     
    #     if ontology and (ontology.get("definitions") or ontology.get("column_metadata")):
    #         vector_store = VectorStore()
    #         vector_store.initialize()
    #         vector_store.build_index(ontology, dataset_id=dataset_id)
    #         
    #         stats = vector_store.get_stats()
    #         print(f"\nâœ… [VectorDB] ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    #         print(f"   - Dataset: {dataset_id}")
    #         print(f"   - Provider: {stats.get('provider')}")
    #         print(f"   - Dimensions: {stats.get('dimensions')}")
    #         print(f"   - Total Embeddings: {stats.get('total', 0)}ê°œ")
    #     else:
    #         print("âš ï¸  [VectorDB] ì„ë² ë”©í•  ë°ì´í„° ì—†ìŒ (ì˜¨í†¨ë¡œì§€ ë¹„ì–´ìˆìŒ)")
    # except Exception as e:
    #     print(f"âš ï¸  [VectorDB] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    #     print("   (pgvector ë¯¸ì„¤ì¹˜ ì‹œ: brew install pgvector)")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()
