#!/usr/bin/env python3
# test_agent_with_interrupt.py
"""
LangGraphì˜ ê³µì‹ Interrupt ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œ Human-in-the-Loop í…ŒìŠ¤íŠ¸
ì—¬ëŸ¬ CSV íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©° Global Contextë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
"""

import sys
import os
import glob

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from agents.graph import build_agent
from langgraph.checkpoint.memory import MemorySaver


def test_with_interrupt(file_path: str):
    """
    LangGraph Interrupt ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ Human Feedback í…ŒìŠ¤íŠ¸
    """
    print("="*80)
    print("ğŸš€ LangGraph Interrupt ë°©ì‹ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # 1. Checkpointer ìƒì„± (State ì €ì¥/ë³µì›ìš©)
    memory = MemorySaver()
    
    # 2. Agent ë¹Œë“œ (checkpointer ì „ë‹¬)
    agent = build_agent(checkpointer=memory)
    
    # 3. ì´ˆê¸° ìƒíƒœ ë° ì„¤ì •
    initial_state = {
        "file_path": file_path,
        "file_type": None,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "logs": [],
        "retry_count": 0,
        "error_message": None,
        "project_context": {
            "master_anchor_name": None,
            "known_aliases": [],
            "example_id_values": []
        }
    }
    
    # Thread ID (ê°™ì€ ì„¸ì…˜ì„ ì¶”ì í•˜ê¸° ìœ„í•¨)
    thread_config = {"configurable": {"thread_id": "test-session-1"}}
    
    print(f"\nğŸ“ íŒŒì¼: {os.path.basename(file_path)}")
    print(f"ğŸ§µ Thread ID: {thread_config['configurable']['thread_id']}\n")
    
    # 4. ì‹¤í–‰ (Interrupt ë°œìƒ ì‹œ ë©ˆì¶¤)
    try:
        print("â–¶ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n")
        
        # stream()ìœ¼ë¡œ ë‹¨ê³„ë³„ ì‹¤í–‰ í™•ì¸
        for event in agent.stream(initial_state, thread_config, stream_mode="values"):
            # ê° ë…¸ë“œ ì‹¤í–‰ í›„ state ì¶œë ¥
            if "logs" in event and event["logs"]:
                print(f"ğŸ“ {event['logs'][-1]}")
            
            # needs_human_review ì²´í¬
            if event.get("needs_human_review"):
                print("\n" + "ğŸ›‘"*40)
                print("âš ï¸  HUMAN REVIEW REQUIRED - Workflow Interrupted")
                print("ğŸ›‘"*40)
                
                question = event.get("human_question", "í™•ì¸ í•„ìš”")
                print(f"\nì§ˆë¬¸:\n{question}\n")
                
                # ì‚¬ìš©ì ì…ë ¥
                user_feedback = input(">>> Anchor ì»¬ëŸ¼ëª… ì…ë ¥: ").strip()
                
                if not user_feedback:
                    print("âš ï¸  ì…ë ¥ ì—†ìŒ. 'unknown' ì‚¬ìš©")
                    user_feedback = "unknown"
                
                print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                
                # 5. State ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰ (ê°™ì€ thread_id ì‚¬ìš©)
                print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                
                # Noneìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ì „ state ìœ ì§€í•˜ë©´ì„œ íŠ¹ì • í•„ë“œë§Œ ë³€ê²½
                update_state = {
                    "human_feedback": user_feedback,
                    "needs_human_review": False
                }
                
                # ì¬ì‹¤í–‰ (update í›„)
                for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                    if "logs" in event2 and event2["logs"]:
                        last_log = event2["logs"][-1]
                        if last_log not in event.get("logs", []):
                            print(f"ğŸ“ {last_log}")
        
        print("\n" + "="*80)
        print("âœ… WORKFLOW COMPLETED")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


def test_multiple_files_with_interrupt(file_paths: list):
    """
    ì—¬ëŸ¬ CSV íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (Global Context ìœ ì§€)
    """
    print("\n" + "ğŸŒ"*40)
    print("ğŸŒ MULTI-FILE PROCESSING with Global Context")
    print("ğŸŒ"*40)
    
    # Ontology Manager ì´ˆê¸°í™” ë° ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ë¡œë“œ
    from src.utils.ontology_manager import get_ontology_manager
    ontology_mgr = get_ontology_manager()
    
    print("\nğŸ“š [Ontology] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ í™•ì¸ ì¤‘...")
    shared_ontology = ontology_mgr.load()
    
    # Checkpointer ìƒì„± (ëª¨ë“  íŒŒì¼ì´ ê³µìœ )
    memory = MemorySaver()
    agent = build_agent(checkpointer=memory)
    
    # ê³µìœ  Project Context
    shared_context = {
        "master_anchor_name": None,
        "known_aliases": [],
        "example_id_values": []
    }
    
    results = []
    
    for idx, file_path in enumerate(file_paths, 1):
        print(f"\n\n{'#'*80}")
        print(f"# File {idx}/{len(file_paths)}: {os.path.basename(file_path)}")
        print(f"{'#'*80}")
        
        # Thread ID (ê° íŒŒì¼ë§ˆë‹¤ ë‹¤ë¥¸ ì„¸ì…˜)
        thread_config = {"configurable": {"thread_id": f"file-{idx}"}}
        
        # ì´ˆê¸° ìƒíƒœ (ì´ì „ íŒŒì¼ì˜ Context ì „ë‹¬)
        initial_state = {
            "file_path": file_path,
            "file_type": None,
            "raw_metadata": {},
            "finalized_anchor": None,
            "finalized_schema": [],
            "needs_human_review": False,
            "human_question": "",
            "human_feedback": None,
            "logs": [],
            "retry_count": 0,
            "error_message": None,
            "project_context": shared_context.copy(),
            # [NEW] ê³µìœ  Ontology Context (ëˆ„ì ë¨)
            "ontology_context": shared_ontology.copy(),
            "skip_indexing": False
        }
        
        try:
            print(f"\nâ–¶ï¸  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n")
            
            # ì‹¤í–‰
            final_state = None
            for event in agent.stream(initial_state, thread_config, stream_mode="values"):
                # ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
                if "logs" in event and event["logs"]:
                    last_log = event["logs"][-1]
                    if not final_state or last_log not in final_state.get("logs", []):
                        print(f"ğŸ“ {last_log}")
                
                final_state = event
                
                # Human Review í•„ìš” ì‹œ
                if event.get("needs_human_review"):
                    print("\n" + "ğŸ›‘"*40)
                    print("âš ï¸  HUMAN REVIEW REQUIRED")
                    print("ğŸ›‘"*40)
                    
                    question = event.get("human_question", "í™•ì¸ í•„ìš”")
                    print(f"\nì§ˆë¬¸:\n{question}\n")
                    
                    # ì‚¬ìš©ì ì…ë ¥
                    user_feedback = input(">>> Anchor ì»¬ëŸ¼ëª… ì…ë ¥ (Enter=skip): ").strip()
                    
                    if not user_feedback:
                        print("âš ï¸  ì…ë ¥ ì—†ìŒ. ìë™ ì²˜ë¦¬ (3ë²ˆ ì¬ì‹œë„ í›„ ê°•ì œ ì§„í–‰)")
                        continue  # ìë™ ëª¨ë“œ
                    
                    print(f"\nâœ… ì…ë ¥ë°›ìŒ: '{user_feedback}'")
                    
                    # State ì—…ë°ì´íŠ¸ í›„ ì¬ì‹¤í–‰
                    print("\nğŸ”„ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹¤í–‰...\n")
                    update_state = {
                        "human_feedback": user_feedback,
                        "needs_human_review": False
                    }
                    
                    # ì¬ì‹¤í–‰
                    for event2 in agent.stream(update_state, thread_config, stream_mode="values"):
                        if "logs" in event2 and event2["logs"]:
                            last_log = event2["logs"][-1]
                            if last_log not in event.get("logs", []):
                                print(f"ğŸ“ {last_log}")
                        final_state = event2
            
            # ì„±ê³µ
            if final_state:
                # Context ì—…ë°ì´íŠ¸ (ë‹¤ìŒ íŒŒì¼ì„ ìœ„í•´)
                shared_context = final_state.get('project_context', shared_context)
                shared_ontology = final_state.get('ontology_context', shared_ontology)
                
                results.append({
                    'file': file_path,
                    'success': True,
                    'anchor': final_state.get('finalized_anchor'),
                    'was_metadata': final_state.get('skip_indexing', False)
                })
                
                print(f"\nâœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(file_path)}")
                print(f"ğŸ”„ Global Context ì—…ë°ì´íŠ¸:")
                print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
                print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
                print(f"ğŸ”„ Ontology Context:")
                print(f"   - ìš©ì–´ ìˆ˜: {len(shared_ontology.get('definitions', {}))}ê°œ")
                print(f"   - íŒŒì¼ íƒœê·¸: {len(shared_ontology.get('file_tags', {}))}ê°œ")
            else:
                results.append({'file': file_path, 'success': False})
                
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
            results.append({'file': file_path, 'success': False})
    
    # ìµœì¢… ìš”ì•½
    print("\n\n" + "="*80)
    print("ğŸ“Š FINAL SUMMARY - All Files")
    print("="*80)
    print(f"\nâœ… Successfully processed: {sum(1 for r in results if r['success'])}/{len(results)} files")
    
    # ë©”íƒ€ë°ì´í„° vs ë°ì´í„° ë¶„ë¦¬
    metadata_files = [r for r in results if r.get('was_metadata')]
    data_files = [r for r in results if not r.get('was_metadata')]
    
    print(f"\nğŸ“– Metadata Files: {len(metadata_files)}ê°œ")
    for r in metadata_files:
        print(f"   â€¢ {os.path.basename(r['file'])} â†’ ì˜¨í†¨ë¡œì§€ ì¶”ê°€ë¨")
    
    print(f"\nğŸ“Š Data Files: {len(data_files)}ê°œ")
    for r in data_files:
        print(f"   â€¢ {os.path.basename(r['file'])}")
        if r.get('anchor'):
            anchor = r['anchor']
            print(f"      â†’ Anchor: {anchor.get('column_name')} (mapped: {anchor.get('mapped_to_master', 'N/A')})")
    
    print(f"\nğŸŒ Final Global Context:")
    print(f"   - Master Anchor: {shared_context.get('master_anchor_name')}")
    print(f"   - Known Aliases: {shared_context.get('known_aliases')}")
    
    print(f"\nğŸ“š Ontology Context:")
    print(f"   - ì´ ìš©ì–´: {len(shared_ontology.get('definitions', {}))}ê°œ")
    print(f"   - ê´€ê³„: {len(shared_ontology.get('relationships', []))}ê°œ")
    print(f"   - ê³„ì¸µ: {len(shared_ontology.get('hierarchy', []))}ê°œ")
    print(f"   - íƒœê·¸ëœ íŒŒì¼: {len(shared_ontology.get('file_tags', {}))}ê°œ")
    
    # ì˜¨í†¨ë¡œì§€ ìš”ì•½ ì¶œë ¥
    print(ontology_mgr.export_summary())


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    from pathlib import Path
    
    data_dir = Path(__file__).parent / "data" / "raw" / "INSPIRE_130K_1.3"
    
    # raw ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸°
    csv_files = sorted(glob.glob(str(data_dir / "*.csv")))
    
    if not csv_files:
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    print(f"\nğŸ“ Found {len(csv_files)} CSV files:")
    for f in csv_files:
        print(f"   - {os.path.basename(f)}")
    
    # ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬
    test_multiple_files_with_interrupt(csv_files)
    
    # ìºì‹œ í†µê³„ ì¶œë ¥ (ì „ì—­ ìºì‹œ import)
    from src.utils.llm_cache import get_llm_cache
    cache = get_llm_cache()
    cache.print_stats()


if __name__ == "__main__":
    main()

