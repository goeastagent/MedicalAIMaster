#!/usr/bin/env python
"""
Phase 0 + 0.5 í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (LangGraph ê¸°ë°˜)

Phase 0: Data Catalog (íŒŒì¼ ìŠ¤ìº” â†’ DB ì €ì¥)
Phase 0.5: Schema Aggregation (ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„ â†’ LLM ë°°ì¹˜ ì¤€ë¹„)
"""

import sys
import os
import glob
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from langgraph.checkpoint.memory import MemorySaver

from agents.graph import build_phase05_only_agent
from agents.nodes.aggregator import run_aggregation, get_aggregation_stats
from database.schema_catalog import init_catalog_schema
from config import Phase05Config


def get_input_files():
    """
    test_agent_with_interrupt.pyì™€ ë™ì¼í•œ íŒŒì¼ ëª©ë¡ ë°˜í™˜
    """
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # VitalDB ë°ì´í„°ë§Œ ì²˜ë¦¬ (CSV + Signal ì²˜ìŒ 2ê°œ)
    all_files = vital_csv_files + vital_signal_files[:2]
    
    return all_files, vital_csv_files, vital_signal_files


def test_phase05_workflow(all_files: list):
    """
    LangGraphë¥¼ ì‚¬ìš©í•œ Phase 0 + 0.5 ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    """
    print("=" * 60)
    print("ğŸ”„ Running Phase 0 + 0.5 via LangGraph Workflow")
    print("=" * 60)
    
    print(f"\nğŸ“ Input Files: {len(all_files)}ê°œ")
    for f in all_files[:5]:
        print(f"   - {os.path.basename(f)}")
    if len(all_files) > 5:
        print(f"   ... and {len(all_files) - 5} more")
    print()
    
    # LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ
    memory = MemorySaver()
    agent = build_phase05_only_agent(checkpointer=memory)
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        # Phase 0 í•„ìˆ˜ í•„ë“œ
        "input_files": all_files,
        "phase0_result": None,
        "phase0_file_ids": [],
        
        # Phase 0.5 í•„ìˆ˜ í•„ë“œ
        "phase05_result": None,
        "unique_columns": [],
        "llm_batches": [],
        
        # ê¸°íƒ€ í•„ë“œ (AgentState í˜¸í™˜)
        "current_dataset_id": "test_dataset",
        "current_table_name": None,
        "data_catalog": {},
        "classification_result": None,
        "processing_progress": {
            "phase": "phase0",
            "metadata_processed": [],
            "data_processed": [],
            "current_file": None,
            "current_file_index": 0,
            "total_files": len(all_files)
        },
        "file_path": "",
        "file_type": None,
        "raw_metadata": {},
        "entity_identification": None,
        "finalized_schema": [],
        "entity_understanding": None,
        "needs_human_review": False,
        "human_question": "",
        "human_feedback": None,
        "review_type": None,
        "conversation_history": {},
        "logs": [],
        "ontology_context": {},
        "skip_indexing": False,
        "retry_count": 0,
        "error_message": None,
        "project_context": {}
    }
    
    # Thread ì„¤ì •
    thread_config = {"configurable": {"thread_id": "phase05-test-1"}}
    
    print("â–¶ï¸  LangGraph Phase 0 + 0.5 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...\n")
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    final_state = None
    for event in agent.stream(initial_state, thread_config, stream_mode="values"):
        final_state = event
    
    return final_state


def test_show_aggregation_result(final_state: dict):
    """ì§‘ê³„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Aggregation Result Details")
    print("=" * 60)
    
    if not final_state:
        print("âŒ No final state available")
        return
    
    # Phase 0.5 ê²°ê³¼
    phase05_result = final_state.get("phase05_result", {})
    unique_columns = final_state.get("unique_columns", [])
    llm_batches = final_state.get("llm_batches", [])
    
    print(f"\nğŸ“ˆ Summary:")
    print(f"   Total columns in DB: {phase05_result.get('total_columns_in_db', 0):,}")
    print(f"   Unique columns: {phase05_result.get('unique_column_count', 0):,}")
    print(f"   Batch size: {phase05_result.get('batch_size', 0)}")
    print(f"   Total batches: {phase05_result.get('total_batches', 0)}")
    
    # ìœ ë‹ˆí¬ ì»¬ëŸ¼ ìƒì„¸ (ì²˜ìŒ 20ê°œ)
    if unique_columns:
        print(f"\nğŸ“‹ Unique Columns (top 20 by frequency):")
        print("-" * 90)
        print(f"  {'#':<4} {'Column Name':<35} {'Type':<12} {'Freq':<6} {'Stats'}")
        print("-" * 90)
        
        for i, col in enumerate(unique_columns[:20]):
            name = col.get('original_name', '?')[:33]
            col_type = col.get('column_type', 'unknown')[:10]
            freq = col.get('frequency', 0)
            
            # í†µê³„ ìš”ì•½
            stats = []
            if col.get('avg_min') is not None:
                stats.append(f"range=[{col.get('avg_min'):.1f}, {col.get('avg_max'):.1f}]")
            if col.get('avg_unique_ratio') is not None:
                stats.append(f"unique_ratio={col.get('avg_unique_ratio'):.2f}")
            if col.get('sample_values'):
                values = list(col['sample_values'].keys())[:3]
                stats.append(f"values={values}")
            
            stats_str = ", ".join(stats) if stats else "-"
            print(f"  {i+1:<4} {name:<35} {col_type:<12} {freq:<6} {stats_str}")
        
        if len(unique_columns) > 20:
            print(f"  ... and {len(unique_columns) - 20} more")
    
    # ë°°ì¹˜ ì •ë³´
    if llm_batches:
        print(f"\nğŸ“¦ LLM Batches Preview:")
        print("-" * 50)
        for i, batch in enumerate(llm_batches[:3]):
            batch_cols = [c.get('original_name', '?') for c in batch[:5]]
            batch_preview = ", ".join(batch_cols)
            if len(batch) > 5:
                batch_preview += f" ... (+{len(batch) - 5})"
            print(f"   Batch {i+1}: [{batch_preview}] ({len(batch)} columns)")
        
        if len(llm_batches) > 3:
            print(f"   ... and {len(llm_batches) - 3} more batches")
    
    print()


def test_show_column_type_distribution(unique_columns: list):
    """ì»¬ëŸ¼ íƒ€ì…ë³„ ë¶„í¬"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Column Type Distribution")
    print("=" * 60)
    
    if not unique_columns:
        print("âŒ No unique columns available")
        return
    
    # íƒ€ì…ë³„ ì§‘ê³„
    type_counts = {}
    for col in unique_columns:
        col_type = col.get('column_type', 'unknown')
        type_counts[col_type] = type_counts.get(col_type, 0) + 1
    
    total = len(unique_columns)
    print(f"\n{'Type':<15} {'Count':<10} {'Percentage'}")
    print("-" * 40)
    
    for col_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
        pct = count / total * 100
        bar = "â–ˆ" * int(pct / 5)
        print(f"{col_type:<15} {count:<10} {pct:5.1f}% {bar}")
    
    print(f"\n{'Total':<15} {total}")
    print()


def test_standalone_aggregation():
    """ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (LangGraph ì—†ì´)"""
    print("\n" + "=" * 60)
    print("ğŸ”§ Standalone Aggregation Test (without LangGraph)")
    print("=" * 60)
    
    result = run_aggregation(verbose=True)
    
    print(f"\nğŸ“Š Stats:")
    stats = result.get('stats', {})
    for key, value in stats.items():
        print(f"   {key}: {value}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("Phase 0 + 0.5 Schema Aggregation Test")
    print(f"(Batch Size: {Phase05Config.BATCH_SIZE})")
    print("=" * 60 + "\n")
    
    # ì…ë ¥ íŒŒì¼ ë¡œë“œ
    all_files, vital_csv_files, vital_signal_files = get_input_files()
    
    print(f"ğŸ“ Found files:")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ (using first 2)")
    print(f"   â¡ï¸  Total to process: {len(all_files)}ê°œ")
    print()
    
    if not all_files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return 1
    
    try:
        # 1. ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” (Phase 0 í…ŒìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ ë°ì´í„°ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        print("=" * 60)
        print("1. Ensuring Schema Exists")
        print("=" * 60)
        init_catalog_schema(reset=True)  # ê¹¨ë—í•œ ìƒíƒœë¡œ ì‹œì‘
        print("âœ“ Schema ready\n")
        
        # 2. LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (Phase 0 + 0.5)
        final_state = test_phase05_workflow(all_files)
        
        # 3. ì§‘ê³„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        test_show_aggregation_result(final_state)
        
        # 4. ì»¬ëŸ¼ íƒ€ì… ë¶„í¬
        unique_columns = final_state.get("unique_columns", []) if final_state else []
        test_show_column_type_distribution(unique_columns)
        
        # 5. Stateì— ì €ì¥ëœ file_ids í™•ì¸
        print("=" * 60)
        print("ğŸ“‹ State Summary")
        print("=" * 60)
        
        if final_state:
            file_ids = final_state.get("phase0_file_ids", [])
            print(f"\n   phase0_file_ids: {len(file_ids)} files")
            
            unique_cols = final_state.get("unique_columns", [])
            print(f"   unique_columns: {len(unique_cols)} columns")
            
            batches = final_state.get("llm_batches", [])
            print(f"   llm_batches: {len(batches)} batches")
            print(f"      â†’ Ready for Phase 1 LLM processing!")
        
        print("\n" + "=" * 60)
        print("âœ… All tests completed!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

