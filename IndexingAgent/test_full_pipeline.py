#!/usr/bin/env python3
"""
Test Full Pipeline: Phase 0 â†’ Phase 0.5 â†’ Phase 1

Phase 0ë¶€í„° Phase 1ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
Human ReviewëŠ” CLIì—ì„œ ì§ì ‘ input()ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (no interrupt).
"""

import os
import sys
import glob
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.database.connection import get_db_manager
from src.database.schema_catalog import init_catalog_schema
from src.config import Phase1Config


# =============================================================================
# Input File Loader (same as test_phase0.py)
# =============================================================================

def get_input_files():
    """
    test_phase0.py / test_agent_with_interrupt.pyì™€ ë™ì¼í•œ íŒŒì¼ ëª©ë¡ ë°˜í™˜
    
    Returns:
        tuple: (all_files, vital_csv_files, vital_signal_files, inspire_files)
    """
    data_dir = Path(__file__).parent / "data" / "raw"
    
    # CSV íŒŒì¼
    inspire_files = sorted(glob.glob(str(data_dir / "INSPIRE_130K_1.3/*.csv")))
    vital_csv_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/*.csv")))
    vital_signal_files = sorted(glob.glob(str(data_dir / "Open_VitalDB_1.0.0/vital_files/*.vital")))
    
    # VitalDB ë°ì´í„°ë§Œ ì²˜ë¦¬ (CSV + Signal ì²˜ìŒ 2ê°œ)
    all_files = vital_csv_files + vital_signal_files[:2]
    
    return all_files, vital_csv_files, vital_signal_files, inspire_files


# =============================================================================
# Helper Functions
# =============================================================================

def print_separator(title: str = "", char: str = "=", width: int = 70):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    if title:
        padding = (width - len(title) - 2) // 2
        print(f"\n{char * padding} {title} {char * padding}")
    else:
        print(char * width)


def show_db_status():
    """DB ìƒíƒœ ì¶œë ¥"""
    print_separator("DB Status Check", "-")
    
    db = get_db_manager()
    conn = None
    cursor = None
    
    try:
        conn = db.get_connection()
        cursor = conn.cursor()
        
        # file_catalog í†µê³„
        cursor.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(semantic_type) as with_semantic,
                AVG(llm_confidence) as avg_conf
            FROM file_catalog
        """)
        row = cursor.fetchone()
        
        print(f"\nğŸ“ file_catalog:")
        print(f"   Total files: {row[0]}")
        print(f"   With semantic: {row[1]}")
        print(f"   Avg confidence: {row[2]:.2f}" if row[2] else "   Avg confidence: N/A")
        
        # column_metadata í†µê³„
        cursor.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(semantic_name) as with_semantic,
                AVG(llm_confidence) as avg_conf
            FROM column_metadata
        """)
        row = cursor.fetchone()
        
        print(f"\nğŸ“Š column_metadata:")
        print(f"   Total columns: {row[0]}")
        print(f"   With semantic: {row[1]}")
        print(f"   Avg confidence: {row[2]:.2f}" if row[2] else "   Avg confidence: N/A")
        
        conn.commit()
        
    except Exception as e:
        print(f"   âŒ Error querying DB: {e}")
        if conn:
            conn.rollback()
    finally:
        if cursor:
            cursor.close()


def show_sample_results():
    """ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥"""
    print_separator("Sample Results", "-")
    
    db = get_db_manager()
    conn = None
    cursor = None
    
    try:
        conn = db.get_connection()
        cursor = conn.cursor()
        
        # íŒŒì¼ ì „ì²´
        cursor.execute("""
            SELECT file_name, semantic_type, semantic_name, domain, llm_confidence
            FROM file_catalog
            WHERE llm_confidence IS NOT NULL
            ORDER BY llm_confidence DESC
        """)
        rows = cursor.fetchall()
        
        if rows:
            print(f"\nğŸ“ File Analysis Results (all {len(rows)} files):")
            for r in rows:
                conf = f"{r[4]:.2f}" if r[4] else "N/A"
                print(f"   {r[0]:30} â†’ {r[1] or 'N/A':20} [{r[3] or 'N/A':15}] conf={conf}")
        
        # ì»¬ëŸ¼ ìƒ˜í”Œ
        cursor.execute("""
            SELECT original_name, semantic_name, concept_category, llm_confidence
            FROM column_metadata
            WHERE llm_confidence IS NOT NULL
            ORDER BY llm_confidence DESC
            LIMIT 10
        """)
        rows = cursor.fetchall()
        
        if rows:
            print(f"\nğŸ“Š Column Analysis Results (top 10):")
            for r in rows:
                conf = f"{r[3]:.2f}" if r[3] else "N/A"
                print(f"   {r[0]:30} â†’ {r[1] or 'N/A':25} [{r[2] or 'N/A':15}] conf={conf}")
        
        conn.commit()
        
    except Exception as e:
        print(f"   âŒ Error: {e}")
        if conn:
            conn.rollback()
    finally:
        if cursor:
            cursor.close()


def reset_database():
    """DB ì´ˆê¸°í™”"""
    print_separator("Resetting Database", "-")
    
    try:
        init_catalog_schema(reset=True)
        print("   âœ… Database reset complete")
    except Exception as e:
        print(f"   âŒ Error resetting database: {e}")
        raise


# =============================================================================
# Main Test
# =============================================================================

def run_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    # ì…ë ¥ íŒŒì¼ ë¡œë“œ (test_phase0.pyì™€ ë™ì¼)
    all_files, vital_csv_files, vital_signal_files, inspire_files = get_input_files()
    
    print_separator("ğŸš€ FULL PIPELINE TEST", "=", 70)
    print(f"   Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   Input files: {len(all_files)}")
    print(f"   Confidence threshold: {Phase1Config.CONFIDENCE_THRESHOLD}")
    print(f"   Max review retries: {Phase1Config.MAX_REVIEW_RETRIES}")
    
    # íŒŒì¼ êµ¬ì„± ì¶œë ¥
    print(f"\nğŸ“ Found files:")
    print(f"   ğŸ“Š VitalDB CSV: {len(vital_csv_files)}ê°œ")
    print(f"   ğŸ“ˆ VitalDB Signal: {len(vital_signal_files)}ê°œ (using first 2)")
    print(f"   ğŸ“‹ INSPIRE CSV: {len(inspire_files)}ê°œ (not used)")
    print(f"   â¡ï¸  Total to process: {len(all_files)}ê°œ")
    
    # 1. DB ì´ˆê¸°í™”
    reset_database()
    
    # 2. íŒŒì¼ ì¡´ì¬ í™•ì¸
    print_separator("Checking Input Files", "-")
    valid_files = []
    
    for f in all_files:
        fp = Path(f)
        if fp.exists():
            print(f"   âœ… {fp.name} ({fp.stat().st_size / 1024:.1f} KB)")
            valid_files.append(str(fp))
        else:
            print(f"   âŒ {fp.name} (not found)")
    
    if not valid_files:
        print("\nâŒ No valid input files found!")
        return
    
    # 3. LangGraph Agent ë¹Œë“œ ë° ì‹¤í–‰
    print_separator("Building LangGraph Agent", "-")
    
    from src.agents.graph import build_phase1_only_agent
    from src.agents.state import AgentState
    
    agent = build_phase1_only_agent()
    print("   âœ… Agent built successfully")
    
    # 4. Initial State ì„¤ì •
    initial_state: AgentState = {
        "messages": [],
        "input_files": valid_files,
        "input_directory": None,
        "phase0_result": None,
        "phase0_file_ids": [],
        "phase05_result": None,
        "unique_columns": [],
        "unique_files": [],
        "column_batches": [],
        "file_batches": [],
        "phase1_result": None,
        "column_semantic_mappings": [],
        "file_semantic_mappings": [],
        "phase1_all_batch_states": [],
        "phase1_review_queue": None,
        "phase1_current_batch": None,
        "phase1_human_feedback": None,
    }
    
    config = {"configurable": {"thread_id": "full_pipeline_test_001"}}
    
    # 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    print_separator("ğŸ¬ Running Pipeline", "=", 70)
    print("\nğŸ’¡ When prompted for Human Review:")
    print("   [1] accept  - Accept current analysis")
    print("   [2] correct - Provide corrections (JSON)")
    print("   [3] skip    - Skip this batch")
    print("   [q] quit    - Exit immediately")
    print("")
    
    started_at = datetime.now()
    
    try:
        # ë‹¨ìˆœ invoke - interrupt ì—†ì´ ë…¸ë“œ ë‚´ë¶€ì—ì„œ Human Review ìˆ˜í–‰
        final_state = agent.invoke(initial_state, config)
        
        # 6. ê²°ê³¼ ì¶œë ¥
        print_separator("ğŸ“Š Pipeline Results", "=", 70)
        
        # Phase 0 ê²°ê³¼
        phase0 = final_state.get("phase0_result", {})
        print(f"\nğŸ“ Phase 0 (Data Catalog):")
        print(f"   Files processed: {phase0.get('processed_files', 0)}")
        print(f"   Files skipped: {phase0.get('skipped_files', 0)}")
        print(f"   Files failed: {phase0.get('failed_files', 0)}")
        
        # Phase 0.5 ê²°ê³¼
        phase05 = final_state.get("phase05_result", {})
        print(f"\nğŸ“‹ Phase 0.5 (Aggregation):")
        print(f"   Unique columns: {phase05.get('unique_column_count', 0)}")
        print(f"   Unique files: {phase05.get('unique_file_count', 0)}")
        print(f"   Column batches: {phase05.get('column_batch_count', 0)}")
        print(f"   File batches: {phase05.get('file_batch_count', 0)}")
        
        # Phase 1 ê²°ê³¼
        phase1 = final_state.get("phase1_result", {})
        print(f"\nğŸ§  Phase 1 (Semantic Analysis):")
        print(f"   Columns analyzed: {phase1.get('total_columns_analyzed', 0)}")
        print(f"      - High confidence: {phase1.get('columns_high_conf', 0)}")
        print(f"      - Low confidence: {phase1.get('columns_low_conf', 0)}")
        print(f"   Files analyzed: {phase1.get('total_files_analyzed', 0)}")
        print(f"      - High confidence: {phase1.get('files_high_conf', 0)}")
        print(f"      - Low confidence: {phase1.get('files_low_conf', 0)}")
        print(f"   Review requests: {phase1.get('total_review_requests', 0)}")
        print(f"   Re-analyzes: {phase1.get('total_reanalyzes', 0)}")
        print(f"   Force accepted: {phase1.get('batches_force_accepted', 0)}")
        print(f"   Total LLM calls: {phase1.get('total_llm_calls', 0)}")
        
        # Duration
        ended_at = datetime.now()
        duration = (ended_at - started_at).total_seconds()
        print(f"\nâ±ï¸ Total Duration: {duration:.1f}s")
        
        # DB ìƒíƒœ
        show_db_status()
        
        # ìƒ˜í”Œ ê²°ê³¼
        show_sample_results()
        
        print_separator("âœ… TEST COMPLETED SUCCESSFULLY", "=", 70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Pipeline interrupted by user")
        show_db_status()
        
    except Exception as e:
        print(f"\nâŒ Pipeline error: {e}")
        import traceback
        traceback.print_exc()
        show_db_status()
        raise


# =============================================================================
# Entry Point
# =============================================================================

if __name__ == "__main__":
    run_full_pipeline()