# src/agents/base/mixins.py
"""
Mixins for agent nodes

Provides reusable functionality:
- LLMMixin: LLM client integration with retry logic
- DatabaseMixin: Database connection management
- LoggingMixin: Enhanced logging capabilities
"""

from typing import Dict, Any, Optional, List, Type
import json
from pydantic import BaseModel


# =============================================================================
# LLMMixin
# =============================================================================

class LLMMixin:
    """
    LLM í˜¸ì¶œ ê¸°ëŠ¥ ì œê³µ
    
    ì‚¬ìš©ë²•:
        class MyNode(BaseNode, LLMMixin):
            requires_llm = True
            
            def execute(self, state):
                response = self.call_llm(prompt, max_tokens=2000)
                return {"result": response}
    """
    
    _llm_client = None
    
    @property
    def llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ (lazy initialization)"""
        if self._llm_client is None:
            from src.utils.llm_client import get_llm_client
            self._llm_client = get_llm_client()
        return self._llm_client
    
    def call_llm(
        self,
        prompt: str,
        max_tokens: int = 4000,
        temperature: float = 0.1,
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """
        LLM í˜¸ì¶œ (í…ìŠ¤íŠ¸ ì‘ë‹µ)
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„ (0.0 ~ 1.0)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            return self.llm_client.ask(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                system_prompt=system_prompt
            )
        except Exception as e:
            if hasattr(self, 'log'):
                self.log(f"LLM call failed: {e}", "âŒ")
            return None
    
    def call_llm_json(
        self,
        prompt: str,
        max_tokens: int = 4000,
        temperature: float = 0.1,
        max_retries: int = 3,
        retry_delay: float = 1.0
    ) -> Optional[Dict[str, Any]]:
        """
        LLM í˜¸ì¶œ (JSON ì‘ë‹µ)
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            íŒŒì‹±ëœ JSON dict ë˜ëŠ” None
        """
        import time
        
        for attempt in range(max_retries):
            try:
                response = self.llm_client.ask_json(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if response is not None:
                    return response
                    
            except Exception as e:
                if hasattr(self, 'log'):
                    self.log(f"LLM JSON call failed (attempt {attempt + 1}): {e}", "âš ï¸")
                    
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
        
        return None
    
    def call_llm_with_schema(
        self,
        prompt: str,
        response_model: Type[BaseModel],
        max_tokens: int = 4000,
        temperature: float = 0.1,
        max_retries: int = 3
    ) -> Optional[BaseModel]:
        """
        LLM í˜¸ì¶œ í›„ Pydantic ëª¨ë¸ë¡œ íŒŒì‹±
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            response_model: Pydantic ëª¨ë¸ í´ë˜ìŠ¤
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            Pydantic ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        response = self.call_llm_json(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            max_retries=max_retries
        )
        
        if response is None:
            return None
        
        try:
            return response_model.model_validate(response)
        except Exception as e:
            if hasattr(self, 'log'):
                self.log(f"Failed to parse LLM response to {response_model.__name__}: {e}", "âŒ")
            return None


# =============================================================================
# DatabaseMixin
# =============================================================================

class DatabaseMixin:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
    
    ì‚¬ìš©ë²•:
        class MyNode(BaseNode, DatabaseMixin):
            def execute(self, state):
                with self.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT ...")
    """
    
    _db_manager = None
    
    @property
    def db_manager(self):
        """DB ë§¤ë‹ˆì € (lazy initialization)"""
        if self._db_manager is None:
            from src.database import get_db_manager
            self._db_manager = get_db_manager()
        return self._db_manager
    
    def get_connection(self):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° íšë“
        
        Returns:
            psycopg2 connection
        """
        return self.db_manager.get_connection()
    
    def execute_query(
        self,
        query: str,
        params: tuple = None,
        fetch: str = "all"
    ) -> Optional[List[Any]]:
        """
        ì¿¼ë¦¬ ì‹¤í–‰ í—¬í¼
        
        Args:
            query: SQL ì¿¼ë¦¬
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            fetch: "all", "one", "none" ì¤‘ í•˜ë‚˜
            
        Returns:
            ì¿¼ë¦¬ ê²°ê³¼
        """
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(query, params)
            
            if fetch == "all":
                return cursor.fetchall()
            elif fetch == "one":
                return cursor.fetchone()
            else:
                conn.commit()
                return None
                
        except Exception as e:
            conn.rollback()
            if hasattr(self, 'log'):
                self.log(f"Query failed: {e}", "âŒ")
            raise
    
    def execute_many(
        self,
        query: str,
        params_list: List[tuple]
    ) -> int:
        """
        ì—¬ëŸ¬ í–‰ ì‚½ì…/ì—…ë°ì´íŠ¸
        
        Args:
            query: SQL ì¿¼ë¦¬
            params_list: íŒŒë¼ë¯¸í„° ëª©ë¡
            
        Returns:
            ì²˜ë¦¬ëœ í–‰ ìˆ˜
        """
        if not params_list:
            return 0
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.executemany(query, params_list)
            conn.commit()
            return len(params_list)
        except Exception as e:
            conn.rollback()
            if hasattr(self, 'log'):
                self.log(f"Batch query failed: {e}", "âŒ")
            raise


# =============================================================================
# LoggingMixin
# =============================================================================

class LoggingMixin:
    """
    í–¥ìƒëœ ë¡œê¹… ê¸°ëŠ¥
    
    BaseNodeì— ê¸°ë³¸ log() ë©”ì„œë“œê°€ ìˆì§€ë§Œ,
    ì´ Mixinì€ ì¶”ê°€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    _log_buffer: List[str] = []
    _verbose: bool = True
    
    def set_verbose(self, verbose: bool):
        """ë¡œê¹… ì¶œë ¥ ì—¬ë¶€ ì„¤ì •"""
        self._verbose = verbose
    
    def log_section(self, title: str):
        """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
        msg = f"\n--- {title} ---"
        self._log_buffer.append(msg)
        if self._verbose:
            print(msg)
    
    def log_progress(self, current: int, total: int, message: str = ""):
        """ì§„í–‰ë¥  ë¡œê¹…"""
        pct = (current / total * 100) if total > 0 else 0
        msg = f"   [{current}/{total}] ({pct:.0f}%) {message}"
        if self._verbose:
            print(msg, end='\r')
    
    def log_stats(self, stats: Dict[str, Any], title: str = "Statistics"):
        """í†µê³„ ì¶œë ¥"""
        lines = [f"\nğŸ“Š {title}:"]
        for key, value in stats.items():
            if isinstance(value, float):
                lines.append(f"   {key}: {value:.2f}")
            else:
                lines.append(f"   {key}: {value}")
        
        msg = "\n".join(lines)
        self._log_buffer.append(msg)
        if self._verbose:
            print(msg)
    
    def log_table(self, headers: List[str], rows: List[List[Any]], max_rows: int = 10):
        """í…Œì´ë¸” í˜•íƒœ ì¶œë ¥"""
        if not rows:
            return
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ê³„ì‚°
        widths = [len(h) for h in headers]
        for row in rows[:max_rows]:
            for i, cell in enumerate(row):
                widths[i] = max(widths[i], len(str(cell)))
        
        # í—¤ë”
        header_line = " | ".join(h.ljust(widths[i]) for i, h in enumerate(headers))
        separator = "-+-".join("-" * w for w in widths)
        
        lines = [header_line, separator]
        
        # í–‰
        for row in rows[:max_rows]:
            line = " | ".join(str(cell).ljust(widths[i]) for i, cell in enumerate(row))
            lines.append(line)
        
        if len(rows) > max_rows:
            lines.append(f"... and {len(rows) - max_rows} more rows")
        
        msg = "\n".join(lines)
        self._log_buffer.append(msg)
        if self._verbose:
            print(msg)
    
    def get_log_buffer(self) -> List[str]:
        """ëˆ„ì ëœ ë¡œê·¸ ë°˜í™˜"""
        return list(self._log_buffer)
    
    def clear_log_buffer(self):
        """ë¡œê·¸ ë²„í¼ ì´ˆê¸°í™”"""
        self._log_buffer = []

