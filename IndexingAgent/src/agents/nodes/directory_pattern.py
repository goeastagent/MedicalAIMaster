"""
Phase 1C: Directory Pattern Analysis Node

ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ , íŒŒì¼ëª…ì—ì„œ ID/ê°’ì„ ì¶”ì¶œí•˜ì—¬ 
ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ê´€ê³„ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

âœ… LLM ì‚¬ìš©:
  1. íŒŒì¼ëª… íŒ¨í„´ ì‹ë³„
  2. íŒ¨í„´ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ê°’ì´ Data Dictionaryì˜ ì–´ë–¤ ì»¬ëŸ¼ê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë‹¨

ì…ë ¥ (DBì—ì„œ ì½ê¸°):
  - directory_catalog.filename_samples (Phase -1ì—ì„œ ìˆ˜ì§‘)
  - column_metadata (Phase 1Aì—ì„œ ë¶„ì„ë¨)

ì¶œë ¥ (DBì— ì €ì¥):
  - directory_catalog.filename_pattern, filename_columns
  - file_catalog.filename_values (ë°°ì¹˜ ì—…ë°ì´íŠ¸)
"""

import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.database.connection import get_db_manager
from src.config import Phase1CConfig


# =============================================================================
# ì „ì—­ ë¦¬ì†ŒìŠ¤
# =============================================================================

_db_manager = None
_llm_client = None


def _get_db():
    """DB Manager ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _db_manager
    if _db_manager is None:
        _db_manager = get_db_manager()
    return _db_manager


def _get_llm():
    """LLM Client ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_client
    if _llm_client is None:
        from src.utils.llm_client import get_llm_client
        _llm_client = get_llm_client()
    return _llm_client


# =============================================================================
# LLM í”„ë¡¬í”„íŠ¸ (ì˜ì–´)
# =============================================================================

SYSTEM_PROMPT = """You are a medical dataset filename pattern analysis expert.

## Task
Given directory filename samples and a data dictionary, analyze:
1. Identify filename patterns for each directory
2. Determine which columns from the data dictionary match values extractable from filenames

## Output Format (JSON)
{
    "directories": [
        {
            "dir_id": "uuid-string",
            "has_pattern": true,
            "pattern": "{caseid:integer}.vital",
            "pattern_regex": "^(\\\\d+)\\\\.vital$",
            "columns": [
                {
                    "name": "caseid",
                    "type": "integer",
                    "position": 1,
                    "matched_column": "caseid",
                    "match_confidence": 0.95,
                    "match_reasoning": "Numeric value in filename matches caseid format in clinical_data"
                }
            ],
            "confidence": 0.95,
            "reasoning": "All 6388 files follow {number}.vital pattern"
        },
        {
            "dir_id": "uuid-string-2",
            "has_pattern": false,
            "pattern": null,
            "pattern_regex": null,
            "columns": [],
            "confidence": 0.9,
            "reasoning": "Various CSV files with no consistent naming pattern"
        }
    ]
}

## Rules
1. pattern_regex must be a valid PostgreSQL regex (use \\\\ for backslash in JSON)
2. position is 1-indexed capture group number
3. type should be "integer" or "text"
4. Only set has_pattern=true if a clear, consistent pattern exists
5. matched_column should reference exact column name from data dictionary
6. If no matching column found in data dictionary, set matched_column to null
"""

USER_PROMPT_TEMPLATE = """
## Data Dictionary
The following tables and columns are available in this dataset:

{data_dictionary}

## Directories to Analyze

{directories_info}

Analyze the filename patterns for each directory and match extractable values to data dictionary columns.
"""


# =============================================================================
# DB ì¡°íšŒ í•¨ìˆ˜ (íŒŒì¼ ì½ê¸° ì—†ìŒ - DBì—ì„œë§Œ ì¡°íšŒ)
# =============================================================================

def _get_directories_for_analysis() -> List[Dict]:
    """
    Query directories from directory_catalog (DB)
    
    Data source: directory_catalog table (populated by Phase -1)
    - filename_samples: collected during Phase -1 directory scan
    - file_extensions: counted during Phase -1
    - dir_type: classified during Phase -1
    """
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT dir_id, dir_path, dir_name, file_count, 
               file_extensions, filename_samples, dir_type
        FROM directory_catalog
        WHERE file_count >= %s
          AND filename_pattern IS NULL
        ORDER BY file_count DESC
    """, (Phase1CConfig.MIN_FILES_FOR_PATTERN,))
    
    directories = []
    for row in cursor.fetchall():
        samples = row[5] if row[5] else []
        # LLMì— ì „ë‹¬í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ
        limited_samples = samples[:Phase1CConfig.MAX_SAMPLES_PER_DIR]
        
        directories.append({
            "dir_id": str(row[0]),
            "dir_path": row[1],
            "dir_name": row[2],
            "file_count": row[3],
            "file_extensions": row[4] if row[4] else {},
            "filename_samples": limited_samples,
            "dir_type": row[6]
        })
    
    return directories


def _collect_data_dictionary() -> Dict[str, Any]:
    """
    Collect data dictionary from DB (Phase 1A/1B results)
    
    Data source: 
    - file_catalog: primary_entity, entity_identifier_column (from Phase 1A)
    - column_metadata: semantic_name, description, concept_category (from Phase 1B)
    
    NO file reading - all from DB
    """
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            fc.file_name,
            fc.primary_entity,
            fc.entity_identifier_column,
            cm.original_name,
            cm.semantic_name,
            cm.description,
            cm.value_distribution
        FROM file_catalog fc
        JOIN column_metadata cm ON fc.file_id = cm.file_id
        WHERE fc.is_metadata = FALSE
          AND (cm.description IS NOT NULL OR cm.semantic_name IS NOT NULL)
        ORDER BY fc.file_name, cm.col_id
    """)
    
    # Group by table
    tables = {}
    for row in cursor.fetchall():
        file_name = row[0]
        if file_name not in tables:
            tables[file_name] = {
                "primary_entity": row[1],
                "entity_identifier": row[2],
                "columns": []
            }
        
        # value_distributionì—ì„œ ìƒ˜í”Œ ê°’ ì¶”ì¶œ
        value_dist = row[6] if row[6] else {}
        examples = value_dist.get('samples', []) if isinstance(value_dist, dict) else []
        
        tables[file_name]["columns"].append({
            "name": row[3],
            "type": row[4],
            "description": row[5],
            "examples": examples
        })
    
    return tables


def _collect_data_dictionary_simple() -> Dict[str, Any]:
    """
    Data Dictionary ê°„ë‹¨ ë²„ì „ - Phase 1A ê²°ê³¼ê°€ ì—†ì–´ë„ ë™ì‘
    
    column_metadataì—ì„œ ì§ì ‘ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
    """
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    dict_entries = {}
    
    # data_dictionary í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ì¡°íšŒ
    try:
        cursor.execute("""
            SELECT 
                parameter_key,
                parameter_desc,
                parameter_unit,
                source_file_name
            FROM data_dictionary
            ORDER BY parameter_key
        """)
        
        for row in cursor.fetchall():
            key = row[0]
            if key not in dict_entries:
                dict_entries[key] = {
                    "description": row[1],
                    "unit": row[2],
                    "source": row[3]
                }
    except Exception as e:
        print(f"   âš ï¸ data_dictionary table not available: {e}")
        conn.rollback()
    
    # column_metadataì—ì„œ ID ê´€ë ¨ ì»¬ëŸ¼ ìˆ˜ì§‘
    cursor.execute("""
        SELECT DISTINCT
            fc.file_name,
            cm.original_name,
            cm.data_type,
            cm.value_distribution
        FROM file_catalog fc
        JOIN column_metadata cm ON fc.file_id = cm.file_id
        WHERE fc.is_metadata = FALSE
          AND (
              LOWER(cm.original_name) LIKE '%%id%%' 
              OR LOWER(cm.original_name) LIKE '%%case%%'
              OR LOWER(cm.original_name) LIKE '%%subject%%'
          )
        ORDER BY fc.file_name
    """)
    
    id_columns = {}
    for row in cursor.fetchall():
        file_name = row[0]
        if file_name not in id_columns:
            id_columns[file_name] = []
        
        # value_distributionì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
        value_dist = row[3] if row[3] else {}
        examples = value_dist.get('samples', []) if isinstance(value_dist, dict) else []
        
        id_columns[file_name].append({
            "name": row[1],
            "type": row[2],
            "examples": examples
        })
    
    return {
        "dictionary_entries": dict_entries,
        "id_columns_by_file": id_columns
    }


# =============================================================================
# ë°°ì¹˜ ì²˜ë¦¬
# =============================================================================

def _batch_directories(directories: List[Dict], batch_size: int) -> List[List[Dict]]:
    """ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ë¶„í• """
    batches = []
    for i in range(0, len(directories), batch_size):
        batches.append(directories[i:i + batch_size])
    return batches


# =============================================================================
# LLM ë¶„ì„
# =============================================================================

def _analyze_batch(
    directories: List[Dict], 
    data_dictionary: Dict
) -> List[Dict]:
    """
    Analyze directory batch with LLM
    
    Input: All from DB (directories from directory_catalog, data_dictionary from column_metadata)
    Output: Pattern analysis results
    """
    llm = _get_llm()
    
    # Build directories info for prompt
    dirs_info_parts = []
    for i, d in enumerate(directories):
        samples_str = "\n".join([f"  - {s}" for s in d['filename_samples']])
        dirs_info_parts.append(
            f"### Directory {i+1}: {d['dir_name']}\n"
            f"- dir_id: {d['dir_id']}\n"
            f"- File count: {d['file_count']}\n"
            f"- Extensions: {json.dumps(d['file_extensions'])}\n"
            f"- Type: {d['dir_type']}\n"
            f"- Filename samples:\n{samples_str}"
        )
    
    dirs_info = "\n\n".join(dirs_info_parts)
    
    user_prompt = USER_PROMPT_TEMPLATE.format(
        data_dictionary=json.dumps(data_dictionary, indent=2, ensure_ascii=False),
        directories_info=dirs_info
    )
    
    full_prompt = f"{SYSTEM_PROMPT}\n\n{user_prompt}"
    
    try:
        result = llm.ask_json(full_prompt)
        
        if result.get("error"):
            print(f"   âŒ LLM returned error: {result.get('error')}")
            return []
        
        return result.get("directories", [])
        
    except Exception as e:
        print(f"   âŒ LLM call error: {e}")
        return []


# =============================================================================
# DB ì €ì¥
# =============================================================================

def _save_pattern_results(results: List[Dict]):
    """Save pattern analysis results to directory_catalog"""
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    saved_count = 0
    
    for r in results:
        try:
            cursor.execute("""
                UPDATE directory_catalog
                SET filename_pattern = %s,
                    filename_columns = %s,
                    pattern_confidence = %s,
                    pattern_reasoning = %s,
                    pattern_analyzed_at = NOW()
                WHERE dir_id = %s
            """, (
                r.get("pattern"),
                json.dumps(r.get("columns", [])),
                r.get("confidence"),
                r.get("reasoning"),
                r["dir_id"]
            ))
            saved_count += 1
        except Exception as e:
            print(f"   âŒ Error saving pattern for dir_id={r.get('dir_id')}: {e}")
            conn.rollback()
            continue
    
    conn.commit()
    print(f"   ğŸ’¾ Saved {saved_count} pattern results to directory_catalog")


def _update_filename_values(results: List[Dict]):
    """
    Batch update file_catalog.filename_values
    
    Uses PostgreSQL regex to extract values from file_name column (already in DB)
    NO file system access - pure DB operation
    
    Note: regexp_matches is a set-returning function, so we use substring instead
    """
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    updated_total = 0
    
    for r in results:
        if not r.get("has_pattern") or not r.get("columns"):
            continue
        
        dir_id = r["dir_id"]
        pattern_regex = r.get("pattern_regex")
        
        if not pattern_regex:
            continue
        
        for col in r["columns"]:
            col_name = col.get("name")
            if not col_name:
                continue
                
            col_type = col.get("type", "text")
            
            try:
                # PostgreSQL substringì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ìº¡ì²˜ ê·¸ë£¹ ì¶”ì¶œ
                # substring(file_name from 'pattern')ì€ ì²« ë²ˆì§¸ ìº¡ì²˜ ê·¸ë£¹ ë°˜í™˜
                if col_type == "integer":
                    # ì •ìˆ˜í˜• ìºìŠ¤íŒ…
                    cursor.execute("""
                        UPDATE file_catalog
                        SET filename_values = CASE 
                            WHEN file_name ~ %s THEN
                                COALESCE(filename_values, '{}'::jsonb) || 
                                jsonb_build_object(%s, substring(file_name from %s)::integer)
                            ELSE filename_values
                        END
                        WHERE dir_id = %s
                          AND file_name ~ %s
                    """, (
                        pattern_regex,  # CASE WHEN condition
                        col_name,       # jsonb key
                        pattern_regex,  # substring pattern (extracts first capture group)
                        dir_id,         # WHERE dir_id
                        pattern_regex   # WHERE file_name ~
                    ))
                else:
                    # í…ìŠ¤íŠ¸í˜•
                    cursor.execute("""
                        UPDATE file_catalog
                        SET filename_values = CASE 
                            WHEN file_name ~ %s THEN
                                COALESCE(filename_values, '{}'::jsonb) || 
                                jsonb_build_object(%s, substring(file_name from %s))
                            ELSE filename_values
                        END
                        WHERE dir_id = %s
                          AND file_name ~ %s
                    """, (
                        pattern_regex,
                        col_name,
                        pattern_regex,
                        dir_id,
                        pattern_regex
                    ))
                
                updated_total += cursor.rowcount
                
            except Exception as e:
                print(f"   âŒ Error updating filename_values for dir_id={dir_id}, col={col_name}: {e}")
                conn.rollback()
                continue
        
        conn.commit()
    
    print(f"   ğŸ’¾ Updated filename_values for {updated_total} files")


# =============================================================================
# LangGraph Node Function
# =============================================================================

def phase1c_directory_pattern_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1C] Directory Pattern Analysis Node
    
    All data is read from DB (no file re-reading):
    - directory_catalog: filename_samples, file_extensions (from Phase -1)
    - column_metadata: column info with semantic descriptions (from Phase 1A)
    
    Steps:
    1. Query directories from directory_catalog
    2. Query data dictionary from column_metadata / data_dictionary
    3. Analyze patterns with LLM
    4. Save results to directory_catalog
    5. Batch update file_catalog.filename_values
    
    Args:
        state: AgentState
    
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ:
        - phase1c_result: ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        - phase1c_dir_patterns: {dir_id: pattern_info}
    """
    print("\n" + "=" * 60)
    print("ğŸ“ Phase 1C: Directory Pattern Analysis")
    print("=" * 60)
    
    started_at = datetime.now()
    
    # 1. ë¶„ì„ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì¡°íšŒ (DBì—ì„œ)
    print("\n   ğŸ“‚ Querying directories from DB...")
    directories = _get_directories_for_analysis()
    
    if not directories:
        print("   âš ï¸ No directories to analyze (all already analyzed or file_count < MIN_FILES)")
        return {
            "phase1c_result": {
                "status": "skipped",
                "reason": "no_directories",
                "total_dirs": 0,
                "analyzed_dirs": 0,
                "patterns_found": 0
            },
            "phase1c_dir_patterns": {},
            "logs": ["âš ï¸ [Phase 1C] No directories to analyze"]
        }
    
    print(f"   ğŸ“‚ Found {len(directories)} directories to analyze:")
    for d in directories:
        print(f"      - {d['dir_name']} ({d['file_count']} files, type={d['dir_type']})")
    
    # 2. Data Dictionary ìˆ˜ì§‘ (DBì—ì„œ)
    print("\n   ğŸ“– Collecting data dictionary from DB...")
    data_dictionary = _collect_data_dictionary()
    
    if not data_dictionary:
        # Phase 1A ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨ ë²„ì „ ì‚¬ìš©
        print("   âš ï¸ No semantic data from Phase 1A, using simple dictionary")
        data_dictionary = _collect_data_dictionary_simple()
    
    print(f"   ğŸ“– Data dictionary: {len(data_dictionary)} tables/entries")
    
    # 3. ë°°ì¹˜ ì²˜ë¦¬
    print(f"\n   ğŸ¤– Analyzing patterns with LLM (batch_size={Phase1CConfig.MAX_DIRS_PER_BATCH})...")
    
    all_results = []
    batches = _batch_directories(directories, Phase1CConfig.MAX_DIRS_PER_BATCH)
    
    for i, batch in enumerate(batches):
        print(f"      Batch {i+1}/{len(batches)}: {len(batch)} directories")
        batch_result = _analyze_batch(batch, data_dictionary)
        all_results.extend(batch_result)
        print(f"      âœ… Got {len(batch_result)} results")
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n   ğŸ’¾ Saving pattern results to directory_catalog...")
    _save_pattern_results(all_results)
    
    # 5. filename_values ë°°ì¹˜ ì—…ë°ì´íŠ¸
    print("\n   ğŸ’¾ Updating file_catalog.filename_values...")
    _update_filename_values(all_results)
    
    # ê²°ê³¼ ìš”ì•½
    completed_at = datetime.now()
    duration = (completed_at - started_at).total_seconds()
    
    patterns_found = sum(1 for r in all_results if r.get("has_pattern"))
    
    result = {
        "status": "completed",
        "total_dirs": len(directories),
        "analyzed_dirs": len(all_results),
        "patterns_found": patterns_found,
        "started_at": started_at.isoformat(),
        "completed_at": completed_at.isoformat(),
        "duration_seconds": duration
    }
    
    dir_patterns = {r["dir_id"]: r for r in all_results}
    
    print(f"\nâœ… Phase 1C Complete!")
    print(f"   ğŸ“ Directories analyzed: {len(all_results)}/{len(directories)}")
    print(f"   ğŸ” Patterns found: {patterns_found}")
    for r in all_results:
        if r.get("has_pattern"):
            print(f"      - {r.get('dir_id', 'unknown')[:8]}: {r.get('pattern')} (conf={r.get('confidence', 0):.2f})")
    print(f"   â±ï¸  Duration: {duration:.1f}s")
    print("=" * 60 + "\n")
    
    return {
        "phase1c_result": result,
        "phase1c_dir_patterns": dir_patterns,
        "logs": [
            f"ğŸ“ [Phase 1C] Analyzed {len(all_results)} directories, "
            f"found {patterns_found} patterns"
        ]
    }


# =============================================================================
# í¸ì˜ í•¨ìˆ˜
# =============================================================================

def run_phase1c_standalone() -> Dict[str, Any]:
    """
    Phase 1C ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼
    """
    state = {}
    return phase1c_directory_pattern_node(state)

