# src/agents/nodes/directory_pattern/node.py
"""
Directory Pattern Analysis Node

ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ , íŒŒì¼ëª…ì—ì„œ ID/ê°’ì„ ì¶”ì¶œí•˜ì—¬ 
ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ê´€ê³„ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

âœ… LLM ì‚¬ìš©:
  1. íŒŒì¼ëª… íŒ¨í„´ ì‹ë³„
  2. íŒ¨í„´ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ê°’ì´ Data Dictionaryì˜ ì–´ë–¤ ì»¬ëŸ¼ê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë‹¨

ì…ë ¥ (DBì—ì„œ ì½ê¸°):
  - directory_catalog.filename_samples (ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘)
  - column_metadata (ì´ì „ ë‹¨ê³„ì—ì„œ ë¶„ì„ë¨)

ì¶œë ¥ (DBì— ì €ì¥):
  - directory_catalog.filename_pattern, filename_columns
  - file_catalog.filename_values (ë°°ì¹˜ ì—…ë°ì´íŠ¸)
"""

import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from ...state import AgentState
from ...base import BaseNode, LLMMixin, DatabaseMixin
from ...registry import register_node
from src.config import DirectoryPatternConfig
from .prompts import DirectoryPatternPrompt


@register_node
class DirectoryPatternNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Directory Pattern Analysis Node (LLM-based)
    
    ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ ,
    íŒŒì¼ëª…ì—ì„œ ID/ê°’ì„ ì¶”ì¶œí•˜ì—¬ ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ê´€ê³„ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
    
    Input (DBì—ì„œ ì½ê¸°):
        - directory_catalog.filename_samples (ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘)
        - column_metadata (ì´ì „ ë‹¨ê³„ì—ì„œ ë¶„ì„ë¨)
    
    Output (DBì— ì €ì¥):
        - directory_catalog.filename_pattern, filename_columns
        - file_catalog.filename_values (ë°°ì¹˜ ì—…ë°ì´íŠ¸)
    """
    
    name = "directory_pattern"
    description = "ë””ë ‰í† ë¦¬ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„"
    order = 700
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = DirectoryPatternPrompt
    
    def _get_directories_for_analysis(self) -> List[Dict]:
        """
        Query directories from directory_catalog (DB)
        
        Uses: DirectoryRepository.get_directories_for_analysis()
        
        Data source: directory_catalog table (populated by previous step)
        - filename_samples: collected during directory scan
        - file_extensions: counted during scan
        - dir_type: classified during scan
        """
        try:
            directories = self.directory_repo.get_directories_for_analysis(
                min_files=DirectoryPatternConfig.MIN_FILES_FOR_PATTERN
            )
            
            # LLMì— ì „ë‹¬í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ
            for d in directories:
                samples = d.get('filename_samples', [])
                d['filename_samples'] = samples[:DirectoryPatternConfig.MAX_SAMPLES_PER_DIR]
            
            return directories
        except Exception as e:
            self.log(f"âŒ Error getting directories: {e}")
            return []
    
    def _collect_data_dictionary(self) -> Dict[str, Any]:
        """
        Collect data dictionary from DB (previous steps results)
        
        Uses: DirectoryRepository.get_data_dictionary_for_pattern()
        - file_catalog: primary_entity, entity_identifier_column
        - column_metadata + parameter: semantic_name, description, concept_category
        
        NO file reading - all from DB
        """
        return self.directory_repo.get_data_dictionary_for_pattern()
    
    def _collect_data_dictionary_simple(self) -> Dict[str, Any]:
        """
        Data Dictionary ê°„ë‹¨ ë²„ì „ - ì´ì „ ë‹¨ê³„ ê²°ê³¼ê°€ ì—†ì–´ë„ ë™ì‘
        
        Uses: DirectoryRepository.get_data_dictionary_simple()
        """
        return self.directory_repo.get_data_dictionary_simple()
    
    def _batch_directories(self, directories: List[Dict], batch_size: int) -> List[List[Dict]]:
        """ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(directories), batch_size):
            batches.append(directories[i:i + batch_size])
        return batches
    
    def _analyze_batch(
        self,
        directories: List[Dict], 
        data_dictionary: Dict
    ) -> List[Dict]:
        """
        Analyze directory batch with LLM
        
        Input: All from DB (directories from directory_catalog, data_dictionary from column_metadata)
        Output: Pattern analysis results
        """
        # Build directories info for prompt
        # Note: dir_idëŠ” LLMì— ë³´ë‚´ì§€ ì•ŠìŒ (ì™¸ë¶€ì—ì„œ ê´€ë¦¬)
        dirs_info_parts = []
        for i, d in enumerate(directories):
            samples_str = "\n".join([f"  - {s}" for s in d['filename_samples']])
            dirs_info_parts.append(
                f"### Directory {i+1}: {d['dir_name']}\n"
                f"- File count: {d['file_count']}\n"
                f"- Extensions: {json.dumps(d['file_extensions'])}\n"
                f"- Type: {d['dir_type']}\n"
                f"- Filename samples:\n{samples_str}"
            )
        
        dirs_info = "\n\n".join(dirs_info_parts)
        
        # PromptTemplate ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_class.build(
            data_dictionary=json.dumps(data_dictionary, indent=2, ensure_ascii=False),
            directories_info=dirs_info
        )
        
        # dir_name â†’ dir_id ë§¤í•‘ ìƒì„± (LLM ì™¸ë¶€ì—ì„œ ê´€ë¦¬)
        name_to_id = {d['dir_name']: d['dir_id'] for d in directories}
        
        try:
            result = self.call_llm_json(prompt)
            
            if result.get("error"):
                self.log(f"âŒ LLM returned error: {result.get('error')}", indent=1)
                return []
            
            # LLM ê²°ê³¼ì— dir_id ì¶”ê°€ (dir_nameìœ¼ë¡œ ë§¤í•‘)
            llm_results = result.get("directories", [])
            for r in llm_results:
                dir_name = r.get("dir_name")
                if dir_name and dir_name in name_to_id:
                    r["dir_id"] = name_to_id[dir_name]
                else:
                    self.log(f"âš ï¸ Unknown dir_name from LLM: {dir_name}", indent=1)
            
            return llm_results
            
        except Exception as e:
            self.log(f"âŒ LLM call error: {e}", indent=1)
            return []
    
    def _save_pattern_results(self, results: List[Dict]):
        """
        Save pattern analysis results to directory_catalog
        
        Uses: DirectoryRepository.save_pattern_results()
        """
        saved_count = self.directory_repo.save_pattern_results(results)
        self.log(f"ğŸ’¾ Saved {saved_count} pattern results to directory_catalog", indent=1)
    
    def _update_filename_values(self, results: List[Dict]):
        """
        Batch update file_catalog.filename_values
        
        Uses: FileRepository.update_filename_values_by_pattern()
        """
        updated_total = 0
        
        for r in results:
            if not r.get("has_pattern") or not r.get("columns"):
                continue
            
            dir_id = r["dir_id"]
            pattern_regex = r.get("pattern_regex")
            
            if not pattern_regex:
                continue
            
            updated = self.file_repo.update_filename_values_by_pattern(
                dir_id=dir_id,
                pattern_regex=pattern_regex,
                columns=r["columns"]
            )
            updated_total += updated
        
        self.log(f"ğŸ’¾ Updated filename_values for {updated_total} files", indent=1)
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Directory Pattern Analysis ì‹¤í–‰
        
        All data is read from DB (no file re-reading):
        - directory_catalog: filename_samples, file_extensions
        - column_metadata: column info with semantic descriptions
        
        Steps:
        1. Query directories from directory_catalog
        2. Query data dictionary from column_metadata / data_dictionary
        3. Analyze patterns with LLM
        4. Save results to directory_catalog
        5. Batch update file_catalog.filename_values
        """
        started_at = datetime.now()
        
        # 1. ë¶„ì„ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì¡°íšŒ (DBì—ì„œ)
        self.log("ğŸ“‚ Querying directories from DB...")
        directories = self._get_directories_for_analysis()
        
        if not directories:
            self.log("âš ï¸ No directories to analyze (all already analyzed or file_count < MIN_FILES)", indent=1)
            return {
                "directory_pattern_result": {
                    "status": "skipped",
                    "reason": "no_directories",
                    "total_dirs": 0,
                    "analyzed_dirs": 0,
                    "patterns_found": 0
                },
                "directory_patterns": {},
                "logs": ["âš ï¸ [Directory Pattern] No directories to analyze"]
            }
        
        self.log(f"ğŸ“‚ Found {len(directories)} directories to analyze:", indent=1)
        for d in directories:
            self.log(f"- {d['dir_name']} ({d['file_count']} files, type={d['dir_type']})", indent=2)
        
        # 2. Data Dictionary ìˆ˜ì§‘ (DBì—ì„œ) - ë‘ ì†ŒìŠ¤ ë³‘í•©
        self.log("ğŸ“– Collecting data dictionary from DB...")
        
        # semantic ì •ë³´ (parameter í…Œì´ë¸” ê¸°ë°˜ - ì»¬ëŸ¼ë³„ ì˜ë¯¸)
        semantic_dict = self._collect_data_dictionary()
        
        # data_dictionary í…Œì´ë¸” ê¸°ë°˜ ì •ë³´ (caseid ë“± íŒŒë¼ë¯¸í„° ì •ì˜ í¬í•¨)
        simple_dict = self._collect_data_dictionary_simple()
        
        # ë³‘í•©: ë‘ ì •ë³´ë¥¼ ëª¨ë‘ LLMì— ì „ë‹¬
        data_dictionary = {
            "tables": semantic_dict,  # fileë³„ ì»¬ëŸ¼ semantic ì •ë³´
            **simple_dict  # dictionary_entries (caseid ë“±), id_columns_by_file
        }
        
        dict_entries_count = len(simple_dict.get('dictionary_entries', {}))
        self.log(f"ğŸ“– Data dictionary: {len(semantic_dict)} tables, {dict_entries_count} dictionary entries", indent=1)
        
        # 3. ë°°ì¹˜ ì²˜ë¦¬
        self.log(f"ğŸ¤– Analyzing patterns with LLM (batch_size={DirectoryPatternConfig.MAX_DIRS_PER_BATCH})...")
        
        all_results = []
        batches = self._batch_directories(directories, DirectoryPatternConfig.MAX_DIRS_PER_BATCH)
        
        for i, batch in enumerate(batches):
            self.log(f"Batch {i+1}/{len(batches)}: {len(batch)} directories", indent=1)
            batch_result = self._analyze_batch(batch, data_dictionary)
            all_results.extend(batch_result)
            self.log(f"âœ… Got {len(batch_result)} results", indent=2)
        
        # 4. ê²°ê³¼ ì €ì¥
        self.log("ğŸ’¾ Saving pattern results to directory_catalog...")
        self._save_pattern_results(all_results)
        
        # 5. filename_values ë°°ì¹˜ ì—…ë°ì´íŠ¸
        self.log("ğŸ’¾ Updating file_catalog.filename_values...")
        self._update_filename_values(all_results)
        
        # ê²°ê³¼ ìš”ì•½
        completed_at = datetime.now()
        duration = (completed_at - started_at).total_seconds()
        
        patterns_found = sum(1 for r in all_results if r.get("has_pattern"))
        
        result = {
            "status": "completed",
            "total_dirs": len(directories),
            "analyzed_dirs": len(all_results),
            "patterns_found": patterns_found,
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_seconds": duration
        }
        
        dir_patterns = {r["dir_id"]: r for r in all_results}
        
        self.log(f"ğŸ“ Directories analyzed: {len(all_results)}/{len(directories)}", indent=1)
        self.log(f"ğŸ” Patterns found: {patterns_found}", indent=1)
        for r in all_results:
            if r.get("has_pattern"):
                self.log(f"- {r.get('dir_id', 'unknown')[:8]}: {r.get('pattern')} (conf={r.get('confidence', 0):.2f})", indent=2)
        self.log(f"â±ï¸  Duration: {duration:.1f}s", indent=1)
        
        return {
            "directory_pattern_result": result,
            "directory_patterns": dir_patterns,
            "logs": [
                f"ğŸ“ [Directory Pattern] Analyzed {len(all_results)} directories, "
                f"found {patterns_found} patterns"
            ]
        }
    
    @classmethod
    def run_standalone(cls) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ (í…ŒìŠ¤íŠ¸ìš©)
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        return node({})

