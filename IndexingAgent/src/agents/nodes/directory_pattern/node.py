# src/agents/nodes/directory_pattern/node.py
"""
Directory Pattern Analysis Node

ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ , íŒŒì¼ëª…ì—ì„œ ID/ê°’ì„ ì¶”ì¶œí•˜ì—¬ 
ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ê´€ê³„ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

âœ… LLM ì‚¬ìš©:
  1. íŒŒì¼ëª… íŒ¨í„´ ì‹ë³„
  2. íŒ¨í„´ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ê°’ì´ Data Dictionaryì˜ ì–´ë–¤ ì»¬ëŸ¼ê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë‹¨

ì…ë ¥ (DBì—ì„œ ì½ê¸°):
  - directory_catalog.filename_samples (ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘)
  - column_metadata (ì´ì „ ë‹¨ê³„ì—ì„œ ë¶„ì„ë¨)
  - file_group (ê·¸ë£¹í™”ëœ íŒŒì¼ ì •ë³´)

ì¶œë ¥ (DBì— ì €ì¥):
  - directory_catalog.filename_pattern, filename_columns
  - file_catalog.filename_values (ë°°ì¹˜ ì—…ë°ì´íŠ¸)
  - file_group.grouping_criteria.pattern_regex (íŒ¨í„´ ì •ë³´)

ìˆ˜ì •ëœ ë¡œì§ (v2):
  Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬ (ìƒ˜í”Œ LLM ë¶„ì„ + íŒ¨í„´ ì „íŒŒ)
  Phase 2: ë¹„ê·¸ë£¹ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë‹¨ìœ„ LLM ë¶„ì„)
"""

import re
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from ...base import BaseNode, LLMMixin, DatabaseMixin
from ...registry import register_node
from src.config import DirectoryPatternConfig
from shared.database.repositories import FileGroupRepository
from .prompts import DirectoryPatternPrompt, GroupPatternPrompt


@register_node
class DirectoryPatternNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Directory Pattern Analysis Node (LLM-based)
    
    ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ ,
    íŒŒì¼ëª…ì—ì„œ ID/ê°’ì„ ì¶”ì¶œí•˜ì—¬ ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ê´€ê³„ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
    
    ìˆ˜ì •ëœ ë¡œì§ (v2):
        Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬ (ìƒ˜í”Œ LLM ë¶„ì„ + íŒ¨í„´ ì „íŒŒ)
        Phase 2: ë¹„ê·¸ë£¹ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë‹¨ìœ„ LLM ë¶„ì„)
    
    Input (DBì—ì„œ ì½ê¸°):
        - directory_catalog.filename_samples (ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘)
        - column_metadata (ì´ì „ ë‹¨ê³„ì—ì„œ ë¶„ì„ë¨)
        - file_group (ê·¸ë£¹í™”ëœ íŒŒì¼ ì •ë³´)
    
    Output (DBì— ì €ì¥):
        - directory_catalog.filename_pattern, filename_columns
        - file_catalog.filename_values (ë°°ì¹˜ ì—…ë°ì´íŠ¸)
        - file_group.grouping_criteria.pattern_regex (íŒ¨í„´ ì •ë³´)
    """
    
    name = "directory_pattern"
    description = "ë””ë ‰í† ë¦¬ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„"
    order = 700
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = DirectoryPatternPrompt
    group_prompt_class = GroupPatternPrompt
    
    def __init__(self):
        super().__init__()
        self._group_repo: Optional[FileGroupRepository] = None
    
    def _get_group_repo(self) -> FileGroupRepository:
        """FileGroupRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._group_repo is None:
            self._group_repo = FileGroupRepository()
        return self._group_repo
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 1: ê·¸ë£¹ íŒ¨í„´ ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _process_group_pattern(
        self, 
        group: Dict, 
        data_dictionary: Dict
    ) -> Dict[str, Any]:
        """
        ê·¸ë£¹ì˜ íŒŒì¼ëª… íŒ¨í„´ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ê³  ì „ì²´ íŒŒì¼ì— ì ìš©
        
        Args:
            group: ê·¸ë£¹ ì •ë³´ (group_id, group_name, all_filenames ë“±)
            data_dictionary: Data Dictionary ì •ë³´
        
        Returns:
            {
                'success': bool,
                'pattern_regex': str or None,
                'files_updated': int,
                'needs_review': bool,
                'review_type': str or None
            }
        """
        group_id = group['group_id']
        group_name = group['group_name']
        all_filenames = group.get('all_filenames', [])
        file_count = group.get('file_count', 0)
        criteria = group.get('grouping_criteria', {})
        extensions = criteria.get('extensions', [])
        
        self.log(f"ğŸ“¦ Analyzing group: {group_name} ({file_count} files)", indent=1)
        
        # ìƒ˜í”Œ íŒŒì¼ëª… ì„ íƒ (ìµœëŒ€ 10ê°œ)
        sample_filenames = self._select_sample_filenames(all_filenames, sample_size=10)
        
        if len(sample_filenames) < 3:
            self.log(f"âš ï¸ Not enough samples for group {group_name}", indent=2)
            return {'success': False, 'needs_review': False}
        
        # LLM ë¶„ì„
        llm_result = self._call_llm_for_group_pattern(
            group_name=group_name,
            file_count=file_count,
            extensions=extensions,
            sample_filenames=sample_filenames,
            data_dictionary=data_dictionary
        )
        
        if not llm_result or not llm_result.get('has_pattern'):
            self.log(f"âŒ No pattern found for group {group_name}", indent=2)
            return {'success': False, 'needs_review': False}
        
        pattern_regex = llm_result.get('pattern_regex')
        columns = llm_result.get('columns', [])
        sample_extractions = llm_result.get('sample_extractions', [])
        confidence = llm_result.get('confidence', 0.0)
        
        # íŒ¨í„´ ê²€ì¦
        validation_result = self._validate_pattern(
            pattern_regex=pattern_regex,
            columns=columns,
            sample_filenames=sample_filenames,
            llm_extractions=sample_extractions
        )
        
        if not validation_result['valid']:
            # ê²€ì¦ ì‹¤íŒ¨ â†’ human review í•„ìš”
            self.log(f"âš ï¸ Pattern validation failed: {validation_result['reason']}", indent=2)
            
            group_repo = self._get_group_repo()
            group_repo.mark_needs_human_review(
                group_id=group_id,
                review_type='pattern_validation_failed',
                review_context={
                    'pattern_regex': pattern_regex,
                    'columns': columns,
                    'failed_samples': validation_result.get('failed_samples', []),
                    'llm_extractions': sample_extractions,
                    'validation_reason': validation_result['reason']
                },
                reasoning=f"Pattern validation failed: {validation_result['reason']}"
            )
            
            return {
                'success': False,
                'needs_review': True,
                'review_type': 'pattern_validation_failed'
            }
        
        # ë‚®ì€ ì‹ ë¢°ë„ ì²´í¬
        if confidence < 0.7:
            self.log(f"âš ï¸ Low confidence ({confidence:.2f}) for group {group_name}", indent=2)
            
            group_repo = self._get_group_repo()
            group_repo.mark_needs_human_review(
                group_id=group_id,
                review_type='low_confidence',
                review_context={
                    'pattern_regex': pattern_regex,
                    'columns': columns,
                    'sample_extractions': sample_extractions,
                    'confidence': confidence,
                    'reasoning': llm_result.get('reasoning')
                },
                reasoning=f"Low pattern confidence: {confidence:.2f}"
            )
            
            return {
                'success': False,
                'needs_review': True,
                'review_type': 'low_confidence'
            }
        
        # ê²€ì¦ ì„±ê³µ â†’ íŒ¨í„´ ì €ì¥ ë° ì „ì²´ íŒŒì¼ì— ì ìš©
        group_repo = self._get_group_repo()
        group_repo.update_group_pattern(
            group_id=group_id,
            pattern_regex=pattern_regex,
            pattern_columns=columns,
            confidence=confidence
        )
        
        # ì „ì²´ íŒŒì¼ì— filename_values ì ìš©
        files_updated = self._apply_pattern_to_group_files(
            group_id=group_id,
            pattern_regex=pattern_regex,
            columns=columns
        )
        
        self.log(f"âœ… Pattern applied: {pattern_regex} â†’ {files_updated} files", indent=2)
        
        return {
            'success': True,
            'pattern_regex': pattern_regex,
            'files_updated': files_updated,
            'needs_review': False
        }
    
    def _select_sample_filenames(
        self, 
        filenames: List[str], 
        sample_size: int = 10
    ) -> List[str]:
        """
        ëŒ€í‘œ ìƒ˜í”Œ íŒŒì¼ëª… ì„ íƒ
        
        ì „ëµ: first 2 + last 2 + random middle
        """
        if len(filenames) <= sample_size:
            return filenames
        
        # ì •ë ¬ëœ ìƒíƒœì—ì„œ ì„ íƒ
        sorted_names = sorted(filenames)
        
        result = []
        # First 2
        result.extend(sorted_names[:2])
        # Last 2
        result.extend(sorted_names[-2:])
        
        # Middle samples (ë‚˜ë¨¸ì§€)
        remaining = sample_size - len(result)
        if remaining > 0:
            middle = sorted_names[2:-2]
            step = max(1, len(middle) // remaining)
            for i in range(0, len(middle), step):
                if len(result) < sample_size:
                    result.append(middle[i])
        
        return result
    
    def _call_llm_for_group_pattern(
        self,
        group_name: str,
        file_count: int,
        extensions: List[str],
        sample_filenames: List[str],
        data_dictionary: Dict
    ) -> Optional[Dict]:
        """
        ê·¸ë£¹ ìƒ˜í”Œ íŒŒì¼ëª…ì— ëŒ€í•´ LLM íŒ¨í„´ ë¶„ì„ í˜¸ì¶œ
        """
        sample_str = "\n".join([f"- {fn}" for fn in sample_filenames])
        
        prompt = self.group_prompt_class.build(
            group_name=group_name,
            file_count=file_count,
            extensions=json.dumps(extensions),
            sample_filenames=sample_str,
            data_dictionary=json.dumps(data_dictionary, indent=2, ensure_ascii=False)
        )
        
        try:
            result = self.call_llm_json(prompt)
            if result.get("error"):
                self.log(f"âŒ LLM error: {result.get('error')}", indent=2)
                return None
            return result
        except Exception as e:
            self.log(f"âŒ LLM call failed: {e}", indent=2)
            return None
    
    def _validate_pattern(
        self,
        pattern_regex: str,
        columns: List[Dict],
        sample_filenames: List[str],
        llm_extractions: List[Dict]
    ) -> Dict[str, Any]:
        """
        LLMì´ ì œì‹œí•œ íŒ¨í„´ì´ ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦
        
        ê²€ì¦ ê¸°ì¤€:
        1. regexê°€ ìœ íš¨í•œê°€?
        2. ëª¨ë“  ìƒ˜í”Œ íŒŒì¼ëª…ì— ë§¤ì¹­ë˜ëŠ”ê°€?
        3. ì¶”ì¶œëœ ê°’ì´ LLMì´ ì œì‹œí•œ ê°’ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?
        
        Returns:
            {'valid': bool, 'reason': str, 'failed_samples': list}
        """
        # 1. Regex ìœ íš¨ì„± ê²€ì‚¬
        try:
            compiled = re.compile(pattern_regex)
        except re.error as e:
            return {
                'valid': False,
                'reason': f"Invalid regex: {e}",
                'failed_samples': []
            }
        
        failed_samples = []
        
        # 2. ëª¨ë“  ìƒ˜í”Œì— ë§¤ì¹­ í…ŒìŠ¤íŠ¸
        for filename in sample_filenames:
            match = compiled.match(filename)
            if not match:
                failed_samples.append({
                    'filename': filename,
                    'error': 'No match'
                })
                continue
            
            # 3. LLM ì¶”ì¶œ ê²°ê³¼ì™€ ë¹„êµ (ìˆëŠ” ê²½ìš°)
            llm_result = next(
                (e for e in llm_extractions if e.get('filename') == filename),
                None
            )
            
            if llm_result:
                for col in columns:
                    col_name = col.get('name')
                    position = col.get('position', 1)
                    
                    try:
                        extracted = match.group(position)
                    except IndexError:
                        failed_samples.append({
                            'filename': filename,
                            'error': f"Capture group {position} not found"
                        })
                        continue
                    
                    expected = llm_result.get('values', {}).get(col_name)
                    if expected and extracted != expected:
                        failed_samples.append({
                            'filename': filename,
                            'error': f"{col_name}: expected '{expected}', got '{extracted}'"
                        })
        
        # 80% ì´ìƒ ì„±ê³µì´ë©´ ê²€ì¦ í†µê³¼
        success_rate = 1 - (len(failed_samples) / len(sample_filenames)) if sample_filenames else 0
        
        if success_rate >= 0.8:
            return {'valid': True, 'reason': 'OK', 'failed_samples': []}
        else:
            return {
                'valid': False,
                'reason': f"Pattern matched only {success_rate*100:.0f}% of samples",
                'failed_samples': failed_samples
            }
    
    def _apply_pattern_to_group_files(
        self,
        group_id: str,
        pattern_regex: str,
        columns: List[Dict]
    ) -> int:
        """
        ê²€ì¦ëœ íŒ¨í„´ì„ ê·¸ë£¹ ë‚´ ëª¨ë“  íŒŒì¼ì— ì ìš©í•˜ì—¬ filename_values ì—…ë°ì´íŠ¸
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ìˆ˜
        """
        # file_repoë¥¼ í†µí•´ ë°°ì¹˜ ì—…ë°ì´íŠ¸
        return self.file_repo.update_filename_values_by_group_pattern(
            group_id=group_id,
            pattern_regex=pattern_regex,
            columns=columns
        )
    
    def _get_full_data_dictionary(self) -> Dict:
        """
        Data Dictionary ì „ì²´ ìˆ˜ì§‘ (ë‘ ì†ŒìŠ¤ ë³‘í•©)
        """
        # semantic ì •ë³´ (parameter í…Œì´ë¸” ê¸°ë°˜)
        semantic_dict = self._collect_data_dictionary()
        
        # data_dictionary í…Œì´ë¸” ê¸°ë°˜ ì •ë³´
        simple_dict = self._collect_data_dictionary_simple()
        
        return {
            "tables": semantic_dict,
            **simple_dict
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 2: ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë¶„ì„ (ungrouped)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_directories_for_analysis(self) -> List[Dict]:
        """
        Query directories from directory_catalog (DB)
        
        Uses: DirectoryRepository.get_directories_for_analysis()
        
        Data source: directory_catalog table (populated by previous step)
        - filename_samples: collected during directory scan
        - file_extensions: counted during scan
        - dir_type: classified during scan
        """
        try:
            directories = self.directory_repo.get_directories_for_analysis(
                min_files=DirectoryPatternConfig.MIN_FILES_FOR_PATTERN
            )
            
            # LLMì— ì „ë‹¬í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ
            for d in directories:
                samples = d.get('filename_samples', [])
                d['filename_samples'] = samples[:DirectoryPatternConfig.MAX_SAMPLES_PER_DIR]
            
            return directories
        except Exception as e:
            self.log(f"âŒ Error getting directories: {e}")
            return []
    
    def _collect_data_dictionary(self) -> Dict[str, Any]:
        """
        Collect data dictionary from DB (previous steps results)
        
        Uses: DirectoryRepository.get_data_dictionary_for_pattern()
        - column_metadata + parameter: semantic_name, description, concept_category
        
        NO file reading - all from DB
        """
        return self.directory_repo.get_data_dictionary_for_pattern()
    
    def _collect_data_dictionary_simple(self) -> Dict[str, Any]:
        """
        Data Dictionary ê°„ë‹¨ ë²„ì „ - ì´ì „ ë‹¨ê³„ ê²°ê³¼ê°€ ì—†ì–´ë„ ë™ì‘
        
        Uses: DirectoryRepository.get_data_dictionary_simple()
        """
        return self.directory_repo.get_data_dictionary_simple()
    
    def _batch_directories(self, directories: List[Dict], batch_size: int) -> List[List[Dict]]:
        """ë””ë ‰í† ë¦¬ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(directories), batch_size):
            batches.append(directories[i:i + batch_size])
        return batches
    
    def _analyze_batch(
        self,
        directories: List[Dict], 
        data_dictionary: Dict
    ) -> List[Dict]:
        """
        Analyze directory batch with LLM
        
        Input: All from DB (directories from directory_catalog, data_dictionary from column_metadata)
        Output: Pattern analysis results
        """
        # Build directories info for prompt
        # Note: dir_idëŠ” LLMì— ë³´ë‚´ì§€ ì•ŠìŒ (ì™¸ë¶€ì—ì„œ ê´€ë¦¬)
        dirs_info_parts = []
        for i, d in enumerate(directories):
            samples_str = "\n".join([f"  - {s}" for s in d['filename_samples']])
            dirs_info_parts.append(
                f"### Directory {i+1}: {d['dir_name']}\n"
                f"- File count: {d['file_count']}\n"
                f"- Extensions: {json.dumps(d['file_extensions'])}\n"
                f"- Type: {d['dir_type']}\n"
                f"- Filename samples:\n{samples_str}"
            )
        
        dirs_info = "\n\n".join(dirs_info_parts)
        
        # PromptTemplate ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_class.build(
            data_dictionary=json.dumps(data_dictionary, indent=2, ensure_ascii=False),
            directories_info=dirs_info
        )
        
        # dir_name â†’ dir_id ë§¤í•‘ ìƒì„± (LLM ì™¸ë¶€ì—ì„œ ê´€ë¦¬)
        name_to_id = {d['dir_name']: d['dir_id'] for d in directories}
        
        try:
            result = self.call_llm_json(prompt)
            
            if result.get("error"):
                self.log(f"âŒ LLM returned error: {result.get('error')}", indent=1)
                return []
            
            # LLM ê²°ê³¼ì— dir_id ì¶”ê°€ (dir_nameìœ¼ë¡œ ë§¤í•‘)
            llm_results = result.get("directories", [])
            for r in llm_results:
                dir_name = r.get("dir_name")
                if dir_name and dir_name in name_to_id:
                    r["dir_id"] = name_to_id[dir_name]
                else:
                    self.log(f"âš ï¸ Unknown dir_name from LLM: {dir_name}", indent=1)
            
            return llm_results
            
        except Exception as e:
            self.log(f"âŒ LLM call error: {e}", indent=1)
            return []
    
    def _save_pattern_results(self, results: List[Dict]):
        """
        Save pattern analysis results to directory_catalog
        
        Uses: DirectoryRepository.save_pattern_results()
        """
        saved_count = self.directory_repo.save_pattern_results(results)
        self.log(f"ğŸ’¾ Saved {saved_count} pattern results to directory_catalog", indent=1)
    
    def _update_filename_values(self, results: List[Dict]):
        """
        Batch update file_catalog.filename_values
        
        Uses: FileRepository.update_filename_values_by_pattern()
        """
        updated_total = 0
        
        for r in results:
            if not r.get("has_pattern") or not r.get("columns"):
                continue
            
            dir_id = r["dir_id"]
            pattern_regex = r.get("pattern_regex")
            
            if not pattern_regex:
                continue
            
            updated = self.file_repo.update_filename_values_by_pattern(
                dir_id=dir_id,
                pattern_regex=pattern_regex,
                columns=r["columns"]
            )
            updated_total += updated
        
        self.log(f"ğŸ’¾ Updated filename_values for {updated_total} files", indent=1)
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Directory Pattern Analysis ì‹¤í–‰ (v2)
        
        ìˆ˜ì •ëœ ë¡œì§:
        Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬ (ìƒ˜í”Œ LLM ë¶„ì„ + íŒ¨í„´ ì „íŒŒ)
        Phase 2: ë¹„ê·¸ë£¹ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë‹¨ìœ„ LLM ë¶„ì„)
        
        Steps:
        1. Phase 1: confirmed ê·¸ë£¹ë“¤ì˜ íŒ¨í„´ ë¶„ì„
           - ìƒ˜í”Œ íŒŒì¼ëª…ìœ¼ë¡œ LLM ë¶„ì„
           - íŒ¨í„´ ê²€ì¦ í›„ ì „ì²´ ê·¸ë£¹ì— ì ìš©
        2. Phase 2: ë¹„ê·¸ë£¹ ë””ë ‰í† ë¦¬ ë¶„ì„
           - ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë‹¨ìœ„ LLM ë¶„ì„
        3. ê²°ê³¼ ì €ì¥ ë° filename_values ì—…ë°ì´íŠ¸
        """
        started_at = datetime.now()
        
        # í†µê³„ ì´ˆê¸°í™”
        groups_processed = 0
        groups_patterns_found = 0
        groups_need_review = 0
        dirs_processed = 0
        dirs_patterns_found = 0
        llm_calls = 0
        files_updated = 0
        
        # Data Dictionary ìˆ˜ì§‘ (Phase 1, 2 ëª¨ë‘ ì‚¬ìš©)
        self.log("ğŸ“– Collecting data dictionary from DB...")
        data_dictionary = self._get_full_data_dictionary()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.log("=" * 50)
        self.log("ğŸ“¦ Phase 1: Processing file groups...")
        
        group_repo = self._get_group_repo()
        groups = group_repo.get_confirmed_groups_for_pattern_analysis()
        
        if groups:
            self.log(f"ğŸ“¦ Found {len(groups)} groups to analyze", indent=1)
            
            for group in groups:
                group_result = self._process_group_pattern(group, data_dictionary)
                groups_processed += 1
                llm_calls += 1
                
                if group_result['success']:
                    groups_patterns_found += 1
                    files_updated += group_result.get('files_updated', 0)
                    self.log(f"âœ… {group['group_name']}: pattern found, {group_result['files_updated']} files updated", indent=2)
                elif group_result.get('needs_review'):
                    groups_need_review += 1
                    self.log(f"âš ï¸ {group['group_name']}: needs human review ({group_result.get('review_type')})", indent=2)
                else:
                    self.log(f"âŒ {group['group_name']}: no pattern found", indent=2)
        else:
            self.log("âš ï¸ No groups to analyze", indent=1)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Phase 2: ë¹„ê·¸ë£¹ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.log("=" * 50)
        self.log("ğŸ“‚ Phase 2: Processing ungrouped directories...")
        
        directories = self._get_directories_for_analysis()
        
        if directories:
            self.log(f"ğŸ“‚ Found {len(directories)} directories to analyze:", indent=1)
            for d in directories:
                self.log(f"- {d['dir_name']} ({d['file_count']} files)", indent=2)
            
            # ë°°ì¹˜ ì²˜ë¦¬
            all_results = []
            batches = self._batch_directories(directories, DirectoryPatternConfig.MAX_DIRS_PER_BATCH)
            
            for i, batch in enumerate(batches):
                self.log(f"Batch {i+1}/{len(batches)}: {len(batch)} directories", indent=2)
                batch_result = self._analyze_batch(batch, data_dictionary)
                all_results.extend(batch_result)
                llm_calls += 1
            
            # ê²°ê³¼ ì €ì¥
            self._save_pattern_results(all_results)
            self._update_filename_values(all_results)
            
            dirs_processed = len(all_results)
            dirs_patterns_found = sum(1 for r in all_results if r.get("has_pattern"))
        else:
            self.log("âš ï¸ No ungrouped directories to analyze", indent=1)
            all_results = []
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ê²°ê³¼ ìš”ì•½
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        completed_at = datetime.now()
        duration = (completed_at - started_at).total_seconds()
        
        result = {
            "status": "completed",
            # Phase 1 ê²°ê³¼
            "groups_processed": groups_processed,
            "groups_patterns_found": groups_patterns_found,
            "groups_need_review": groups_need_review,
            # Phase 2 ê²°ê³¼
            "dirs_processed": dirs_processed,
            "dirs_patterns_found": dirs_patterns_found,
            # ì „ì²´ í†µê³„
            "total_patterns_found": groups_patterns_found + dirs_patterns_found,
            "llm_calls": llm_calls,
            "files_updated": files_updated,
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_seconds": duration
        }
        
        dir_patterns = {r.get("dir_id", "unknown"): r for r in all_results}
        
        self.log("=" * 50)
        self.log("ğŸ“Š Summary:")
        self.log(f"ğŸ“¦ Groups: {groups_processed} processed, {groups_patterns_found} patterns, {groups_need_review} need review", indent=1)
        self.log(f"ğŸ“‚ Directories: {dirs_processed} processed, {dirs_patterns_found} patterns", indent=1)
        self.log(f"ğŸ“ Files updated: {files_updated}", indent=1)
        self.log(f"ğŸ¤– LLM calls: {llm_calls}", indent=1)
        self.log(f"â±ï¸  Duration: {duration:.1f}s", indent=1)
        
        return {
            "directory_pattern_result": result,
            "directory_patterns": dir_patterns,
            "logs": [
                f"ğŸ“ [Directory Pattern] Groups: {groups_patterns_found}/{groups_processed}, "
                f"Dirs: {dirs_patterns_found}/{dirs_processed}, "
                f"Files: {files_updated}"
            ]
        }
    
    @classmethod
    def run_standalone(cls) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ (í…ŒìŠ¤íŠ¸ìš©)
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        return node({})

