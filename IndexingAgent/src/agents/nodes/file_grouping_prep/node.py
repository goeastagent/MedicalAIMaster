# src/agents/nodes/file_grouping_prep/node.py
"""
File Grouping Prep Node

ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒ¨í„´ì„ ê´€ì°°í•©ë‹ˆë‹¤.
íŒë‹¨ì€ í•˜ì§€ ì•Šê³ , LLM ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

ìˆ˜ì§‘í•˜ëŠ” ì •ë³´:
- ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ìˆ˜, í™•ì¥ì ë¶„í¬
- íŒŒì¼ëª… ìƒ˜í”Œ ë° ê¸¸ì´ í†µê³„
- íŒŒì¼ í¬ê¸° í†µê³„
- ê´€ì°°ëœ íŒ¨í„´ (ìˆ«ì, ë‚ ì§œ ë“±) - íŒë‹¨ ì—†ì´ ê´€ì°°ë§Œ

ì¶œë ¥:
- grouping_prep_result: ë””ë ‰í† ë¦¬ë³„ ìš”ì•½ ì •ë³´
- directories_for_grouping: LLMì—ê²Œ ì „ë‹¬í•  ë””ë ‰í† ë¦¬ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
"""

import re
from typing import Dict, Any, List, Optional
from datetime import datetime
from collections import defaultdict

from src.database.repositories import DirectoryRepository, FileRepository

from ...base import BaseNode, DatabaseMixin
from ...registry import register_node


@register_node
class FileGroupingPrepNode(BaseNode, DatabaseMixin):
    """
    File Grouping Prep Node (Rule-based)
    
    ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒ¨í„´ì„ ê´€ì°°í•©ë‹ˆë‹¤.
    LLMì´ ê·¸ë£¹í•‘ ì „ëµì„ ê²°ì •í•  ìˆ˜ ìˆë„ë¡ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    """
    
    name = "file_grouping_prep"
    description = "ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ í†µê³„ ìˆ˜ì§‘ ë° íŒ¨í„´ ê´€ì°° (LLM ì…ë ¥ ì¤€ë¹„)"
    order = 250
    requires_llm = False
    
    # =========================================================================
    # Configuration
    # =========================================================================
    
    # ìƒ˜í”Œë§ ì„¤ì •
    MAX_FILENAME_SAMPLES = 10  # LLMì—ê²Œ ì „ë‹¬í•  ìµœëŒ€ íŒŒì¼ëª… ìƒ˜í”Œ ìˆ˜
    MIN_FILES_FOR_ANALYSIS = 2  # ë¶„ì„ ëŒ€ìƒ ìµœì†Œ íŒŒì¼ ìˆ˜
    
    # =========================================================================
    # Repository Access (Lazy Initialization)
    # =========================================================================
    
    @property
    def dir_repo(self) -> DirectoryRepository:
        """DirectoryRepository ì¸ìŠ¤í„´ìŠ¤ (lazy)"""
        if not hasattr(self, '_dir_repo') or self._dir_repo is None:
            self._dir_repo = DirectoryRepository()
        return self._dir_repo
    
    @property
    def file_repo(self) -> FileRepository:
        """FileRepository ì¸ìŠ¤í„´ìŠ¤ (lazy)"""
        if not hasattr(self, '_file_repo') or self._file_repo is None:
            self._file_repo = FileRepository()
        return self._file_repo
    
    # =========================================================================
    # Main Execution
    # =========================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒ¨í„´ì„ ê´€ì°°
        
        Returns:
            - grouping_prep_result: ìš”ì•½ í†µê³„
            - directories_for_grouping: LLM ì…ë ¥ìš© ë””ë ‰í† ë¦¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        self.log("=" * 60)
        self.log("ğŸ“ íŒŒì¼ ê·¸ë£¹í•‘ ì¤€ë¹„ (íŒ¨í„´ ê´€ì°°)")
        self.log("=" * 60)
        
        # 1. ë””ë ‰í† ë¦¬ ëª©ë¡ ì¡°íšŒ
        directories = self._get_directories_with_files()
        self.log(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {len(directories)}ê°œ")
        
        # 2. ê° ë””ë ‰í† ë¦¬ë³„ ìƒì„¸ ë¶„ì„
        directories_for_grouping = []
        total_files = 0
        
        for dir_info in directories:
            dir_summary = self._analyze_directory(dir_info)
            
            if dir_summary:
                directories_for_grouping.append(dir_summary)
                total_files += dir_summary['file_count']
                
                # ë¡œê·¸ ì¶œë ¥
                self._log_directory_summary(dir_summary)
        
        # 3. ê²°ê³¼ êµ¬ì„±
        result = {
            "total_directories": len(directories_for_grouping),
            "total_files_analyzed": total_files,
            "directories_with_patterns": sum(
                1 for d in directories_for_grouping 
                if d.get('observed_patterns')
            ),
            "prepared_at": datetime.now().isoformat()
        }
        
        self.log("=" * 60)
        self.log("âœ… ì¤€ë¹„ ì™„ë£Œ!")
        self.log(f"â†’ {result['total_directories']}ê°œ ë””ë ‰í† ë¦¬ ë¶„ì„", indent=1)
        self.log(f"â†’ {result['total_files_analyzed']}ê°œ íŒŒì¼ ê´€ì°°", indent=1)
        self.log(f"â†’ {result['directories_with_patterns']}ê°œ ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ê°ì§€", indent=1)
        self.log("=" * 60)
        
        return {
            "grouping_prep_result": result,
            "directories_for_grouping": directories_for_grouping
        }
    
    # =========================================================================
    # Directory Analysis (via Repository)
    # =========================================================================
    
    def _get_directories_with_files(self) -> List[Dict]:
        """íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ëª©ë¡ ì¡°íšŒ (via DirectoryRepository)"""
        return self.dir_repo.get_directories_with_files(min_files=self.MIN_FILES_FOR_ANALYSIS)
    
    def _get_files_in_directory(self, dir_id: str) -> List[Dict]:
        """íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ (via FileRepository)"""
        return self.file_repo.get_files_by_dir_id(dir_id)
    
    def _analyze_directory(self, dir_info: Dict) -> Optional[Dict]:
        """
        ë””ë ‰í† ë¦¬ ë¶„ì„ - íŒ¨í„´ ê´€ì°° (íŒë‹¨ ì—†ì´)
        
        Returns:
            ë””ë ‰í† ë¦¬ ìš”ì•½ ì •ë³´ (LLM ì…ë ¥ìš©)
        """
        dir_id = str(dir_info['dir_id'])
        file_count = dir_info.get('actual_file_count', 0)
        
        # ìµœì†Œ íŒŒì¼ ìˆ˜ ì²´í¬
        if file_count < self.MIN_FILES_FOR_ANALYSIS:
            return None
        
        # íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        files = self._get_files_in_directory(dir_id)
        
        if not files:
            return None
        
        # í™•ì¥ìë³„ ë¶„ë¥˜
        ext_distribution = self._get_extension_distribution(files)
        
        # íŒŒì¼ëª… ë¶„ì„
        filename_analysis = self._analyze_filenames(files)
        
        # íŒŒì¼ í¬ê¸° í†µê³„
        size_stats = self._get_size_statistics(files)
        
        # íŒ¨í„´ ê´€ì°° (íŒë‹¨ ì—†ì´ ê´€ì°°ëœ ì‚¬ì‹¤ë§Œ)
        observed_patterns = self._observe_patterns(files)
        
        # ì´ë¯¸ ê·¸ë£¹í™”ëœ íŒŒì¼ ìˆ˜
        already_grouped = sum(1 for f in files if f.get('group_id'))
        
        return {
            "dir_id": dir_id,
            "dir_path": dir_info['dir_path'],
            "dir_name": dir_info['dir_name'],
            "file_count": len(files),
            "already_grouped_count": already_grouped,
            
            # í™•ì¥ì ë¶„í¬
            "extension_distribution": ext_distribution,
            
            # íŒŒì¼ëª… ë¶„ì„
            "filename_samples": filename_analysis['samples'],
            "filename_length_stats": filename_analysis['length_stats'],
            "common_prefix": filename_analysis['common_prefix'],
            "common_suffix": filename_analysis['common_suffix'],
            
            # í¬ê¸° í†µê³„
            "size_stats": size_stats,
            
            # ê´€ì°°ëœ íŒ¨í„´ (íŒë‹¨ ì—†ì´)
            "observed_patterns": observed_patterns
        }
    
    # =========================================================================
    # Analysis Helpers
    # =========================================================================
    
    def _get_extension_distribution(self, files: List[Dict]) -> Dict[str, int]:
        """í™•ì¥ìë³„ íŒŒì¼ ìˆ˜ ê³„ì‚°"""
        distribution = defaultdict(int)
        for f in files:
            ext = f.get('file_extension', 'unknown') or 'no_extension'
            distribution[ext] += 1
        return dict(distribution)
    
    def _analyze_filenames(self, files: List[Dict]) -> Dict[str, Any]:
        """íŒŒì¼ëª… ë¶„ì„"""
        filenames = [f['file_name'] for f in files if f.get('file_name')]
        
        if not filenames:
            return {
                'samples': [],
                'length_stats': {},
                'common_prefix': '',
                'common_suffix': ''
            }
        
        # ìƒ˜í”Œ ì¶”ì¶œ (ì²« Nê°œ, ì¤‘ê°„, ë§ˆì§€ë§‰)
        samples = self._get_representative_samples(filenames)
        
        # ê¸¸ì´ í†µê³„
        lengths = [len(fn) for fn in filenames]
        length_stats = {
            'min': min(lengths),
            'max': max(lengths),
            'avg': round(sum(lengths) / len(lengths), 1)
        }
        
        # ê³µí†µ prefix/suffix ì°¾ê¸°
        common_prefix = self._find_common_prefix(filenames)
        common_suffix = self._find_common_suffix(filenames)
        
        return {
            'samples': samples,
            'length_stats': length_stats,
            'common_prefix': common_prefix,
            'common_suffix': common_suffix
        }
    
    def _get_representative_samples(self, filenames: List[str]) -> List[str]:
        """ëŒ€í‘œ ìƒ˜í”Œ ì¶”ì¶œ"""
        if len(filenames) <= self.MAX_FILENAME_SAMPLES:
            return sorted(filenames)
        
        # ì²« 3ê°œ, ì¤‘ê°„ 4ê°œ, ë§ˆì§€ë§‰ 3ê°œ
        sorted_names = sorted(filenames)
        n = len(sorted_names)
        
        samples = []
        samples.extend(sorted_names[:3])  # ì²˜ìŒ
        
        mid_start = n // 2 - 2
        samples.extend(sorted_names[mid_start:mid_start + 4])  # ì¤‘ê°„
        
        samples.extend(sorted_names[-3:])  # ë§ˆì§€ë§‰
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return sorted(list(set(samples)))
    
    def _find_common_prefix(self, strings: List[str]) -> str:
        """ê³µí†µ prefix ì°¾ê¸°"""
        if not strings:
            return ''
        
        prefix = strings[0]
        for s in strings[1:]:
            while not s.startswith(prefix):
                prefix = prefix[:-1]
                if not prefix:
                    return ''
        return prefix
    
    def _find_common_suffix(self, strings: List[str]) -> str:
        """ê³µí†µ suffix ì°¾ê¸° (í™•ì¥ì í¬í•¨)"""
        if not strings:
            return ''
        
        reversed_strings = [s[::-1] for s in strings]
        reversed_suffix = self._find_common_prefix(reversed_strings)
        return reversed_suffix[::-1]
    
    def _get_size_statistics(self, files: List[Dict]) -> Dict[str, Any]:
        """íŒŒì¼ í¬ê¸° í†µê³„"""
        sizes = [f.get('file_size_bytes', 0) or 0 for f in files]
        
        if not sizes:
            return {}
        
        return {
            'min_bytes': min(sizes),
            'max_bytes': max(sizes),
            'avg_bytes': round(sum(sizes) / len(sizes)),
            'total_bytes': sum(sizes),
            # MB ë‹¨ìœ„ë„ ì œê³µ
            'min_mb': round(min(sizes) / (1024 * 1024), 2),
            'max_mb': round(max(sizes) / (1024 * 1024), 2),
            'avg_mb': round(sum(sizes) / len(sizes) / (1024 * 1024), 2)
        }
    
    # =========================================================================
    # Pattern Observation (No Judgment)
    # =========================================================================
    
    def _observe_patterns(self, files: List[Dict]) -> List[Dict]:
        """
        íŒ¨í„´ ê´€ì°° (íŒë‹¨ ì—†ì´ ê´€ì°°ëœ ì‚¬ì‹¤ë§Œ ê¸°ë¡)
        
        ê´€ì°° í•­ëª©:
        - ìˆ«ìë¡œë§Œ ëœ íŒŒì¼ëª… (1.csv, 2.csv, ...)
        - ìˆ«ì prefix/suffix (case_001.csv, record_2024.csv)
        - ë‚ ì§œ íŒ¨í„´ (2024-01-01.log)
        - ë¶„í•  íŒŒì¼ íŒ¨í„´ (table_1.csv, table_2.csv)
        - ìŒ íŒŒì¼ íŒ¨í„´ (.hea + .dat)
        
        Returns:
            ê´€ì°°ëœ íŒ¨í„´ ëª©ë¡ (íŒë‹¨ ì—†ì´ ì‚¬ì‹¤ë§Œ)
        """
        patterns = []
        filenames = [f['file_name'] for f in files if f.get('file_name')]
        
        if not filenames:
            return patterns
        
        # 1. ìˆœìˆ˜ ìˆ«ì íŒŒì¼ëª… ê´€ì°°
        numeric_only = self._observe_numeric_only_pattern(filenames)
        if numeric_only:
            patterns.append(numeric_only)
        
        # 2. ìˆ«ì í¬í•¨ íŒ¨í„´ ê´€ì°°
        numeric_parts = self._observe_numeric_parts_pattern(filenames)
        if numeric_parts:
            patterns.append(numeric_parts)
        
        # 3. ë‚ ì§œ íŒ¨í„´ ê´€ì°°
        date_pattern = self._observe_date_pattern(filenames)
        if date_pattern:
            patterns.append(date_pattern)
        
        # 4. ë¶„í•  íŒŒì¼ íŒ¨í„´ ê´€ì°° (base_1, base_2)
        partition_pattern = self._observe_partition_pattern(filenames)
        if partition_pattern:
            patterns.append(partition_pattern)
        
        # 5. í™•ì¥ì ìŒ ê´€ì°° (.hea + .dat)
        paired_ext = self._observe_paired_extensions(files)
        if paired_ext:
            patterns.append(paired_ext)
        
        return patterns
    
    def _observe_numeric_only_pattern(self, filenames: List[str]) -> Optional[Dict]:
        """ìˆœìˆ˜ ìˆ«ì íŒŒì¼ëª… ê´€ì°° (1.vital, 2.vital)"""
        # í™•ì¥ì ì œê±° í›„ ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸
        numeric_count = 0
        numeric_values = []
        
        for fn in filenames:
            name_without_ext = fn.rsplit('.', 1)[0] if '.' in fn else fn
            if name_without_ext.isdigit():
                numeric_count += 1
                numeric_values.append(int(name_without_ext))
        
        if numeric_count == 0:
            return None
        
        ratio = numeric_count / len(filenames)
        
        if ratio < 0.5:  # 50% ë¯¸ë§Œì´ë©´ ë¬´ì‹œ
            return None
        
        return {
            'type': 'numeric_only',
            'description': 'íŒŒì¼ëª…ì´ ìˆœìˆ˜ ìˆ«ì (ì˜ˆ: 1.ext, 2.ext)',
            'matching_count': numeric_count,
            'total_count': len(filenames),
            'ratio': round(ratio, 2),
            'value_range': {
                'min': min(numeric_values) if numeric_values else None,
                'max': max(numeric_values) if numeric_values else None
            }
        }
    
    def _observe_numeric_parts_pattern(self, filenames: List[str]) -> Optional[Dict]:
        """ìˆ«ì ë¶€ë¶„ì´ í¬í•¨ëœ íŒ¨í„´ ê´€ì°° (case_001.csv)"""
        # ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
        pattern = re.compile(r'(\d+)')
        
        files_with_numbers = 0
        number_positions = defaultdict(int)  # ìœ„ì¹˜ë³„ ì¹´ìš´íŠ¸
        
        for fn in filenames:
            name_without_ext = fn.rsplit('.', 1)[0] if '.' in fn else fn
            matches = list(pattern.finditer(name_without_ext))
            
            if matches:
                files_with_numbers += 1
                # ìˆ«ìì˜ ìƒëŒ€ì  ìœ„ì¹˜ ê¸°ë¡
                for m in matches:
                    rel_pos = m.start() / len(name_without_ext) if name_without_ext else 0
                    position = 'start' if rel_pos < 0.3 else ('end' if rel_pos > 0.7 else 'middle')
                    number_positions[position] += 1
        
        if files_with_numbers == 0:
            return None
        
        ratio = files_with_numbers / len(filenames)
        
        if ratio < 0.5:
            return None
        
        return {
            'type': 'numeric_parts',
            'description': 'íŒŒì¼ëª…ì— ìˆ«ì ë¶€ë¶„ í¬í•¨ (ì˜ˆ: case_001.csv)',
            'matching_count': files_with_numbers,
            'total_count': len(filenames),
            'ratio': round(ratio, 2),
            'number_positions': dict(number_positions)
        }
    
    def _observe_date_pattern(self, filenames: List[str]) -> Optional[Dict]:
        """ë‚ ì§œ íŒ¨í„´ ê´€ì°°"""
        # ì¼ë°˜ì ì¸ ë‚ ì§œ íŒ¨í„´ë“¤
        date_patterns = [
            (r'\d{4}-\d{2}-\d{2}', 'YYYY-MM-DD'),
            (r'\d{4}\d{2}\d{2}', 'YYYYMMDD'),
            (r'\d{2}-\d{2}-\d{4}', 'DD-MM-YYYY'),
            (r'\d{2}/\d{2}/\d{4}', 'DD/MM/YYYY'),
        ]
        
        for pattern, format_name in date_patterns:
            regex = re.compile(pattern)
            matching = [fn for fn in filenames if regex.search(fn)]
            
            if len(matching) >= len(filenames) * 0.5:
                return {
                    'type': 'date_pattern',
                    'description': f'ë‚ ì§œ íŒ¨í„´ ê°ì§€ ({format_name})',
                    'format': format_name,
                    'matching_count': len(matching),
                    'total_count': len(filenames),
                    'ratio': round(len(matching) / len(filenames), 2)
                }
        
        return None
    
    def _observe_partition_pattern(self, filenames: List[str]) -> Optional[Dict]:
        """ë¶„í•  íŒŒì¼ íŒ¨í„´ ê´€ì°° (table_1.csv, table_2.csv)"""
        # base_N ë˜ëŠ” base-N íŒ¨í„´
        pattern = re.compile(r'^(.+)[_-](\d+)\.(\w+)$')
        
        base_names = defaultdict(list)
        
        for fn in filenames:
            match = pattern.match(fn)
            if match:
                base, num, ext = match.groups()
                base_names[f"{base}.{ext}"].append(int(num))
        
        # 2ê°œ ì´ìƒì˜ íŒŒí‹°ì…˜ì´ ìˆëŠ” ë² ì´ìŠ¤ë§Œ
        partitioned = {k: v for k, v in base_names.items() if len(v) >= 2}
        
        if not partitioned:
            return None
        
        return {
            'type': 'partitioned',
            'description': 'ë¶„í•  íŒŒì¼ íŒ¨í„´ (ì˜ˆ: table_1.csv, table_2.csv)',
            'base_tables': [
                {
                    'base_name': base,
                    'partition_count': len(nums),
                    'partition_range': {'min': min(nums), 'max': max(nums)}
                }
                for base, nums in partitioned.items()
            ]
        }
    
    def _observe_paired_extensions(self, files: List[Dict]) -> Optional[Dict]:
        """í™•ì¥ì ìŒ ê´€ì°° (.hea + .dat)"""
        # íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)ìœ¼ë¡œ ê·¸ë£¹í™”
        by_stem = defaultdict(list)
        
        for f in files:
            fn = f.get('file_name', '')
            if '.' in fn:
                stem = fn.rsplit('.', 1)[0]
                ext = fn.rsplit('.', 1)[1]
                by_stem[stem].append(ext)
        
        # 2ê°œ ì´ìƒì˜ í™•ì¥ìë¥¼ ê°€ì§„ stem
        paired_stems = {k: v for k, v in by_stem.items() if len(v) >= 2}
        
        if not paired_stems:
            return None
        
        # ê°€ì¥ í”í•œ í™•ì¥ì ì¡°í•©
        ext_combinations = defaultdict(int)
        for exts in paired_stems.values():
            combo = tuple(sorted(set(exts)))
            ext_combinations[combo] += 1
        
        most_common = max(ext_combinations.items(), key=lambda x: x[1])
        
        return {
            'type': 'paired_extensions',
            'description': 'ë™ì¼ íŒŒì¼ëª…ì— ì—¬ëŸ¬ í™•ì¥ì (ì˜ˆ: record.hea + record.dat)',
            'paired_count': len(paired_stems),
            'most_common_pair': list(most_common[0]),
            'pair_frequency': most_common[1]
        }
    
    # =========================================================================
    # Logging Helpers
    # =========================================================================
    
    def _log_directory_summary(self, summary: Dict):
        """ë””ë ‰í† ë¦¬ ìš”ì•½ ë¡œê·¸"""
        self.log(f"\nğŸ“‚ {summary['dir_name']}/", indent=1)
        self.log(f"íŒŒì¼ ìˆ˜: {summary['file_count']}", indent=2)
        
        if summary.get('already_grouped_count'):
            self.log(f"ì´ë¯¸ ê·¸ë£¹í™”ë¨: {summary['already_grouped_count']}", indent=2)
        
        # í™•ì¥ì ë¶„í¬
        ext_dist = summary.get('extension_distribution', {})
        if ext_dist:
            ext_str = ', '.join(f"{k}: {v}" for k, v in ext_dist.items())
            self.log(f"í™•ì¥ì: {ext_str}", indent=2)
        
        # íŒŒì¼ëª… ìƒ˜í”Œ
        samples = summary.get('filename_samples', [])
        if samples:
            sample_str = ', '.join(samples[:5])
            if len(samples) > 5:
                sample_str += f" ... (+{len(samples) - 5})"
            self.log(f"ìƒ˜í”Œ: {sample_str}", indent=2)
        
        # ê´€ì°°ëœ íŒ¨í„´
        patterns = summary.get('observed_patterns', [])
        if patterns:
            self.log("ê´€ì°°ëœ íŒ¨í„´:", indent=2)
            for p in patterns:
                self.log(f"- {p['type']}: {p['description']} ({p.get('ratio', 0)*100:.0f}%)", indent=3)
    
    # =========================================================================
    # Standalone Execution
    # =========================================================================
    
    @classmethod
    def run_standalone(cls, verbose: bool = True) -> Dict[str, Any]:
        """ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)"""
        node = cls()
        result = node.execute({})
        
        if verbose:
            for log in node._logs:
                print(log)
        
        return result

