# src/agents/nodes/aggregator.py
"""
Phase 3: Schema Aggregation Node

DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª…ê³¼ ëŒ€í‘œ í†µê³„ë¥¼ ì§‘ê³„í•˜ì—¬
Phase 1ì˜ ë°°ì¹˜ LLM í˜¸ì¶œì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª… ì¶”ì¶œ (GROUP BY original_name)
- ëŒ€í‘œ í†µê³„ ì§‘ê³„ (AVG min/max/mean, sample values)
- ë°°ì¹˜ ë¶„í•  (config.BATCH_SIZE ë‹¨ìœ„)
"""

import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.database.connection import get_db_manager
from src.config import Phase3Config




# =============================================================================
# SQL ì¿¼ë¦¬
# =============================================================================

# ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„ ì¿¼ë¦¬ (Continuous)
AGGREGATE_CONTINUOUS_SQL = """
SELECT 
    cm.original_name,
    cm.column_type,
    COUNT(DISTINCT cm.file_id) as frequency,
    
    -- ìˆ˜ì¹˜í˜• í†µê³„ ì§‘ê³„
    AVG((cm.column_info->>'min')::float) as avg_min,
    AVG((cm.column_info->>'max')::float) as avg_max,
    AVG((cm.column_info->>'mean')::float) as avg_mean,
    AVG((cm.column_info->>'std')::float) as avg_std,
    MAX(cm.column_info->>'unit') as sample_unit,
    
    -- ìƒ˜í”Œ file_id (ìµœëŒ€ Nê°œ)
    (SELECT array_agg(DISTINCT sub.file_id::text)
     FROM (
         SELECT file_id FROM column_metadata 
         WHERE original_name = cm.original_name 
         LIMIT %s
     ) sub
    ) as sample_file_ids

FROM column_metadata cm
WHERE cm.column_type IN ('continuous', 'waveform')
GROUP BY cm.original_name, cm.column_type
ORDER BY frequency DESC;
"""

# ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„ ì¿¼ë¦¬ (Categorical)
AGGREGATE_CATEGORICAL_SQL = """
SELECT 
    cm.original_name,
    cm.column_type,
    COUNT(DISTINCT cm.file_id) as frequency,
    
    -- ë²”ì£¼í˜• í†µê³„ ì§‘ê³„
    AVG((cm.column_info->>'unique_count')::float) as avg_unique_count,
    AVG((cm.column_info->>'unique_ratio')::float) as avg_unique_ratio,
    
    -- ëŒ€í‘œ ê°’ (ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
    (SELECT sub.value_distribution
     FROM column_metadata sub 
     WHERE sub.original_name = cm.original_name 
       AND sub.value_distribution != '{}'::jsonb
     LIMIT 1
    ) as sample_distribution,
    
    -- ìƒ˜í”Œ file_id
    (SELECT array_agg(DISTINCT sub.file_id::text)
     FROM (
         SELECT file_id FROM column_metadata 
         WHERE original_name = cm.original_name 
         LIMIT %s
     ) sub
    ) as sample_file_ids

FROM column_metadata cm
WHERE cm.column_type = 'categorical'
GROUP BY cm.original_name, cm.column_type
ORDER BY frequency DESC;
"""

# ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„ ì¿¼ë¦¬ (Datetime)
AGGREGATE_DATETIME_SQL = """
SELECT 
    cm.original_name,
    cm.column_type,
    COUNT(DISTINCT cm.file_id) as frequency,
    
    -- ë‚ ì§œ ë²”ìœ„
    MIN(cm.column_info->>'min_date') as min_date,
    MAX(cm.column_info->>'max_date') as max_date,
    
    -- ìƒ˜í”Œ file_id
    (SELECT array_agg(DISTINCT sub.file_id::text)
     FROM (
         SELECT file_id FROM column_metadata 
         WHERE original_name = cm.original_name 
         LIMIT %s
     ) sub
    ) as sample_file_ids

FROM column_metadata cm
WHERE cm.column_type = 'datetime'
GROUP BY cm.original_name, cm.column_type
ORDER BY frequency DESC;
"""

# ì „ì²´ ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„ (í†µí•© ì¿¼ë¦¬ - ë‹¨ìˆœí™”ëœ ë²„ì „)
AGGREGATE_ALL_SQL = """
SELECT 
    cm.original_name,
    cm.column_type,
    COUNT(DISTINCT cm.file_id) as frequency,
    
    -- í†µê³„ (JSONì—ì„œ ì¶”ì¶œ)
    AVG((cm.column_info->>'min')::float) as avg_min,
    AVG((cm.column_info->>'max')::float) as avg_max,
    AVG((cm.column_info->>'mean')::float) as avg_mean,
    AVG((cm.column_info->>'unique_count')::float) as avg_unique_count,
    AVG((cm.column_info->>'unique_ratio')::float) as avg_unique_ratio,
    MAX(cm.column_info->>'unit') as sample_unit,
    
    -- ëŒ€í‘œ ê°’ ë¶„í¬ (ì²« ë²ˆì§¸ ìœ íš¨ê°’)
    (SELECT sub.value_distribution
     FROM column_metadata sub 
     WHERE sub.original_name = cm.original_name 
       AND sub.value_distribution IS NOT NULL 
       AND sub.value_distribution != '{}'::jsonb
     LIMIT 1
    ) as sample_distribution

FROM column_metadata cm
GROUP BY cm.original_name, cm.column_type
ORDER BY frequency DESC, cm.original_name;
"""


# =============================================================================
# í•µì‹¬ í•¨ìˆ˜
# =============================================================================

def aggregate_unique_columns() -> List[Dict[str, Any]]:
    """
    DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª…ê³¼ ëŒ€í‘œ í†µê³„ ì¶”ì¶œ
    
    Returns:
        List of unique columns with aggregated stats
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        # í†µí•© ì¿¼ë¦¬ ì‹¤í–‰
        cursor.execute(AGGREGATE_ALL_SQL)
        rows = cursor.fetchall()
        
        # ì»¬ëŸ¼ëª… ì¶”ì¶œ
        col_names = [desc[0] for desc in cursor.description]
        
        unique_columns = []
        for row in rows:
            row_dict = dict(zip(col_names, row))
            
            # í†µê³„ ì •ë¦¬
            column_info = {
                "original_name": row_dict["original_name"],
                "column_type": row_dict["column_type"] or "unknown",
                "frequency": row_dict["frequency"] or 0,
            }
            
            # ìˆ˜ì¹˜í˜• í†µê³„
            if row_dict.get("avg_min") is not None:
                column_info["avg_min"] = round(row_dict["avg_min"], 2)
            if row_dict.get("avg_max") is not None:
                column_info["avg_max"] = round(row_dict["avg_max"], 2)
            if row_dict.get("avg_mean") is not None:
                column_info["avg_mean"] = round(row_dict["avg_mean"], 2)
            
            # ë²”ì£¼í˜• í†µê³„
            if row_dict.get("avg_unique_count") is not None:
                column_info["avg_unique_count"] = round(row_dict["avg_unique_count"], 1)
            if row_dict.get("avg_unique_ratio") is not None:
                column_info["avg_unique_ratio"] = round(row_dict["avg_unique_ratio"], 3)
            
            # ë‹¨ìœ„
            if row_dict.get("sample_unit"):
                column_info["sample_unit"] = row_dict["sample_unit"]
            
            # ëŒ€í‘œ ê°’ ë¶„í¬ (ìµœëŒ€ Nê°œ)
            sample_dist = row_dict.get("sample_distribution")
            if sample_dist and isinstance(sample_dist, dict):
                # ìƒìœ„ Nê°œ ê°’ë§Œ ì¶”ì¶œ
                max_samples = Phase3Config.MAX_SAMPLE_VALUES
                top_values = dict(list(sample_dist.items())[:max_samples])
                if top_values:
                    column_info["sample_values"] = top_values
            
            unique_columns.append(column_info)
        
        return unique_columns
        
    except Exception as e:
        print(f"[Aggregator] Error aggregating columns: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()


def prepare_llm_batches(
    unique_columns: List[Dict[str, Any]], 
    batch_size: Optional[int] = None
) -> List[List[Dict[str, Any]]]:
    """
    ìœ ë‹ˆí¬ ì»¬ëŸ¼ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ”
    
    Args:
        unique_columns: ìœ ë‹ˆí¬ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ë‹¹ ì»¬ëŸ¼ ìˆ˜ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
    
    Returns:
        List of batches (ê° ë°°ì¹˜ëŠ” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸)
    """
    if batch_size is None:
        batch_size = Phase3Config.BATCH_SIZE
    
    batches = []
    for i in range(0, len(unique_columns), batch_size):
        batch = unique_columns[i:i + batch_size]
        batches.append(batch)
    
    return batches


def get_aggregation_stats() -> Dict[str, Any]:
    """
    ì§‘ê³„ í†µê³„ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    stats = {}
    
    try:
        # ì „ì²´ ì»¬ëŸ¼ ìˆ˜
        cursor.execute("SELECT COUNT(*) FROM column_metadata")
        stats["total_columns"] = cursor.fetchone()[0]
        
        # ìœ ë‹ˆí¬ ì»¬ëŸ¼ ìˆ˜
        cursor.execute("SELECT COUNT(DISTINCT original_name) FROM column_metadata")
        stats["unique_columns"] = cursor.fetchone()[0]
        
        # column_typeë³„ ìœ ë‹ˆí¬ ìˆ˜
        cursor.execute("""
            SELECT column_type, COUNT(DISTINCT original_name) 
            FROM column_metadata 
            GROUP BY column_type
        """)
        stats["unique_by_type"] = dict(cursor.fetchall())
        
        # íŒŒì¼ ìˆ˜
        cursor.execute("SELECT COUNT(*) FROM file_catalog")
        stats["total_files"] = cursor.fetchone()[0]
        
    except Exception as e:
        print(f"[Aggregator] Error getting stats: {e}")
        stats["error"] = str(e)
    
    return stats


# =============================================================================
# LangGraph Node Function
# =============================================================================

def phase3_aggregation_node(state: AgentState) -> Dict[str, Any]:
    """
    Phase 0.5: Schema Aggregation ë…¸ë“œ
    
    DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ê³¼ íŒŒì¼ì„ ì§‘ê³„í•˜ê³  Phase 1 LLM ë°°ì¹˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    Input (from state):
        - phase0_file_ids: Phase 0ì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ IDë“¤ (ì°¸ê³ ìš©)
    
    Output (to state):
        - phase05_result: ì§‘ê³„ ê²°ê³¼ ìš”ì•½
        - unique_columns: ìœ ë‹ˆí¬ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        - unique_files: ìœ ë‹ˆí¬ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        - column_batches: ì»¬ëŸ¼ LLM ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
        - file_batches: íŒŒì¼ LLM ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    from src.config import Phase5Config
    
    print("\n" + "=" * 60)
    print("ğŸ”„ Phase 3: Schema Aggregation")
    print("=" * 60)
    
    # 1. ì§‘ê³„ í†µê³„ ì¡°íšŒ
    stats = get_aggregation_stats()
    print(f"\nğŸ“Š Current DB Stats:")
    print(f"   Total files: {stats.get('total_files', 0):,}")
    print(f"   Total columns: {stats.get('total_columns', 0):,}")
    print(f"   Unique columns: {stats.get('unique_columns', 0):,}")
    
    if stats.get('unique_by_type'):
        print(f"   By type: {stats.get('unique_by_type')}")
    
    # =========================================================================
    # 2. ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„
    # =========================================================================
    print(f"\nğŸ” Aggregating unique columns...")
    unique_columns = aggregate_unique_columns()
    print(f"   âœ… Found {len(unique_columns)} unique columns")
    
    # ì»¬ëŸ¼ ë°°ì¹˜ ì¤€ë¹„
    column_batch_size = Phase5Config.COLUMN_BATCH_SIZE
    column_batches = prepare_llm_batches(unique_columns, column_batch_size)
    print(f"\nğŸ“¦ Column LLM Batches:")
    print(f"   Batch size: {column_batch_size}")
    print(f"   Total batches: {len(column_batches)}")
    
    # ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 5ê°œ ì»¬ëŸ¼)
    if unique_columns:
        print(f"\nğŸ“ Sample columns (top 5 by frequency):")
        for col in unique_columns[:5]:
            freq = col.get('frequency', 0)
            col_type = col.get('column_type', 'unknown')
            name = col.get('original_name', '?')
            
            stat_str = ""
            if col.get('avg_min') is not None:
                stat_str = f"range: [{col.get('avg_min'):.1f}, {col.get('avg_max'):.1f}]"
            elif col.get('sample_values'):
                values = list(col['sample_values'].keys())[:3]
                stat_str = f"values: {values}"
            
            print(f"   - {name} ({col_type}, freq={freq}) {stat_str}")
    
    # =========================================================================
    # 3. íŒŒì¼ ì§‘ê³„
    # =========================================================================
    print(f"\nğŸ” Aggregating files for semantic analysis...")
    unique_files = aggregate_unique_files()
    print(f"   âœ… Found {len(unique_files)} files to analyze")
    
    # íŒŒì¼ ë°°ì¹˜ ì¤€ë¹„
    file_batch_size = Phase5Config.FILE_BATCH_SIZE
    file_batches = prepare_file_batches(unique_files, file_batch_size)
    print(f"\nğŸ“¦ File LLM Batches:")
    print(f"   Batch size: {file_batch_size}")
    print(f"   Total batches: {len(file_batches)}")
    
    # ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 5ê°œ íŒŒì¼)
    if unique_files:
        print(f"\nğŸ“ Sample files:")
        for f in unique_files[:5]:
            name = f.get('file_name', '?')
            cols = f.get('column_count', 0)
            ptype = f.get('processor_type', '?')
            print(f"   - {name} ({ptype}, {cols} columns)")
    
    # =========================================================================
    # 4. ê²°ê³¼ êµ¬ì„±
    # =========================================================================
    result = {
        "total_columns_in_db": stats.get('total_columns', 0),
        "unique_column_count": len(unique_columns),
        "unique_file_count": len(unique_files),
        "column_batch_size": column_batch_size,
        "file_batch_size": file_batch_size,
        "column_batches": len(column_batches),
        "file_batches": len(file_batches),
        "aggregated_at": datetime.now().isoformat(),
        "stats": stats
    }
    
    print(f"\nâœ… Phase 3 Complete!")
    print(f"   â†’ {len(unique_columns)} unique columns â†’ {len(column_batches)} batches")
    print(f"   â†’ {len(unique_files)} files â†’ {len(file_batches)} batches")
    print(f"   â†’ Ready for Phase 4 LLM analysis!")
    print("=" * 60 + "\n")
    
    return {
        "phase3_result": result,
        "unique_columns": unique_columns,
        "unique_files": unique_files,
        "column_batches": column_batches,
        "file_batches": file_batches
    }


# =============================================================================
# íŒŒì¼ ì§‘ê³„ í•¨ìˆ˜ (Phase 1 File Analysisìš©)
# =============================================================================

AGGREGATE_FILES_SQL = """
SELECT 
    fc.file_id,
    fc.file_name,
    fc.file_extension,
    fc.processor_type,
    fc.file_size_mb,
    fc.file_metadata,
    
    -- ì»¬ëŸ¼ ì •ë³´ ìš”ì•½
    COUNT(cm.col_id) as column_count,
    ARRAY_AGG(DISTINCT cm.original_name) as column_names,
    ARRAY_AGG(DISTINCT cm.column_type) as column_types
    
FROM file_catalog fc
LEFT JOIN column_metadata cm ON fc.file_id = cm.file_id
WHERE fc.semantic_type IS NULL  -- ì•„ì§ ë¶„ì„ ì•ˆ ëœ íŒŒì¼ë§Œ
GROUP BY fc.file_id, fc.file_name, fc.file_extension, 
         fc.processor_type, fc.file_size_mb, fc.file_metadata
ORDER BY fc.file_name;
"""


def aggregate_unique_files() -> List[Dict[str, Any]]:
    """
    Phase 1 File Analysisë¥¼ ìœ„í•´ íŒŒì¼ ì •ë³´ ì§‘ê³„
    
    Returns:
        List of file info dicts
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute(AGGREGATE_FILES_SQL)
        rows = cursor.fetchall()
        
        col_names = [desc[0] for desc in cursor.description]
        
        files = []
        for row in rows:
            row_dict = dict(zip(col_names, row))
            
            file_info = {
                "file_id": str(row_dict["file_id"]),
                "file_name": row_dict["file_name"],
                "file_extension": row_dict["file_extension"],
                "processor_type": row_dict["processor_type"],
                "file_size_mb": float(row_dict["file_size_mb"]) if row_dict["file_size_mb"] else 0,
                "column_count": row_dict["column_count"] or 0,
                "column_names": row_dict["column_names"][:20] if row_dict["column_names"] else [],  # ì²˜ìŒ 20ê°œë§Œ
                "column_types": list(set(row_dict["column_types"])) if row_dict["column_types"] else []
            }
            
            # file_metadataì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
            metadata = row_dict.get("file_metadata", {}) or {}
            if metadata:
                file_info["row_count"] = metadata.get("row_count")
                file_info["duration_seconds"] = metadata.get("duration_seconds")
            
            files.append(file_info)
        
        return files
        
    except Exception as e:
        print(f"[Aggregator] Error aggregating files: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()


def prepare_file_batches(
    files: List[Dict[str, Any]],
    batch_size: Optional[int] = None
) -> List[List[Dict[str, Any]]]:
    """
    íŒŒì¼ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ”
    
    Args:
        files: íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
    """
    from src.config import Phase5Config
    
    if batch_size is None:
        batch_size = Phase5Config.FILE_BATCH_SIZE
    
    batches = []
    for i in range(0, len(files), batch_size):
        batch = files[i:i + batch_size]
        batches.append(batch)
    
    return batches


# =============================================================================
# í¸ì˜ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
# =============================================================================

def run_aggregation(verbose: bool = True) -> Dict[str, Any]:
    """
    Phase 0.5 ì‹¤í–‰ (ë…ë¦½ ì‹¤í–‰ìš©)
    
    Returns:
        Dict with unique_columns, batches, and stats
    """
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ”„ Running Schema Aggregation...")
        print("=" * 60)
    
    # ì»¬ëŸ¼ ì§‘ê³„
    unique_columns = aggregate_unique_columns()
    column_batches = prepare_llm_batches(unique_columns)
    
    # íŒŒì¼ ì§‘ê³„
    unique_files = aggregate_unique_files()
    file_batches = prepare_file_batches(unique_files)
    
    # í†µê³„
    stats = get_aggregation_stats()
    
    result = {
        "unique_columns": unique_columns,
        "column_batches": column_batches,
        "unique_files": unique_files,
        "file_batches": file_batches,
        "stats": stats,
        "unique_column_count": len(unique_columns),
        "unique_file_count": len(unique_files),
        "column_batch_count": len(column_batches),
        "file_batch_count": len(file_batches)
    }
    
    if verbose:
        print(f"\nâœ… Aggregation Complete:")
        print(f"   Unique columns: {len(unique_columns)} â†’ {len(column_batches)} batches")
        print(f"   Unique files: {len(unique_files)} â†’ {len(file_batches)} batches")
    
    return result

