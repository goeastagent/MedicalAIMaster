# src/agents/nodes/aggregator.py
"""
Schema Aggregation Node

DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª…ê³¼ ëŒ€í‘œ í†µê³„ë¥¼ ì§‘ê³„í•˜ì—¬
LLM ë°°ì¹˜ í˜¸ì¶œì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª… ì¶”ì¶œ (GROUP BY original_name)
- ëŒ€í‘œ í†µê³„ ì§‘ê³„ (AVG min/max/mean, sample values)
- ë°°ì¹˜ ë¶„í•  (config.BATCH_SIZE ë‹¨ìœ„)
"""

from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.database.connection import get_db_manager
from src.config import SchemaAggregationConfig, MetadataSemanticConfig

from ..base import BaseNode, DatabaseMixin
from ..registry import register_node


@register_node
class SchemaAggregationNode(BaseNode, DatabaseMixin):
    """
    Schema Aggregation Node (Rule-based)
    
    DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª…ê³¼ ëŒ€í‘œ í†µê³„ë¥¼ ì§‘ê³„í•˜ì—¬
    LLM ë°°ì¹˜ í˜¸ì¶œì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    """
    
    name = "schema_aggregation"
    description = "ìœ ë‹ˆí¬ ì»¬ëŸ¼/íŒŒì¼ ì§‘ê³„ ë° LLM ë°°ì¹˜ ì¤€ë¹„"
    order = 300
    requires_llm = False
    
    # =========================================================================
    # SQL Queries
    # =========================================================================
    
    AGGREGATE_ALL_SQL = """
    SELECT 
        cm.original_name,
        cm.column_type,
        COUNT(DISTINCT cm.file_id) as frequency,
        
        -- í†µê³„ (JSONì—ì„œ ì¶”ì¶œ)
        AVG((cm.column_info->>'min')::float) as avg_min,
        AVG((cm.column_info->>'max')::float) as avg_max,
        AVG((cm.column_info->>'mean')::float) as avg_mean,
        AVG((cm.column_info->>'unique_count')::float) as avg_unique_count,
        AVG((cm.column_info->>'unique_ratio')::float) as avg_unique_ratio,
        MAX(cm.column_info->>'unit') as sample_unit,
        
        -- ëŒ€í‘œ ê°’ ë¶„í¬ (ì²« ë²ˆì§¸ ìœ íš¨ê°’)
        (SELECT sub.value_distribution
         FROM column_metadata sub 
         WHERE sub.original_name = cm.original_name 
           AND sub.value_distribution IS NOT NULL 
           AND sub.value_distribution != '{}'::jsonb
         LIMIT 1
        ) as sample_distribution

    FROM column_metadata cm
    GROUP BY cm.original_name, cm.column_type
    ORDER BY frequency DESC, cm.original_name;
    """
    
    AGGREGATE_FILES_SQL = """
    SELECT 
        fc.file_id,
        fc.file_name,
        fc.file_extension,
        fc.processor_type,
        fc.file_size_mb,
        fc.file_metadata,
        
        -- ì»¬ëŸ¼ ì •ë³´ ìš”ì•½
        COUNT(cm.col_id) as column_count,
        ARRAY_AGG(DISTINCT cm.original_name) as column_names,
        ARRAY_AGG(DISTINCT cm.column_type) as column_types
        
    FROM file_catalog fc
    LEFT JOIN column_metadata cm ON fc.file_id = cm.file_id
    WHERE fc.semantic_type IS NULL  -- ì•„ì§ ë¶„ì„ ì•ˆ ëœ íŒŒì¼ë§Œ
    GROUP BY fc.file_id, fc.file_name, fc.file_extension, 
             fc.processor_type, fc.file_size_mb, fc.file_metadata
    ORDER BY fc.file_name;
    """
    
    # =========================================================================
    # Main Execution
    # =========================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ìœ ë‹ˆí¬ ì»¬ëŸ¼ê³¼ íŒŒì¼ì„ ì§‘ê³„í•˜ê³  LLM ë°°ì¹˜ë¥¼ ì¤€ë¹„
        
        Args:
            state: AgentState
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ:
            - schema_aggregation_result: ì§‘ê³„ ê²°ê³¼ ìš”ì•½
            - unique_columns: ìœ ë‹ˆí¬ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            - unique_files: ìœ ë‹ˆí¬ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            - column_batches: ì»¬ëŸ¼ LLM ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            - file_batches: íŒŒì¼ LLM ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        print("\n" + "=" * 60)
        print("ðŸ”„ [Schema Aggregation] ìœ ë‹ˆí¬ ì»¬ëŸ¼/íŒŒì¼ ì§‘ê³„")
        print("=" * 60)
        
        # 1. ì§‘ê³„ í†µê³„ ì¡°íšŒ
        stats = self._get_aggregation_stats()
        print(f"\nðŸ“Š Current DB Stats:")
        print(f"   Total files: {stats.get('total_files', 0):,}")
        print(f"   Total columns: {stats.get('total_columns', 0):,}")
        print(f"   Unique columns: {stats.get('unique_columns', 0):,}")
        
        if stats.get('unique_by_type'):
            print(f"   By type: {stats.get('unique_by_type')}")
        
        # 2. ìœ ë‹ˆí¬ ì»¬ëŸ¼ ì§‘ê³„
        print(f"\nðŸ” Aggregating unique columns...")
        unique_columns = self._aggregate_unique_columns()
        print(f"   âœ… Found {len(unique_columns)} unique columns")
        
        # ì»¬ëŸ¼ ë°°ì¹˜ ì¤€ë¹„
        column_batch_size = MetadataSemanticConfig.COLUMN_BATCH_SIZE
        column_batches = self._prepare_batches(unique_columns, column_batch_size)
        print(f"\nðŸ“¦ Column LLM Batches:")
        print(f"   Batch size: {column_batch_size}")
        print(f"   Total batches: {len(column_batches)}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        if unique_columns:
            print(f"\nðŸ“ Sample columns (top 5 by frequency):")
            for col in unique_columns[:5]:
                self._print_column_sample(col)
        
        # 3. íŒŒì¼ ì§‘ê³„
        print(f"\nðŸ” Aggregating files for semantic analysis...")
        unique_files = self._aggregate_unique_files()
        print(f"   âœ… Found {len(unique_files)} files to analyze")
        
        # íŒŒì¼ ë°°ì¹˜ ì¤€ë¹„
        file_batch_size = MetadataSemanticConfig.FILE_BATCH_SIZE
        file_batches = self._prepare_batches(unique_files, file_batch_size)
        print(f"\nðŸ“¦ File LLM Batches:")
        print(f"   Batch size: {file_batch_size}")
        print(f"   Total batches: {len(file_batches)}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        if unique_files:
            print(f"\nðŸ“ Sample files:")
            for f in unique_files[:5]:
                name = f.get('file_name', '?')
                cols = f.get('column_count', 0)
                ptype = f.get('processor_type', '?')
                print(f"   - {name} ({ptype}, {cols} columns)")
        
        # 4. ê²°ê³¼ êµ¬ì„±
        result = {
            "total_columns_in_db": stats.get('total_columns', 0),
            "unique_column_count": len(unique_columns),
            "unique_file_count": len(unique_files),
            "column_batch_size": column_batch_size,
            "file_batch_size": file_batch_size,
            "column_batches": len(column_batches),
            "file_batches": len(file_batches),
            "aggregated_at": datetime.now().isoformat(),
            "stats": stats
        }
        
        print(f"\nâœ… [Schema Aggregation] Complete!")
        print(f"   â†’ {len(unique_columns)} unique columns â†’ {len(column_batches)} batches")
        print(f"   â†’ {len(unique_files)} files â†’ {len(file_batches)} batches")
        print(f"   â†’ Ready for LLM analysis!")
        print("=" * 60 + "\n")
        
        return {
            "schema_aggregation_result": result,
            "unique_columns": unique_columns,
            "unique_files": unique_files,
            "column_batches": column_batches,
            "file_batches": file_batches
        }
    
    # =========================================================================
    # Aggregation Methods
    # =========================================================================
    
    def _aggregate_unique_columns(self) -> List[Dict[str, Any]]:
        """DBì—ì„œ ìœ ë‹ˆí¬ ì»¬ëŸ¼ëª…ê³¼ ëŒ€í‘œ í†µê³„ ì¶”ì¶œ"""
        db = get_db_manager()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(self.AGGREGATE_ALL_SQL)
            rows = cursor.fetchall()
            
            col_names = [desc[0] for desc in cursor.description]
            
            unique_columns = []
            for row in rows:
                row_dict = dict(zip(col_names, row))
                column_info = self._build_column_info(row_dict)
                unique_columns.append(column_info)
            
            return unique_columns
            
        except Exception as e:
            print(f"[Schema Aggregation] Error aggregating columns: {e}")
            conn.rollback()
            raise
        finally:
            cursor.close()
    
    def _aggregate_unique_files(self) -> List[Dict[str, Any]]:
        """íŒŒì¼ ì •ë³´ ì§‘ê³„"""
        db = get_db_manager()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(self.AGGREGATE_FILES_SQL)
            rows = cursor.fetchall()
            
            col_names = [desc[0] for desc in cursor.description]
            
            files = []
            for row in rows:
                row_dict = dict(zip(col_names, row))
                file_info = self._build_file_info(row_dict)
                files.append(file_info)
            
            return files
            
        except Exception as e:
            print(f"[Schema Aggregation] Error aggregating files: {e}")
            conn.rollback()
            raise
        finally:
            cursor.close()
    
    def _get_aggregation_stats(self) -> Dict[str, Any]:
        """ì§‘ê³„ í†µê³„ ì¡°íšŒ"""
        db = get_db_manager()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        stats = {}
        
        try:
            cursor.execute("SELECT COUNT(*) FROM column_metadata")
            stats["total_columns"] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT original_name) FROM column_metadata")
            stats["unique_columns"] = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT column_type, COUNT(DISTINCT original_name) 
                FROM column_metadata 
                GROUP BY column_type
            """)
            stats["unique_by_type"] = dict(cursor.fetchall())
            
            cursor.execute("SELECT COUNT(*) FROM file_catalog")
            stats["total_files"] = cursor.fetchone()[0]
            
        except Exception as e:
            print(f"[Schema Aggregation] Error getting stats: {e}")
            stats["error"] = str(e)
        
        return stats
    
    # =========================================================================
    # Helper Methods: Build Info Dicts
    # =========================================================================
    
    def _build_column_info(self, row_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        column_info = {
            "original_name": row_dict["original_name"],
            "column_type": row_dict["column_type"] or "unknown",
            "frequency": row_dict["frequency"] or 0,
        }
        
        # ìˆ˜ì¹˜í˜• í†µê³„
        if row_dict.get("avg_min") is not None:
            column_info["avg_min"] = round(row_dict["avg_min"], 2)
        if row_dict.get("avg_max") is not None:
            column_info["avg_max"] = round(row_dict["avg_max"], 2)
        if row_dict.get("avg_mean") is not None:
            column_info["avg_mean"] = round(row_dict["avg_mean"], 2)
        
        # ë²”ì£¼í˜• í†µê³„
        if row_dict.get("avg_unique_count") is not None:
            column_info["avg_unique_count"] = round(row_dict["avg_unique_count"], 1)
        if row_dict.get("avg_unique_ratio") is not None:
            column_info["avg_unique_ratio"] = round(row_dict["avg_unique_ratio"], 3)
        
        # ë‹¨ìœ„
        if row_dict.get("sample_unit"):
            column_info["sample_unit"] = row_dict["sample_unit"]
        
        # ëŒ€í‘œ ê°’ ë¶„í¬
        sample_dist = row_dict.get("sample_distribution")
        if sample_dist and isinstance(sample_dist, dict):
            max_samples = SchemaAggregationConfig.MAX_SAMPLE_VALUES
            top_values = dict(list(sample_dist.items())[:max_samples])
            if top_values:
                column_info["sample_values"] = top_values
        
        return column_info
    
    def _build_file_info(self, row_dict: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        file_info = {
            "file_id": str(row_dict["file_id"]),
            "file_name": row_dict["file_name"],
            "file_extension": row_dict["file_extension"],
            "processor_type": row_dict["processor_type"],
            "file_size_mb": float(row_dict["file_size_mb"]) if row_dict["file_size_mb"] else 0,
            "column_count": row_dict["column_count"] or 0,
            "column_names": row_dict["column_names"][:20] if row_dict["column_names"] else [],
            "column_types": list(set(row_dict["column_types"])) if row_dict["column_types"] else []
        }
        
        # file_metadataì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
        metadata = row_dict.get("file_metadata", {}) or {}
        if metadata:
            file_info["row_count"] = metadata.get("row_count")
            file_info["duration_seconds"] = metadata.get("duration_seconds")
        
        return file_info
    
    # =========================================================================
    # Helper Methods: Batching & Utils
    # =========================================================================
    
    def _prepare_batches(
        self,
        items: List[Dict[str, Any]],
        batch_size: int
    ) -> List[List[Dict[str, Any]]]:
        """ì•„ì´í…œì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batches.append(batch)
        return batches
    
    def _print_column_sample(self, col: Dict[str, Any]):
        """ì»¬ëŸ¼ ìƒ˜í”Œ ì¶œë ¥"""
        freq = col.get('frequency', 0)
        col_type = col.get('column_type', 'unknown')
        name = col.get('original_name', '?')
        
        stat_str = ""
        if col.get('avg_min') is not None:
            stat_str = f"range: [{col.get('avg_min'):.1f}, {col.get('avg_max'):.1f}]"
        elif col.get('sample_values'):
            values = list(col['sample_values'].keys())[:3]
            stat_str = f"values: {values}"
        
        print(f"   - {name} ({col_type}, freq={freq}) {stat_str}")
    
    # =========================================================================
    # Convenience Methods (Standalone Execution)
    # =========================================================================
    
    @classmethod
    def run_standalone(cls, verbose: bool = True) -> Dict[str, Any]:
        """
        ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
        
        Returns:
            Dict with unique_columns, batches, and stats
        """
        node = cls()
        
        if verbose:
            print("\n" + "=" * 60)
            print("ðŸ”„ Running Schema Aggregation...")
            print("=" * 60)
        
        # ì»¬ëŸ¼ ì§‘ê³„
        unique_columns = node._aggregate_unique_columns()
        column_batches = node._prepare_batches(unique_columns, MetadataSemanticConfig.COLUMN_BATCH_SIZE)
        
        # íŒŒì¼ ì§‘ê³„
        unique_files = node._aggregate_unique_files()
        file_batches = node._prepare_batches(unique_files, MetadataSemanticConfig.FILE_BATCH_SIZE)
        
        # í†µê³„
        stats = node._get_aggregation_stats()
        
        result = {
            "unique_columns": unique_columns,
            "column_batches": column_batches,
            "unique_files": unique_files,
            "file_batches": file_batches,
            "stats": stats,
            "unique_column_count": len(unique_columns),
            "unique_file_count": len(unique_files),
            "column_batch_count": len(column_batches),
            "file_batch_count": len(file_batches)
        }
        
        if verbose:
            print(f"\nâœ… Aggregation Complete:")
            print(f"   Unique columns: {len(unique_columns)} â†’ {len(column_batches)} batches")
            print(f"   Unique files: {len(unique_files)} â†’ {len(file_batches)} batches")
        
        return result
    
    @classmethod
    def get_stats(cls) -> Dict[str, Any]:
        """ì§‘ê³„ í†µê³„ ì¡°íšŒ"""
        node = cls()
        return node._get_aggregation_stats()
