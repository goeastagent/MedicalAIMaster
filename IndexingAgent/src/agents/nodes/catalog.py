# src/agents/nodes/catalog.py
"""
File Catalog Node

íŒŒì¼ì„ ìˆœíšŒí•˜ë©° Processorë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  DBì— ì €ìž¥í•©ë‹ˆë‹¤.
LLM í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì €ìž¥ í…Œì´ë¸”:
- file_catalog: íŒŒì¼ ë‹¨ìœ„ ê±°ì‹œì  ì •ë³´
- column_metadata: ì»¬ëŸ¼ ë‹¨ìœ„ ë¯¸ì‹œì  ì •ë³´
"""

import os
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.agents.nodes.common import processors
from src.database import (
    get_db_manager,
    CatalogSchemaManager,
    get_directory_by_path,
)

from ..base import BaseNode, DatabaseMixin
from ..registry import register_node


# í…ìŠ¤íŠ¸ë¡œ ì½ì„ ìˆ˜ ìžˆëŠ” íŒŒì¼ í™•ìž¥ìž
TEXT_READABLE_EXTENSIONS = {'csv', 'tsv', 'txt', 'json', 'xml', 'xlsx', 'xls'}


@register_node
class FileCatalogNode(BaseNode, DatabaseMixin):
    """
    File Catalog Node (Rule-based)
    
    íŒŒì¼ì„ ìˆœíšŒí•˜ë©° Processorë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  DBì— ì €ìž¥í•©ë‹ˆë‹¤.
    LLM í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    name = "file_catalog"
    description = "íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° DB ì €ìž¥"
    order = 200
    requires_llm = False
    
    # =========================================================================
    # Main Execution
    # =========================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ìž¥
        
        Args:
            state: AgentState (input_files í•„ë“œ í•„ìš”)
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (file_catalog_result, catalog_file_ids, logs)
        """
        self.log("=" * 80)
        self.log("ðŸ“¦ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œìž‘")
        self.log("=" * 80)
        
        input_files = state.get("input_files", [])
        
        if not input_files:
            return {
                "logs": ["âŒ [File Catalog] Error: ìž…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."],
                "file_catalog_result": self._empty_result(),
                "catalog_file_ids": [],
                "error_message": "No input files provided"
            }
        
        self.log(f"ðŸ“‚ ì²˜ë¦¬í•  íŒŒì¼: {len(input_files)}ê°œ", indent=1)
        
        # íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰
        result = self._process_files(
            file_paths=input_files,
            skip_unchanged=True,
            verbose=True
        )
        
        # ëª¨ë“  íŒŒì¼ì˜ file_id (ì²˜ë¦¬ + ìŠ¤í‚µ í¬í•¨)
        file_ids = result.get("file_ids", [])
        
        # ë¡œê·¸ ìƒì„±
        logs = [
            f"ðŸ“¦ [File Catalog] ì™„ë£Œ: {result['processed_files']}ê°œ ì²˜ë¦¬, {result['skipped_files']}ê°œ ìŠ¤í‚µ"
        ]
        
        if file_ids:
            short_ids = [fid[:8] for fid in file_ids]
            logs.append(f"   ðŸ“‹ File IDs: {short_ids}")
        
        if result["failed_files"] > 0:
            logs.append(f"   âš ï¸ ì‹¤íŒ¨: {result['failed_files']}ê°œ")
            for r in result["results"]:
                if not r["success"]:
                    logs.append(f"      - {os.path.basename(r['file_path'])}: {r['error']}")
        
        self.log(f"âœ… ì™„ë£Œ: {result['processed_files']}ê°œ ì²˜ë¦¬, {result['skipped_files']}ê°œ ìŠ¤í‚µ, {result['failed_files']}ê°œ ì‹¤íŒ¨")
        if file_ids:
            short_ids = [fid[:8] for fid in file_ids]
            self.log(f"ðŸ“‹ File IDs: {short_ids}", indent=1)
        
        return {
            "logs": logs,
            "file_catalog_result": result,
            "catalog_file_ids": file_ids
        }
    
    # =========================================================================
    # File Processing Methods
    # =========================================================================
    
    def _process_files(
        self,
        file_paths: List[str],
        skip_unchanged: bool = True,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            skip_unchanged: Trueë©´ ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
            verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self._ensure_schema()
        
        total_files = len(file_paths)
        processed_files = 0
        skipped_files = 0
        failed_files = 0
        results = []
        file_ids = []
        
        for i, file_path in enumerate(file_paths):
            if verbose and (i + 1) % 100 == 0:
                self.log(f"Processing {i + 1}/{total_files}...")
            
            file_result = self._process_single_file(file_path, skip_unchanged, verbose)
            results.append(file_result)
            
            if file_result.get("file_id"):
                file_ids.append(file_result["file_id"])
            
            if file_result["success"]:
                if file_result.get("skipped"):
                    skipped_files += 1
                else:
                    processed_files += 1
            else:
                failed_files += 1
        
        if verbose:
            self.log(f"Complete: {processed_files} processed, "
                  f"{skipped_files} skipped, {failed_files} failed")
        
        success_rate = f"{(processed_files + skipped_files) / total_files * 100:.1f}%" if total_files > 0 else "0%"
        
        return {
            "total_files": total_files,
            "processed_files": processed_files,
            "skipped_files": skipped_files,
            "failed_files": failed_files,
            "success_rate": success_rate,
            "file_ids": file_ids,
            "results": results
        }
    
    def _process_single_file(
        self,
        file_path: str,
        skip_unchanged: bool = True,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            skip_unchanged: Trueë©´ ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
            verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
        
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        file_path = os.path.abspath(file_path)
        filename = os.path.basename(file_path)
        
        file_modified_at = self._get_file_modified_time(file_path)
        
        # ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
        if skip_unchanged and file_modified_at:
            existing_id = self._file_unchanged_in_catalog(file_path, file_modified_at)
            if existing_id:
                short_id = existing_id[:8]
                if verbose:
                    self.log(f"â­ï¸ [{short_id}] {filename} (skipped: unchanged)", indent=1)
                return {
                    "file_path": file_path,
                    "success": True,
                    "file_id": existing_id,
                    "column_count": 0,
                    "error": None,
                    "skipped": True
                }
        
        # Processor ì„ íƒ
        processor = self._get_processor(file_path)
        if not processor:
            if verbose:
                self.log(f"âŒ [--------] {filename} (no processor)", indent=1)
            return {
                "file_path": file_path,
                "success": False,
                "file_id": None,
                "column_count": 0,
                "error": f"No processor available for file: {file_path}",
                "skipped": False
            }
        
        db = get_db_manager()
        
        try:
            # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = processor.extract_metadata(file_path)
            
            if "error" in metadata:
                if verbose:
                    self.log(f"âŒ [--------] {filename} ({metadata['error']})", indent=1)
                return {
                    "file_path": file_path,
                    "success": False,
                    "file_id": None,
                    "column_count": 0,
                    "error": metadata["error"],
                    "skipped": False
                }
            
            # 2. file_catalogì— ì €ìž¥
            file_id = self._insert_file_catalog(file_path, metadata)
            
            # 3. column_metadataì— ì €ìž¥
            processor_type = metadata.get("processor_type", "unknown")
            column_details = metadata.get("column_details", [])
            
            if isinstance(column_details, dict):
                column_details = list(column_details.values())
            
            column_count = self._insert_column_metadata(file_id, column_details, processor_type)
            
            # 4. ì»¤ë°‹
            db.commit()
            
            # 5. ê²°ê³¼ ì¶œë ¥
            short_id = file_id[:8]
            if verbose:
                self.log(f"âœ… [{short_id}] {filename} ({column_count} columns)", indent=1)
            
            return {
                "file_path": file_path,
                "success": True,
                "file_id": file_id,
                "column_count": column_count,
                "error": None,
                "skipped": False
            }
            
        except Exception as e:
            db.get_connection().rollback()
            if verbose:
                self.log(f"âŒ [--------] {filename} ({str(e)})", indent=1)
            return {
                "file_path": file_path,
                "success": False,
                "file_id": None,
                "column_count": 0,
                "error": str(e),
                "skipped": False
            }
    
    def _process_directory(
        self,
        directory: str,
        recursive: bool = True,
        skip_unchanged: bool = True,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        
        Args:
            directory: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            recursive: Trueë©´ í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ì²˜ë¦¬
            skip_unchanged: Trueë©´ ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
            verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        file_paths = []
        
        if recursive:
            for root, dirs, files in os.walk(directory):
                for f in files:
                    file_path = os.path.join(root, f)
                    if self._get_processor(file_path):
                        file_paths.append(file_path)
        else:
            for f in os.listdir(directory):
                file_path = os.path.join(directory, f)
                if os.path.isfile(file_path) and self._get_processor(file_path):
                    file_paths.append(file_path)
        
        if verbose:
            self.log(f"Found {len(file_paths)} processable files in {directory}")
        
        return self._process_files(file_paths, skip_unchanged, verbose)
    
    # =========================================================================
    # Helper Methods: Processor & File Info
    # =========================================================================
    
    def _get_processor(self, file_path: str):
        """íŒŒì¼ì— ë§žëŠ” Processor ë°˜í™˜"""
        for processor in processors:
            if processor.can_handle(file_path):
                return processor
        return None
    
    def _is_text_readable(self, file_path: str) -> bool:
        """íŒŒì¼ì´ í…ìŠ¤íŠ¸ë¡œ ì½ì„ ìˆ˜ ìžˆëŠ”ì§€ íŒë‹¨"""
        ext = file_path.lower().split('.')[-1]
        return ext in TEXT_READABLE_EXTENSIONS
    
    def _get_file_modified_time(self, file_path: str) -> Optional[datetime]:
        """íŒŒì¼ì˜ ìµœê·¼ ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
        try:
            mtime = os.path.getmtime(file_path)
            return datetime.fromtimestamp(mtime)
        except:
            return None
    
    # =========================================================================
    # Helper Methods: DB Query
    # =========================================================================
    
    def _file_unchanged_in_catalog(self, file_path: str, modified_time: datetime) -> Optional[str]:
        """íŒŒì¼ì´ ì¹´íƒˆë¡œê·¸ì— ìžˆê³  modified_timeì´ ê°™ì€ì§€ í™•ì¸"""
        db = get_db_manager()
        conn = db.get_connection()
        
        try:
            conn.rollback()
        except:
            pass
        
        cursor = conn.cursor()
        
        try:
            cursor.execute(
                """
                SELECT file_id FROM file_catalog 
                WHERE file_path = %s AND file_modified_at = %s
                """,
                (file_path, modified_time)
            )
            result = cursor.fetchone()
            return str(result[0]) if result else None
        except Exception as e:
            conn.rollback()
            return None
    
    def _get_dir_id_for_file(self, file_path: str) -> Optional[str]:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ê³  dir_id ì¡°íšŒ"""
        dir_path = os.path.dirname(os.path.abspath(file_path))
        dir_info = get_directory_by_path(dir_path)
        return dir_info.get("dir_id") if dir_info else None
    
    # =========================================================================
    # Helper Methods: DB Insert
    # =========================================================================
    
    def _insert_file_catalog(self, file_path: str, metadata: Dict[str, Any]) -> str:
        """file_catalog í…Œì´ë¸”ì— íŒŒì¼ ì •ë³´ ì‚½ìž…"""
        db = get_db_manager()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        processor_type = metadata.get("processor_type", "unknown")
        file_meta = self._extract_file_metadata(metadata, processor_type)
        is_text_readable = self._is_text_readable(file_path)
        file_modified_at = self._get_file_modified_time(file_path)
        dir_id = self._get_dir_id_for_file(file_path)
        
        cursor.execute("""
            INSERT INTO file_catalog (
                file_path, file_name, file_extension, 
                file_size_bytes, file_size_mb, file_modified_at,
                processor_type, is_text_readable, file_metadata, raw_stats,
                dir_id
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
            ON CONFLICT (file_path) DO UPDATE SET
                file_name = EXCLUDED.file_name,
                file_extension = EXCLUDED.file_extension,
                file_size_bytes = EXCLUDED.file_size_bytes,
                file_size_mb = EXCLUDED.file_size_mb,
                file_modified_at = EXCLUDED.file_modified_at,
                processor_type = EXCLUDED.processor_type,
                is_text_readable = EXCLUDED.is_text_readable,
                file_metadata = EXCLUDED.file_metadata,
                raw_stats = EXCLUDED.raw_stats,
                dir_id = EXCLUDED.dir_id
            RETURNING file_id
        """, (
            file_path,
            metadata.get("file_name") or os.path.basename(file_path),
            metadata.get("file_extension") or file_path.split('.')[-1].lower(),
            metadata.get("file_size_bytes"),
            metadata.get("file_size_mb"),
            file_modified_at,
            processor_type,
            is_text_readable,
            json.dumps(file_meta),
            json.dumps(metadata),
            dir_id
        ))
        
        file_id = cursor.fetchone()[0]
        return str(file_id)
    
    def _insert_column_metadata(
        self,
        file_id: str,
        column_details: List[Dict[str, Any]],
        processor_type: str
    ) -> int:
        """column_metadata í…Œì´ë¸”ì— ì»¬ëŸ¼ ì •ë³´ ì‚½ìž…"""
        db = get_db_manager()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        # ê¸°ì¡´ ì»¬ëŸ¼ ì‚­ì œ
        cursor.execute(
            "DELETE FROM column_metadata WHERE file_id = %s",
            (file_id,)
        )
        
        inserted = 0
        
        for col in column_details:
            if isinstance(col, dict):
                col_name = col.get("column_name") or col.get("original_name", "unknown")
                col_type = col.get("column_type", "unknown")
                data_type = col.get("dtype") or col.get("data_type", "")
                
                column_info = self._build_column_info(col, col_type, processor_type)
                value_distribution = self._build_value_distribution(col)
                
                cursor.execute("""
                    INSERT INTO column_metadata (
                        file_id, original_name, column_type, data_type,
                        column_info, value_distribution
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (file_id, original_name) DO UPDATE SET
                        column_type = EXCLUDED.column_type,
                        data_type = EXCLUDED.data_type,
                        column_info = EXCLUDED.column_info,
                        value_distribution = EXCLUDED.value_distribution,
                        updated_at = NOW()
                """, (
                    file_id,
                    col_name,
                    col_type,
                    data_type,
                    json.dumps(column_info),
                    json.dumps(value_distribution)
                ))
                
                inserted += 1
        
        return inserted
    
    # =========================================================================
    # Helper Methods: Metadata Extraction
    # =========================================================================
    
    def _extract_file_metadata(self, metadata: Dict[str, Any], processor_type: str) -> Dict[str, Any]:
        """file_catalog.file_metadataì— ì €ìž¥í•  ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
        file_meta = {}
        
        if processor_type == "tabular":
            file_meta = {
                "row_count": metadata.get("row_count"),
                "column_count": metadata.get("column_count"),
                "quality_summary": metadata.get("quality_summary", {}),
                "column_type_summary": metadata.get("column_type_summary", {}),
                "potential_id_columns": metadata.get("potential_id_columns", []),
                "dtype_distribution": metadata.get("dtype_distribution", {}),
            }
        elif processor_type == "signal":
            file_meta = {
                "duration": metadata.get("duration"),
                "duration_minutes": metadata.get("duration_minutes"),
                "track_count": metadata.get("track_count"),
                "device_count": metadata.get("device_count"),
                "device_names": metadata.get("device_names", []),
                "track_summary": metadata.get("track_summary", {}),
                "sample_rate_summary": metadata.get("sample_rate_summary", {}),
                "recording_info": metadata.get("recording_info", {}),
                "unique_units": metadata.get("unique_units", []),
            }
        
        return file_meta
    
    def _build_column_info(
        self,
        col: Dict[str, Any],
        col_type: str,
        processor_type: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        column_info = {
            "unit": col.get("unit"),
            "sample_rate": col.get("sample_rate"),
            "null_ratio": col.get("null_ratio"),
            "unique_ratio": col.get("unique_ratio"),
            "is_potential_id": col.get("is_potential_id"),
        }
        
        # continuous ì»¬ëŸ¼ í†µê³„
        if col_type == "continuous":
            column_info.update({
                "min": col.get("min"),
                "max": col.get("max"),
                "mean": col.get("mean"),
                "std": col.get("std"),
                "median": col.get("median"),
                "quartiles": col.get("quartiles"),
            })
        
        # Signal ì „ìš© ì •ë³´
        if processor_type == "signal":
            column_info.update({
                "device_name": col.get("device_name"),
                "track_type": col.get("track_type"),
                "display_range": col.get("display_range"),
                "scaling": col.get("scaling"),
                "monitor_type": col.get("monitor_type"),
            })
        
        # Text í†µê³„
        if col.get("text_stats"):
            column_info["text_stats"] = col.get("text_stats")
        
        # Datetime ì •ë³´
        if col.get("is_datetime"):
            column_info.update({
                "is_datetime": True,
                "min_date": col.get("min_date"),
                "max_date": col.get("max_date"),
                "date_range_days": col.get("date_range_days"),
            })
        
        # None ê°’ í•„í„°ë§
        return {k: v for k, v in column_info.items() if v is not None}
    
    def _build_value_distribution(self, col: Dict[str, Any]) -> Dict[str, Any]:
        """ê°’ ë¶„í¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        value_distribution = {}
        if col.get("unique_values"):
            value_distribution["unique_values"] = col.get("unique_values")
        if col.get("value_counts"):
            value_distribution["value_counts"] = col.get("value_counts")
        if col.get("samples"):
            value_distribution["samples"] = col.get("samples")
        return value_distribution
    
    # =========================================================================
    # Helper Methods: Schema & Utils
    # =========================================================================
    
    def _ensure_schema(self):
        """ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ ìƒì„±"""
        db = get_db_manager()
        schema_manager = CatalogSchemaManager(db)
        
        try:
            conn = db.get_connection()
            conn.rollback()
        except:
            pass
        
        if not schema_manager.table_exists('file_catalog'):
            schema_manager.create_tables()
    
    def _empty_result(self) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "total_files": 0,
            "processed_files": 0,
            "skipped_files": 0,
            "failed_files": 0,
            "success_rate": "0%",
            "file_ids": [],
            "results": []
        }
    
    # =========================================================================
    # Convenience Methods (Standalone Execution)
    # =========================================================================
    
    @classmethod
    def run_standalone(
        cls,
        directory: str = None,
        file_paths: List[str] = None,
        recursive: bool = True,
        skip_unchanged: bool = True,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
        
        Args:
            directory: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ (file_pathsê°€ ì—†ì„ ë•Œ)
            file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ )
            recursive: Trueë©´ í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ì²˜ë¦¬
            skip_unchanged: Trueë©´ ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
            verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        node = cls()
        
        if file_paths:
            return node._process_files(file_paths, skip_unchanged, verbose)
        elif directory:
            return node._process_directory(directory, recursive, skip_unchanged, verbose)
        else:
            raise ValueError("Either directory or file_paths must be provided")
    
    @classmethod
    def get_stats(cls) -> dict:
        """ì¹´íƒˆë¡œê·¸ í†µê³„ ì¡°íšŒ"""
        db = get_db_manager()
        schema_manager = CatalogSchemaManager(db)
        return schema_manager.get_stats()
