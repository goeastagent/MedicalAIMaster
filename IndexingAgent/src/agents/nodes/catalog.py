# src/agents/nodes/catalog.py
"""
Phase 2: File Catalog Node

íŒŒì¼ì„ ìˆœíšŒí•˜ë©° Processorë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  DBì— ì €ìž¥í•©ë‹ˆë‹¤.
LLM í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì €ìž¥ í…Œì´ë¸”:
- file_catalog: íŒŒì¼ ë‹¨ìœ„ ê±°ì‹œì  ì •ë³´
- column_metadata: ì»¬ëŸ¼ ë‹¨ìœ„ ë¯¸ì‹œì  ì •ë³´
"""

import os
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.agents.nodes.common import processors
from src.database import (
    get_db_manager,
    CatalogSchemaManager,
    get_directory_by_path,
)




# í…ìŠ¤íŠ¸ë¡œ ì½ì„ ìˆ˜ ìžˆëŠ” íŒŒì¼ í™•ìž¥ìž
TEXT_READABLE_EXTENSIONS = {'csv', 'tsv', 'txt', 'json', 'xml', 'xlsx', 'xls'}


# =============================================================================
# í—¬í¼ í•¨ìˆ˜
# =============================================================================

def _get_processor(file_path: str):
    """íŒŒì¼ì— ë§žëŠ” Processor ë°˜í™˜"""
    for processor in processors:
        if processor.can_handle(file_path):
            return processor
    return None


def _is_text_readable(file_path: str) -> bool:
    """íŒŒì¼ì´ í…ìŠ¤íŠ¸ë¡œ ì½ì„ ìˆ˜ ìžˆëŠ”ì§€ íŒë‹¨"""
    ext = file_path.lower().split('.')[-1]
    return ext in TEXT_READABLE_EXTENSIONS


def _get_file_modified_time(file_path: str) -> Optional[datetime]:
    """íŒŒì¼ì˜ ìµœê·¼ ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
    try:
        mtime = os.path.getmtime(file_path)
        return datetime.fromtimestamp(mtime)
    except:
        return None


def _file_exists_in_catalog(file_path: str) -> Optional[str]:
    """
    íŒŒì¼ì´ ì´ë¯¸ ì¹´íƒˆë¡œê·¸ì— ìžˆëŠ”ì§€ í™•ì¸
    
    Returns:
        file_id (UUID string) if exists, None otherwise
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    cursor.execute(
        "SELECT file_id FROM file_catalog WHERE file_path = %s",
        (file_path,)
    )
    result = cursor.fetchone()
    return str(result[0]) if result else None


def _file_unchanged_in_catalog(file_path: str, modified_time: datetime) -> Optional[str]:
    """
    íŒŒì¼ì´ ì¹´íƒˆë¡œê·¸ì— ìžˆê³  modified_timeì´ ê°™ì€ì§€ í™•ì¸
    
    Returns:
        file_id (UUID string) if unchanged, None otherwise
    """
    db = get_db_manager()
    conn = db.get_connection()
    
    # ì´ì „ íŠ¸ëžœìž­ì…˜ ì˜¤ë¥˜ ì •ë¦¬
    try:
        conn.rollback()
    except:
        pass
    
    cursor = conn.cursor()
    
    try:
        cursor.execute(
            """
            SELECT file_id FROM file_catalog 
            WHERE file_path = %s AND file_modified_at = %s
            """,
            (file_path, modified_time)
        )
        result = cursor.fetchone()
        return str(result[0]) if result else None
    except Exception as e:
        conn.rollback()
        return None


def _extract_file_metadata(metadata: Dict[str, Any], processor_type: str) -> Dict[str, Any]:
    """
    file_catalog.file_metadataì— ì €ìž¥í•  ìš”ì•½ ì •ë³´ ì¶”ì¶œ
    
    ì›ë³¸ ì „ì²´ëŠ” raw_statsì— ì €ìž¥í•˜ê³ , í•µì‹¬ ì •ë³´ë§Œ file_metadataì— ì €ìž¥
    """
    file_meta = {}
    
    if processor_type == "tabular":
        file_meta = {
            "row_count": metadata.get("row_count"),
            "column_count": metadata.get("column_count"),
            "quality_summary": metadata.get("quality_summary", {}),
            "column_type_summary": metadata.get("column_type_summary", {}),
            "potential_id_columns": metadata.get("potential_id_columns", []),
            "dtype_distribution": metadata.get("dtype_distribution", {}),
        }
    elif processor_type == "signal":
        file_meta = {
            "duration": metadata.get("duration"),
            "duration_minutes": metadata.get("duration_minutes"),
            "track_count": metadata.get("track_count"),
            "device_count": metadata.get("device_count"),
            "device_names": metadata.get("device_names", []),
            "track_summary": metadata.get("track_summary", {}),
            "sample_rate_summary": metadata.get("sample_rate_summary", {}),
            "recording_info": metadata.get("recording_info", {}),
            "unique_units": metadata.get("unique_units", []),
        }
    
    return file_meta


# =============================================================================
# DB ì €ìž¥ í•¨ìˆ˜
# =============================================================================

def _get_dir_id_for_file(file_path: str) -> Optional[str]:
    """
    íŒŒì¼ ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ê³  dir_id ì¡°íšŒ
    
    Returns:
        dir_id (UUID string) if found, None otherwise
    """
    dir_path = os.path.dirname(os.path.abspath(file_path))
    dir_info = get_directory_by_path(dir_path)
    return dir_info.get("dir_id") if dir_info else None


def _insert_file_catalog(file_path: str, metadata: Dict[str, Any]) -> str:
    """
    file_catalog í…Œì´ë¸”ì— íŒŒì¼ ì •ë³´ ì‚½ìž…
    
    Returns:
        file_id (UUID string)
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    processor_type = metadata.get("processor_type", "unknown")
    file_meta = _extract_file_metadata(metadata, processor_type)
    is_text_readable = _is_text_readable(file_path)
    file_modified_at = _get_file_modified_time(file_path)
    
    # Phase 1ì—ì„œ ìƒì„±ëœ dir_id ì¡°íšŒ
    dir_id = _get_dir_id_for_file(file_path)
    
    cursor.execute("""
        INSERT INTO file_catalog (
            file_path, file_name, file_extension, 
            file_size_bytes, file_size_mb, file_modified_at,
            processor_type, is_text_readable, file_metadata, raw_stats,
            dir_id
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
        ON CONFLICT (file_path) DO UPDATE SET
            file_name = EXCLUDED.file_name,
            file_extension = EXCLUDED.file_extension,
            file_size_bytes = EXCLUDED.file_size_bytes,
            file_size_mb = EXCLUDED.file_size_mb,
            file_modified_at = EXCLUDED.file_modified_at,
            processor_type = EXCLUDED.processor_type,
            is_text_readable = EXCLUDED.is_text_readable,
            file_metadata = EXCLUDED.file_metadata,
            raw_stats = EXCLUDED.raw_stats,
            dir_id = EXCLUDED.dir_id
        RETURNING file_id
    """, (
        file_path,
        metadata.get("file_name") or os.path.basename(file_path),
        metadata.get("file_extension") or file_path.split('.')[-1].lower(),
        metadata.get("file_size_bytes"),
        metadata.get("file_size_mb"),
        file_modified_at,
        processor_type,
        is_text_readable,
        json.dumps(file_meta),
        json.dumps(metadata),  # ì›ë³¸ ì „ì²´ ë°±ì—…
        dir_id  # Phase 1ì—ì„œ ìƒì„±ëœ dir_id
    ))
    
    file_id = cursor.fetchone()[0]
    return str(file_id)  # UUIDë¥¼ ë¬¸ìžì—´ë¡œ ë°˜í™˜


def _insert_column_metadata(
    file_id: str, 
    column_details: List[Dict[str, Any]],
    processor_type: str
) -> int:
    """
    column_metadata í…Œì´ë¸”ì— ì»¬ëŸ¼ ì •ë³´ ì‚½ìž…
    
    Returns:
        ì‚½ìž…ëœ ì»¬ëŸ¼ ìˆ˜
    """
    db = get_db_manager()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    # ê¸°ì¡´ ì»¬ëŸ¼ ì‚­ì œ (ì—…ë°ì´íŠ¸ ì‹œ)
    cursor.execute(
        "DELETE FROM column_metadata WHERE file_id = %s",
        (file_id,)
    )
    
    inserted = 0
    
    for col in column_details:
        # Tabular: column_detailsê°€ list
        # Signal: column_detailsê°€ dict (track_name -> info)
        if isinstance(col, dict):
            col_name = col.get("column_name") or col.get("original_name", "unknown")
            col_type = col.get("column_type", "unknown")
            data_type = col.get("dtype") or col.get("data_type", "")
            
            # column_info: í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
            column_info = {
                "unit": col.get("unit"),
                "sample_rate": col.get("sample_rate"),
                "null_ratio": col.get("null_ratio"),
                "unique_ratio": col.get("unique_ratio"),
                "is_potential_id": col.get("is_potential_id"),
            }
            
            # í†µê³„ ì •ë³´ ì¶”ê°€ (continuous)
            if col_type == "continuous":
                column_info.update({
                    "min": col.get("min"),
                    "max": col.get("max"),
                    "mean": col.get("mean"),
                    "std": col.get("std"),
                    "median": col.get("median"),
                    "quartiles": col.get("quartiles"),
                })
            
            # Signal ì „ìš© ì •ë³´
            if processor_type == "signal":
                column_info.update({
                    "device_name": col.get("device_name"),
                    "track_type": col.get("track_type"),
                    "display_range": col.get("display_range"),
                    "scaling": col.get("scaling"),
                    "monitor_type": col.get("monitor_type"),
                })
            
            # Text í†µê³„ (ìžˆëŠ” ê²½ìš°)
            if col.get("text_stats"):
                column_info["text_stats"] = col.get("text_stats")
            
            # Datetime ì •ë³´ (ìžˆëŠ” ê²½ìš°)
            if col.get("is_datetime"):
                column_info.update({
                    "is_datetime": True,
                    "min_date": col.get("min_date"),
                    "max_date": col.get("max_date"),
                    "date_range_days": col.get("date_range_days"),
                })
            
            # value_distribution: unique values, value_counts
            value_distribution = {}
            if col.get("unique_values"):
                value_distribution["unique_values"] = col.get("unique_values")
            if col.get("value_counts"):
                value_distribution["value_counts"] = col.get("value_counts")
            if col.get("samples"):
                value_distribution["samples"] = col.get("samples")
            
            # None ê°’ í•„í„°ë§
            column_info = {k: v for k, v in column_info.items() if v is not None}
            
            cursor.execute("""
                INSERT INTO column_metadata (
                    file_id, original_name, column_type, data_type,
                    column_info, value_distribution
                ) VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (file_id, original_name) DO UPDATE SET
                    column_type = EXCLUDED.column_type,
                    data_type = EXCLUDED.data_type,
                    column_info = EXCLUDED.column_info,
                    value_distribution = EXCLUDED.value_distribution,
                    updated_at = NOW()
            """, (
                file_id,
                col_name,
                col_type,
                data_type,
                json.dumps(column_info),
                json.dumps(value_distribution)
            ))
            
            inserted += 1
    
    return inserted


# =============================================================================
# ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ í•¨ìˆ˜
# =============================================================================

def ensure_schema():
    """ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ ìƒì„±"""
    db = get_db_manager()
    schema_manager = CatalogSchemaManager(db)
    
    # ì´ì „ íŠ¸ëžœìž­ì…˜ ì˜¤ë¥˜ ìƒíƒœ ì •ë¦¬
    try:
        conn = db.get_connection()
        conn.rollback()
    except:
        pass
    
    if not schema_manager.table_exists('file_catalog'):
        schema_manager.create_tables()


# =============================================================================
# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
# =============================================================================

def process_single_file(file_path: str, skip_unchanged: bool = True, verbose: bool = True) -> Dict[str, Any]:
    """
    ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    
    Args:
        file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
        skip_unchanged: Trueë©´ file_path + modified_timeì´ ê°™ì€ íŒŒì¼ ìŠ¤í‚µ
        verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
    
    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {file_path, success, file_id, column_count, error, skipped}
    """
    # íŒŒì¼ ê²½ë¡œ ì •ê·œí™” (ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
    file_path = os.path.abspath(file_path)
    filename = os.path.basename(file_path)
    
    # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
    file_modified_at = _get_file_modified_time(file_path)
    
    # ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ (file_path + modified_time ê¸°ì¤€)
    if skip_unchanged and file_modified_at:
        existing_id = _file_unchanged_in_catalog(file_path, file_modified_at)
        if existing_id:
            short_id = existing_id[:8]  # UUID ì•ž 8ìžë¦¬ë§Œ í‘œì‹œ
            if verbose:
                print(f"   â­ï¸ [{short_id}] {filename} (skipped: unchanged)")
            return {
                "file_path": file_path,
                "success": True,
                "file_id": existing_id,
                "column_count": 0,
                "error": None,
                "skipped": True
            }
    
    # Processor ì„ íƒ
    processor = _get_processor(file_path)
    if not processor:
        if verbose:
            print(f"   âŒ [--------] {filename} (no processor)")
        return {
            "file_path": file_path,
            "success": False,
            "file_id": None,
            "column_count": 0,
            "error": f"No processor available for file: {file_path}",
            "skipped": False
        }
    
    db = get_db_manager()
    
    try:
        # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = processor.extract_metadata(file_path)
        
        if "error" in metadata:
            if verbose:
                print(f"   âŒ [--------] {filename} ({metadata['error']})")
            return {
                "file_path": file_path,
                "success": False,
                "file_id": None,
                "column_count": 0,
                "error": metadata["error"],
                "skipped": False
            }
        
        # 2. file_catalogì— ì €ìž¥
        file_id = _insert_file_catalog(file_path, metadata)
        
        # 3. column_metadataì— ì €ìž¥
        processor_type = metadata.get("processor_type", "unknown")
        column_details = metadata.get("column_details", [])
        
        # Signalì˜ ê²½ìš° dictë¥¼ listë¡œ ë³€í™˜
        if isinstance(column_details, dict):
            column_details = list(column_details.values())
        
        column_count = _insert_column_metadata(file_id, column_details, processor_type)
        
        # 4. ì»¤ë°‹
        db.commit()
        
        # 5. file_id ì¶œë ¥ (UUID ì•ž 8ìžë¦¬)
        short_id = file_id[:8]
        if verbose:
            print(f"   âœ… [{short_id}] {filename} ({column_count} columns)")
        
        return {
            "file_path": file_path,
            "success": True,
            "file_id": file_id,
            "column_count": column_count,
            "error": None,
            "skipped": False
        }
        
    except Exception as e:
        db.get_connection().rollback()
        if verbose:
            print(f"   âŒ [--------] {filename} ({str(e)})")
        return {
            "file_path": file_path,
            "success": False,
            "file_id": None,
            "column_count": 0,
            "error": str(e),
            "skipped": False
        }


def process_files(
    file_paths: List[str], 
    skip_unchanged: bool = True,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
    
    Args:
        file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        skip_unchanged: Trueë©´ file_path + modified_timeì´ ê°™ì€ íŒŒì¼ ìŠ¤í‚µ
        verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
    
    Returns:
        Phase 2 ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (file_ids í¬í•¨)
    """
    ensure_schema()
    
    total_files = len(file_paths)
    processed_files = 0
    skipped_files = 0
    failed_files = 0
    results = []
    file_ids = []  # ëª¨ë“  íŒŒì¼ì˜ file_id ìˆ˜ì§‘
    
    for i, file_path in enumerate(file_paths):
        if verbose and (i + 1) % 100 == 0:
            print(f"[Phase 2] Processing {i + 1}/{total_files}...")
        
        file_result = process_single_file(file_path, skip_unchanged, verbose)
        results.append(file_result)
        
        # file_id ìˆ˜ì§‘ (ì„±ê³µí•œ íŒŒì¼ë§Œ)
        if file_result.get("file_id"):
            file_ids.append(file_result["file_id"])
        
        if file_result["success"]:
            if file_result.get("skipped"):
                skipped_files += 1
            else:
                processed_files += 1
        else:
            failed_files += 1
    
    if verbose:
        print(f"[Phase 2] Complete: {processed_files} processed, "
              f"{skipped_files} skipped, {failed_files} failed")
    
    success_rate = f"{(processed_files + skipped_files) / total_files * 100:.1f}%" if total_files > 0 else "0%"
    
    return {
        "total_files": total_files,
        "processed_files": processed_files,
        "skipped_files": skipped_files,
        "failed_files": failed_files,
        "success_rate": success_rate,
        "file_ids": file_ids,  # ëª¨ë“  íŒŒì¼ì˜ file_id
        "results": results
    }


def process_directory(
    directory: str, 
    recursive: bool = True,
    skip_unchanged: bool = True,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    
    Args:
        directory: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        recursive: Trueë©´ í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ì²˜ë¦¬
        skip_unchanged: Trueë©´ file_path + modified_timeì´ ê°™ì€ íŒŒì¼ ìŠ¤í‚µ
        verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
    
    Returns:
        Phase 2 ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    file_paths = []
    
    if recursive:
        for root, dirs, files in os.walk(directory):
            for f in files:
                file_path = os.path.join(root, f)
                if _get_processor(file_path):
                    file_paths.append(file_path)
    else:
        for f in os.listdir(directory):
            file_path = os.path.join(directory, f)
            if os.path.isfile(file_path) and _get_processor(file_path):
                file_paths.append(file_path)
    
    if verbose:
        print(f"[Phase 2] Found {len(file_paths)} processable files in {directory}")
    
    return process_files(file_paths, skip_unchanged, verbose)


# =============================================================================
# íŽ¸ì˜ í•¨ìˆ˜
# =============================================================================

def run_phase2(
    directory: str = None,
    file_paths: List[str] = None,
    recursive: bool = True,
    skip_unchanged: bool = True,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Phase 2 ì‹¤í–‰ - íŽ¸ì˜ í•¨ìˆ˜
    
    Args:
        directory: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ (file_pathsê°€ ì—†ì„ ë•Œ)
        file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ )
        recursive: Trueë©´ í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ì²˜ë¦¬
        skip_unchanged: Trueë©´ file_path + modified_timeì´ ê°™ì€ íŒŒì¼ ìŠ¤í‚µ
        verbose: Trueë©´ ì§„í–‰ ìƒí™© ì¶œë ¥
    
    Returns:
        Phase 2 ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (file_ids í¬í•¨)
    """
    if file_paths:
        return process_files(file_paths, skip_unchanged, verbose)
    elif directory:
        return process_directory(directory, recursive, skip_unchanged, verbose)
    else:
        raise ValueError("Either directory or file_paths must be provided")


def get_catalog_stats() -> dict:
    """ì¹´íƒˆë¡œê·¸ í†µê³„ ì¡°íšŒ"""
    db = get_db_manager()
    schema_manager = CatalogSchemaManager(db)
    return schema_manager.get_stats()


# =============================================================================
# LangGraph Node Function
# =============================================================================

def phase2_file_catalog_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2] File Catalog ë…¸ë“œ - LangGraphìš©
    
    ëª¨ë“  ìž…ë ¥ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ìž¥í•©ë‹ˆë‹¤.
    LLM í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    - file_path + modified_timeì´ ê°™ìœ¼ë©´ ìŠ¤í‚µ
    - ëª¨ë“  íŒŒì¼ì˜ file_idë¥¼ stateì— ë°˜í™˜
    
    Args:
        state: AgentState (input_files í•„ë“œ í•„ìš”)
    
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (phase2_result, phase2_file_ids, logs)
    """
    print("\n" + "="*80)
    print("ðŸ“¦ [PHASE 2] File Catalog - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œìž‘")
    print("="*80)
    
    input_files = state.get("input_files", [])
    
    if not input_files:
        return {
            "logs": ["âŒ [Phase 2] Error: ìž…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."],
            "phase2_result": {
                "total_files": 0,
                "processed_files": 0,
                "skipped_files": 0,
                "failed_files": 0,
                "success_rate": "0%",
                "file_ids": [],
                "results": []
            },
            "phase2_file_ids": [],
            "error_message": "No input files provided"
        }
    
    print(f"   ðŸ“‚ ì²˜ë¦¬í•  íŒŒì¼: {len(input_files)}ê°œ\n")
    
    # Phase 2 ì‹¤í–‰ (ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ì€ ìŠ¤í‚µ)
    result = process_files(
        file_paths=input_files,
        skip_unchanged=True,  # file_path + modified_timeì´ ê°™ìœ¼ë©´ ìŠ¤í‚µ
        verbose=True
    )
    
    # ëª¨ë“  íŒŒì¼ì˜ file_id (ì²˜ë¦¬ + ìŠ¤í‚µ í¬í•¨)
    file_ids = result.get("file_ids", [])
    
    # ë¡œê·¸ ìƒì„±
    logs = [
        f"ðŸ“¦ [Phase 2] ì™„ë£Œ: {result['processed_files']}ê°œ ì²˜ë¦¬, {result['skipped_files']}ê°œ ìŠ¤í‚µ"
    ]
    
    if file_ids:
        # UUID ì•ž 8ìžë¦¬ë§Œ í‘œì‹œ
        short_ids = [fid[:8] for fid in file_ids]
        logs.append(f"   ðŸ“‹ File IDs: {short_ids}")
    
    if result["failed_files"] > 0:
        logs.append(f"   âš ï¸ ì‹¤íŒ¨: {result['failed_files']}ê°œ")
        for r in result["results"]:
            if not r["success"]:
                logs.append(f"      - {os.path.basename(r['file_path'])}: {r['error']}")
    
    print(f"\nâœ… [Phase 2] ì™„ë£Œ: {result['processed_files']}ê°œ ì²˜ë¦¬, {result['skipped_files']}ê°œ ìŠ¤í‚µ, {result['failed_files']}ê°œ ì‹¤íŒ¨")
    if file_ids:
        short_ids = [fid[:8] for fid in file_ids]
        print(f"   ðŸ“‹ File IDs: {short_ids}")
    
    return {
        "logs": logs,
        "phase2_result": result,
        "phase2_file_ids": file_ids  # ëª¨ë“  íŒŒì¼ì˜ file_idë¥¼ stateì— ì €ìž¥
    }


# =============================================================================
# Class-based Node (for NodeRegistry)
# =============================================================================

from ..base import BaseNode, DatabaseMixin
from ..registry import register_node


@register_node
class FileCatalogNode(BaseNode, DatabaseMixin):
    """
    File Catalog Node (Rule-based)
    
    íŒŒì¼ì„ ìˆœíšŒí•˜ë©° Processorë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  DBì— ì €ìž¥í•©ë‹ˆë‹¤.
    LLM í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    name = "file_catalog"
    description = "íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° DB ì €ìž¥"
    order = 200
    requires_llm = False
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì¡´ í•¨ìˆ˜ ìœ„ìž„"""
        return phase2_file_catalog_node(state)
