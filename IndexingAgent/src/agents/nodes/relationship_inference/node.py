# src/agents/nodes/relationship_inference/node.py
"""
Relationship Inference + Neo4j Node

í…Œì´ë¸” ê°„ FK ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ê³  Neo4jì— 3-Level Ontologyë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. FK ê´€ê³„ ì¶”ë¡  (LLM)
2. PostgreSQL table_relationships ì €ì¥
3. Neo4j 3-Level Ontology êµ¬ì¶•:
   - Level 1: RowEntity (í…Œì´ë¸”)
   - Level 2: ConceptCategory (ê°œë… ê·¸ë£¹)
   - Level 3: Parameter (ì»¬ëŸ¼)
"""

import time
from datetime import datetime
from typing import Dict, List, Any, Tuple, Set

from ...models.llm_responses import (
    TableRelationship,
    RelationshipInferenceResult,
)
from ...base import BaseNode, LLMMixin, DatabaseMixin, Neo4jMixin
from ...registry import register_node
from shared.database import OntologySchemaManager
from shared.database.repositories import ParameterRepository, FileGroupRepository
from src.config import RelationshipInferenceConfig, IndexingConfig
from shared.config import LLMConfig, Neo4jConfig
from .prompts import RelationshipInferencePrompt


@register_node
class RelationshipInferenceNode(BaseNode, LLMMixin, DatabaseMixin, Neo4jMixin):
    """
    Relationship Inference + Neo4j Node (LLM-based)
    
    í…Œì´ë¸” ê°„ FK ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ê³  Neo4jì— 3-Level Ontologyë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. FK ê´€ê³„ ì¶”ë¡  (LLM)
    2. PostgreSQL table_relationships ì €ì¥
    3. Neo4j 3-Level Ontology êµ¬ì¶•
    
    Input (from state):
        - entity_identification_result: ì´ì „ ë‹¨ê³„ ì™„ë£Œ ì •ë³´
        - data_files: ë°ì´í„° íŒŒì¼ ëª©ë¡
    
    Output:
        - relationship_inference_result: RelationshipInferenceResult í˜•íƒœ
        - table_relationships: TableRelationship ëª©ë¡
    """
    
    name = "relationship_inference"
    description = "í…Œì´ë¸” ê°„ FK ê´€ê³„ ì¶”ë¡  + Neo4j ì˜¨í†¨ë¡œì§€"
    order = 900
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = RelationshipInferencePrompt
    
    # =============================================================================
    # Data Loading (Using Repository Pattern)
    # =============================================================================
    
    def _load_tables_with_entity_and_columns(self) -> List[Dict[str, Any]]:
        """
        table_entities + column_metadata + file_catalog ì¡°ì¸ ë¡œë“œ
        
        Uses: EntityRepository.get_tables_with_entities(include_semantic=True)
        
        Returns:
            [
                {
                    "file_id": "uuid",
                    "file_name": "clinical_data.csv",
                    "row_represents": "surgery",
                    "entity_identifier": "caseid",
                    "row_count": 6388,
                    "filename_values": {"caseid": 1234},
                    "columns": [...]
                },
                ...
            ]
        """
        try:
            return self.entity_repo.get_tables_with_entities(include_semantic=True)
        except Exception as e:
            self.log(f"âŒ Error loading tables: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _find_shared_columns(self, tables: List[Dict]) -> List[Dict[str, Any]]:
        """
        í…Œì´ë¸” ê°„ ê³µìœ  ì»¬ëŸ¼ ì°¾ê¸° (rule-based FK í›„ë³´)
        
        filename_valuesë„ ê°€ìƒ ì»¬ëŸ¼ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ FK í›„ë³´ì— í¬í•¨
        """
        column_to_tables = {}
        
        for table in tables:
            file_name = table['file_name']
            row_count = table['row_count']
            
            # 1. ì¼ë°˜ ì»¬ëŸ¼
            for col in table['columns']:
                col_name = col['original_name']
                unique_count = col['unique_count']
                
                if col_name not in column_to_tables:
                    column_to_tables[col_name] = []
                
                column_to_tables[col_name].append({
                    "file_name": file_name,
                    "unique_count": unique_count,
                    "row_count": row_count,
                    "source": "column"
                })
            
            # 2. filename_valuesì˜ í‚¤ë„ ê°€ìƒ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
            filename_values = table.get('filename_values', {})
            if filename_values:
                for fv_key, fv_value in filename_values.items():
                    if fv_key not in column_to_tables:
                        column_to_tables[fv_key] = []
                    
                    # ì¤‘ë³µ ë°©ì§€
                    already_exists = any(
                        t['file_name'] == file_name 
                        for t in column_to_tables[fv_key]
                    )
                    if not already_exists:
                        column_to_tables[fv_key].append({
                            "file_name": file_name,
                            "unique_count": 1,
                            "row_count": row_count,
                            "source": "filename",
                            "extracted_value": fv_value
                        })
        
        # 2ê°œ ì´ìƒ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë°˜í™˜
        shared = []
        for col_name, table_list in column_to_tables.items():
            if len(table_list) >= 2:
                shared.append({
                    "column_name": col_name,
                    "tables": table_list
                })
        
        return shared
    
    # =============================================================================
    # LLM Context Building
    # =============================================================================
    
    def _build_tables_context(self, tables: List[Dict]) -> str:
        """LLMìš© í…Œì´ë¸” ì •ë³´ context ìƒì„±"""
        lines = []
        
        for table in tables:
            lines.append(f"\n## {table['file_name']}")
            lines.append(f"- row_represents: {table['row_represents']}")
            lines.append(f"- entity_identifier: {table['entity_identifier'] or '(none)'}")
            lines.append(f"- row_count: {table['row_count']:,}")
            
            # filename_values í‘œì‹œ
            filename_values = table.get('filename_values', {})
            if filename_values:
                lines.append(f"- filename_values (extracted from filename): {filename_values}")
            
            # FK í›„ë³´ ì»¬ëŸ¼ë§Œ í‘œì‹œ (identifier role ë˜ëŠ” FK íŒ¨í„´/ê°œë…)
            fk_candidates = [c for c in table['columns'] 
                            if c.get('column_role') == 'identifier'
                            or c.get('concept_category') in RelationshipInferenceConfig.FK_CANDIDATE_CONCEPTS
                            or any(p in (c['original_name'] or '') for p in RelationshipInferenceConfig.FK_CANDIDATE_PATTERNS)]
            
            if fk_candidates:
                lines.append("- FK candidate columns:")
                for col in fk_candidates:
                    unique_str = f"unique: {col['unique_count']:,}" if col['unique_count'] else "unique: ?"
                    role_str = f"ğŸ”‘" if col.get('column_role') == 'identifier' else ""
                    concept_str = col.get('concept_category') or col.get('column_role') or '-'
                    lines.append(f"    - {col['original_name']} {role_str}({concept_str}) [{unique_str}]")
        
        return "\n".join(lines)
    
    def _build_shared_columns_context(self, shared: List[Dict]) -> str:
        """LLMìš© ê³µìœ  ì»¬ëŸ¼ ì •ë³´ context ìƒì„±"""
        if not shared:
            return "(No shared columns found)"
        
        lines = []
        for item in shared:
            col_name = item['column_name']
            tables = item['tables']
            
            lines.append(f"\n- {col_name}:")
            for t in tables:
                unique_str = f"{t['unique_count']:,}" if t['unique_count'] else "?"
                source = t.get('source', 'column')
                source_str = " [from filename]" if source == 'filename' else ""
                
                if source == 'filename' and 'extracted_value' in t:
                    lines.append(f"    - {t['file_name']}: value={t['extracted_value']}, rows={t['row_count']:,}{source_str}")
                else:
                    lines.append(f"    - {t['file_name']}: unique={unique_str}, rows={t['row_count']:,}{source_str}")
        
        return "\n".join(lines)
    
    # =============================================================================
    # LLM Call
    # =============================================================================
    
    def _call_llm_for_relationships(
        self,
        tables: List[Dict],
        shared: List[Dict]
    ) -> Tuple[List[TableRelationship], int]:
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ FK ê´€ê³„ ì¶”ë¡  (ë°°ì¹˜ ì²˜ë¦¬)
        
        í…Œì´ë¸” ìˆ˜ê°€ MAX_TABLES_PER_BATCHë¥¼ ì´ˆê³¼í•˜ë©´ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Returns:
            (ê´€ê³„ ëª©ë¡, LLM í˜¸ì¶œ íšŸìˆ˜)
        """
        if not tables or len(tables) < 2:
            return [], 0
        
        if not shared:
            self.log("â„¹ï¸ No shared columns - skipping LLM call", indent=1)
            return [], 0
        
        max_tables = RelationshipInferenceConfig.MAX_TABLES_PER_BATCH
        
        # ë°°ì¹˜ê°€ í•„ìš”í•œì§€ í™•ì¸
        if len(tables) <= max_tables:
            # ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            return self._call_llm_for_batch(tables, shared)
        
        # ë°°ì¹˜ ì²˜ë¦¬ í•„ìš”
        self.log(f"ğŸ“¦ Splitting {len(tables)} tables into batches of {max_tables}...", indent=1)
        
        all_results = []
        total_llm_calls = 0
        
        # í…Œì´ë¸”ì„ ë°°ì¹˜ë¡œ ë¶„í• 
        table_batches = [tables[i:i + max_tables] for i in range(0, len(tables), max_tables)]
        
        for batch_idx, table_batch in enumerate(table_batches):
            self.log(f"ğŸ“¤ Batch {batch_idx + 1}/{len(table_batches)} ({len(table_batch)} tables)...", indent=1)
            
            # ì´ ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë“¤ì˜ ì´ë¦„ ì§‘í•©
            batch_file_names = {t['file_name'] for t in table_batch}
            
            # ê³µìœ  ì»¬ëŸ¼ ì¤‘ ì´ ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ í•„í„°ë§
            batch_shared = self._filter_shared_for_batch(shared, batch_file_names)
            
            if not batch_shared:
                self.log(f"   â„¹ï¸ No shared columns in this batch, skipping", indent=1)
                continue
            
            # ë°°ì¹˜ë³„ LLM í˜¸ì¶œ
            batch_results, batch_calls = self._call_llm_for_batch(table_batch, batch_shared)
            all_results.extend(batch_results)
            total_llm_calls += batch_calls
            
            self.log(f"   âœ… Found {len(batch_results)} relationships", indent=1)
        
        # ì¤‘ë³µ ê´€ê³„ ì œê±°
        unique_results = self._deduplicate_relationships(all_results)
        
        return unique_results, total_llm_calls
    
    def _filter_shared_for_batch(
        self,
        shared: List[Dict],
        batch_file_names: Set[str]
    ) -> List[Dict]:
        """ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë“¤ë§Œ í¬í•¨í•˜ë„ë¡ ê³µìœ  ì»¬ëŸ¼ í•„í„°ë§"""
        filtered = []
        
        for item in shared:
            # ì´ ì»¬ëŸ¼ì„ ê³µìœ í•˜ëŠ” í…Œì´ë¸” ì¤‘ ë°°ì¹˜ì— ì†í•˜ëŠ” ê²ƒë§Œ í•„í„°ë§
            batch_tables = [t for t in item['tables'] if t['file_name'] in batch_file_names]
            
            # 2ê°œ ì´ìƒ í…Œì´ë¸”ì´ ê³µìœ í•´ì•¼ FK í›„ë³´
            if len(batch_tables) >= 2:
                filtered.append({
                    "column_name": item['column_name'],
                    "tables": batch_tables
                })
        
        return filtered
    
    def _deduplicate_relationships(
        self,
        relationships: List[TableRelationship]
    ) -> List[TableRelationship]:
        """ì¤‘ë³µ ê´€ê³„ ì œê±° (source-target-column ì¡°í•© ê¸°ì¤€)"""
        seen = set()
        unique = []
        
        for rel in relationships:
            key = (rel.source_table, rel.target_table, rel.source_column, rel.target_column)
            if key not in seen:
                seen.add(key)
                unique.append(rel)
        
        return unique
    
    def _call_llm_for_batch(
        self,
        tables: List[Dict],
        shared: List[Dict]
    ) -> Tuple[List[TableRelationship], int]:
        """
        ë‹¨ì¼ ë°°ì¹˜ì— ëŒ€í•´ LLM í˜¸ì¶œ
        
        Returns:
            (ê´€ê³„ ëª©ë¡, LLM í˜¸ì¶œ íšŸìˆ˜)
        """
        tables_context = self._build_tables_context(tables)
        shared_context = self._build_shared_columns_context(shared)
        
        # PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_class.build(
            tables_context=tables_context,
            shared_columns=shared_context
        )
        
        llm_calls = 0
        results = []
        
        for attempt in range(RelationshipInferenceConfig.MAX_RETRIES):
            try:
                response = self.call_llm_json(prompt, max_tokens=LLMConfig.MAX_TOKENS)
                llm_calls += 1
                
                if response and 'relationships' in response:
                    for rel_data in response['relationships']:
                        rel = TableRelationship(
                            source_table=rel_data.get('source_table', ''),
                            target_table=rel_data.get('target_table', ''),
                            source_column=rel_data.get('source_column', ''),
                            target_column=rel_data.get('target_column', ''),
                            relationship_type=rel_data.get('relationship_type', 'foreign_key'),
                            cardinality=rel_data.get('cardinality', '1:N'),
                            confidence=float(rel_data.get('confidence', 0.0)),
                            reasoning=rel_data.get('reasoning', '')
                        )
                        results.append(rel)
                    
                    return results, llm_calls
                else:
                    self.log(f"   âš ï¸ Invalid LLM response, attempt {attempt + 1}", indent=2)
                    
            except Exception as e:
                self.log(f"   âŒ LLM call failed (attempt {attempt + 1}): {e}", indent=2)
                if attempt < RelationshipInferenceConfig.MAX_RETRIES - 1:
                    time.sleep(RelationshipInferenceConfig.RETRY_DELAY_SECONDS)
        
        return results, llm_calls
    
    # =============================================================================
    # PostgreSQL Save
    # =============================================================================
    
    def _save_relationships_to_postgres(
        self,
        relationships: List[TableRelationship],
        tables: List[Dict]
    ) -> int:
        """
        FK ê´€ê³„ë¥¼ table_relationships í…Œì´ë¸”ì— ì €ì¥
        
        Returns:
            ì €ì¥ëœ ê´€ê³„ ìˆ˜
        """
        if not relationships:
            return 0
        
        # file_name â†’ file_id ë§¤í•‘
        name_to_id = {t['file_name']: t['file_id'] for t in tables}
        
        rel_dicts = []
        for rel in relationships:
            source_id = name_to_id.get(rel.source_table)
            target_id = name_to_id.get(rel.target_table)
            
            if not source_id or not target_id:
                self.log(f"âš ï¸ Table not found: {rel.source_table} or {rel.target_table}", indent=1)
                continue
            
            rel_dicts.append({
                "source_file_id": source_id,
                "target_file_id": target_id,
                "source_column": rel.source_column,
                "target_column": rel.target_column,
                "relationship_type": rel.relationship_type,
                "cardinality": rel.cardinality,
                "confidence": rel.confidence,
                "reasoning": rel.reasoning
            })
        
        if rel_dicts:
            self.entity_repo.save_relationships(rel_dicts)
        
        return len(rel_dicts)
    
    # =============================================================================
    # Neo4j Sync (Using Neo4jMixin)
    # =============================================================================
    
    def _create_row_entity_nodes(self, driver, tables: List[Dict]) -> int:
        """Level 1: RowEntity ë…¸ë“œ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver or not tables:
            return 0
        
        batch_data = [
            {
                "file_id": table['file_id'],
                "file_name": table['file_name'],
                "row_represents": table['row_represents'],
                "entity_identifier": table['entity_identifier'],
                "row_count": table['row_count']
            }
            for table in tables
        ]
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MERGE (e:RowEntity {file_name: row.file_name})
                    SET e.file_id = row.file_id,
                        e.name = row.row_represents,
                        e.identifier_column = row.entity_identifier,
                        e.row_count = row.row_count
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating RowEntity nodes: {e}", indent=2)
            return 0
    
    def _create_concept_category_nodes(self, driver, all_params: List[Dict]) -> int:
        """Level 2: ConceptCategory ë…¸ë“œ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        concepts = list({param['concept'] for param in all_params if param.get('concept')})
        if not concepts:
            return 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $concepts AS name
                    MERGE (c:ConceptCategory {name: name})
                    RETURN count(*) as cnt
                """, {"concepts": concepts})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating ConceptCategory nodes: {e}", indent=2)
            return 0
    
    def _create_parameter_nodes(self, driver, all_params: List[Dict]) -> int:
        """Level 3: Parameter ë…¸ë“œ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver or not all_params:
            return 0
        
        batch_data = [
            {
                "key": param['key'],
                "name": param['name'],
                "unit": param['unit'],
                "concept": param['concept'],
                "is_identifier": param.get('is_identifier', False)
            }
            for param in all_params
        ]
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MERGE (p:Parameter {key: row.key})
                    SET p.name = row.name,
                        p.unit = row.unit,
                        p.concept = row.concept,
                        p.is_identifier = row.is_identifier
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating Parameter nodes: {e}", indent=2)
            return 0
    
    def _create_links_to_edges(self, driver, relationships: List[TableRelationship], tables: List[Dict]) -> int:
        """RowEntity ê°„ FK ê´€ê³„ (LINKS_TO) ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver or not relationships:
            return 0
        
        valid_names = {t['file_name'] for t in tables}
        
        batch_data = [
            {
                "source_name": rel.source_table,
                "target_name": rel.target_table,
                "source_column": rel.source_column,
                "target_column": rel.target_column,
                "cardinality": rel.cardinality,
                "confidence": rel.confidence
            }
            for rel in relationships
            if rel.source_table in valid_names and rel.target_table in valid_names
        ]
        
        if not batch_data:
            return 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (s:RowEntity {file_name: row.source_name})
                    MATCH (t:RowEntity {file_name: row.target_name})
                    MERGE (s)-[r:LINKS_TO]->(t)
                    SET r.source_column = row.source_column,
                        r.target_column = row.target_column,
                        r.cardinality = row.cardinality,
                        r.confidence = row.confidence
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating LINKS_TO edges: {e}", indent=2)
            return 0
    
    def _create_has_concept_edges(self, driver, tables: List[Dict]) -> int:
        """RowEntity â†’ ConceptCategory (HAS_CONCEPT) ì—£ì§€ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
        edges_set = set()
        for table in tables:
            concepts = {col['concept_category'] for col in table['columns'] if col['concept_category']}
            for concept in concepts:
                edges_set.add((table['file_name'], concept))
        
        if not edges_set:
            return 0
        
        batch_data = [{"file_name": fn, "concept": c} for fn, c in edges_set]
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (e:RowEntity {file_name: row.file_name})
                    MATCH (c:ConceptCategory {name: row.concept})
                    MERGE (e)-[:HAS_CONCEPT]->(c)
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating HAS_CONCEPT edges: {e}", indent=2)
            return 0
    
    def _create_contains_edges(self, driver, all_params: List[Dict]) -> int:
        """ConceptCategory â†’ Parameter (CONTAINS) ì—£ì§€ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        batch_data = [
            {"concept": param['concept'], "key": param['key']}
            for param in all_params
            if param.get('concept')
        ]
        
        if not batch_data:
            return 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (c:ConceptCategory {name: row.concept})
                    MATCH (p:Parameter {key: row.key})
                    MERGE (c)-[:CONTAINS]->(p)
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating CONTAINS edges: {e}", indent=2)
            return 0
    
    def _create_has_column_edges(self, driver, tables: List[Dict]) -> int:
        """RowEntity â†’ Parameter (HAS_COLUMN) ì—£ì§€ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬ (ëŒ€ëŸ‰ ìµœì í™”)"""
        if not driver:
            return 0
        
        # ëª¨ë“  ì—£ì§€ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘
        batch_data = []
        for table in tables:
            file_name = table['file_name']
            for col in table['columns']:
                batch_data.append({
                    "file_name": file_name,
                    "key": col['original_name']
                })
        
        if not batch_data:
            return 0
        
        # ëŒ€ëŸ‰ ë°ì´í„°ëŠ” ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        CHUNK_SIZE = 5000
        total_count = 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                for i in range(0, len(batch_data), CHUNK_SIZE):
                    chunk = batch_data[i:i + CHUNK_SIZE]
                    result = session.run("""
                        UNWIND $batch AS row
                        MATCH (e:RowEntity {file_name: row.file_name})
                        MATCH (p:Parameter {key: row.key})
                        MERGE (e)-[:HAS_COLUMN]->(p)
                        RETURN count(*) as cnt
                    """, {"batch": chunk})
                    total_count += result.single()["cnt"]
            return total_count
        except Exception as e:
            self.log(f"âŒ Error creating HAS_COLUMN edges: {e}", indent=2)
            return 0
    
    def _load_filename_column_mappings(self) -> Dict[str, Dict[str, Any]]:
        """
        directory_catalogì—ì„œ filename_columns ë§¤í•‘ ì •ë³´ ë¡œë“œ
        
        Uses: DirectoryRepository.get_filename_column_mappings()
        """
        return self.directory_repo.get_filename_column_mappings()
    
    def _create_filename_value_edges(self, driver, tables: List[Dict]) -> int:
        """filename_valuesì—ì„œ ì¶”ì¶œëœ ê°’ì„ Parameter ë…¸ë“œì™€ FILENAME_VALUE ê´€ê³„ë¡œ ì—°ê²° - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        dir_mappings = self._load_filename_column_mappings()
        
        # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        batch_data = []
        for table in tables:
            filename_values = table.get('filename_values', {})
            if not filename_values:
                continue
            
            file_name = table.get('file_name', '')
            
            for key, value in filename_values.items():
                matched_info = None
                semantic_role = "extracted_value"
                confidence = 0.8
                reasoning = "Extracted from filename pattern"
                
                for dir_name, col_map in dir_mappings.items():
                    if key in col_map:
                        matched_info = col_map[key]
                        break
                
                if matched_info:
                    matched_column = matched_info.get('matched_column') or key
                    confidence = matched_info.get('match_confidence', 0.8)
                    reasoning = matched_info.get('match_reasoning', reasoning)
                else:
                    matched_column = key
                
                # semantic_role ê²°ì •
                matched_lower = matched_column.lower()
                if 'id' in matched_lower or 'case' in matched_lower:
                    semantic_role = "case_identifier"
                elif 'subject' in matched_lower or 'patient' in matched_lower:
                    semantic_role = "subject_identifier"
                elif 'date' in matched_lower or 'time' in matched_lower:
                    semantic_role = "temporal_identifier"
                else:
                    semantic_role = "identifier"
                
                batch_data.append({
                    "file_name": file_name,
                    "param_key": matched_column,
                    "value": str(value) if value is not None else "",
                    "semantic_role": semantic_role,
                    "confidence": confidence,
                    "reasoning": reasoning
                })
        
        if not batch_data:
            return 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (e:RowEntity {file_name: row.file_name})
                    MATCH (p:Parameter {key: row.param_key})
                    MERGE (e)-[r:FILENAME_VALUE]->(p)
                    SET r.value = row.value,
                        r.semantic_role = row.semantic_role,
                        r.source = 'filename',
                        r.confidence = row.confidence,
                        r.reasoning = row.reasoning
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating FILENAME_VALUE edges: {e}", indent=2)
            return 0
    
    # =========================================================================
    # FileGroup ë…¸ë“œ ë° ì—£ì§€ (Q2: group_common íŒŒë¼ë¯¸í„° ì§€ì›)
    # =========================================================================
    
    def _create_file_group_nodes(self, driver) -> int:
        """FileGroup ë…¸ë“œ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        group_repo = FileGroupRepository()
        groups = group_repo.get_all_groups(status='confirmed')
        
        if not groups:
            return 0
        
        batch_data = [
            {
                "group_id": str(group['group_id']),
                "name": group['group_name'],
                "file_count": group.get('file_count', 0),
                "row_represents": group.get('row_represents')
            }
            for group in groups
        ]
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MERGE (fg:FileGroup {group_id: row.group_id})
                    SET fg.name = row.name,
                        fg.file_count = row.file_count,
                        fg.row_represents = row.row_represents
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating FileGroup nodes: {e}", indent=2)
            return 0
    
    def _create_contains_file_edges(self, driver, tables: List[Dict]) -> int:
        """FileGroup â†’ RowEntity (CONTAINS_FILE) ì—£ì§€ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        # tablesì—ì„œ group_idê°€ ìˆëŠ” íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ
        batch_data = [
            {"group_id": str(t['group_id']), "file_name": t['file_name']}
            for t in tables if t.get('group_id')
        ]
        
        if not batch_data:
            return 0
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (fg:FileGroup {group_id: row.group_id})
                    MATCH (r:RowEntity {file_name: row.file_name})
                    MERGE (fg)-[:CONTAINS_FILE]->(r)
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating CONTAINS_FILE edges: {e}", indent=2)
            return 0
    
    def _create_has_common_param_edges(self, driver) -> int:
        """FileGroup â†’ Parameter (HAS_COMMON_PARAM) ì—£ì§€ ìƒì„± - UNWIND ë°°ì¹˜ ì²˜ë¦¬"""
        if not driver:
            return 0
        
        param_repo = ParameterRepository()
        group_params = param_repo.get_group_common_params_for_neo4j()
        
        if not group_params:
            return 0
        
        batch_data = [
            {"group_id": item['group_id'], "param_key": item['param_key']}
            for item in group_params
        ]
        
        try:
            with driver.session(database=Neo4jConfig.DATABASE) as session:
                result = session.run("""
                    UNWIND $batch AS row
                    MATCH (fg:FileGroup {group_id: row.group_id})
                    MATCH (p:Parameter {key: row.param_key})
                    MERGE (fg)-[:HAS_COMMON_PARAM]->(p)
                    RETURN count(*) as cnt
                """, {"batch": batch_data})
                return result.single()["cnt"]
        except Exception as e:
            self.log(f"âŒ Error creating HAS_COMMON_PARAM edges: {e}", indent=2)
            return 0
    
    def _sync_to_neo4j(
        self,
        tables: List[Dict],
        relationships: List[TableRelationship]
    ) -> Dict[str, int]:
        """Neo4j ì „ì²´ ë™ê¸°í™” (Neo4jMixin ì‚¬ìš©)"""
        stats = {
            "row_entity_nodes": 0,
            "file_group_nodes": 0,
            "concept_category_nodes": 0,
            "parameter_nodes": 0,
            "edges_links_to": 0,
            "edges_has_concept": 0,
            "edges_contains": 0,
            "edges_has_column": 0,
            "edges_filename_value": 0,
            "edges_contains_file": 0,
            "edges_has_common_param": 0
        }
        
        if not RelationshipInferenceConfig.NEO4J_ENABLED:
            self.log("â„¹ï¸ Neo4j sync is disabled", indent=1)
            return stats
        
        driver = self.neo4j_driver  # Neo4jMixin ì‚¬ìš©
        if not driver:
            self.log("âš ï¸ Skipping Neo4j sync (connection failed)", indent=1)
            return stats
        
        try:
            self.log("ğŸ“Š Creating Neo4j nodes and edges...", indent=1)
            
            # parameter í…Œì´ë¸”ì—ì„œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ (group_common í¬í•¨)
            param_repo = ParameterRepository()
            all_params = param_repo.get_all_parameters_for_ontology()
            self.log(f"âœ“ Loaded {len(all_params)} parameters from DB", indent=2)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # NODES
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Level 1: RowEntity (ê°œë³„ íŒŒì¼/í…Œì´ë¸”)
            stats["row_entity_nodes"] = self._create_row_entity_nodes(driver, tables)
            self.log(f"âœ“ RowEntity nodes: {stats['row_entity_nodes']}", indent=2)
            
            # FileGroup (íŒŒì¼ ê·¸ë£¹ - group_common íŒŒë¼ë¯¸í„° ì§€ì›)
            stats["file_group_nodes"] = self._create_file_group_nodes(driver)
            self.log(f"âœ“ FileGroup nodes: {stats['file_group_nodes']}", indent=2)
            
            # Level 2: ConceptCategory (parameter í…Œì´ë¸” ê¸°ë°˜)
            stats["concept_category_nodes"] = self._create_concept_category_nodes(driver, all_params)
            self.log(f"âœ“ ConceptCategory nodes: {stats['concept_category_nodes']}", indent=2)
            
            # Level 3: Parameter (parameter í…Œì´ë¸” ê¸°ë°˜)
            stats["parameter_nodes"] = self._create_parameter_nodes(driver, all_params)
            self.log(f"âœ“ Parameter nodes: {stats['parameter_nodes']}", indent=2)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # EDGES
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # FK ê´€ê³„: RowEntity â†’ RowEntity
            stats["edges_links_to"] = self._create_links_to_edges(driver, relationships, tables)
            self.log(f"âœ“ LINKS_TO edges: {stats['edges_links_to']}", indent=2)
            
            # RowEntity â†’ ConceptCategory
            stats["edges_has_concept"] = self._create_has_concept_edges(driver, tables)
            self.log(f"âœ“ HAS_CONCEPT edges: {stats['edges_has_concept']}", indent=2)
            
            # ConceptCategory â†’ Parameter (parameter í…Œì´ë¸” ê¸°ë°˜)
            stats["edges_contains"] = self._create_contains_edges(driver, all_params)
            self.log(f"âœ“ CONTAINS edges: {stats['edges_contains']}", indent=2)
            
            # RowEntity â†’ Parameter (column_name ê¸°ë°˜)
            stats["edges_has_column"] = self._create_has_column_edges(driver, tables)
            self.log(f"âœ“ HAS_COLUMN edges: {stats['edges_has_column']}", indent=2)
            
            # RowEntity â†’ Parameter (filenameì—ì„œ ì¶”ì¶œëœ ê°’)
            stats["edges_filename_value"] = self._create_filename_value_edges(driver, tables)
            self.log(f"âœ“ FILENAME_VALUE edges: {stats['edges_filename_value']}", indent=2)
            
            # FileGroup â†’ RowEntity (ê·¸ë£¹ì— ì†í•œ íŒŒì¼ë“¤)
            stats["edges_contains_file"] = self._create_contains_file_edges(driver, tables)
            self.log(f"âœ“ CONTAINS_FILE edges: {stats['edges_contains_file']}", indent=2)
            
            # FileGroup â†’ Parameter (group_common íŒŒë¼ë¯¸í„°)
            stats["edges_has_common_param"] = self._create_has_common_param_edges(driver)
            self.log(f"âœ“ HAS_COMMON_PARAM edges: {stats['edges_has_common_param']}", indent=2)
            
        finally:
            self.close_neo4j()  # Neo4jMixin ì‚¬ìš©
        
        return stats
    
    # =============================================================================
    # Main Execute
    # =============================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Relationship Inference + Neo4j Sync ì‹¤í–‰
        
        1. table_entities + column_metadata ë¡œë“œ
        2. ê³µìœ  ì»¬ëŸ¼ íƒì§€ (FK í›„ë³´)
        3. LLMìœ¼ë¡œ FK ê´€ê³„ ì¶”ë¡ 
        4. PostgreSQL table_relationships ì €ì¥
        5. Neo4j 3-Level Ontology êµ¬ì¶•
        """
        started_at = datetime.now().isoformat()
        
        # 1. ìŠ¤í‚¤ë§ˆ í™•ì¸
        schema_manager = OntologySchemaManager()
        schema_manager.create_tables()
        
        # 2. í…Œì´ë¸” ì •ë³´ ë¡œë“œ
        self.log("ğŸ“¥ Loading tables with entity and column info...")
        tables = self._load_tables_with_entity_and_columns()
        
        if not tables:
            self.log("âš ï¸ No tables found (run previous step first)")
            return {
                "relationship_inference_result": RelationshipInferenceResult(
                    started_at=started_at,
                    completed_at=datetime.now().isoformat()
                ).model_dump(),
                "table_relationships": []
            }
        
        self.log(f"âœ… Loaded {len(tables)} tables", indent=1)
        for t in tables:
            self.log(f"- {t['file_name']} ({t['row_represents']}, {len(t['columns'])} columns)", indent=2)
        
        # 3. ì´ë¯¸ ë¶„ì„ëœ ê´€ê³„ í™•ì¸ (ìŠ¤í‚µ ë¡œì§)
        relationships = []
        llm_calls = 0
        skipped_relationships = False
        
        if not IndexingConfig.FORCE_REANALYZE:
            existing_count = self.entity_repo.get_relationship_count()
            if existing_count > 0:
                self.log(f"â­ï¸  Skipping LLM inference: {existing_count} relationships already exist", indent=1)
                skipped_relationships = True
                # ê¸°ì¡´ ê´€ê³„ë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
                relationships = self._load_existing_relationships(tables)
        
        if not skipped_relationships:
            # 3. ê³µìœ  ì»¬ëŸ¼ ì°¾ê¸°
            self.log("ğŸ” Finding shared columns...")
            shared_columns = self._find_shared_columns(tables)
            self.log(f"âœ… Found {len(shared_columns)} shared columns", indent=1)
            for sc in shared_columns[:5]:
                self.log(f"- {sc['column_name']} (in {len(sc['tables'])} tables)", indent=2)
            
            # 4. LLMìœ¼ë¡œ FK ê´€ê³„ ì¶”ë¡ 
            self.log("ğŸ¤– Inferring relationships with LLM...")
            relationships, llm_calls = self._call_llm_for_relationships(tables, shared_columns)
            self.log(f"âœ… Found {len(relationships)} relationships (LLM calls: {llm_calls})", indent=1)
        
        # 5. PostgreSQL ì €ì¥ (ìƒˆë¡œ ì¶”ë¡ í•œ ê²½ìš°ì—ë§Œ)
        saved_count = 0
        if not skipped_relationships:
            self.log("ğŸ’¾ Saving to PostgreSQL...")
            saved_count = self._save_relationships_to_postgres(relationships, tables)
            self.log(f"âœ… Saved {saved_count} relationships", indent=1)
        
        # 6. Neo4j ë™ê¸°í™”
        self.log("ğŸ“Š Syncing to Neo4j...")
        neo4j_stats = self._sync_to_neo4j(tables, relationships)
        neo4j_synced = sum(neo4j_stats.values()) > 0
        
        # 7. ê²°ê³¼ í†µê³„
        high_conf = sum(1 for r in relationships if r.confidence >= RelationshipInferenceConfig.CONFIDENCE_THRESHOLD)
        
        # 8. ê²°ê³¼ ì¶œë ¥
        self.log(f"Relationships found: {len(relationships)}", indent=1)
        self.log(f"High confidence (â‰¥{RelationshipInferenceConfig.CONFIDENCE_THRESHOLD}): {high_conf}", indent=1)
        self.log(f"LLM calls: {llm_calls}", indent=1)
        
        if relationships:
            self.log("ğŸ”— Relationships:")
            for rel in relationships:
                conf_emoji = "ğŸŸ¢" if rel.confidence >= RelationshipInferenceConfig.CONFIDENCE_THRESHOLD else "ğŸŸ¡"
                self.log(f"{conf_emoji} {rel.source_table} â†’ {rel.target_table}", indent=1)
                self.log(f"{rel.source_column} â†’ {rel.target_column} ({rel.cardinality})", indent=2)
                self.log(f"confidence: {rel.confidence:.2f}", indent=2)
        
        if neo4j_synced:
            self.log("ğŸ“Š Neo4j Graph:")
            self.log(f"RowEntity nodes: {neo4j_stats['row_entity_nodes']}", indent=1)
            self.log(f"ConceptCategory nodes: {neo4j_stats['concept_category_nodes']}", indent=1)
            self.log(f"Parameter nodes: {neo4j_stats['parameter_nodes']}", indent=1)
            self.log(f"LINKS_TO edges: {neo4j_stats['edges_links_to']}", indent=1)
            self.log(f"HAS_CONCEPT edges: {neo4j_stats['edges_has_concept']}", indent=1)
            self.log(f"CONTAINS edges: {neo4j_stats['edges_contains']}", indent=1)
            self.log(f"HAS_COLUMN edges: {neo4j_stats['edges_has_column']}", indent=1)
        
        # 9. ê²°ê³¼ ë°˜í™˜
        completed_at = datetime.now().isoformat()
        
        phase_result = RelationshipInferenceResult(
            relationships_found=len(relationships),
            relationships_high_conf=high_conf,
            row_entity_nodes=neo4j_stats['row_entity_nodes'],
            concept_category_nodes=neo4j_stats['concept_category_nodes'],
            parameter_nodes=neo4j_stats['parameter_nodes'],
            edges_links_to=neo4j_stats['edges_links_to'],
            edges_has_concept=neo4j_stats['edges_has_concept'],
            edges_contains=neo4j_stats['edges_contains'],
            edges_has_column=neo4j_stats['edges_has_column'],
            llm_calls=llm_calls,
            neo4j_synced=neo4j_synced,
            started_at=started_at,
            completed_at=completed_at
        )
        
        return {
            "relationship_inference_result": phase_result.model_dump(),
            "table_relationships": [r.model_dump() for r in relationships]
        }
    
    # =========================================================================
    # Skip Already Analyzed
    # =========================================================================
    
    def _load_existing_relationships(
        self, 
        tables: List[Dict[str, Any]]
    ) -> List[TableRelationship]:
        """
        ê¸°ì¡´ table_relationshipsì—ì„œ ê´€ê³„ ë¡œë“œ
        
        FORCE_REANALYZE=falseì´ê³  ê´€ê³„ê°€ ì´ë¯¸ ìˆì„ ë•Œ ì‚¬ìš©
        
        Args:
            tables: í…Œì´ë¸” ì •ë³´ (file_name â†’ file_id ë§¤í•‘ìš©)
        
        Returns:
            TableRelationship ëª©ë¡
        """
        existing = self.entity_repo.get_relationships()
        
        relationships = []
        for rel in existing:
            relationships.append(TableRelationship(
                source_table=rel.get('source_name', ''),
                target_table=rel.get('target_name', ''),
                source_column=rel.get('source_column', ''),
                target_column=rel.get('target_column', ''),
                relationship_type=rel.get('relationship_type', 'foreign_key'),
                cardinality=rel.get('cardinality', '1:N'),
                confidence=rel.get('confidence', 0.0),
                reasoning=rel.get('reasoning', '')
            ))
        
        return relationships
    
    @classmethod
    def run_standalone(cls) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ (í…ŒìŠ¤íŠ¸ìš©)
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        return node({})

