# src/agents/nodes/entity_identification/node.py
"""
Entity Identification Node

ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ í–‰ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€(row_represents)ì™€
ê³ ìœ  ì‹ë³„ì ì»¬ëŸ¼(entity_identifier)ì„ ì‹ë³„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLMì„ ì‚¬ìš©í•´ ê° í…Œì´ë¸”ì˜ row_represents ì¶”ë¡  (surgery, patient, lab_result ë“±)
- ì»¬ëŸ¼ í†µê³„(unique count)ë¥¼ í™œìš©í•´ entity_identifier ì‹ë³„
- table_entities í…Œì´ë¸”ì— ê²°ê³¼ ì €ì¥
"""

import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from IndexingAgent.src.models.llm_responses import (
    TableEntityResult,
    EntityIdentificationResult,
)
from shared.langgraph import BaseNode, LLMMixin, DatabaseMixin
from shared.langgraph import register_node
from shared.database import OntologySchemaManager
from shared.database.repositories import FileGroupRepository
from IndexingAgent.src.config import EntityIdentificationConfig, IndexingConfig
from shared.config import LLMConfig
from .prompts import EntityIdentificationPrompt, GroupEntityPrompt


@register_node
class EntityIdentificationNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Entity Identification Node (LLM-based)
    
    ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ í–‰ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€(row_represents)ì™€
    ê³ ìœ  ì‹ë³„ì ì»¬ëŸ¼(entity_identifier)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Input (from state):
        - data_files: is_metadata=falseì¸ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        - data_semantic_result: ì´ì „ ë‹¨ê³„ ì™„ë£Œ ì •ë³´ (column_metadata ë¶„ì„ ì™„ë£Œ)
    
    Output:
        - entity_identification_result: EntityIdentificationResult í˜•íƒœ
        - table_entity_results: TableEntityResult ëª©ë¡
    """
    
    name = "entity_identification"
    description = "Entity ì‹ë³„ (row_represents, entity_identifier)"
    order = 800
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = EntityIdentificationPrompt
    group_prompt_class = GroupEntityPrompt
    
    def __init__(self):
        super().__init__()
        self._group_repo: Optional[FileGroupRepository] = None
    
    def _get_group_repo(self) -> FileGroupRepository:
        """FileGroupRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._group_repo is None:
            self._group_repo = FileGroupRepository()
        return self._group_repo
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 1: ê·¸ë£¹ Entity ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _analyze_group_entity(self, group: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ê·¸ë£¹ì˜ ìƒ˜í”Œ íŒŒì¼ë¡œ Entity ë¶„ì„
        
        Args:
            group: ê·¸ë£¹ ì •ë³´ (group_id, group_name, grouping_criteria ë“±)
        
        Returns:
            {
                'row_represents': str,
                'entity_identifier_source': str,  # 'filename' or 'content'
                'entity_identifier_key': str,     # 'caseid'
                'confidence': float,
                'reasoning': str,
                'sample_file_ids': list
            }
            ë˜ëŠ” None (ë¶„ì„ ì‹¤íŒ¨)
        """
        group_id = group['group_id']
        group_name = group['group_name']
        criteria = group.get('grouping_criteria', {})
        file_count = group.get('file_count', 0)
        sample_file_ids = group.get('sample_file_ids', [])
        
        self.log(f"ğŸ“¦ Analyzing group: {group_name} ({file_count} files)", indent=1)
        
        # ìƒ˜í”Œ íŒŒì¼ ì„ íƒ
        group_repo = self._get_group_repo()
        
        if sample_file_ids:
            # ê¸°ì¡´ ìƒ˜í”Œ íŒŒì¼ ì‚¬ìš©
            sample_file_id = sample_file_ids[0]
            sample_file = self.file_repo.get_file_by_id(sample_file_id)
        else:
            # ê·¸ë£¹ì—ì„œ ìƒ˜í”Œ íŒŒì¼ ì„ íƒ
            sample_files = group_repo.get_sample_files_for_analysis(group_id, sample_size=1)
            if not sample_files:
                self.log(f"âš ï¸ No sample files for group {group_name}", indent=2)
                return None
            sample_file = sample_files[0]
            sample_file_id = sample_file['file_id']
        
        if not sample_file:
            self.log(f"âš ï¸ Sample file not found for group {group_name}", indent=2)
            return None
        
        sample_file_path = sample_file.get('file_path')
        self.log(f"ğŸ¯ Sample file: {sample_file_path.split('/')[-1] if sample_file_path else 'unknown'}", indent=2)
        
        # ìƒ˜í”Œ íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ë¡œë“œ
        files_info = self._load_data_files_with_columns([sample_file_path])
        if not files_info:
            self.log(f"âš ï¸ No column info for sample file", indent=2)
            return None
        
        sample_info = files_info[0]
        
        # ê·¸ë£¹ ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        group_context = self._build_group_entity_context(
            group_name=group_name,
            file_count=file_count,
            criteria=criteria,
            sample_info=sample_info
        )
        
        # LLM í˜¸ì¶œ
        prompt = self.group_prompt_class.build(group_context=group_context)
        
        try:
            response = self.call_llm_json(prompt)
            
            if not response or response.get('error'):
                self.log(f"âŒ LLM error: {response.get('error') if response else 'No response'}", indent=2)
                return None
            
            # ì‘ë‹µ íŒŒì‹±
            row_represents = response.get('row_represents', 'unknown')
            entity_source = response.get('entity_identifier_source', 'filename')
            entity_key = response.get('entity_identifier_key')
            confidence = float(response.get('confidence', 0.0))
            reasoning = response.get('reasoning', '')
            
            # pattern_columnsì—ì„œ entity_key ì¶”ì¶œ (fallback)
            if not entity_key:
                pattern_columns = criteria.get('pattern_columns', [])
                if pattern_columns:
                    entity_key = pattern_columns[0].get('name')
            
            self.log(f"âœ… row_represents: {row_represents}", indent=2)
            self.log(f"âœ… entity_identifier: {entity_source}/{entity_key}", indent=2)
            self.log(f"âœ… confidence: {confidence:.2f}", indent=2)
            
            return {
                'row_represents': row_represents,
                'entity_identifier_source': entity_source,
                'entity_identifier_key': entity_key,
                'confidence': confidence,
                'reasoning': reasoning,
                'sample_file_ids': [sample_file_id]
            }
            
        except Exception as e:
            self.log(f"âŒ LLM call failed: {e}", indent=2)
            return None
    
    def _build_group_entity_context(
        self,
        group_name: str,
        file_count: int,
        criteria: Dict,
        sample_info: Dict
    ) -> str:
        """
        ê·¸ë£¹ Entity ë¶„ì„ìš© LLM ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        """
        lines = []
        
        # ê·¸ë£¹ ì •ë³´
        lines.append("## File Group Information")
        lines.append(f"- Group Name: {group_name}")
        lines.append(f"- Total Files: {file_count}")
        lines.append(f"- Extensions: {criteria.get('extensions', [])}")
        
        # íŒ¨í„´ ì •ë³´
        pattern_regex = criteria.get('pattern_regex')
        pattern_columns = criteria.get('pattern_columns', [])
        if pattern_regex:
            lines.append(f"- Filename Pattern: {pattern_regex}")
            if pattern_columns:
                cols_str = ', '.join([c.get('name', '?') for c in pattern_columns])
                lines.append(f"- Pattern Columns: {cols_str}")
        
        # ìƒ˜í”Œ íŒŒì¼ ì •ë³´
        lines.append("")
        lines.append("## Sample File")
        lines.append(f"- File Name: {sample_info.get('file_name', 'unknown')}")
        lines.append(f"- Row Count: {sample_info.get('row_count', 0):,}")
        
        # filename_values
        filename_values = sample_info.get('filename_values', {})
        if filename_values:
            lines.append("- Filename-extracted values:")
            for key, value in filename_values.items():
                lines.append(f"  - {key}: {value}")
        
        # ì»¬ëŸ¼ ì •ë³´
        columns = sample_info.get('columns', [])
        if columns:
            lines.append("")
            lines.append("## Sample File Columns")
            
            # identifier ì»¬ëŸ¼ ë¨¼ì €
            id_cols = [c for c in columns if c.get('column_role') == 'identifier']
            param_cols = [c for c in columns if c.get('column_role') == 'parameter_name']
            other_cols = [c for c in columns if c.get('column_role') not in ('identifier', 'parameter_name')]
            
            if id_cols:
                lines.append("Identifier columns:")
                for col in id_cols:
                    unique = col.get('unique_count')
                    lines.append(f"  - {col['original_name']} ğŸ”‘ (unique: {unique:,})" if unique else f"  - {col['original_name']} ğŸ”‘")
            
            if param_cols:
                lines.append("Parameter columns (first 10):")
                for col in param_cols[:10]:
                    semantic = col.get('semantic_name') or col['original_name']
                    lines.append(f"  - {col['original_name']} ({semantic})")
                if len(param_cols) > 10:
                    lines.append(f"  ... and {len(param_cols) - 10} more")
            
            if other_cols:
                lines.append("Other columns (first 5):")
                for col in other_cols[:5]:
                    lines.append(f"  - {col['original_name']} [{col.get('column_type', '-')}]")
                if len(other_cols) > 5:
                    lines.append(f"  ... and {len(other_cols) - 5} more")
        
        return "\n".join(lines)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 2: ë¹„ê·¸ë£¹ íŒŒì¼ ë¶„ì„ (ê¸°ì¡´ ë¡œì§)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _load_data_files_with_columns(self, data_files: List[str]) -> List[Dict[str, Any]]:
        """
        ë°ì´í„° íŒŒì¼ê³¼ ê·¸ ì»¬ëŸ¼ ì •ë³´ë¥¼ DBì—ì„œ ë¡œë“œ
        
        Uses:
          - FileRepository.get_files_by_paths()
          - ColumnRepository.get_columns_with_stats()
          - ColumnRepository.get_columns_with_semantic()
        
        Returns:
            [
                {
                    "file_id": "uuid",
                    "file_name": "clinical_data.csv",
                    "row_count": 6388,
                    "columns": [
                        {
                            "original_name": "caseid",
                            "semantic_name": "Case ID",
                            "column_type": "categorical",
                            "concept_category": "Identifiers",
                            "unique_count": 6388,
                            ...
                        },
                        ...
                    ]
                },
                ...
            ]
        """
        if not data_files:
            return []
        
        files_info = []
        
        try:
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            files = self.file_repo.get_files_by_paths(data_files)
            
            for f in files:
                file_id = f['file_id']
                file_name = f['file_name']
                file_path = f['file_path']
                
                # row_count ì¶”ì¶œ
                metadata = f.get('file_metadata', {})
                row_count = metadata.get('row_count', 0) if metadata else 0
                
                # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ (í†µê³„ + semantic)
                # get_columns_with_stats() + get_columns_with_semantic() ë³‘í•©
                cols_stats = self.column_repo.get_columns_with_stats(file_id)
                cols_semantic = self.column_repo.get_columns_with_semantic(file_id)
                
                # ë³‘í•©: col_id ê¸°ì¤€
                semantic_map = {c['col_id']: c for c in cols_semantic}
                
                columns = []
                for col in cols_stats:
                    col_id = col['col_id']
                    sem_info = semantic_map.get(col_id, {})
                    
                    columns.append({
                        "original_name": col['original_name'],
                        "column_role": col.get('column_role'),
                        "semantic_name": sem_info.get('semantic_name'),
                        "column_type": col.get('column_type'),
                        "concept_category": sem_info.get('concept_category'),
                        "unique_count": col.get('unique_count'),
                        "column_info": col.get('column_info', {})
                    })
                
                # filename_values ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’ë“¤)
                filename_values = f.get('filename_values', {})
                
                files_info.append({
                    "file_id": file_id,
                    "file_name": file_name,
                    "row_count": row_count or 0,
                    "file_path": file_path,
                    "columns": columns,
                    "filename_values": filename_values
                })
        
        except Exception as e:
            self.log(f"âŒ Error loading data files: {e}")
            import traceback
            traceback.print_exc()
        
        return files_info
    
    def _build_tables_context(self, files_info: List[Dict[str, Any]]) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ìš© í…Œì´ë¸” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        column_roleì„ í™œìš©í•˜ì—¬ identifier í›„ë³´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.
        filename_valuesë„ í¬í•¨í•˜ì—¬ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Args:
            files_info: _load_data_files_with_columns()ì˜ ê²°ê³¼
        
        Returns:
            í¬ë§·ëœ ë¬¸ìì—´
        """
        lines = []
        
        for file_info in files_info:
            file_name = file_info['file_name']
            row_count = file_info['row_count']
            columns = file_info['columns']
            filename_values = file_info.get('filename_values', {})
            
            lines.append(f"\n## {file_name}")
            lines.append(f"Rows: {row_count:,}")
            
            # filename_values í‘œì‹œ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’)
            if filename_values:
                lines.append("Filename-extracted values (embedded in filename):")
                for key, value in filename_values.items():
                    lines.append(f"  - {key}: {value}")
            
            lines.append("Columns:")
            
            max_cols = EntityIdentificationConfig.MAX_COLUMNS_PER_TABLE
            display_cols = columns[:max_cols] if max_cols > 0 else columns
            
            for col in display_cols:
                name = col['original_name']
                col_role = col.get('column_role')
                semantic = col.get('semantic_name') or name
                concept = col.get('concept_category')
                col_type = col.get('column_type') or '-'
                unique_count = col.get('unique_count')
                
                # column_roleì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                if col_role == 'identifier':
                    # identifierëŠ” ê°•ì¡° í‘œì‹œ
                    line = f"  - {name} ğŸ”‘[IDENTIFIER]"
                    if unique_count is not None:
                        line += f" unique: {unique_count:,}"
                        # row_countì™€ ë¹„êµí•˜ì—¬ unique identifier í›„ë³´ í‘œì‹œ
                        if row_count > 0 and unique_count == row_count:
                            line += " â† matches row count!"
                elif col_role == 'parameter_name':
                    # parameterëŠ” semantic ì •ë³´ í¬í•¨
                    line = f"  - {name}"
                    if semantic and semantic != name:
                        line += f" ({semantic})"
                    if concept:
                        line += f" [{concept}]"
                elif col_role == 'timestamp':
                    line = f"  - {name} [timestamp]"
                elif col_role == 'attribute':
                    line = f"  - {name} [attribute]"
                    if unique_count is not None:
                        line += f" unique: {unique_count:,}"
                else:
                    # ê¸°íƒ€ ì»¬ëŸ¼
                    line = f"  - {name} [{col_type}]"
                    if EntityIdentificationConfig.SHOW_UNIQUE_COUNTS and unique_count is not None:
                        line += f" unique: {unique_count:,}"
                
                lines.append(line)
            
            if max_cols > 0 and len(columns) > max_cols:
                lines.append(f"  ... and {len(columns) - max_cols} more columns")
        
        return "\n".join(lines)
    
    def _call_llm_for_entity_identification(
        self,
        files_info: List[Dict[str, Any]]
    ) -> Tuple[List[TableEntityResult], int]:
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ Entity ì‹ë³„
        
        Args:
            files_info: í…Œì´ë¸” ì •ë³´ ëª©ë¡
        
        Returns:
            (ê²°ê³¼ ëª©ë¡, LLM í˜¸ì¶œ íšŸìˆ˜)
        """
        if not files_info:
            return [], 0
        
        tables_context = self._build_tables_context(files_info)
        
        # PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_class.build(tables_context=tables_context)
        
        self.log(f"ğŸ“¤ Sending {len(files_info)} tables to LLM...", indent=1)
        
        llm_calls = 0
        results = []
        
        for attempt in range(EntityIdentificationConfig.MAX_RETRIES):
            try:
                response = self.call_llm_json(
                    prompt,
                    max_tokens=LLMConfig.MAX_TOKENS
                )
                llm_calls += 1
                
                if response and 'tables' in response:
                    for table_data in response['tables']:
                        result = TableEntityResult(
                            file_name=table_data.get('file_name', ''),
                            row_represents=table_data.get('row_represents', 'unknown'),
                            entity_identifier=table_data.get('entity_identifier'),
                            confidence=float(table_data.get('confidence', 0.0)),
                            reasoning=table_data.get('reasoning', '')
                        )
                        results.append(result)
                    
                    return results, llm_calls
                else:
                    self.log(f"âš ï¸ Invalid LLM response format, attempt {attempt + 1}", indent=1)
                    
            except Exception as e:
                self.log(f"âŒ LLM call failed (attempt {attempt + 1}): {e}", indent=1)
                if attempt < EntityIdentificationConfig.MAX_RETRIES - 1:
                    time.sleep(EntityIdentificationConfig.RETRY_DELAY_SECONDS)
        
        return results, llm_calls
    
    def _save_table_entities(
        self,
        files_info: List[Dict[str, Any]],
        llm_results: List[TableEntityResult]
    ) -> int:
        """
        LLM ê²°ê³¼ë¥¼ table_entities í…Œì´ë¸”ì— ì €ì¥
        
        Args:
            files_info: íŒŒì¼ ì •ë³´ (file_id í¬í•¨)
            llm_results: LLM ë¶„ì„ ê²°ê³¼
        
        Returns:
            ì €ì¥ëœ ì—”í‹°í‹° ìˆ˜
        """
        # file_name â†’ file_id ë§¤í•‘ ìƒì„±
        name_to_info = {f['file_name']: f for f in files_info}
        
        entities_to_save = []
        
        for result in llm_results:
            file_info = name_to_info.get(result.file_name)
            if not file_info:
                self.log(f"âš ï¸ File not found: {result.file_name}", indent=1)
                continue
            
            entities_to_save.append({
                "file_id": file_info['file_id'],
                "row_represents": result.row_represents,
                "entity_identifier": result.entity_identifier,
                "confidence": result.confidence,
                "reasoning": result.reasoning
            })
        
        if entities_to_save:
            self.entity_repo.save_table_entities(entities_to_save)
        
        return len(entities_to_save)
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Entity Identification ì‹¤í–‰ (v2)
        
        ìˆ˜ì •ëœ ë¡œì§:
        Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬ (ê·¸ë£¹ ë‹¨ìœ„ ë¶„ì„ + ì „íŒŒ)
        Phase 2: ë¹„ê·¸ë£¹ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ê°œë³„ ë¶„ì„)
        """
        started_at = datetime.now().isoformat()
        
        # Ontology ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
        schema_manager = OntologySchemaManager()
        schema_manager.create_tables()
        
        # í†µê³„ ì´ˆê¸°í™”
        groups_processed = 0
        group_files_propagated = 0
        ungrouped_files_processed = 0
        total_llm_calls = 0
        all_results: List[TableEntityResult] = []
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Phase 1: ê·¸ë£¹í™”ëœ íŒŒì¼ ì²˜ë¦¬
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.log("=" * 50)
        self.log("ğŸ“¦ Phase 1: Processing file groups...")
        
        group_repo = self._get_group_repo()
        groups = group_repo.get_groups_for_entity_analysis()
        groups_skipped = 0
        
        if groups:
            self.log(f"ğŸ“¦ Found {len(groups)} groups to analyze", indent=1)
            
            # Skip already analyzed groups (FORCE_REANALYZE=falseì¸ ê²½ìš°)
            groups_to_process = groups
            if not IndexingConfig.FORCE_REANALYZE:
                groups_to_process, groups_skipped = self._filter_unanalyzed_groups_entity(groups)
                if groups_skipped > 0:
                    self.log(f"â­ï¸  Skipping {groups_skipped} already analyzed groups", indent=1)
            
            for group in groups_to_process:
                group_result = self._analyze_group_entity(group)
                total_llm_calls += 1
                
                if group_result:
                    groups_processed += 1
                    
                    # file_group í…Œì´ë¸” ì—…ë°ì´íŠ¸
                    group_repo.update_group_analysis(
                        group_id=group['group_id'],
                        row_represents=group_result['row_represents'],
                        entity_identifier_source=group_result['entity_identifier_source'],
                        entity_identifier_key=group_result['entity_identifier_key'],
                        confidence=group_result['confidence'],
                        reasoning=group_result['reasoning'],
                        sample_file_ids=group_result.get('sample_file_ids')
                    )
                    
                    # ê·¸ë£¹ ë‚´ ëª¨ë“  íŒŒì¼ì— table_entities ì „íŒŒ
                    propagated = self.entity_repo.bulk_save_group_entities(
                        group_id=group['group_id'],
                        row_represents=group_result['row_represents'],
                        entity_identifier_key=group_result['entity_identifier_key'],
                        confidence=group_result['confidence'],
                        reasoning=group_result['reasoning']
                    )
                    group_files_propagated += propagated
                    
                    self.log(f"âœ… {group['group_name']}: {group_result['row_represents']} â†’ {propagated} files", indent=2)
                else:
                    self.log(f"âš ï¸ {group['group_name']}: Analysis failed", indent=2)
        else:
            self.log("âš ï¸ No groups to analyze", indent=1)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Phase 2: ë¹„ê·¸ë£¹ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.log("=" * 50)
        self.log("ğŸ“„ Phase 2: Processing ungrouped files...")
        
        ungrouped_files = self.file_repo.get_ungrouped_data_files()
        ungrouped_skipped = 0
        
        if ungrouped_files:
            self.log(f"ğŸ“„ Found {len(ungrouped_files)} ungrouped files to analyze", indent=1)
            
            # Skip already analyzed files (FORCE_REANALYZE=falseì¸ ê²½ìš°)
            if not IndexingConfig.FORCE_REANALYZE:
                original_count = len(ungrouped_files)
                ungrouped_files = self._filter_unanalyzed_files_entity(ungrouped_files)
                ungrouped_skipped = original_count - len(ungrouped_files)
                if ungrouped_skipped > 0:
                    self.log(f"â­ï¸  Skipping {ungrouped_skipped} already analyzed files", indent=1)
            
            if ungrouped_files:
                # íŒŒì¼ ì •ë³´ ë¡œë“œ
                files_info = self._load_data_files_with_columns(ungrouped_files)
                
                if files_info:
                    # ë°°ì¹˜ ì²˜ë¦¬
                    batch_size = EntityIdentificationConfig.TABLE_BATCH_SIZE
                    for i in range(0, len(files_info), batch_size):
                        batch = files_info[i:i + batch_size]
                        batch_num = i // batch_size + 1
                        total_batches = (len(files_info) + batch_size - 1) // batch_size
                        
                        self.log(f"ğŸ“¦ Batch {batch_num}/{total_batches} ({len(batch)} tables)", indent=2)
                        
                        results, llm_calls = self._call_llm_for_entity_identification(batch)
                        all_results.extend(results)
                        total_llm_calls += llm_calls
                    
                    # DB ì €ì¥
                    saved_count = self._save_table_entities(files_info, all_results)
                    ungrouped_files_processed = saved_count
                    self.log(f"âœ… Saved {saved_count} table entities", indent=2)
            else:
                self.log("âœ… All ungrouped files already analyzed", indent=1)
        else:
            self.log("âš ï¸ No ungrouped files to analyze", indent=1)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ê²°ê³¼ ìš”ì•½
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        completed_at = datetime.now().isoformat()
        
        # í†µê³„ ê³„ì‚°
        entities_identified = sum(1 for r in all_results if r.row_represents != 'unknown')
        identifiers_found = sum(1 for r in all_results if r.entity_identifier is not None)
        high_conf = sum(1 for r in all_results if r.confidence >= EntityIdentificationConfig.CONFIDENCE_THRESHOLD)
        low_conf = sum(1 for r in all_results if r.confidence < EntityIdentificationConfig.CONFIDENCE_THRESHOLD)
        
        self.log("=" * 50)
        self.log("ğŸ“Š Summary:")
        self.log(f"ğŸ“¦ Groups processed: {groups_processed}", indent=1)
        self.log(f"ğŸ“ Group files propagated: {group_files_propagated}", indent=1)
        self.log(f"ğŸ“„ Ungrouped files processed: {ungrouped_files_processed}", indent=1)
        self.log(f"ğŸ¤– LLM calls: {total_llm_calls}", indent=1)
        self.log(f"ğŸ¯ High confidence: {high_conf}", indent=1)
        
        # ê²°ê³¼ ì¶œë ¥ (ë¹„ê·¸ë£¹ íŒŒì¼ë§Œ)
        if all_results:
            self.log("ğŸ“‹ Ungrouped Entity Results:")
            for result in all_results[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                identifier_str = result.entity_identifier or "(none)"
                conf_emoji = "ğŸŸ¢" if result.confidence >= EntityIdentificationConfig.CONFIDENCE_THRESHOLD else "ğŸŸ¡"
                self.log(f"{conf_emoji} {result.file_name}: {result.row_represents} [{identifier_str}]", indent=1)
            if len(all_results) > 10:
                self.log(f"... and {len(all_results) - 10} more", indent=1)
        
        phase_result = EntityIdentificationResult(
            total_tables=groups_processed + ungrouped_files_processed,
            tables_analyzed=groups_processed + len(all_results),
            entities_identified=groups_processed + entities_identified,
            identifiers_found=groups_processed + identifiers_found,
            high_confidence=groups_processed + high_conf,
            low_confidence=low_conf,
            llm_calls=total_llm_calls,
            started_at=started_at,
            completed_at=completed_at
        )
        
        return {
            "entity_identification_result": phase_result.model_dump(),
            "table_entity_results": [r.model_dump() for r in all_results],
            "groups_processed": groups_processed,
            "group_files_propagated": group_files_propagated,
            "logs": [
                f"ğŸ¯ [Entity Identification] Groups: {groups_processed} ({group_files_propagated} files), "
                f"Ungrouped: {ungrouped_files_processed}, LLM calls: {total_llm_calls}"
            ]
        }
    
    # =========================================================================
    # Skip Already Analyzed
    # =========================================================================
    
    def _filter_unanalyzed_groups_entity(
        self, 
        groups: List[Dict[str, Any]]
    ) -> tuple:
        """
        ì´ë¯¸ Entity ë¶„ì„ì´ ì™„ë£Œëœ ê·¸ë£¹ í•„í„°ë§
        
        Args:
            groups: ê·¸ë£¹ ëª©ë¡
        
        Returns:
            (ë¶„ì„í•  ê·¸ë£¹ ëª©ë¡, ìŠ¤í‚µëœ ê·¸ë£¹ ìˆ˜)
        """
        if not groups:
            return [], 0
        
        group_repo = self._get_group_repo()
        
        # llm_analyzed_atì´ NULLì¸ ê·¸ë£¹ë§Œ (get_groups_for_entity_analysisê°€ ì´ë¯¸ í•„í„°ë§í•˜ì§€ë§Œ í™•ì¸ìš©)
        to_process = []
        for group in groups:
            group_id = group.get('group_id')
            full_group = group_repo.get_group_by_id(group_id)
            if full_group and full_group.get('llm_analyzed_at') is None:
                to_process.append(group)
        
        skipped_count = len(groups) - len(to_process)
        return to_process, skipped_count
    
    def _filter_unanalyzed_files_entity(
        self, 
        file_paths: List[str]
    ) -> List[str]:
        """
        ì´ë¯¸ Entity ë¶„ì„ì´ ì™„ë£Œëœ íŒŒì¼ í•„í„°ë§
        
        table_entitiesì— í•´ë‹¹ íŒŒì¼ì´ ì—†ëŠ” ê²ƒë§Œ ë°˜í™˜
        
        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ëª©ë¡
        
        Returns:
            ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
        """
        if not file_paths:
            return []
        
        # table_entitiesì— ì—†ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
        to_process = []
        for file_path in file_paths:
            if not self.entity_repo.has_entity_for_file_path(file_path):
                to_process.append(file_path)
        
        return to_process
    
    @classmethod
    def run_standalone(cls, data_files: List[str]) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ
        
        Args:
            data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        initial_state = {
            'data_files': data_files
        }
        return node(initial_state)

