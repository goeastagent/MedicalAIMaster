# src/agents/nodes/entity_identification/node.py
"""
Entity Identification Node

ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ í–‰ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€(row_represents)ì™€
ê³ ìœ  ì‹ë³„ì ì»¬ëŸ¼(entity_identifier)ì„ ì‹ë³„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLMì„ ì‚¬ìš©í•´ ê° í…Œì´ë¸”ì˜ row_represents ì¶”ë¡  (surgery, patient, lab_result ë“±)
- ì»¬ëŸ¼ í†µê³„(unique count)ë¥¼ í™œìš©í•´ entity_identifier ì‹ë³„
- table_entities í…Œì´ë¸”ì— ê²°ê³¼ ì €ì¥
"""

import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from ...state import AgentState
from ...models.llm_responses import (
    TableEntityResult,
    EntityIdentificationResponse,
    EntityIdentificationResult,
)
from ...base import BaseNode, LLMMixin, DatabaseMixin
from ...registry import register_node
from src.database import OntologySchemaManager
from src.config import EntityIdentificationConfig, LLMConfig
from .prompts import EntityIdentificationPrompt


@register_node
class EntityIdentificationNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Entity Identification Node (LLM-based)
    
    ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ í–‰ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€(row_represents)ì™€
    ê³ ìœ  ì‹ë³„ì ì»¬ëŸ¼(entity_identifier)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Input (from state):
        - data_files: is_metadata=falseì¸ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        - data_semantic_result: ì´ì „ ë‹¨ê³„ ì™„ë£Œ ì •ë³´ (column_metadata ë¶„ì„ ì™„ë£Œ)
    
    Output:
        - entity_identification_result: EntityIdentificationResult í˜•íƒœ
        - table_entity_results: TableEntityResult ëª©ë¡
    """
    
    name = "entity_identification"
    description = "Entity ì‹ë³„ (row_represents, entity_identifier)"
    order = 800
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = EntityIdentificationPrompt
    
    def _load_data_files_with_columns(self, data_files: List[str]) -> List[Dict[str, Any]]:
        """
        ë°ì´í„° íŒŒì¼ê³¼ ê·¸ ì»¬ëŸ¼ ì •ë³´ë¥¼ DBì—ì„œ ë¡œë“œ
        
        Uses:
          - FileRepository.get_files_by_paths()
          - ColumnRepository.get_columns_with_stats()
          - ColumnRepository.get_columns_with_semantic()
        
        Returns:
            [
                {
                    "file_id": "uuid",
                    "file_name": "clinical_data.csv",
                    "row_count": 6388,
                    "columns": [
                        {
                            "original_name": "caseid",
                            "semantic_name": "Case ID",
                            "column_type": "categorical",
                            "concept_category": "Identifiers",
                            "unique_count": 6388,
                            ...
                        },
                        ...
                    ]
                },
                ...
            ]
        """
        if not data_files:
            return []
        
        files_info = []
        
        try:
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            files = self.file_repo.get_files_by_paths(data_files)
            
            for f in files:
                file_id = f['file_id']
                file_name = f['file_name']
                file_path = f['file_path']
                
                # row_count ì¶”ì¶œ
                metadata = f.get('file_metadata', {})
                row_count = metadata.get('row_count', 0) if metadata else 0
                
                # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ (í†µê³„ + semantic)
                # get_columns_with_stats() + get_columns_with_semantic() ë³‘í•©
                cols_stats = self.column_repo.get_columns_with_stats(file_id)
                cols_semantic = self.column_repo.get_columns_with_semantic(file_id)
                
                # ë³‘í•©: col_id ê¸°ì¤€
                semantic_map = {c['col_id']: c for c in cols_semantic}
                
                columns = []
                for col in cols_stats:
                    col_id = col['col_id']
                    sem_info = semantic_map.get(col_id, {})
                    
                    columns.append({
                        "original_name": col['original_name'],
                        "column_role": col.get('column_role'),
                        "semantic_name": sem_info.get('semantic_name'),
                        "column_type": col.get('column_type'),
                        "concept_category": sem_info.get('concept_category'),
                        "unique_count": col.get('unique_count'),
                        "column_info": col.get('column_info', {})
                    })
                
                # filename_values ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’ë“¤)
                filename_values = f.get('filename_values', {})
                
                files_info.append({
                    "file_id": file_id,
                    "file_name": file_name,
                    "row_count": row_count or 0,
                    "file_path": file_path,
                    "columns": columns,
                    "filename_values": filename_values
                })
        
        except Exception as e:
            self.log(f"âŒ Error loading data files: {e}")
            import traceback
            traceback.print_exc()
        
        return files_info
    
    def _build_tables_context(self, files_info: List[Dict[str, Any]]) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ìš© í…Œì´ë¸” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        column_roleì„ í™œìš©í•˜ì—¬ identifier í›„ë³´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.
        filename_valuesë„ í¬í•¨í•˜ì—¬ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Args:
            files_info: _load_data_files_with_columns()ì˜ ê²°ê³¼
        
        Returns:
            í¬ë§·ëœ ë¬¸ìì—´
        """
        lines = []
        
        for file_info in files_info:
            file_name = file_info['file_name']
            row_count = file_info['row_count']
            columns = file_info['columns']
            filename_values = file_info.get('filename_values', {})
            
            lines.append(f"\n## {file_name}")
            lines.append(f"Rows: {row_count:,}")
            
            # filename_values í‘œì‹œ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ê°’)
            if filename_values:
                lines.append("Filename-extracted values (embedded in filename):")
                for key, value in filename_values.items():
                    lines.append(f"  - {key}: {value}")
            
            lines.append("Columns:")
            
            max_cols = EntityIdentificationConfig.MAX_COLUMNS_PER_TABLE
            display_cols = columns[:max_cols] if max_cols > 0 else columns
            
            for col in display_cols:
                name = col['original_name']
                col_role = col.get('column_role')
                semantic = col.get('semantic_name') or name
                concept = col.get('concept_category')
                col_type = col.get('column_type') or '-'
                unique_count = col.get('unique_count')
                
                # column_roleì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                if col_role == 'identifier':
                    # identifierëŠ” ê°•ì¡° í‘œì‹œ
                    line = f"  - {name} ğŸ”‘[IDENTIFIER]"
                    if unique_count is not None:
                        line += f" unique: {unique_count:,}"
                        # row_countì™€ ë¹„êµí•˜ì—¬ unique identifier í›„ë³´ í‘œì‹œ
                        if row_count > 0 and unique_count == row_count:
                            line += " â† matches row count!"
                elif col_role == 'parameter_name':
                    # parameterëŠ” semantic ì •ë³´ í¬í•¨
                    line = f"  - {name}"
                    if semantic and semantic != name:
                        line += f" ({semantic})"
                    if concept:
                        line += f" [{concept}]"
                elif col_role == 'timestamp':
                    line = f"  - {name} [timestamp]"
                elif col_role == 'attribute':
                    line = f"  - {name} [attribute]"
                    if unique_count is not None:
                        line += f" unique: {unique_count:,}"
                else:
                    # ê¸°íƒ€ ì»¬ëŸ¼
                    line = f"  - {name} [{col_type}]"
                    if EntityIdentificationConfig.SHOW_UNIQUE_COUNTS and unique_count is not None:
                        line += f" unique: {unique_count:,}"
                
                lines.append(line)
            
            if max_cols > 0 and len(columns) > max_cols:
                lines.append(f"  ... and {len(columns) - max_cols} more columns")
        
        return "\n".join(lines)
    
    def _call_llm_for_entity_identification(
        self,
        files_info: List[Dict[str, Any]]
    ) -> Tuple[List[TableEntityResult], int]:
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ Entity ì‹ë³„
        
        Args:
            files_info: í…Œì´ë¸” ì •ë³´ ëª©ë¡
        
        Returns:
            (ê²°ê³¼ ëª©ë¡, LLM í˜¸ì¶œ íšŸìˆ˜)
        """
        if not files_info:
            return [], 0
        
        tables_context = self._build_tables_context(files_info)
        
        # PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_class.build(tables_context=tables_context)
        
        self.log(f"ğŸ“¤ Sending {len(files_info)} tables to LLM...", indent=1)
        
        llm_calls = 0
        results = []
        
        for attempt in range(EntityIdentificationConfig.MAX_RETRIES):
            try:
                response = self.call_llm_json(
                    prompt,
                    max_tokens=LLMConfig.MAX_TOKENS
                )
                llm_calls += 1
                
                if response and 'tables' in response:
                    for table_data in response['tables']:
                        result = TableEntityResult(
                            file_name=table_data.get('file_name', ''),
                            row_represents=table_data.get('row_represents', 'unknown'),
                            entity_identifier=table_data.get('entity_identifier'),
                            confidence=float(table_data.get('confidence', 0.0)),
                            reasoning=table_data.get('reasoning', '')
                        )
                        results.append(result)
                    
                    return results, llm_calls
                else:
                    self.log(f"âš ï¸ Invalid LLM response format, attempt {attempt + 1}", indent=1)
                    
            except Exception as e:
                self.log(f"âŒ LLM call failed (attempt {attempt + 1}): {e}", indent=1)
                if attempt < EntityIdentificationConfig.MAX_RETRIES - 1:
                    time.sleep(EntityIdentificationConfig.RETRY_DELAY_SECONDS)
        
        return results, llm_calls
    
    def _save_table_entities(
        self,
        files_info: List[Dict[str, Any]],
        llm_results: List[TableEntityResult]
    ) -> int:
        """
        LLM ê²°ê³¼ë¥¼ table_entities í…Œì´ë¸”ì— ì €ì¥
        
        Args:
            files_info: íŒŒì¼ ì •ë³´ (file_id í¬í•¨)
            llm_results: LLM ë¶„ì„ ê²°ê³¼
        
        Returns:
            ì €ì¥ëœ ì—”í‹°í‹° ìˆ˜
        """
        # file_name â†’ file_id ë§¤í•‘ ìƒì„±
        name_to_info = {f['file_name']: f for f in files_info}
        
        entities_to_save = []
        
        for result in llm_results:
            file_info = name_to_info.get(result.file_name)
            if not file_info:
                self.log(f"âš ï¸ File not found: {result.file_name}", indent=1)
                continue
            
            entities_to_save.append({
                "file_id": file_info['file_id'],
                "row_represents": result.row_represents,
                "entity_identifier": result.entity_identifier,
                "confidence": result.confidence,
                "reasoning": result.reasoning
            })
        
        if entities_to_save:
            self.entity_repo.save_table_entities(entities_to_save)
        
        return len(entities_to_save)
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Entity Identification ì‹¤í–‰
        
        ë°ì´í„° íŒŒì¼ì˜ í–‰ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€(row_represents)ì™€
        ê³ ìœ  ì‹ë³„ì ì»¬ëŸ¼(entity_identifier)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
        """
        started_at = datetime.now().isoformat()
        
        # 1. ë°ì´í„° íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        data_files = state.get('data_files', [])
        
        if not data_files:
            self.log("â„¹ï¸  No data files to analyze")
            return {
                "entity_identification_result": EntityIdentificationResult(
                    started_at=started_at,
                    completed_at=datetime.now().isoformat()
                ).model_dump(),
                "table_entity_results": []
            }
        
        self.log(f"ğŸ“ Data files to analyze: {len(data_files)}")
        for f in data_files[:5]:
            self.log(f"- {f}", indent=1)
        if len(data_files) > 5:
            self.log(f"... and {len(data_files) - 5} more", indent=1)
        
        # 2. Ontology ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
        schema_manager = OntologySchemaManager()
        schema_manager.create_tables()
        
        # 3. ë°ì´í„° íŒŒì¼ê³¼ ì»¬ëŸ¼ ì •ë³´ ë¡œë“œ
        self.log("ğŸ“¥ Loading data files with column info...")
        files_info = self._load_data_files_with_columns(data_files)
        
        if not files_info:
            self.log("âš ï¸  No file info loaded from database")
            return {
                "entity_identification_result": EntityIdentificationResult(
                    total_tables=len(data_files),
                    started_at=started_at,
                    completed_at=datetime.now().isoformat()
                ).model_dump(),
                "table_entity_results": []
            }
        
        self.log(f"âœ… Loaded {len(files_info)} files with column info", indent=1)
        
        # 4. LLM í˜¸ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)
        self.log("ğŸ¤– Calling LLM for entity identification...")
        
        all_results: List[TableEntityResult] = []
        total_llm_calls = 0
        
        # ë°°ì¹˜ í¬ê¸°ì— ë”°ë¼ ë¶„í• 
        batch_size = EntityIdentificationConfig.TABLE_BATCH_SIZE
        for i in range(0, len(files_info), batch_size):
            batch = files_info[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(files_info) + batch_size - 1) // batch_size
            
            self.log(f"ğŸ“¦ Batch {batch_num}/{total_batches} ({len(batch)} tables)", indent=1)
            
            results, llm_calls = self._call_llm_for_entity_identification(batch)
            all_results.extend(results)
            total_llm_calls += llm_calls
            
            self.log(f"âœ… Got {len(results)} results", indent=2)
        
        # 5. DB ì €ì¥
        self.log("ğŸ’¾ Saving to table_entities...")
        saved_count = self._save_table_entities(files_info, all_results)
        self.log(f"âœ… Saved {saved_count} table entities", indent=1)
        
        # 6. í†µê³„ ê³„ì‚°
        entities_identified = sum(1 for r in all_results if r.row_represents != 'unknown')
        identifiers_found = sum(1 for r in all_results if r.entity_identifier is not None)
        high_conf = sum(1 for r in all_results if r.confidence >= EntityIdentificationConfig.CONFIDENCE_THRESHOLD)
        low_conf = sum(1 for r in all_results if r.confidence < EntityIdentificationConfig.CONFIDENCE_THRESHOLD)
        
        # 7. ê²°ê³¼ ì¶œë ¥
        self.log(f"Total tables: {len(files_info)}", indent=1)
        self.log(f"Analyzed: {len(all_results)}", indent=1)
        self.log(f"Entities identified: {entities_identified}", indent=1)
        self.log(f"With unique identifier: {identifiers_found}", indent=1)
        self.log(f"High confidence (â‰¥{EntityIdentificationConfig.CONFIDENCE_THRESHOLD}): {high_conf}", indent=1)
        self.log(f"Low confidence: {low_conf}", indent=1)
        self.log(f"LLM calls: {total_llm_calls}", indent=1)
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        self.log("ğŸ“‹ Entity Results:")
        for result in all_results:
            identifier_str = result.entity_identifier or "(none)"
            conf_emoji = "ğŸŸ¢" if result.confidence >= EntityIdentificationConfig.CONFIDENCE_THRESHOLD else "ğŸŸ¡"
            self.log(f"{conf_emoji} {result.file_name}", indent=1)
            self.log(f"row_represents: {result.row_represents}", indent=2)
            self.log(f"entity_identifier: {identifier_str}", indent=2)
            self.log(f"confidence: {result.confidence:.2f}", indent=2)
        
        # 8. ê²°ê³¼ ë°˜í™˜
        completed_at = datetime.now().isoformat()
        
        phase_result = EntityIdentificationResult(
            total_tables=len(files_info),
            tables_analyzed=len(all_results),
            entities_identified=entities_identified,
            identifiers_found=identifiers_found,
            high_confidence=high_conf,
            low_confidence=low_conf,
            llm_calls=total_llm_calls,
            started_at=started_at,
            completed_at=completed_at
        )
        
        return {
            "entity_identification_result": phase_result.model_dump(),
            "table_entity_results": [r.model_dump() for r in all_results]
        }
    
    @classmethod
    def run_standalone(cls, data_files: List[str]) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ
        
        Args:
            data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        initial_state = {
            'data_files': data_files
        }
        return node(initial_state)

