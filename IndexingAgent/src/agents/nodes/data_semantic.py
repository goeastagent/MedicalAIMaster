# src/agents/nodes/data_semantic.py
"""
Phase 1B: Data Semantic Analysis Node

ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ 
data_dictionaryì™€ ì—°ê²°í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLMì„ ì‚¬ìš©í•´ ê° ì»¬ëŸ¼ì˜ semantic_name, unit, description ì¶”ë¡ 
- data_dictionaryì˜ parameter_keyì™€ ë§¤ì¹­ ì‹œë„
- column_metadataì— ê²°ê³¼ ì €ì¥ (dict_entry_id, dict_match_status)
- íŒŒì¼ë‹¹ ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ LLM í˜¸ì¶œ
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from ..state import AgentState
from ..models.llm_responses import (
    ColumnSemanticResult,
    DataSemanticResponse,
    DataSemanticResult,
)
from src.database.connection import get_db_manager
from src.utils.llm_client import get_llm_client
from src.config import Phase6Config, LLMConfig


# =============================================================================
# LLM Prompt Template
# =============================================================================

COLUMN_SEMANTIC_PROMPT = """You are a Medical Data Expert analyzing clinical data columns.

[Task]
Analyze each column and provide semantic information.
Use the Parameter Dictionary and column statistics to make accurate judgments.

{dict_section}

[File: {file_name}]
Type: {file_type}
Rows: {row_count}

[Columns to Analyze with Statistics]
{columns_info}

[CRITICAL RULES for dict_entry_key]
1. MUST be EXACTLY one of the keys from "EXACT Parameter Keys" above (if provided)
2. Copy the key exactly as shown (including "/" and special characters)
3. If no matching key exists â†’ set to null
4. If uncertain (confidence < 0.7) â†’ set to null
5. Use column statistics (min/max/values) to help identify the correct match

[Output Format]
Return ONLY valid JSON (no markdown, no explanation):
{{
  "columns": [
    {{
      "original_name": "age",
      "semantic_name": "Age",
      "unit": "years",
      "description": "Patient age at time of surgery",
      "concept_category": "Demographics",
      "dict_entry_key": "age",
      "match_confidence": 0.99,
      "reasoning": "Exact name match, values 20-90 consistent with age"
    }},
    {{
      "original_name": "unknown_col",
      "semantic_name": "Unknown Parameter",
      "unit": null,
      "description": "Unable to determine meaning",
      "concept_category": "Other",
      "dict_entry_key": null,
      "match_confidence": 0.0,
      "reasoning": "No matching parameter found in dictionary"
    }}
  ]
}}
"""

DICT_SECTION_TEMPLATE = """[EXACT Parameter Keys - Use these values ONLY]
{dict_keys_list}

[Parameter Definitions]
{dict_context}
"""

DICT_SECTION_EMPTY = """[Note]
No parameter dictionary is available for this dataset.
Infer semantic meaning from column names and statistics using your medical knowledge.
Set dict_entry_key to null for all columns.
"""


# =============================================================================
# Helper Functions
# =============================================================================

def _load_data_dictionary(db) -> List[Dict[str, Any]]:
    """
    data_dictionary í…Œì´ë¸”ì—ì„œ ëª¨ë“  ì—”íŠ¸ë¦¬ ë¡œë“œ
    
    Returns:
        List of dict with keys: dict_id, parameter_key, parameter_desc, parameter_unit, extra_info
    """
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            SELECT dict_id, parameter_key, parameter_desc, parameter_unit, extra_info
            FROM data_dictionary
            ORDER BY parameter_key
        """)
        rows = cursor.fetchall()
        
        entries = []
        for row in rows:
            dict_id, key, desc, unit, extra = row
            entries.append({
                'dict_id': str(dict_id),
                'parameter_key': key,
                'parameter_desc': desc,
                'parameter_unit': unit,
                'extra_info': extra if isinstance(extra, dict) else {}
            })
        
        return entries
        
    except Exception as e:
        print(f"   âš ï¸ Error loading data_dictionary: {e}")
        return []


def _build_dict_context(dictionary: List[Dict]) -> Tuple[str, str, Dict[str, str]]:
    """
    data_dictionaryë¥¼ LLM context ë¬¸ìì—´ë¡œ ë³€í™˜
    
    Returns:
        (dict_keys_list, dict_context, key_to_id_map)
    """
    if not dictionary:
        return "", "", {}
    
    # Key ëª©ë¡ (ì •í™•í•œ ë§¤ì¹­ìš©)
    keys = [f'"{e["parameter_key"]}"' for e in dictionary]
    dict_keys_list = ", ".join(keys)
    
    # ìƒì„¸ ì •ì˜ (LLMì´ ì˜ë¯¸ íŒŒì•…ìš©)
    lines = []
    key_to_id_map = {}
    
    for entry in dictionary:
        key = entry['parameter_key']
        desc = entry['parameter_desc'] or ''
        unit = entry['parameter_unit'] or '-'
        extra = entry.get('extra_info', {})
        
        key_to_id_map[key] = entry['dict_id']
        
        line = f'- "{key}": {desc}'
        if unit and unit != '-':
            line += f' ({unit})'
        if extra:
            extra_items = list(extra.items())[:2]  # ìµœëŒ€ 2ê°œ
            if extra_items:
                extra_str = ", ".join(f"{k}={v}" for k, v in extra_items)
                line += f' [{extra_str}]'
        lines.append(line)
    
    dict_context = "\n".join(lines)
    
    return dict_keys_list, dict_context, key_to_id_map


def _get_columns_with_stats(db, file_id: str) -> List[Dict]:
    """
    íŠ¹ì • íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ì™€ í†µê³„ë¥¼ ì¡°íšŒ
    
    Returns:
        List of column info dicts
    """
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            SELECT col_id, original_name, column_type, data_type, 
                   column_info, value_distribution
            FROM column_metadata
            WHERE file_id = %s
            ORDER BY col_id
        """, (file_id,))
        
        rows = cursor.fetchall()
        columns = []
        
        for row in rows:
            col_id, name, col_type, dtype, col_info, val_dist = row
            columns.append({
                'col_id': col_id,
                'original_name': name,
                'column_type': col_type or 'unknown',
                'data_type': dtype or 'unknown',
                'column_info': col_info if isinstance(col_info, dict) else {},
                'value_distribution': val_dist if isinstance(val_dist, dict) else {}
            })
        
        return columns
        
    except Exception as e:
        print(f"   âš ï¸ Error loading columns: {e}")
        return []


def _build_columns_info(columns: List[Dict], config: Phase6Config) -> str:
    """
    ì»¬ëŸ¼ ì •ë³´ + í†µê³„ë¥¼ LLM context ë¬¸ìì—´ë¡œ ë³€í™˜
    
    Args:
        columns: ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        config: Phase1B ì„¤ì • (í‘œì‹œ ê°œìˆ˜ ì œí•œ)
    
    Returns:
        í¬ë§·ëœ ì»¬ëŸ¼ ì •ë³´ ë¬¸ìì—´
    """
    lines = []
    
    for col in columns:
        name = col['original_name']
        dtype = col['data_type']
        col_type = col['column_type']
        info = col.get('column_info', {}) or {}
        dist = col.get('value_distribution', {}) or {}
        
        # ê¸°ë³¸ ì •ë³´
        line = f"- {name} ({dtype}, {col_type})"
        details = []
        
        # Continuous: min, max, mean
        if col_type == 'continuous':
            min_val = info.get('min')
            max_val = info.get('max')
            mean_val = info.get('mean')
            if min_val is not None and max_val is not None:
                range_str = f"range: [{min_val:.2f}, {max_val:.2f}]"
                if mean_val is not None:
                    range_str += f", mean: {mean_val:.2f}"
                details.append(range_str)
        
        # Categorical: unique values
        if col_type == 'categorical':
            unique_vals = dist.get('unique_values', [])
            n_unique = len(unique_vals)
            if n_unique > 0:
                max_show = config.MAX_UNIQUE_VALUES_DISPLAY
                if n_unique <= max_show:
                    details.append(f"values ({n_unique}): {unique_vals}")
                else:
                    details.append(f"values ({n_unique} unique): {unique_vals[:max_show]}...")
        
        # Datetime: date range
        if info.get('is_datetime'):
            min_dt = info.get('min_date')
            max_dt = info.get('max_date')
            if min_dt:
                details.append(f"date_range: [{min_dt}, {max_dt}]")
        
        # Samples
        samples = dist.get('samples', [])[:config.MAX_SAMPLES_DISPLAY]
        if samples:
            details.append(f"samples: {samples}")
        
        # ì¡°í•©
        if details:
            line += "\n    " + "\n    ".join(details)
        
        lines.append(line)
    
    return "\n".join(lines)


def _resolve_dict_entry_id(
    llm_key: Optional[str],
    key_to_id_map: Dict[str, str]
) -> Tuple[Optional[str], str]:
    """
    LLMì´ ë°˜í™˜í•œ keyë¥¼ dict_idì™€ statusë¡œ ë³€í™˜
    
    Args:
        llm_key: LLMì´ ë°˜í™˜í•œ dict_entry_key (None ê°€ëŠ¥)
        key_to_id_map: {parameter_key: dict_id} ë§¤í•‘
    
    Returns:
        (dict_id or None, status)
        status: 'matched', 'not_found', 'null_from_llm'
    """
    if llm_key is None:
        return (None, 'null_from_llm')
    
    if llm_key in key_to_id_map:
        return (key_to_id_map[llm_key], 'matched')
    
    # LLMì´ keyë¥¼ ë°˜í™˜í–ˆì§€ë§Œ dictionaryì— ì—†ìŒ
    print(f"   âš ï¸ Key '{llm_key}' not found in dictionary")
    return (None, 'not_found')


def _call_llm_for_semantic(
    llm_client,
    file_info: Dict,
    columns: List[Dict],
    dict_keys_list: str,
    dict_context: str,
    config: Phase6Config
) -> Optional[DataSemanticResponse]:
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì‹œë§¨í‹± ë¶„ì„ ìˆ˜í–‰
    
    Args:
        llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        file_info: íŒŒì¼ ì •ë³´ (file_name, file_type, row_count)
        columns: ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡
        dict_keys_list: dictionary key ëª©ë¡ ë¬¸ìì—´
        dict_context: dictionary ìƒì„¸ ì •ë³´ ë¬¸ìì—´
        config: Phase1B ì„¤ì •
    
    Returns:
        DataSemanticResponse or None
    """
    # Dictionary section êµ¬ì„±
    if dict_keys_list:
        dict_section = DICT_SECTION_TEMPLATE.format(
            dict_keys_list=dict_keys_list,
            dict_context=dict_context
        )
    else:
        dict_section = DICT_SECTION_EMPTY
    
    # Columns info êµ¬ì„±
    columns_info = _build_columns_info(columns, config)
    
    # Prompt êµ¬ì„±
    prompt = COLUMN_SEMANTIC_PROMPT.format(
        dict_section=dict_section,
        file_name=file_info.get('file_name', 'unknown'),
        file_type=file_info.get('file_type', 'tabular'),
        row_count=file_info.get('row_count', 'unknown'),
        columns_info=columns_info
    )
    
    try:
        response = llm_client.ask_json(
            prompt=prompt,
            max_tokens=LLMConfig.MAX_TOKENS_COLUMN_ANALYSIS
        )
        
        if not response:
            return None
        
        # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
        columns_data = response.get('columns', [])
        column_results = []
        for col_data in columns_data:
            try:
                col_result = ColumnSemanticResult(**col_data)
                column_results.append(col_result)
            except Exception as e:
                print(f"   âš ï¸ Error parsing column result: {e}")
                continue
        
        return DataSemanticResponse(
            columns=column_results,
            file_summary=response.get('file_summary')
        )
        
    except json.JSONDecodeError as e:
        print(f"   âŒ JSON parsing error: {e}")
        return None
    except Exception as e:
        print(f"   âŒ LLM call error: {e}")
        return None


def _update_column_metadata_batch(
    db,
    file_id: str,
    results: List[ColumnSemanticResult],
    key_to_id_map: Dict[str, str]
) -> Dict[str, int]:
    """
    column_metadata í…Œì´ë¸”ì„ ë°°ì¹˜ ì—…ë°ì´íŠ¸
    
    Args:
        db: DB ë§¤ë‹ˆì €
        file_id: íŒŒì¼ ID
        results: LLM ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        key_to_id_map: {parameter_key: dict_id} ë§¤í•‘
    
    Returns:
        í†µê³„ dict: {matched: n, not_found: n, null_from_llm: n}
    """
    conn = db.get_connection()
    cursor = conn.cursor()
    
    stats = {'matched': 0, 'not_found': 0, 'null_from_llm': 0}
    now = datetime.now()
    
    try:
        for result in results:
            # dict_entry_id í•´ì„
            dict_id, status = _resolve_dict_entry_id(
                result.dict_entry_key,
                key_to_id_map
            )
            stats[status] = stats.get(status, 0) + 1
            
            # UPDATE ì¿¼ë¦¬
            cursor.execute("""
                UPDATE column_metadata
                SET semantic_name = %s,
                    unit = %s,
                    description = %s,
                    concept_category = %s,
                    dict_entry_id = %s,
                    dict_match_status = %s,
                    match_confidence = %s,
                    llm_confidence = %s,
                    llm_analyzed_at = %s
                WHERE file_id = %s AND original_name = %s
            """, (
                result.semantic_name,
                result.unit,
                result.description,
                result.concept_category,
                dict_id,
                status,
                result.match_confidence,
                result.match_confidence,  # llm_confidenceë„ ë™ì¼í•˜ê²Œ
                now,
                file_id,
                result.original_name
            ))
        
        conn.commit()
        return stats
        
    except Exception as e:
        conn.rollback()
        print(f"   âŒ Error updating column_metadata: {e}")
        raise


def _get_data_files_info(db, data_files: List[str]) -> List[Dict]:
    """
    ë°ì´í„° íŒŒì¼ë“¤ì˜ ì •ë³´ ì¡°íšŒ
    
    Args:
        db: DB ë§¤ë‹ˆì €
        data_files: íŒŒì¼ ê²½ë¡œ ëª©ë¡
    
    Returns:
        íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (file_id, file_path, file_name, row_count ë“±)
    """
    if not data_files:
        return []
    
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        # íŒŒì¼ ê²½ë¡œë¡œ ì¡°íšŒ
        placeholders = ','.join(['%s'] * len(data_files))
        cursor.execute(f"""
            SELECT file_id, file_path, file_name, processor_type, raw_stats
            FROM file_catalog
            WHERE file_path IN ({placeholders})
            ORDER BY file_name
        """, tuple(data_files))
        
        rows = cursor.fetchall()
        files = []
        
        for row in rows:
            file_id, path, name, proc_type, raw_stats = row
            stats = raw_stats if isinstance(raw_stats, dict) else {}
            files.append({
                'file_id': str(file_id),
                'file_path': path,
                'file_name': name,
                'file_type': proc_type or 'tabular',
                'row_count': stats.get('row_count', 'unknown')
            })
        
        return files
        
    except Exception as e:
        print(f"   âš ï¸ Error loading file info: {e}")
        return []


# =============================================================================
# Main Node Function
# =============================================================================

def phase6_data_semantic_node(state: AgentState) -> AgentState:
    """
    Phase 1B: Data Semantic Analysis Node
    
    ë°ì´í„° íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  data_dictionaryì™€ ì—°ê²°
    
    Input State:
        - data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ëª©ë¡
        - (DB) data_dictionary: Phase 1Aì—ì„œ ìƒì„±ëœ parameter definitions
        - (DB) column_metadata: Phase 0ì—ì„œ ìƒì„±ëœ ì»¬ëŸ¼ ì •ë³´ + í†µê³„
    
    Output State:
        - phase1b_result: DataSemanticResult
        - data_semantic_entries: ë¶„ì„ëœ ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        - (DB) column_metadata ì—…ë°ì´íŠ¸: semantic_name, unit, dict_entry_id ë“±
    """
    print("\n" + "="*60)
    print("ğŸ”¬ Phase 6: Data Semantic Analysis")
    print("="*60)
    
    started_at = datetime.now().isoformat()
    config = Phase6Config
    
    # ë°ì´í„° íŒŒì¼ ëª©ë¡
    data_files = state.get('data_files', [])
    
    if not data_files:
        print("âš ï¸ No data files to analyze")
        return {
            **state,
            'phase6_result': DataSemanticResult(
                total_data_files=0,
                started_at=started_at,
                completed_at=datetime.now().isoformat()
            ).dict(),
            'data_semantic_entries': []
        }
    
    print(f"ğŸ“ Data files to analyze: {len(data_files)}")
    
    # DB ë° LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    db = get_db_manager()
    llm_client = get_llm_client()
    
    # 1. data_dictionary ë¡œë“œ
    print("\nğŸ“– Loading data dictionary...")
    dictionary = _load_data_dictionary(db)
    print(f"   Found {len(dictionary)} parameter definitions")
    
    # Dictionary context êµ¬ì„±
    dict_keys_list, dict_context, key_to_id_map = _build_dict_context(dictionary)
    
    # 2. íŒŒì¼ ì •ë³´ ì¡°íšŒ
    files_info = _get_data_files_info(db, data_files)
    print(f"   Loaded info for {len(files_info)} files")
    
    # ê²°ê³¼ ì¶”ì 
    total_columns = 0
    total_matched = 0
    total_not_found = 0
    total_null_from_llm = 0
    columns_by_file = {}
    llm_calls = 0
    batches_processed = 0
    all_entries = []
    
    # 3. íŒŒì¼ë³„ ì²˜ë¦¬
    for file_info in files_info:
        file_id = file_info['file_id']
        file_name = file_info['file_name']
        
        print(f"\nğŸ“„ Processing: {file_name}")
        
        # ì»¬ëŸ¼ ì •ë³´ ë¡œë“œ
        columns = _get_columns_with_stats(db, file_id)
        n_cols = len(columns)
        print(f"   Columns: {n_cols}")
        
        if not columns:
            continue
        
        columns_by_file[file_name] = n_cols
        total_columns += n_cols
        
        # ë°°ì¹˜ ë¶„í•  (ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´)
        batch_size = config.COLUMN_BATCH_SIZE
        batches = [columns[i:i+batch_size] for i in range(0, n_cols, batch_size)]
        
        if len(batches) > 1:
            print(f"   Splitting into {len(batches)} batches (batch_size={batch_size})")
        
        file_results = []
        
        for batch_idx, batch_cols in enumerate(batches):
            if len(batches) > 1:
                print(f"   Batch {batch_idx + 1}/{len(batches)} ({len(batch_cols)} columns)")
            
            # LLM í˜¸ì¶œ
            response = _call_llm_for_semantic(
                llm_client,
                file_info,
                batch_cols,
                dict_keys_list,
                dict_context,
                config
            )
            llm_calls += 1
            batches_processed += 1
            
            if response and response.columns:
                # DB ì—…ë°ì´íŠ¸
                stats = _update_column_metadata_batch(
                    db, file_id, response.columns, key_to_id_map
                )
                
                total_matched += stats.get('matched', 0)
                total_not_found += stats.get('not_found', 0)
                total_null_from_llm += stats.get('null_from_llm', 0)
                
                file_results.extend([c.dict() for c in response.columns])
                
                print(f"   âœ“ Analyzed {len(response.columns)} columns "
                      f"(matched: {stats.get('matched', 0)}, "
                      f"not_found: {stats.get('not_found', 0)}, "
                      f"null: {stats.get('null_from_llm', 0)})")
            else:
                print(f"   âš ï¸ No results from LLM")
        
        # ê²°ê³¼ ì €ì¥
        all_entries.extend(file_results)
    
    # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
    completed_at = datetime.now().isoformat()
    
    result = DataSemanticResult(
        total_data_files=len(files_info),
        processed_files=len(files_info),
        total_columns_analyzed=total_columns,
        columns_matched=total_matched,
        columns_not_found=total_not_found,
        columns_null_from_llm=total_null_from_llm,
        columns_by_file=columns_by_file,
        batches_processed=batches_processed,
        llm_calls=llm_calls,
        started_at=started_at,
        completed_at=completed_at
    )
    
    print("\n" + "="*60)
    print("âœ… Phase 6 Complete!")
    print(f"   Files processed: {result.processed_files}")
    print(f"   Columns analyzed: {result.total_columns_analyzed}")
    print(f"   Dictionary matches: {result.columns_matched}")
    print(f"   Not found in dict: {result.columns_not_found}")
    print(f"   Null from LLM: {result.columns_null_from_llm}")
    print(f"   LLM calls: {result.llm_calls}")
    print(f"   Batches: {result.batches_processed}")
    print("="*60)
    
    return {
        **state,
        'phase6_result': result.dict(),
        'data_semantic_entries': all_entries
    }

