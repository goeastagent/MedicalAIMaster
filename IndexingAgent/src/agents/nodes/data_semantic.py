# src/agents/nodes/data_semantic.py
"""
Data Semantic Analysis Node

ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ 
data_dictionaryì™€ ì—°ê²°í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLMì„ ì‚¬ìš©í•´ ê° ì»¬ëŸ¼ì˜ semantic_name, unit, description ì¶”ë¡ 
- data_dictionaryì˜ parameter_keyì™€ ë§¤ì¹­ ì‹œë„
- column_metadataì— ê²°ê³¼ ì €ì¥ (dict_entry_id, dict_match_status)
- íŒŒì¼ë‹¹ ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ LLM í˜¸ì¶œ
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from ..state import AgentState
from ..models.llm_responses import (
    ColumnSemanticResult,
    DataSemanticResponse,
    DataSemanticResult,
)
from ..base import BaseNode, LLMMixin, DatabaseMixin
from ..registry import register_node
from src.database import (
    FileRepository,
    ColumnRepository,
    DictionaryRepository,
)
from src.config import DataSemanticConfig, LLMConfig


# =============================================================================
# LLM Prompt Templates
# =============================================================================

COLUMN_SEMANTIC_PROMPT = """You are a Medical Data Expert analyzing clinical data columns.

[Task]
Analyze each column and provide semantic information.
Use the Parameter Dictionary and column statistics to make accurate judgments.

{dict_section}

[File: {file_name}]
Type: {file_type}
Rows: {row_count}

[Columns to Analyze with Statistics]
{columns_info}

[CRITICAL RULES for dict_entry_key]
1. MUST be EXACTLY one of the keys from "EXACT Parameter Keys" above (if provided)
2. Copy the key exactly as shown (including "/" and special characters)
3. If no matching key exists â†’ set to null
4. If uncertain (confidence < 0.7) â†’ set to null
5. Use column statistics (min/max/values) to help identify the correct match

[Output Format]
Return ONLY valid JSON (no markdown, no explanation):
{{
  "columns": [
    {{
      "original_name": "age",
      "semantic_name": "Age",
      "unit": "years",
      "description": "Patient age at time of surgery",
      "concept_category": "Demographics",
      "dict_entry_key": "age",
      "match_confidence": 0.99,
      "reasoning": "Exact name match, values 20-90 consistent with age"
    }},
    {{
      "original_name": "unknown_col",
      "semantic_name": "Unknown Parameter",
      "unit": null,
      "description": "Unable to determine meaning",
      "concept_category": "Other",
      "dict_entry_key": null,
      "match_confidence": 0.0,
      "reasoning": "No matching parameter found in dictionary"
    }}
  ]
}}
"""

DICT_SECTION_TEMPLATE = """[EXACT Parameter Keys - Use these values ONLY]
{dict_keys_list}

[Parameter Definitions]
{dict_context}
"""

DICT_SECTION_EMPTY = """[Note]
No parameter dictionary is available for this dataset.
Infer semantic meaning from column names and statistics using your medical knowledge.
Set dict_entry_key to null for all columns.
"""


# =============================================================================
# Class-based Node
# =============================================================================

@register_node
class DataSemanticNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Data Semantic Analysis Node (LLM-based)
    
    ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ 
    data_dictionaryì™€ ì—°ê²°í•©ë‹ˆë‹¤.
    
    Input State:
        - data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ëª©ë¡
        - (DB) data_dictionary: ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ parameter definitions
        - (DB) column_metadata: ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ì»¬ëŸ¼ ì •ë³´ + í†µê³„
    
    Output State:
        - data_semantic_result: DataSemanticResult
        - data_semantic_entries: ë¶„ì„ëœ ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        - (DB) column_metadata ì—…ë°ì´íŠ¸: semantic_name, unit, dict_entry_id ë“±
    """
    
    name = "data_semantic"
    description = "ë°ì´í„° íŒŒì¼ ì»¬ëŸ¼ ì˜ë¯¸ ë¶„ì„"
    order = 600
    requires_llm = True
    
    def __init__(self):
        super().__init__()
        self._file_repo: Optional[FileRepository] = None
        self._col_repo: Optional[ColumnRepository] = None
        self._dict_repo: Optional[DictionaryRepository] = None
    
    def _get_repositories(self) -> Tuple[FileRepository, ColumnRepository, DictionaryRepository]:
        """Repository ì¸ìŠ¤í„´ìŠ¤ë“¤ ë°˜í™˜ (lazy initialization)"""
        if self._file_repo is None:
            self._file_repo = FileRepository()
            self._col_repo = ColumnRepository()
            self._dict_repo = DictionaryRepository()
        return self._file_repo, self._col_repo, self._dict_repo
    
    def _load_dictionary_with_context(self) -> Tuple[List[Dict], str, str, Dict[str, str]]:
        """
        data_dictionary ë¡œë“œ + LLM context ìƒì„±
        
        Returns:
            (dictionary_entries, dict_keys_list, dict_context, key_to_id_map)
        """
        _, _, dict_repo = self._get_repositories()
        
        dictionary = dict_repo.get_all_entries()
        dict_keys_list, dict_context, key_to_id_map = dict_repo.build_llm_context()
        
        return dictionary, dict_keys_list, dict_context, key_to_id_map
    
    def _get_columns_with_stats(self, file_id: str) -> List[Dict]:
        """
        íŠ¹ì • íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ì™€ í†µê³„ë¥¼ ì¡°íšŒ
        
        Returns:
            List of column info dicts
        """
        _, col_repo, _ = self._get_repositories()
        
        try:
            return col_repo.get_columns_with_stats(file_id)
        except Exception as e:
            self.log(f"âš ï¸ Error loading columns: {e}", indent=1)
            return []
    
    def _build_columns_info(self, columns: List[Dict]) -> str:
        """
        ì»¬ëŸ¼ ì •ë³´ + í†µê³„ë¥¼ LLM context ë¬¸ìì—´ë¡œ ë³€í™˜
        
        Args:
            columns: ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í¬ë§·ëœ ì»¬ëŸ¼ ì •ë³´ ë¬¸ìì—´
        """
        config = DataSemanticConfig
        lines = []
        
        for col in columns:
            name = col['original_name']
            dtype = col['data_type']
            col_type = col['column_type']
            info = col.get('column_info', {}) or {}
            dist = col.get('value_distribution', {}) or {}
            
            # ê¸°ë³¸ ì •ë³´
            line = f"- {name} ({dtype}, {col_type})"
            details = []
            
            # Continuous: min, max, mean
            if col_type == 'continuous':
                min_val = info.get('min')
                max_val = info.get('max')
                mean_val = info.get('mean')
                if min_val is not None and max_val is not None:
                    range_str = f"range: [{min_val:.2f}, {max_val:.2f}]"
                    if mean_val is not None:
                        range_str += f", mean: {mean_val:.2f}"
                    details.append(range_str)
            
            # Categorical: unique values
            if col_type == 'categorical':
                unique_vals = dist.get('unique_values', [])
                n_unique = len(unique_vals)
                if n_unique > 0:
                    max_show = config.MAX_UNIQUE_VALUES_DISPLAY
                    if n_unique <= max_show:
                        details.append(f"values ({n_unique}): {unique_vals}")
                    else:
                        details.append(f"values ({n_unique} unique): {unique_vals[:max_show]}...")
            
            # Datetime: date range
            if info.get('is_datetime'):
                min_dt = info.get('min_date')
                max_dt = info.get('max_date')
                if min_dt:
                    details.append(f"date_range: [{min_dt}, {max_dt}]")
            
            # Samples
            samples = dist.get('samples', [])[:config.MAX_SAMPLES_DISPLAY]
            if samples:
                details.append(f"samples: {samples}")
            
            # ì¡°í•©
            if details:
                line += "\n    " + "\n    ".join(details)
            
            lines.append(line)
        
        return "\n".join(lines)
    
    def _call_llm_for_semantic(
        self,
        file_info: Dict,
        columns: List[Dict],
        dict_keys_list: str,
        dict_context: str
    ) -> Optional[DataSemanticResponse]:
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì‹œë§¨í‹± ë¶„ì„ ìˆ˜í–‰
        
        Args:
            file_info: íŒŒì¼ ì •ë³´ (file_name, file_type, row_count)
            columns: ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡
            dict_keys_list: dictionary key ëª©ë¡ ë¬¸ìì—´
            dict_context: dictionary ìƒì„¸ ì •ë³´ ë¬¸ìì—´
        
        Returns:
            DataSemanticResponse or None
        """
        # Dictionary section êµ¬ì„±
        if dict_keys_list:
            dict_section = DICT_SECTION_TEMPLATE.format(
                dict_keys_list=dict_keys_list,
                dict_context=dict_context
            )
        else:
            dict_section = DICT_SECTION_EMPTY
        
        # Columns info êµ¬ì„±
        columns_info = self._build_columns_info(columns)
        
        # Prompt êµ¬ì„±
        prompt = COLUMN_SEMANTIC_PROMPT.format(
            dict_section=dict_section,
            file_name=file_info.get('file_name', 'unknown'),
            file_type=file_info.get('file_type', 'tabular'),
            row_count=file_info.get('row_count', 'unknown'),
            columns_info=columns_info
        )
        
        try:
            response = self.call_llm_json(
                prompt=prompt,
                max_tokens=LLMConfig.MAX_TOKENS_COLUMN_ANALYSIS
            )
            
            if not response:
                return None
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            columns_data = response.get('columns', [])
            column_results = []
            for col_data in columns_data:
                try:
                    col_result = ColumnSemanticResult(**col_data)
                    column_results.append(col_result)
                except Exception as e:
                    self.log(f"âš ï¸ Error parsing column result: {e}", indent=1)
                    continue
            
            return DataSemanticResponse(
                columns=column_results,
                file_summary=response.get('file_summary')
            )
            
        except json.JSONDecodeError as e:
            self.log(f"âŒ JSON parsing error: {e}", indent=1)
            return None
        except Exception as e:
            self.log(f"âŒ LLM call error: {e}", indent=1)
            return None
    
    def _update_column_metadata_batch(
        self,
        file_id: str,
        results: List[ColumnSemanticResult],
        key_to_id_map: Dict[str, str]
    ) -> Dict[str, int]:
        """
        column_metadata í…Œì´ë¸”ì„ ë°°ì¹˜ ì—…ë°ì´íŠ¸
        
        Args:
            file_id: íŒŒì¼ ID
            results: LLM ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            key_to_id_map: {parameter_key: dict_id} ë§¤í•‘
        
        Returns:
            í†µê³„ dict: {matched: n, not_found: n, null_from_llm: n}
        """
        _, col_repo, dict_repo = self._get_repositories()
        
        # LLM ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸ìš© dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        updates = []
        for result in results:
            # dict_entry_id í•´ì„
            dict_id, status = dict_repo.resolve_dict_entry_id(
                result.dict_entry_key,
                key_to_id_map
            )
            
            updates.append({
                'original_name': result.original_name,
                'semantic_name': result.semantic_name,
                'unit': result.unit,
                'description': result.description,
                'concept_category': result.concept_category,
                'dict_entry_id': dict_id,
                'dict_match_status': status,
                'match_confidence': result.match_confidence
            })
        
        # Repositoryë¥¼ í†µí•´ ì¼ê´„ ì—…ë°ì´íŠ¸
        return col_repo.batch_update_semantic_info(file_id, updates)
    
    def _get_data_files_info(self, data_files: List[str]) -> List[Dict]:
        """
        ë°ì´í„° íŒŒì¼ë“¤ì˜ ì •ë³´ ì¡°íšŒ
        
        Args:
            data_files: íŒŒì¼ ê²½ë¡œ ëª©ë¡
        
        Returns:
            íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (file_id, file_path, file_name, row_count ë“±)
        """
        if not data_files:
            return []
        
        file_repo, _, _ = self._get_repositories()
        
        try:
            files_data = file_repo.get_files_by_paths(data_files)
            
            files = []
            for f in files_data:
                raw_stats = f.get('raw_stats', {})
                files.append({
                    'file_id': f['file_id'],
                    'file_path': f['file_path'],
                    'file_name': f['file_name'],
                    'file_type': f.get('processor_type') or 'tabular',
                    'row_count': raw_stats.get('row_count', 'unknown')
                })
            
            return files
            
        except Exception as e:
            self.log(f"âš ï¸ Error loading file info: {e}", indent=1)
            return []
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Data Semantic Analysis ì‹¤í–‰
        
        ë°ì´í„° íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  data_dictionaryì™€ ì—°ê²°
        """
        started_at = datetime.now().isoformat()
        config = DataSemanticConfig
        
        # ë°ì´í„° íŒŒì¼ ëª©ë¡
        data_files = state.get('data_files', [])
        
        if not data_files:
            self.log("âš ï¸ No data files to analyze")
            return {
                **state,
                'data_semantic_result': DataSemanticResult(
                    total_data_files=0,
                    started_at=started_at,
                    completed_at=datetime.now().isoformat()
                ).dict(),
                'data_semantic_entries': []
            }
        
        self.log(f"ğŸ“ Data files to analyze: {len(data_files)}")
        
        # 1. data_dictionary ë¡œë“œ
        self.log("ğŸ“– Loading data dictionary...")
        dictionary, dict_keys_list, dict_context, key_to_id_map = self._load_dictionary_with_context()
        self.log(f"Found {len(dictionary)} parameter definitions", indent=1)
        
        # 2. íŒŒì¼ ì •ë³´ ì¡°íšŒ
        files_info = self._get_data_files_info(data_files)
        self.log(f"Loaded info for {len(files_info)} files", indent=1)
        
        # ê²°ê³¼ ì¶”ì 
        total_columns = 0
        total_matched = 0
        total_not_found = 0
        total_null_from_llm = 0
        columns_by_file = {}
        llm_calls = 0
        batches_processed = 0
        all_entries = []
        
        # 3. íŒŒì¼ë³„ ì²˜ë¦¬
        for file_info in files_info:
            file_id = file_info['file_id']
            file_name = file_info['file_name']
            
            self.log(f"ğŸ“„ Processing: {file_name}")
            
            # ì»¬ëŸ¼ ì •ë³´ ë¡œë“œ
            columns = self._get_columns_with_stats(file_id)
            n_cols = len(columns)
            self.log(f"Columns: {n_cols}", indent=1)
            
            if not columns:
                continue
            
            columns_by_file[file_name] = n_cols
            total_columns += n_cols
            
            # ë°°ì¹˜ ë¶„í•  (ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´)
            batch_size = config.COLUMN_BATCH_SIZE
            batches = [columns[i:i+batch_size] for i in range(0, n_cols, batch_size)]
            
            if len(batches) > 1:
                self.log(f"Splitting into {len(batches)} batches (batch_size={batch_size})", indent=1)
            
            file_results = []
            
            for batch_idx, batch_cols in enumerate(batches):
                if len(batches) > 1:
                    self.log(f"Batch {batch_idx + 1}/{len(batches)} ({len(batch_cols)} columns)", indent=1)
                
                # LLM í˜¸ì¶œ
                response = self._call_llm_for_semantic(
                    file_info,
                    batch_cols,
                    dict_keys_list,
                    dict_context
                )
                llm_calls += 1
                batches_processed += 1
                
                if response and response.columns:
                    # DB ì—…ë°ì´íŠ¸
                    stats = self._update_column_metadata_batch(
                        file_id, response.columns, key_to_id_map
                    )
                    
                    total_matched += stats.get('matched', 0)
                    total_not_found += stats.get('not_found', 0)
                    total_null_from_llm += stats.get('null_from_llm', 0)
                    
                    file_results.extend([c.dict() for c in response.columns])
                    
                    self.log(
                        f"âœ“ Analyzed {len(response.columns)} columns "
                        f"(matched: {stats.get('matched', 0)}, "
                        f"not_found: {stats.get('not_found', 0)}, "
                        f"null: {stats.get('null_from_llm', 0)})",
                        indent=1
                    )
                else:
                    self.log("âš ï¸ No results from LLM", indent=1)
            
            # ê²°ê³¼ ì €ì¥
            all_entries.extend(file_results)
        
        # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
        completed_at = datetime.now().isoformat()
        
        result = DataSemanticResult(
            total_data_files=len(files_info),
            processed_files=len(files_info),
            total_columns_analyzed=total_columns,
            columns_matched=total_matched,
            columns_not_found=total_not_found,
            columns_null_from_llm=total_null_from_llm,
            columns_by_file=columns_by_file,
            batches_processed=batches_processed,
            llm_calls=llm_calls,
            started_at=started_at,
            completed_at=completed_at
        )
        
        self.log(f"Files processed: {result.processed_files}", indent=1)
        self.log(f"Columns analyzed: {result.total_columns_analyzed}", indent=1)
        self.log(f"Dictionary matches: {result.columns_matched}", indent=1)
        self.log(f"Not found in dict: {result.columns_not_found}", indent=1)
        self.log(f"Null from LLM: {result.columns_null_from_llm}", indent=1)
        self.log(f"LLM calls: {result.llm_calls}", indent=1)
        self.log(f"Batches: {result.batches_processed}", indent=1)
        
        return {
            **state,
            'data_semantic_result': result.dict(),
            'data_semantic_entries': all_entries
        }
    
    @classmethod
    def run_standalone(cls, data_files: List[str]) -> Dict[str, Any]:
        """
        ë‹¨ë… ì‹¤í–‰ìš© ë©”ì„œë“œ
        
        Args:
            data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ state
        """
        node = cls()
        initial_state = {
            'data_files': data_files
        }
        return node(initial_state)
