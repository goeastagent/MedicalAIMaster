# src/agents/nodes/data_semantic.py
"""
Phase 6: Data Semantic Analysis Node

ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ 
data_dictionaryì™€ ì—°ê²°í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLMì„ ì‚¬ìš©í•´ ê° ì»¬ëŸ¼ì˜ semantic_name, unit, description ì¶”ë¡ 
- data_dictionaryì˜ parameter_keyì™€ ë§¤ì¹­ ì‹œë„
- column_metadataì— ê²°ê³¼ ì €ì¥ (dict_entry_id, dict_match_status)
- íŒŒì¼ë‹¹ ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ LLM í˜¸ì¶œ
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from ..state import AgentState
from ..models.llm_responses import (
    ColumnSemanticResult,
    DataSemanticResponse,
    DataSemanticResult,
)
from src.database import (
    FileRepository,
    ColumnRepository,
    DictionaryRepository,
)
from src.utils.llm_client import get_llm_client
from src.config import Phase6Config, LLMConfig


# Repository ì‹±ê¸€í†¤
_file_repo = None
_col_repo = None
_dict_repo = None

def _get_repositories():
    """Repository ì¸ìŠ¤í„´ìŠ¤ë“¤ ë°˜í™˜"""
    global _file_repo, _col_repo, _dict_repo
    if _file_repo is None:
        _file_repo = FileRepository()
        _col_repo = ColumnRepository()
        _dict_repo = DictionaryRepository()
    return _file_repo, _col_repo, _dict_repo


# =============================================================================
# LLM Prompt Template
# =============================================================================

COLUMN_SEMANTIC_PROMPT = """You are a Medical Data Expert analyzing clinical data columns.

[Task]
Analyze each column and provide semantic information.
Use the Parameter Dictionary and column statistics to make accurate judgments.

{dict_section}

[File: {file_name}]
Type: {file_type}
Rows: {row_count}

[Columns to Analyze with Statistics]
{columns_info}

[CRITICAL RULES for dict_entry_key]
1. MUST be EXACTLY one of the keys from "EXACT Parameter Keys" above (if provided)
2. Copy the key exactly as shown (including "/" and special characters)
3. If no matching key exists â†’ set to null
4. If uncertain (confidence < 0.7) â†’ set to null
5. Use column statistics (min/max/values) to help identify the correct match

[Output Format]
Return ONLY valid JSON (no markdown, no explanation):
{{
  "columns": [
    {{
      "original_name": "age",
      "semantic_name": "Age",
      "unit": "years",
      "description": "Patient age at time of surgery",
      "concept_category": "Demographics",
      "dict_entry_key": "age",
      "match_confidence": 0.99,
      "reasoning": "Exact name match, values 20-90 consistent with age"
    }},
    {{
      "original_name": "unknown_col",
      "semantic_name": "Unknown Parameter",
      "unit": null,
      "description": "Unable to determine meaning",
      "concept_category": "Other",
      "dict_entry_key": null,
      "match_confidence": 0.0,
      "reasoning": "No matching parameter found in dictionary"
    }}
  ]
}}
"""

DICT_SECTION_TEMPLATE = """[EXACT Parameter Keys - Use these values ONLY]
{dict_keys_list}

[Parameter Definitions]
{dict_context}
"""

DICT_SECTION_EMPTY = """[Note]
No parameter dictionary is available for this dataset.
Infer semantic meaning from column names and statistics using your medical knowledge.
Set dict_entry_key to null for all columns.
"""


# =============================================================================
# Helper Functions (Repository ì‚¬ìš©)
# =============================================================================

def _load_dictionary_with_context() -> Tuple[List[Dict], str, str, Dict[str, str]]:
    """
    data_dictionary ë¡œë“œ + LLM context ìƒì„± (Repository ì‚¬ìš©)
    
    Returns:
        (dictionary_entries, dict_keys_list, dict_context, key_to_id_map)
    """
    _, _, dict_repo = _get_repositories()
    
    dictionary = dict_repo.get_all_entries()
    dict_keys_list, dict_context, key_to_id_map = dict_repo.build_llm_context()
    
    return dictionary, dict_keys_list, dict_context, key_to_id_map


# Legacy í•¨ìˆ˜ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
def _load_data_dictionary(db) -> List[Dict[str, Any]]:
    """
    data_dictionary í…Œì´ë¸”ì—ì„œ ëª¨ë“  ì—”íŠ¸ë¦¬ ë¡œë“œ (Legacy - Repository ì‚¬ìš©)
    """
    _, _, dict_repo = _get_repositories()
    return dict_repo.get_all_entries()


def _build_dict_context(dictionary: List[Dict]) -> Tuple[str, str, Dict[str, str]]:
    """
    data_dictionaryë¥¼ LLM context ë¬¸ìì—´ë¡œ ë³€í™˜ (Legacy í˜¸í™˜)
    
    Returns:
        (dict_keys_list, dict_context, key_to_id_map)
    """
    if not dictionary:
        return "", "", {}
    
    # Key ëª©ë¡ (ì •í™•í•œ ë§¤ì¹­ìš©)
    keys = [f'"{e["parameter_key"]}"' for e in dictionary]
    dict_keys_list = ", ".join(keys)
    
    # ìƒì„¸ ì •ì˜ (LLMì´ ì˜ë¯¸ íŒŒì•…ìš©)
    lines = []
    key_to_id_map = {}
    
    for entry in dictionary:
        key = entry['parameter_key']
        desc = entry['parameter_desc'] or ''
        unit = entry['parameter_unit'] or '-'
        extra = entry.get('extra_info', {})
        
        key_to_id_map[key] = entry['dict_id']
        
        line = f'- "{key}": {desc}'
        if unit and unit != '-':
            line += f' ({unit})'
        if extra:
            extra_items = list(extra.items())[:2]  # ìµœëŒ€ 2ê°œ
            if extra_items:
                extra_str = ", ".join(f"{k}={v}" for k, v in extra_items)
                line += f' [{extra_str}]'
        lines.append(line)
    
    dict_context = "\n".join(lines)
    
    return dict_keys_list, dict_context, key_to_id_map


def _get_columns_with_stats(db, file_id: str) -> List[Dict]:
    """
    íŠ¹ì • íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ì™€ í†µê³„ë¥¼ ì¡°íšŒ (Repository ì‚¬ìš©)
    
    Returns:
        List of column info dicts
    """
    _, col_repo, _ = _get_repositories()
    
    try:
        return col_repo.get_columns_with_stats(file_id)
    except Exception as e:
        print(f"   âš ï¸ Error loading columns: {e}")
        return []


def _build_columns_info(columns: List[Dict], config: Phase6Config) -> str:
    """
    ì»¬ëŸ¼ ì •ë³´ + í†µê³„ë¥¼ LLM context ë¬¸ìì—´ë¡œ ë³€í™˜
    
    Args:
        columns: ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        config: Phase6Config ì„¤ì • (í‘œì‹œ ê°œìˆ˜ ì œí•œ)
    
    Returns:
        í¬ë§·ëœ ì»¬ëŸ¼ ì •ë³´ ë¬¸ìì—´
    """
    lines = []
    
    for col in columns:
        name = col['original_name']
        dtype = col['data_type']
        col_type = col['column_type']
        info = col.get('column_info', {}) or {}
        dist = col.get('value_distribution', {}) or {}
        
        # ê¸°ë³¸ ì •ë³´
        line = f"- {name} ({dtype}, {col_type})"
        details = []
        
        # Continuous: min, max, mean
        if col_type == 'continuous':
            min_val = info.get('min')
            max_val = info.get('max')
            mean_val = info.get('mean')
            if min_val is not None and max_val is not None:
                range_str = f"range: [{min_val:.2f}, {max_val:.2f}]"
                if mean_val is not None:
                    range_str += f", mean: {mean_val:.2f}"
                details.append(range_str)
        
        # Categorical: unique values
        if col_type == 'categorical':
            unique_vals = dist.get('unique_values', [])
            n_unique = len(unique_vals)
            if n_unique > 0:
                max_show = config.MAX_UNIQUE_VALUES_DISPLAY
                if n_unique <= max_show:
                    details.append(f"values ({n_unique}): {unique_vals}")
                else:
                    details.append(f"values ({n_unique} unique): {unique_vals[:max_show]}...")
        
        # Datetime: date range
        if info.get('is_datetime'):
            min_dt = info.get('min_date')
            max_dt = info.get('max_date')
            if min_dt:
                details.append(f"date_range: [{min_dt}, {max_dt}]")
        
        # Samples
        samples = dist.get('samples', [])[:config.MAX_SAMPLES_DISPLAY]
        if samples:
            details.append(f"samples: {samples}")
        
        # ì¡°í•©
        if details:
            line += "\n    " + "\n    ".join(details)
        
        lines.append(line)
    
    return "\n".join(lines)


# NOTE: _resolve_dict_entry_id í•¨ìˆ˜ëŠ” DictionaryRepository.resolve_dict_entry_id()ë¡œ ì´ë™ë¨


def _call_llm_for_semantic(
    llm_client,
    file_info: Dict,
    columns: List[Dict],
    dict_keys_list: str,
    dict_context: str,
    config: Phase6Config
) -> Optional[DataSemanticResponse]:
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì‹œë§¨í‹± ë¶„ì„ ìˆ˜í–‰
    
    Args:
        llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        file_info: íŒŒì¼ ì •ë³´ (file_name, file_type, row_count)
        columns: ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡
        dict_keys_list: dictionary key ëª©ë¡ ë¬¸ìì—´
        dict_context: dictionary ìƒì„¸ ì •ë³´ ë¬¸ìì—´
        config: Phase6Config ì„¤ì •
    
    Returns:
        DataSemanticResponse or None
    """
    # Dictionary section êµ¬ì„±
    if dict_keys_list:
        dict_section = DICT_SECTION_TEMPLATE.format(
            dict_keys_list=dict_keys_list,
            dict_context=dict_context
        )
    else:
        dict_section = DICT_SECTION_EMPTY
    
    # Columns info êµ¬ì„±
    columns_info = _build_columns_info(columns, config)
    
    # Prompt êµ¬ì„±
    prompt = COLUMN_SEMANTIC_PROMPT.format(
        dict_section=dict_section,
        file_name=file_info.get('file_name', 'unknown'),
        file_type=file_info.get('file_type', 'tabular'),
        row_count=file_info.get('row_count', 'unknown'),
        columns_info=columns_info
    )
    
    try:
        response = llm_client.ask_json(
            prompt=prompt,
            max_tokens=LLMConfig.MAX_TOKENS_COLUMN_ANALYSIS
        )
        
        if not response:
            return None
        
        # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
        columns_data = response.get('columns', [])
        column_results = []
        for col_data in columns_data:
            try:
                col_result = ColumnSemanticResult(**col_data)
                column_results.append(col_result)
            except Exception as e:
                print(f"   âš ï¸ Error parsing column result: {e}")
                continue
        
        return DataSemanticResponse(
            columns=column_results,
            file_summary=response.get('file_summary')
        )
        
    except json.JSONDecodeError as e:
        print(f"   âŒ JSON parsing error: {e}")
        return None
    except Exception as e:
        print(f"   âŒ LLM call error: {e}")
        return None


def _update_column_metadata_batch(
    db,  # unused, í•˜ìœ„ í˜¸í™˜ì„±
    file_id: str,
    results: List[ColumnSemanticResult],
    key_to_id_map: Dict[str, str]
) -> Dict[str, int]:
    """
    column_metadata í…Œì´ë¸”ì„ ë°°ì¹˜ ì—…ë°ì´íŠ¸ (Repository ì‚¬ìš©)
    
    Args:
        db: DB ë§¤ë‹ˆì € (unused, í•˜ìœ„ í˜¸í™˜ì„±)
        file_id: íŒŒì¼ ID
        results: LLM ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        key_to_id_map: {parameter_key: dict_id} ë§¤í•‘
    
    Returns:
        í†µê³„ dict: {matched: n, not_found: n, null_from_llm: n}
    """
    _, col_repo, dict_repo = _get_repositories()
    
    # LLM ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸ìš© dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    updates = []
    for result in results:
        # dict_entry_id í•´ì„
        dict_id, status = dict_repo.resolve_dict_entry_id(
            result.dict_entry_key,
            key_to_id_map
        )
        
        updates.append({
            'original_name': result.original_name,
            'semantic_name': result.semantic_name,
            'unit': result.unit,
            'description': result.description,
            'concept_category': result.concept_category,
            'dict_entry_id': dict_id,
            'dict_match_status': status,
            'match_confidence': result.match_confidence
        })
    
    # Repositoryë¥¼ í†µí•´ ì¼ê´„ ì—…ë°ì´íŠ¸
    return col_repo.batch_update_semantic_info(file_id, updates)


def _get_data_files_info(db, data_files: List[str]) -> List[Dict]:
    """
    ë°ì´í„° íŒŒì¼ë“¤ì˜ ì •ë³´ ì¡°íšŒ (Repository ì‚¬ìš©)
    
    Args:
        db: DB ë§¤ë‹ˆì € (unused, í•˜ìœ„ í˜¸í™˜ì„±)
        data_files: íŒŒì¼ ê²½ë¡œ ëª©ë¡
    
    Returns:
        íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (file_id, file_path, file_name, row_count ë“±)
    """
    if not data_files:
        return []
    
    file_repo, _, _ = _get_repositories()
    
    try:
        files_data = file_repo.get_files_by_paths(data_files)
        
        files = []
        for f in files_data:
            raw_stats = f.get('raw_stats', {})
            files.append({
                'file_id': f['file_id'],
                'file_path': f['file_path'],
                'file_name': f['file_name'],
                'file_type': f.get('processor_type') or 'tabular',
                'row_count': raw_stats.get('row_count', 'unknown')
            })
        
        return files
        
    except Exception as e:
        print(f"   âš ï¸ Error loading file info: {e}")
        return []


# =============================================================================
# Main Node Function
# =============================================================================

def phase6_data_semantic_node(state: AgentState) -> AgentState:
    """
    Phase 6: Data Semantic Analysis Node
    
    ë°ì´í„° íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  data_dictionaryì™€ ì—°ê²°
    
    Input State:
        - data_files: ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ ëª©ë¡
        - (DB) data_dictionary: Phase 5ì—ì„œ ìƒì„±ëœ parameter definitions
        - (DB) column_metadata: Phase 2ì—ì„œ ìƒì„±ëœ ì»¬ëŸ¼ ì •ë³´ + í†µê³„
    
    Output State:
        - phase6_result: DataSemanticResult
        - data_semantic_entries: ë¶„ì„ëœ ì»¬ëŸ¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        - (DB) column_metadata ì—…ë°ì´íŠ¸: semantic_name, unit, dict_entry_id ë“±
    """
    print("\n" + "="*60)
    print("ğŸ”¬ Phase 6: Data Semantic Analysis")
    print("="*60)
    
    started_at = datetime.now().isoformat()
    config = Phase6Config
    
    # ë°ì´í„° íŒŒì¼ ëª©ë¡
    data_files = state.get('data_files', [])
    
    if not data_files:
        print("âš ï¸ No data files to analyze")
        return {
            **state,
            'phase6_result': DataSemanticResult(
                total_data_files=0,
                started_at=started_at,
                completed_at=datetime.now().isoformat()
            ).dict(),
            'data_semantic_entries': []
        }
    
    print(f"ğŸ“ Data files to analyze: {len(data_files)}")
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    llm_client = get_llm_client()
    
    # 1. data_dictionary ë¡œë“œ (Repository ì‚¬ìš©)
    print("\nğŸ“– Loading data dictionary...")
    dictionary, dict_keys_list, dict_context, key_to_id_map = _load_dictionary_with_context()
    print(f"   Found {len(dictionary)} parameter definitions")
    
    # 2. íŒŒì¼ ì •ë³´ ì¡°íšŒ (Repository ì‚¬ìš©)
    files_info = _get_data_files_info(None, data_files)  # db íŒŒë¼ë¯¸í„°ëŠ” ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨
    print(f"   Loaded info for {len(files_info)} files")
    
    # ê²°ê³¼ ì¶”ì 
    total_columns = 0
    total_matched = 0
    total_not_found = 0
    total_null_from_llm = 0
    columns_by_file = {}
    llm_calls = 0
    batches_processed = 0
    all_entries = []
    
    # 3. íŒŒì¼ë³„ ì²˜ë¦¬
    for file_info in files_info:
        file_id = file_info['file_id']
        file_name = file_info['file_name']
        
        print(f"\nğŸ“„ Processing: {file_name}")
        
        # ì»¬ëŸ¼ ì •ë³´ ë¡œë“œ (Repository ì‚¬ìš©)
        columns = _get_columns_with_stats(None, file_id)  # db íŒŒë¼ë¯¸í„°ëŠ” ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨
        n_cols = len(columns)
        print(f"   Columns: {n_cols}")
        
        if not columns:
            continue
        
        columns_by_file[file_name] = n_cols
        total_columns += n_cols
        
        # ë°°ì¹˜ ë¶„í•  (ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´)
        batch_size = config.COLUMN_BATCH_SIZE
        batches = [columns[i:i+batch_size] for i in range(0, n_cols, batch_size)]
        
        if len(batches) > 1:
            print(f"   Splitting into {len(batches)} batches (batch_size={batch_size})")
        
        file_results = []
        
        for batch_idx, batch_cols in enumerate(batches):
            if len(batches) > 1:
                print(f"   Batch {batch_idx + 1}/{len(batches)} ({len(batch_cols)} columns)")
            
            # LLM í˜¸ì¶œ
            response = _call_llm_for_semantic(
                llm_client,
                file_info,
                batch_cols,
                dict_keys_list,
                dict_context,
                config
            )
            llm_calls += 1
            batches_processed += 1
            
            if response and response.columns:
                # DB ì—…ë°ì´íŠ¸
                stats = _update_column_metadata_batch(
                    None, file_id, response.columns, key_to_id_map  # db íŒŒë¼ë¯¸í„°ëŠ” ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨
                )
                
                total_matched += stats.get('matched', 0)
                total_not_found += stats.get('not_found', 0)
                total_null_from_llm += stats.get('null_from_llm', 0)
                
                file_results.extend([c.dict() for c in response.columns])
                
                print(f"   âœ“ Analyzed {len(response.columns)} columns "
                      f"(matched: {stats.get('matched', 0)}, "
                      f"not_found: {stats.get('not_found', 0)}, "
                      f"null: {stats.get('null_from_llm', 0)})")
            else:
                print(f"   âš ï¸ No results from LLM")
        
        # ê²°ê³¼ ì €ì¥
        all_entries.extend(file_results)
    
    # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
    completed_at = datetime.now().isoformat()
    
    result = DataSemanticResult(
        total_data_files=len(files_info),
        processed_files=len(files_info),
        total_columns_analyzed=total_columns,
        columns_matched=total_matched,
        columns_not_found=total_not_found,
        columns_null_from_llm=total_null_from_llm,
        columns_by_file=columns_by_file,
        batches_processed=batches_processed,
        llm_calls=llm_calls,
        started_at=started_at,
        completed_at=completed_at
    )
    
    print("\n" + "="*60)
    print("âœ… Phase 6 Complete!")
    print(f"   Files processed: {result.processed_files}")
    print(f"   Columns analyzed: {result.total_columns_analyzed}")
    print(f"   Dictionary matches: {result.columns_matched}")
    print(f"   Not found in dict: {result.columns_not_found}")
    print(f"   Null from LLM: {result.columns_null_from_llm}")
    print(f"   LLM calls: {result.llm_calls}")
    print(f"   Batches: {result.batches_processed}")
    print("="*60)
    
    return {
        **state,
        'phase6_result': result.dict(),
        'data_semantic_entries': all_entries
    }


# =============================================================================
# Class-based Node (for NodeRegistry)
# =============================================================================

from ..base import BaseNode, LLMMixin, DatabaseMixin
from ..registry import register_node


@register_node
class DataSemanticNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Data Semantic Analysis Node (LLM-based)
    
    ë°ì´í„° íŒŒì¼(is_metadata=false)ì˜ ì»¬ëŸ¼ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ 
    data_dictionaryì™€ ì—°ê²°í•©ë‹ˆë‹¤.
    """
    
    name = "data_semantic"
    description = "ë°ì´í„° íŒŒì¼ ì»¬ëŸ¼ ì˜ë¯¸ ë¶„ì„"
    order = 600
    requires_llm = True
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì¡´ í•¨ìˆ˜ ìœ„ì„"""
        return phase6_data_semantic_node(state)

