# src/agents/nodes/metadata_semantic.py
"""
Phase 1A: MetaData Semantic Analysis Node

metadata íŒŒì¼ì—ì„œ key-desc-unitì„ ì¶”ì¶œí•˜ì—¬ data_dictionaryì— ì €ì¥í•©ë‹ˆë‹¤.

âœ… LLM ì‚¬ìš©:
  1. ì»¬ëŸ¼ ì—­í•  ì¶”ë¡  (ì–´ë–¤ ì»¬ëŸ¼ì´ key/desc/unitì¸ì§€)
  2. ë¹„êµ¬ì¡°í™” TXT íŒŒì¼ íŒŒì‹± (ì¶”í›„ ì§€ì›)
"""

import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

from src.agents.state import AgentState
from src.database.connection import get_db_manager
from src.database.schema_dictionary import (
    ensure_dictionary_schema,
    insert_dictionary_entries_batch,
    DictionarySchemaManager,
)
from src.config import Phase1Config, LLMConfig
from src.agents.models.llm_responses import (
    ColumnRoleMapping,
    ColumnRoleMappingResponse,
    DataDictionaryEntry,
    MetadataSemanticResult,
)


# =============================================================================
# ì „ì—­ ë¦¬ì†ŒìŠ¤
# =============================================================================

_db_manager = None
_llm_client = None


def _get_db():
    """DB Manager ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _db_manager
    if _db_manager is None:
        _db_manager = get_db_manager()
    return _db_manager


def _get_llm():
    """LLM Client ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_client
    if _llm_client is None:
        from src.utils.llm_client import get_llm_client
        _llm_client = get_llm_client()
    return _llm_client


# =============================================================================
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# =============================================================================

COLUMN_ROLE_PROMPT = """You are a Medical Data Expert analyzing a metadata/dictionary file.

[Task]
Analyze this file and identify which column serves which role:

- **key_column**: The column containing parameter names/codes (e.g., "age", "hr", "sbp")
  This is the main identifier column that other data files will reference.
  
- **desc_column**: The column containing descriptions or definitions
  Human-readable explanations of what each parameter means.
  
- **unit_column**: The column containing measurement units (e.g., "years", "bpm", "mmHg")
  May be empty or null for some parameters.
  
- **extra_columns**: Other useful columns mapped to their semantic role
  Examples: {{"category": "Category", "reference_value": "Reference value", "data_source": "Data Source"}}

[File Info]
File: {file_name}
Columns: {column_names}

[Columns with Sample Values]
{columns_info}

[Sample Rows (first 5)]
{sample_rows}

[Output Format]
Return ONLY valid JSON (no markdown, no explanation):
{{
  "key_column": "Parameter",
  "desc_column": "Description",
  "unit_column": "Unit",
  "extra_columns": {{"category": "Category", "reference": "Reference value"}},
  "confidence": 0.95,
  "reasoning": "Parameter column contains unique identifiers, Description has explanations, Unit has measurement units"
}}
"""


# =============================================================================
# íŒŒì¼/ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
# =============================================================================

def _get_metadata_file_details(file_path: str) -> Optional[Dict[str, Any]]:
    """
    metadata íŒŒì¼ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì»¬ëŸ¼ë³„ unique values í¬í•¨)
    
    Returns:
        {
            "file_id": str,
            "file_name": str,
            "file_path": str,
            "row_count": int,
            "columns": [
                {
                    "name": str,
                    "dtype": str,
                    "all_unique_values": List[str],  # ëª¨ë“  unique values
                    "n_unique": int
                }
            ],
            "sample_rows": List[Dict]  # ì²« 5í–‰
        }
    """
    db = _get_db()
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´
        cursor.execute("""
            SELECT file_id, file_name, file_path, file_metadata, raw_stats
            FROM file_catalog
            WHERE file_path = %s
        """, (file_path,))
        
        row = cursor.fetchone()
        if not row:
            # file_nameìœ¼ë¡œë„ ì‹œë„
            cursor.execute("""
                SELECT file_id, file_name, file_path, file_metadata, raw_stats
                FROM file_catalog
                WHERE file_name = %s
            """, (file_path.split('/')[-1],))
            row = cursor.fetchone()
        
        if not row:
            return None
        
        file_id, file_name, file_path_db, file_metadata, raw_stats = row
        
        # row_count ì¶”ì¶œ
        metadata = file_metadata if isinstance(file_metadata, dict) else {}
        raw = raw_stats if isinstance(raw_stats, dict) else {}
        row_count = metadata.get('row_count') or raw.get('row_count', 0)
        
        # sample_rows ì¶”ì¶œ
        sample_rows = raw.get('sample_rows', [])
        
        # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT original_name, data_type, column_type, value_distribution
            FROM column_metadata
            WHERE file_id = %s
            ORDER BY col_id
        """, (file_id,))
        
        columns = []
        for col_row in cursor.fetchall():
            col_name, dtype, col_type, value_dist = col_row
            
            # value_distributionì—ì„œ unique_values ì¶”ì¶œ
            dist = value_dist if isinstance(value_dist, dict) else {}
            unique_values = dist.get('unique_values', [])
            samples = dist.get('samples', [])
            
            # unique_valuesê°€ ì—†ìœ¼ë©´ samples ì‚¬ìš©
            all_unique = unique_values if unique_values else samples
            
            columns.append({
                "name": col_name,
                "dtype": dtype or "unknown",
                "column_type": col_type or "unknown",
                "all_unique_values": all_unique,
                "n_unique": len(all_unique)
            })
        
        return {
            "file_id": str(file_id),
            "file_name": file_name,
            "file_path": file_path_db or file_path,
            "row_count": row_count,
            "columns": columns,
            "sample_rows": sample_rows[:5] if sample_rows else []
        }
        
    except Exception as e:
        print(f"   âŒ Error getting metadata file details: {e}")
        return None


def _build_columns_info_text(columns: List[Dict[str, Any]]) -> str:
    """
    ì»¬ëŸ¼ ì •ë³´ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    ì˜ˆì‹œ:
    1. "Parameter" [categorical, 82 unique values]
       dtype: object
       unique_ratio: 1.0
       All unique values: ["age", "sex", "height", ...]
    """
    lines = []
    
    for i, col in enumerate(columns, 1):
        col_name = col['name']
        col_type = col.get('column_type', 'unknown')
        n_unique = col.get('n_unique', 0)
        dtype = col.get('dtype', 'unknown')
        unique_vals = col.get('all_unique_values', [])
        
        lines.append(f'{i}. "{col_name}" [{col_type}, {n_unique} unique values]')
        lines.append(f"   dtype: {dtype}")
        
        # unique values í‘œì‹œ (ìµœëŒ€ 20ê°œ)
        if unique_vals:
            vals_display = unique_vals[:20]
            vals_str = [str(v)[:50] for v in vals_display]
            lines.append(f"   Sample values: {vals_str}")
            if len(unique_vals) > 20:
                lines.append(f"   ... and {len(unique_vals) - 20} more values")
        
        lines.append("")
    
    return "\n".join(lines)


def _build_sample_rows_text(sample_rows: List[Dict], columns: List[Dict]) -> str:
    """ìƒ˜í”Œ í–‰ì„ í…Œì´ë¸” í˜•ì‹ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not sample_rows:
        return "(No sample rows available)"
    
    col_names = [c['name'] for c in columns]
    
    lines = []
    
    # í—¤ë”
    header = " | ".join(col_names[:8])  # ìµœëŒ€ 8ê°œ ì»¬ëŸ¼ë§Œ
    lines.append(header)
    lines.append("-" * len(header))
    
    # ë°ì´í„° í–‰
    for row in sample_rows[:5]:
        if isinstance(row, dict):
            values = [str(row.get(c, ''))[:20] for c in col_names[:8]]
        else:
            values = [str(v)[:20] for v in list(row)[:8]]
        lines.append(" | ".join(values))
    
    return "\n".join(lines)


# =============================================================================
# LLM í˜¸ì¶œ
# =============================================================================

def _call_llm_for_column_roles(
    file_info: Dict[str, Any]
) -> Optional[ColumnRoleMapping]:
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì—­í•  ì¶”ë¡ 
    
    Returns:
        ColumnRoleMapping or None
    """
    llm = _get_llm()
    
    file_name = file_info['file_name']
    columns = file_info['columns']
    sample_rows = file_info.get('sample_rows', [])
    
    column_names = [c['name'] for c in columns]
    columns_info_text = _build_columns_info_text(columns)
    sample_rows_text = _build_sample_rows_text(sample_rows, columns)
    
    prompt = COLUMN_ROLE_PROMPT.format(
        file_name=file_name,
        column_names=", ".join(column_names),
        columns_info=columns_info_text,
        sample_rows=sample_rows_text
    )
    
    try:
        data = llm.ask_json(prompt, max_tokens=LLMConfig.MAX_TOKENS)
        
        if data.get("error"):
            print(f"   âŒ LLM returned error: {data.get('error')}")
            return None
        
        return ColumnRoleMapping(
            key_column=data.get('key_column', ''),
            desc_column=data.get('desc_column'),
            unit_column=data.get('unit_column'),
            extra_columns=data.get('extra_columns', {}),
            confidence=data.get('confidence', 0.8),
            reasoning=data.get('reasoning', '')
        )
        
    except Exception as e:
        print(f"   âŒ LLM call error: {e}")
        return None


# =============================================================================
# Data Dictionary ì¶”ì¶œ
# =============================================================================

def _extract_dictionary_entries(
    file_info: Dict[str, Any],
    column_mapping: ColumnRoleMapping
) -> List[Dict[str, Any]]:
    """
    íŒŒì¼ì—ì„œ data_dictionary ì—”íŠ¸ë¦¬ ì¶”ì¶œ
    
    ì»¬ëŸ¼ ì—­í•  ë§¤í•‘ì„ ê¸°ë°˜ìœ¼ë¡œ ê° í–‰ì„ key-desc-unit ì—”íŠ¸ë¦¬ë¡œ ë³€í™˜
    """
    file_id = file_info['file_id']
    file_name = file_info['file_name']
    columns = file_info['columns']
    
    # ì»¬ëŸ¼ëª… â†’ unique_values ë§¤í•‘
    col_values = {c['name']: c.get('all_unique_values', []) for c in columns}
    
    key_col = column_mapping.key_column
    desc_col = column_mapping.desc_column
    unit_col = column_mapping.unit_column
    extra_cols = column_mapping.extra_columns  # {role: col_name}
    
    # key ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ì¶œ ë¶ˆê°€
    if not key_col or key_col not in col_values:
        print(f"   âš ï¸ Key column '{key_col}' not found in file")
        return []
    
    # raw_statsì—ì„œ sample_rows ê°€ì ¸ì˜¤ê¸° (ì „ì²´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¢‹ìŒ)
    # í˜„ì¬ëŠ” unique_valuesë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œ
    
    # ë°©ë²• 1: sample_rowsê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    sample_rows = file_info.get('sample_rows', [])
    
    entries = []
    
    if sample_rows:
        # sample_rowsì—ì„œ ì¶”ì¶œ
        for row in sample_rows:
            if not isinstance(row, dict):
                continue
            
            key_val = row.get(key_col)
            if not key_val:
                continue
            
            desc_val = row.get(desc_col) if desc_col else None
            unit_val = row.get(unit_col) if unit_col else None
            
            # extra_info êµ¬ì„±
            extra_info = {}
            for role, col_name in extra_cols.items():
                if col_name in row:
                    extra_info[role] = row[col_name]
            
            entries.append({
                'source_file_id': file_id,
                'source_file_name': file_name,
                'parameter_key': str(key_val),
                'parameter_desc': str(desc_val) if desc_val else None,
                'parameter_unit': str(unit_val) if unit_val else None,
                'extra_info': extra_info,
                'llm_confidence': column_mapping.confidence
            })
    else:
        # unique_values ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ (ì œí•œì )
        # key ì»¬ëŸ¼ì˜ ê° ê°’ì— ëŒ€í•´ ì—”íŠ¸ë¦¬ ìƒì„±
        key_values = col_values.get(key_col, [])
        desc_values = col_values.get(desc_col, []) if desc_col else []
        unit_values = col_values.get(unit_col, []) if unit_col else []
        
        # ê°’ë“¤ì„ ë§¤ì¹­ (ê°™ì€ ì¸ë±ìŠ¤ë¼ë¦¬ - ìˆœì„œê°€ ìœ ì§€ëœë‹¤ê³  ê°€ì •)
        for i, key_val in enumerate(key_values):
            desc_val = desc_values[i] if i < len(desc_values) else None
            unit_val = unit_values[i] if i < len(unit_values) else None
            
            entries.append({
                'source_file_id': file_id,
                'source_file_name': file_name,
                'parameter_key': str(key_val),
                'parameter_desc': str(desc_val) if desc_val else None,
                'parameter_unit': str(unit_val) if unit_val else None,
                'extra_info': {},
                'llm_confidence': column_mapping.confidence
            })
    
    return entries


def _extract_from_raw_data(file_info: Dict[str, Any], column_mapping: ColumnRoleMapping) -> List[Dict[str, Any]]:
    """
    raw_statsì— ì €ì¥ëœ ì „ì²´ ë°ì´í„°ì—ì„œ dictionary ì—”íŠ¸ë¦¬ ì¶”ì¶œ
    
    Phase 0ì—ì„œ ì €ì¥í•œ raw_statsë¥¼ í™œìš©
    """
    import pandas as pd
    import os
    
    file_id = file_info['file_id']
    file_name = file_info['file_name']
    file_path = file_info['file_path']
    
    key_col = column_mapping.key_column
    desc_col = column_mapping.desc_column
    unit_col = column_mapping.unit_column
    extra_cols = column_mapping.extra_columns
    
    entries = []
    
    try:
        # íŒŒì¼ ì§ì ‘ ì½ê¸° (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
        if os.path.exists(file_path):
            ext = file_path.lower().split('.')[-1]
            
            if ext == 'csv':
                df = pd.read_csv(file_path)
            elif ext == 'tsv':
                df = pd.read_csv(file_path, sep='\t')
            elif ext in ['xlsx', 'xls']:
                df = pd.read_excel(file_path)
            else:
                print(f"   âš ï¸ Unsupported file type: {ext}")
                return []
            
            # ê° í–‰ì—ì„œ ì—”íŠ¸ë¦¬ ì¶”ì¶œ
            for _, row in df.iterrows():
                key_val = row.get(key_col)
                if pd.isna(key_val) or key_val == '':
                    continue
                
                desc_val = row.get(desc_col) if desc_col else None
                unit_val = row.get(unit_col) if unit_col else None
                
                # NaN ì²˜ë¦¬
                if pd.isna(desc_val):
                    desc_val = None
                if pd.isna(unit_val):
                    unit_val = None
                
                # extra_info êµ¬ì„±
                extra_info = {}
                for role, col_name in extra_cols.items():
                    if col_name in row.index:
                        val = row[col_name]
                        if not pd.isna(val):
                            extra_info[role] = str(val)
                
                entries.append({
                    'source_file_id': file_id,
                    'source_file_name': file_name,
                    'parameter_key': str(key_val),
                    'parameter_desc': str(desc_val) if desc_val else None,
                    'parameter_unit': str(unit_val) if unit_val else None,
                    'extra_info': extra_info,
                    'llm_confidence': column_mapping.confidence
                })
            
            print(f"      ğŸ“„ Extracted {len(entries)} entries from file")
            
    except Exception as e:
        print(f"   âŒ Error reading file: {e}")
    
    return entries


# =============================================================================
# LangGraph Node Function
# =============================================================================

def metadata_semantic_node(state: AgentState) -> Dict[str, Any]:
    """
    Phase 1A: ë©”íƒ€ë°ì´í„° íŒŒì¼ì—ì„œ key-desc-unit ì¶”ì¶œ
    
    ì…ë ¥: state.metadata_files (Phase 0.7ì—ì„œ ë¶„ë¥˜ëœ metadata íŒŒì¼ë“¤)
    
    ì²˜ë¦¬:
    1. ê° metadata íŒŒì¼ì— ëŒ€í•´:
       a. íŒŒì¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ
       b. LLM í˜¸ì¶œ â†’ ì»¬ëŸ¼ ì—­í•  ì¶”ë¡ 
       c. íŒŒì¼ì—ì„œ dictionary ì—”íŠ¸ë¦¬ ì¶”ì¶œ
       d. data_dictionary í…Œì´ë¸”ì— ì €ì¥
    
    ì¶œë ¥:
    - phase1a_result: ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    - data_dictionary_entries: ì¶”ì¶œëœ ëª¨ë“  ì—”íŠ¸ë¦¬
    """
    print("\n" + "=" * 60)
    print("ğŸ“– Phase 1A: MetaData Semantic Analysis")
    print("=" * 60)
    
    started_at = datetime.now()
    
    # data_dictionary í…Œì´ë¸” í™•ì¸/ìƒì„±
    ensure_dictionary_schema()
    
    # metadata íŒŒì¼ ëª©ë¡
    metadata_files = state.get("metadata_files", [])
    
    if not metadata_files:
        print("   âš ï¸ No metadata files to process")
        return {
            "phase1a_result": {
                "total_metadata_files": 0,
                "processed_files": 0,
                "total_entries_extracted": 0,
                "error": "No metadata files"
            },
            "data_dictionary_entries": [],
            "logs": ["âš ï¸ [Phase 1A] No metadata files to process"]
        }
    
    print(f"   ğŸ“‚ Metadata files to process: {len(metadata_files)}")
    for f in metadata_files:
        print(f"      - {f.split('/')[-1]}")
    
    all_entries = []
    entries_by_file = {}
    processed_files = 0
    llm_calls = 0
    
    for file_path in metadata_files:
        file_name = file_path.split('/')[-1]
        print(f"\n   ğŸ“„ Processing: {file_name}")
        
        # 1. íŒŒì¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        file_info = _get_metadata_file_details(file_path)
        if not file_info:
            print(f"      âŒ Failed to get file details")
            continue
        
        print(f"      Columns: {[c['name'] for c in file_info['columns']]}")
        
        # 2. LLM í˜¸ì¶œ â†’ ì»¬ëŸ¼ ì—­í•  ì¶”ë¡ 
        print(f"      ğŸ¤– Calling LLM for column role mapping...")
        column_mapping = _call_llm_for_column_roles(file_info)
        llm_calls += 1
        
        if not column_mapping:
            print(f"      âŒ Failed to get column mapping")
            continue
        
        print(f"      âœ… Column roles identified (conf={column_mapping.confidence:.2f}):")
        print(f"         key: {column_mapping.key_column}")
        print(f"         desc: {column_mapping.desc_column}")
        print(f"         unit: {column_mapping.unit_column}")
        if column_mapping.extra_columns:
            print(f"         extra: {column_mapping.extra_columns}")
        
        # 3. Dictionary ì—”íŠ¸ë¦¬ ì¶”ì¶œ (íŒŒì¼ ì§ì ‘ ì½ê¸°)
        entries = _extract_from_raw_data(file_info, column_mapping)
        
        if not entries:
            # ëŒ€ì•ˆ: sample_rows/unique_values ê¸°ë°˜ ì¶”ì¶œ
            entries = _extract_dictionary_entries(file_info, column_mapping)
        
        if entries:
            # 4. DBì— ì €ì¥
            print(f"      ğŸ’¾ Saving {len(entries)} entries to data_dictionary...")
            inserted = insert_dictionary_entries_batch(entries)
            print(f"      âœ… Saved {inserted} entries")
            
            all_entries.extend(entries)
            entries_by_file[file_name] = len(entries)
            processed_files += 1
        else:
            print(f"      âš ï¸ No entries extracted")
    
    # ê²°ê³¼ ìš”ì•½
    completed_at = datetime.now()
    duration = (completed_at - started_at).total_seconds()
    
    result = MetadataSemanticResult(
        total_metadata_files=len(metadata_files),
        processed_files=processed_files,
        total_entries_extracted=len(all_entries),
        entries_by_file=entries_by_file,
        llm_calls=llm_calls,
        started_at=started_at.isoformat(),
        completed_at=completed_at.isoformat()
    )
    
    print(f"\nâœ… Phase 1A Complete!")
    print(f"   ğŸ“ Processed files: {processed_files}/{len(metadata_files)}")
    print(f"   ğŸ“ Total entries: {len(all_entries)}")
    for fname, count in entries_by_file.items():
        print(f"      - {fname}: {count} entries")
    print(f"   ğŸ¤– LLM calls: {llm_calls}")
    print(f"   â±ï¸  Duration: {duration:.1f}s")
    print("=" * 60 + "\n")
    
    # í†µê³„ ì¶œë ¥
    schema_manager = DictionarySchemaManager()
    stats = schema_manager.get_stats()
    print(f"   ğŸ“Š Data Dictionary Stats: {stats}")
    
    return {
        "phase1a_result": result.model_dump(),
        "data_dictionary_entries": all_entries,
        "logs": [
            f"ğŸ“– [Phase 1A] Extracted {len(all_entries)} entries from "
            f"{processed_files} metadata files"
        ]
    }


# =============================================================================
# í¸ì˜ í•¨ìˆ˜
# =============================================================================

def run_metadata_semantic_standalone(metadata_files: List[str] = None) -> Dict[str, Any]:
    """
    Phase 1A ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
    
    Args:
        metadata_files: ì²˜ë¦¬í•  metadata íŒŒì¼ ê²½ë¡œ ëª©ë¡
                       Noneì´ë©´ DBì—ì„œ is_metadata=trueì¸ íŒŒì¼ ì¡°íšŒ
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼
    """
    if metadata_files is None:
        # DBì—ì„œ is_metadata=trueì¸ íŒŒì¼ ì¡°íšŒ
        db = _get_db()
        conn = db.get_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT file_path FROM file_catalog 
            WHERE is_metadata = true 
            ORDER BY file_name
        """)
        metadata_files = [row[0] for row in cursor.fetchall()]
    
    # State ì‹œë®¬ë ˆì´ì…˜
    state = {
        "metadata_files": metadata_files
    }
    
    return metadata_semantic_node(state)

