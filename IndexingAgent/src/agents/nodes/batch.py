# src/agents/nodes/batch.py
"""
Batch Processing Nodes - 2-Phase Workflow
"""

import os
from datetime import datetime
from typing import Dict, Any, List

from src.agents.state import (
    AgentState, FileClassification, ClassificationResult, ProcessingProgress
)
from src.agents.nodes.common import processors, ontology_manager
from src.agents.helpers.llm_helpers import ask_llm_is_metadata
from src.agents.helpers.metadata_helpers import (
    build_lightweight_classification_context,  # NEW: ê²½ëŸ‰ ë¶„ë¥˜ìš©
    parse_metadata_content,
    # NEW: Hybrid Approach - LLM Enrichment
    extract_relevant_context,
    enrich_definitions_with_llm,
    infer_concept_relationships,
)
from src.config import HumanReviewConfig, ProcessingConfig, MetadataEnrichmentConfig


def batch_classifier_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1] ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ë…¸ë“œ
    """
    print("\n" + "="*80)
    print("ğŸ“‹ [BATCH CLASSIFIER] Phase 1 ì‹œì‘ - ì „ì²´ íŒŒì¼ ë¶„ë¥˜")
    print("="*80)
    
    input_files = state.get("input_files", [])
    
    if not input_files:
        return {
            "logs": ["âŒ Error: ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."],
            "error_message": "No input files provided"
        }
    
    print(f"   ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼: {len(input_files)}ê°œ")
    
    classifications: Dict[str, FileClassification] = {}
    metadata_files: List[str] = []
    data_files: List[str] = []
    uncertain_files: List[str] = []
    
    for idx, file_path in enumerate(input_files):
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(filename)[1].lower()
        print(f"\n   [{idx+1}/{len(input_files)}] {filename}")
        
        try:
            # Rule-based: Signal files are always data
            if file_ext.lstrip('.') in ProcessingConfig.SIGNAL_EXTENSIONS:
                classifications[file_path] = {
                    "file_path": file_path,
                    "filename": filename,
                    "classification": "data",
                    "confidence": 1.0,
                    "reasoning": f"Signal file ({file_ext}) - always transactional data",
                    "indicators": {"file_type": "signal"},
                    "needs_review": False,
                    "human_confirmed": False
                }
                data_files.append(file_path)
                print(f"      ğŸ“ˆ Signal ë°ì´í„° (100% - rule-based)")
                continue
            
            # Check if file extension is supported (without full extraction)
            if not any(p.can_handle(file_path) for p in processors):
                print(f"      âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
                classifications[file_path] = {
                    "file_path": file_path,
                    "filename": filename,
                    "classification": "unknown",
                    "confidence": 0.0,
                    "reasoning": "Unsupported file format",
                    "indicators": {},
                    "needs_review": True,
                    "human_confirmed": False
                }
                uncertain_files.append(file_path)
                continue
            
            # NEW: ê²½ëŸ‰ context ìƒì„± (extract_metadata ì—†ì´ ì§ì ‘ íŒŒì¼ì—ì„œ ìƒ˜í”Œ ì½ê¸°)
            # ì´ë¡œì¨ metadata/data ë¶„ë¥˜ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ì „ì²´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì€ loaderì—ì„œ ìˆ˜í–‰
            context = build_lightweight_classification_context(file_path)
            meta_result = ask_llm_is_metadata(context)
            
            confidence = meta_result.get("confidence", 0.0)
            is_metadata = meta_result.get("is_metadata", False)
            reasoning = meta_result.get("reasoning", "")
            indicators = meta_result.get("indicators", {})
            
            classification_type = "metadata" if is_metadata else "data"
            needs_review = confidence < HumanReviewConfig.CLASSIFICATION_CONFIDENCE_THRESHOLD
            
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": classification_type,
                "confidence": confidence,
                "reasoning": reasoning,
                "indicators": indicators,
                "needs_review": needs_review,
                "human_confirmed": False
            }
            
            if needs_review:
                uncertain_files.append(file_path)
                print(f"      âš ï¸ ë¶ˆí™•ì‹¤: {classification_type} ({confidence:.1%})")
            elif is_metadata:
                metadata_files.append(file_path)
                print(f"      ğŸ“– ë©”íƒ€ë°ì´í„° ({confidence:.1%})")
            else:
                data_files.append(file_path)
                print(f"      ğŸ“Š ë°ì´í„° ({confidence:.1%})")
                
        except Exception as e:
            print(f"      âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": "unknown",
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
                "indicators": {},
                "needs_review": True,
                "human_confirmed": False
            }
            uncertain_files.append(file_path)
    
    classification_result: ClassificationResult = {
        "total_files": len(input_files),
        "metadata_files": metadata_files,
        "data_files": data_files,
        "uncertain_files": uncertain_files,
        "classifications": classifications
    }
    
    processing_progress: ProcessingProgress = {
        "phase": "classification",
        "metadata_processed": [],
        "data_processed": [],
        "current_file": None,
        "current_file_index": 0,
        "total_files": len(input_files)
    }
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ:")
    print(f"   - ë©”íƒ€ë°ì´í„°: {len(metadata_files)}ê°œ")
    print(f"   - ë°ì´í„°: {len(data_files)}ê°œ")
    print(f"   - ë¶ˆí™•ì‹¤: {len(uncertain_files)}ê°œ")
    print("="*80)
    
    # NOTE: ë¶ˆí™•ì‹¤í•œ íŒŒì¼ì´ ìˆì–´ë„ ì—¬ê¸°ì„œëŠ” human_questionì„ ì„¤ì •í•˜ì§€ ì•ŠìŒ
    # classification_review_nodeì—ì„œ interrupt()ë¥¼ í†µí•´ ì§ì ‘ ì²˜ë¦¬í•¨
    
    return {
        "classification_result": classification_result,
        "processing_progress": processing_progress,
        "logs": [
            f"ğŸ“‹ [Phase1] ë¶„ë¥˜ ì™„ë£Œ: ë©”íƒ€ë°ì´í„° {len(metadata_files)}ê°œ, "
            f"ë°ì´í„° {len(data_files)}ê°œ, ë¶ˆí™•ì‹¤ {len(uncertain_files)}ê°œ"
        ]
    }


def classification_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1-2] ë¶„ë¥˜ í™•ì¸ ë…¸ë“œ (Human-in-the-Loop with interrupt())
    
    interrupt()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¸ë“œ ë‚´ë¶€ì—ì„œ ì§ì ‘ human inputì„ ë°›ìŠµë‹ˆë‹¤.
    - ì§ˆë¬¸ ìƒì„± â†’ interrupt() í˜¸ì¶œ â†’ ì‚¬ìš©ì ì‘ë‹µ ìˆ˜ì‹  â†’ ì²˜ë¦¬
    - ëŒ€í™” íˆìŠ¤í† ë¦¬ ìë™ ì €ì¥
    """
    from langgraph.types import interrupt
    from src.agents.nodes.common import (
        add_conversation_turn, 
        create_empty_conversation_history
    )
    
    print("\n" + "="*80)
    print("ğŸ§‘ [CLASSIFICATION REVIEW] Human-in-the-Loop")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    uncertain_files = classification_result.get("uncertain_files", [])
    classifications = classification_result.get("classifications", {})
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)
    dataset_id = state.get("current_dataset_id", "unknown")
    conversation_history = state.get("conversation_history")
    if not conversation_history:
        conversation_history = create_empty_conversation_history(dataset_id)
    
    # ë¶ˆí™•ì‹¤í•œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if not uncertain_files:
        print("   âœ… ë¶ˆí™•ì‹¤í•œ íŒŒì¼ ì—†ìŒ - ë¦¬ë·° ìŠ¤í‚µ")
        return {
            "needs_human_review": False,
            "logs": ["âœ… [Review] ëª¨ë“  íŒŒì¼ ë¶„ë¥˜ í™•ì •"]
        }
    
    # =========================================================================
    # Human-in-the-Loop: ë¶ˆí™•ì‹¤í•œ íŒŒì¼ì´ ìˆìœ¼ë©´ interruptë¡œ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
    # =========================================================================
    
    # ë°˜ë³µ ì²˜ë¦¬ (ì—¬ëŸ¬ ë¼ìš´ë“œ ê°€ëŠ¥)
    remaining_uncertain = uncertain_files.copy()
    current_classifications = classifications.copy()
    
    while remaining_uncertain:
        # 1. ì§ˆë¬¸ ìƒì„±
        question = _generate_classification_question(remaining_uncertain, current_classifications)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± (Knowledge Graphìš©)
        context_snapshot = {
            "uncertain_files": remaining_uncertain.copy(),
            "classifications": {
                fp: {
                    "classification": clf.get("classification"),
                    "confidence": clf.get("confidence"),
                    "reasoning": clf.get("reasoning", "")[:200]
                }
                for fp, clf in current_classifications.items()
                if fp in remaining_uncertain
            },
            "total_metadata": len(classification_result.get("metadata_files", [])),
            "total_data": len(classification_result.get("data_files", []))
        }
        
        print(f"\n   â“ {len(remaining_uncertain)}ê°œ íŒŒì¼ì— ëŒ€í•´ ì‚¬ìš©ì í™•ì¸ ìš”ì²­")
        print("="*80)
        
        # 3. interrupt() í˜¸ì¶œ - ì—¬ê¸°ì„œ ê·¸ë˜í”„ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ê³  ì‚¬ìš©ì ì‘ë‹µì„ ê¸°ë‹¤ë¦¼
        # ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ììœ ë¡­ê²Œ ì‘ë‹µí•˜ë©´ LLMì´ íŒŒì‹±í•¨
        human_response = interrupt({
            "type": "classification_review",
            "question": question,
            "uncertain_files": remaining_uncertain,
            "context": context_snapshot
        })
        
        # 4. ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬
        print(f"\n   ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì‹ : '{human_response}'")
        
        # 5. í”¼ë“œë°± íŒŒì‹± ë° ë¶„ë¥˜ ì—…ë°ì´íŠ¸
        current_classifications = _parse_classification_feedback(
            feedback=human_response,
            classifications=current_classifications,
            uncertain_files=remaining_uncertain
        )
        
        # 6. ê²°ê³¼ì— ë”°ë¥¸ ë¶„ë¥˜
        new_metadata_files = []
        new_data_files = []
        new_remaining_uncertain = []
        
        for file_path in remaining_uncertain:
            clf = current_classifications.get(file_path, {})
            if clf.get("human_confirmed"):
                if clf["classification"] == "metadata":
                    new_metadata_files.append(file_path)
                elif clf["classification"] == "data":
                    new_data_files.append(file_path)
                # unknownì€ ì œì™¸ë¨ (skip)
            elif clf.get("needs_review"):
                new_remaining_uncertain.append(file_path)
            elif clf["classification"] == "metadata":
                new_metadata_files.append(file_path)
            elif clf["classification"] == "data":
                new_data_files.append(file_path)
        
        # 7. ì—ì´ì „íŠ¸ ì•¡ì…˜ ê²°ì •
        action_parts = []
        if new_metadata_files:
            action_parts.append(f"ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¥˜: {len(new_metadata_files)}ê°œ")
        if new_data_files:
            action_parts.append(f"ë°ì´í„°ë¡œ ë¶„ë¥˜: {len(new_data_files)}ê°œ")
        skipped = len(remaining_uncertain) - len(new_metadata_files) - len(new_data_files) - len(new_remaining_uncertain)
        if skipped > 0:
            action_parts.append(f"ì œì™¸: {skipped}ê°œ")
        agent_action = ", ".join(action_parts) if action_parts else "ë³€ê²½ ì—†ìŒ"
        
        # 8. ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ê¸°ë¡ + ìë™ ì €ì¥
        conversation_history = add_conversation_turn(
            history=conversation_history,
            review_type="classification",
            agent_question=question,
            human_response=human_response,
            agent_action=agent_action,
            file_path=", ".join([os.path.basename(f) for f in remaining_uncertain[:3]]),
            context_summary=f"ë¶ˆí™•ì‹¤í•œ íŒŒì¼ {len(remaining_uncertain)}ê°œ ë¶„ë¥˜ í™•ì¸",
            context_snapshot=context_snapshot,
            auto_save=True
        )
        
        print(f"   âœ… ë¶„ë¥˜ ì—…ë°ì´íŠ¸: {agent_action}")
        
        # ë‹¤ìŒ ë¼ìš´ë“œ ì¤€ë¹„
        remaining_uncertain = new_remaining_uncertain
        
        # ì¶”ê°€ í™•ì¸ì´ í•„ìš” ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
        if not remaining_uncertain:
            break
    
    # =========================================================================
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    # =========================================================================
    
    # ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ ì§‘ê³„
    final_metadata = classification_result.get("metadata_files", []).copy()
    final_data = classification_result.get("data_files", []).copy()
    
    for file_path, clf in current_classifications.items():
        if clf.get("human_confirmed") or not clf.get("needs_review"):
            if clf["classification"] == "metadata" and file_path not in final_metadata:
                final_metadata.append(file_path)
            elif clf["classification"] == "data" and file_path not in final_data:
                final_data.append(file_path)
    
    updated_result: ClassificationResult = {
        "total_files": classification_result["total_files"],
        "metadata_files": final_metadata,
        "data_files": final_data,
        "uncertain_files": [],  # ëª¨ë‘ ì²˜ë¦¬ë¨
        "classifications": current_classifications
    }
    
    progress = state.get("processing_progress", {})
    progress["phase"] = "classification_review"
    
    print(f"\n   âœ… ë¶„ë¥˜ í™•ì • ì™„ë£Œ")
    print(f"      - ë©”íƒ€ë°ì´í„°: {len(final_metadata)}ê°œ")
    print(f"      - ë°ì´í„°: {len(final_data)}ê°œ")
    print("="*80)
    
    return {
        "classification_result": updated_result,
        "processing_progress": progress,
        "conversation_history": conversation_history,
        "needs_human_review": False,
        "human_feedback": None,
        "logs": [f"âœ… [Review] ë¶„ë¥˜ í™•ì • ì™„ë£Œ - ë©”íƒ€ë°ì´í„°: {len(final_metadata)}ê°œ, ë°ì´í„°: {len(final_data)}ê°œ"]
    }


def _generate_classification_question(
    uncertain_files: List[str], 
    classifications: Dict[str, FileClassification]
) -> str:
    """
    Generate user-friendly classification review question using LLM.
    Only asks about uncertain files with low confidence.
    """
    from src.utils.llm_client import get_llm_client
    
    # Prepare file summaries for uncertain files only
    file_summaries = []
    for idx, file_path in enumerate(uncertain_files[:10], 1):
        clf = classifications.get(file_path, {})
        filename = clf.get("filename", os.path.basename(file_path))
        predicted = clf.get("classification", "unknown")
        confidence = clf.get("confidence", 0.0)
        reasoning = clf.get("reasoning", "")[:150]
        
        file_summaries.append({
            "index": idx,
            "filename": filename,
            "predicted": predicted,
            "confidence": f"{confidence:.0%}",
            "reasoning": reasoning
        })
    
    # LLM prompt for question generation
    prompt = f"""You are a UI assistant for a medical data classification system.
Generate a concise, user-friendly question asking the user to verify file classifications.

[UNCERTAIN FILES - Need Review]
{_format_file_summaries_for_prompt(file_summaries)}

[TASK]
Create a clear question in Korean that:
1. Lists each file with: number, filename, AI prediction (ğŸ“–=metadata, ğŸ“Š=data), confidence, and brief reason
2. Explains response options: "ok" to approve all, or specify changes like "1ë²ˆ ë°ì´í„°" or "2ë²ˆ ì œì™¸"

Keep it concise. Output plain text only (no JSON):"""

    try:
        llm = get_llm_client()
        generated_question = llm.ask_text(prompt)
        return generated_question.strip()
    except Exception as e:
        print(f"   âš ï¸ LLM question generation failed, using fallback: {e}")
        return _generate_fallback_question(uncertain_files, classifications)


def _format_file_summaries_for_prompt(file_summaries: List[Dict]) -> str:
    """Format file summaries for LLM prompt"""
    lines = []
    for fs in file_summaries:
        lines.append(
            f"File {fs['index']}: {fs['filename']}\n"
            f"  - Prediction: {fs['predicted']} ({fs['confidence']})\n"
            f"  - Reason: {fs['reasoning'] or 'N/A'}"
        )
    return "\n".join(lines)


def _generate_fallback_question(
    uncertain_files: List[str], 
    classifications: Dict[str, FileClassification]
) -> str:
    """LLM ì‹¤íŒ¨ì‹œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ í…œí”Œë¦¿ ì§ˆë¬¸"""
    question_parts = [
        "ğŸ“‹ **íŒŒì¼ ë¶„ë¥˜ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤**\n",
        "ì•„ë˜ íŒŒì¼ë“¤ì˜ ë¶„ë¥˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\n"
    ]
    
    for idx, file_path in enumerate(uncertain_files[:5], 1):
        clf = classifications.get(file_path, {})
        filename = clf.get("filename", os.path.basename(file_path))
        predicted = clf.get("classification", "unknown")
        confidence = clf.get("confidence", 0.0)
        reasoning = clf.get("reasoning", "")[:100]
        
        pred_emoji = "ğŸ“–" if predicted == "metadata" else "ğŸ“Š" if predicted == "data" else "â“"
        pred_text = "ë©”íƒ€ë°ì´í„°" if predicted == "metadata" else "ë°ì´í„°" if predicted == "data" else "ì•Œ ìˆ˜ ì—†ìŒ"
        
        question_parts.append(
            f"\n**{idx}. {filename}**\n"
            f"   - AI ì˜ˆì¸¡: {pred_emoji} {pred_text} ({confidence:.0%})\n"
            f"   - íŒë‹¨ ê·¼ê±°: {reasoning}...\n"
        )
    
    if len(uncertain_files) > 5:
        question_parts.append(f"\n... ì™¸ {len(uncertain_files) - 5}ê°œ íŒŒì¼\n")
    
    question_parts.append(
        "\n**ì‘ë‹µ ë°©ë²•:**\n"
        "- ëª¨ë‘ ë§ìœ¼ë©´: `í™•ì¸`, `ok`, `ëª¨ë‘ ë§ì•„` ë“±\n"
        "- ìˆ˜ì • í•„ìš”: `1ë²ˆ ë°ì´í„°`, `2ë²ˆ ë©”íƒ€ë°ì´í„°ë¡œ ë³€ê²½` ë“± ìì—°ì–´ë¡œ\n"
        "- íŒŒì¼ ì œì™¸: `3ë²ˆ ì œì™¸`, `3ë²ˆ ìŠ¤í‚µ` ë“±\n"
    )
    
    return "".join(question_parts)

def _parse_classification_feedback(
    feedback: str, 
    classifications: Dict[str, FileClassification],
    uncertain_files: List[str]
) -> Dict[str, FileClassification]:
    """
    Parse user's natural language feedback using LLM and update classifications.
    
    Handles various response formats:
    - Simple approval: "ok", "í™•ì¸", "ë§ì•„"
    - Index-based: "1ë²ˆ ë°ì´í„°", "2ë²ˆì€ ë©”íƒ€ë°ì´í„°ì•¼"
    - Filename-based: "clinical_infoëŠ” ë°ì´í„°ê³  parametersëŠ” ë©”íƒ€ë°ì´í„°ì•¼"
    - Descriptive: "ì»¬ëŸ¼ ì„¤ëª…ì´ ìˆëŠ” íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°ì•¼", "ì‹¤ì œ í™˜ì ê¸°ë¡ì´ ìˆëŠ”ê±´ ë°ì´í„°"
    - Mixed explanations with classification hints
    """
    from src.utils.llm_client import get_llm_client
    import json
    
    updated = classifications.copy()
    feedback_stripped = feedback.strip()
    
    if not feedback_stripped:
        return updated
    
    # Prepare detailed file context for LLM
    file_context = []
    for idx, file_path in enumerate(uncertain_files, 1):
        clf = classifications.get(file_path, {})
        filename = clf.get("filename", os.path.basename(file_path))
        current_type = clf.get("classification", "unknown")
        confidence = clf.get("confidence", 0.0)
        reasoning = clf.get("reasoning", "")[:100]
        
        file_context.append({
            "index": idx,
            "filename": filename,
            "file_path": file_path,
            "ai_prediction": current_type,
            "confidence": f"{confidence:.0%}",
            "ai_reasoning": reasoning
        })
    
    # Enhanced LLM prompt for flexible natural language parsing
    prompt = f"""You are parsing user feedback about file classification for a medical data indexing system.

[CONTEXT - FILES UNDER REVIEW]
{json.dumps(file_context, indent=2)}

[USER'S RESPONSE]
"{feedback_stripped}"

[YOUR TASK]
Analyze the user's natural language response and extract classification decisions.
The user may:
1. Approve all AI predictions ("ok", "í™•ì¸", "ë§ì•„", "ê·¸ëŒ€ë¡œ í•´")
2. Refer to files by index number ("1ë²ˆì€ ë°ì´í„°", "2ë²ˆ ë©”íƒ€ë°ì´í„°ë¡œ")
3. Refer to files by filename ("clinical_infoëŠ” ë°ì´í„°ì•¼", "parameters íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°")
4. Give descriptive explanations ("ì»¬ëŸ¼ ì„¤ëª…ì´ ìˆëŠ” íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°", "ì‹¤ì œ í™˜ì ë°ì´í„°ê°€ ìˆëŠ”ê±´ data")
5. Provide mixed responses with partial approvals and corrections

[CLASSIFICATION DEFINITIONS]
- METADATA: Files that DESCRIBE other data (column definitions, parameter lists, codebooks, data dictionaries)
- DATA: Files containing actual records/measurements (patient records, clinical data, transactions)
- SKIP: Files to exclude from processing

[OUTPUT FORMAT - JSON ONLY]
{{
    "action": "approve_all" | "modify" | "partial_approve",
    "changes": [
        {{
            "index": <1-based index>,
            "filename": "<filename for reference>",
            "new_type": "metadata" | "data" | "skip",
            "reason": "<brief reason extracted from user feedback>"
        }}
    ],
    "unmentioned_files": "approve" | "keep_uncertain",
    "summary": "<brief summary of what you understood>"
}}

RULES:
- If user approves everything: {{"action": "approve_all", "changes": [], "summary": "..."}}
- If user mentions specific files: extract each file's new classification
- If user gives general rules (e.g., "files with column descriptions are metadata"): apply to matching files
- Match filenames flexibly (partial match OK, case-insensitive)
- "unmentioned_files": "approve" if user seems satisfied with AI predictions for unmentioned files
- "unmentioned_files": "keep_uncertain" if only mentioned files should be updated
"""

    try:
        llm = get_llm_client()
        parsed_result = llm.ask_json(prompt)
        
        action = parsed_result.get("action", "unclear")
        summary = parsed_result.get("summary", "")
        
        print(f"   ğŸ§  [Parser] LLM ë¶„ì„ ê²°ê³¼: {summary}")
        
        if action == "approve_all":
            # ì „ì²´ ìŠ¹ì¸
            print("   âœ… [Parser] ì „ì²´ ìŠ¹ì¸")
            for file_path in uncertain_files:
                if file_path in updated:
                    updated[file_path]["human_confirmed"] = True
                    updated[file_path]["needs_review"] = False
            return updated
        
        elif action in ["modify", "partial_approve"]:
            changes = parsed_result.get("changes", [])
            unmentioned = parsed_result.get("unmentioned_files", "keep_uncertain")
            
            print(f"   âœï¸ [Parser] {len(changes)}ê°œ íŒŒì¼ ë¶„ë¥˜ ê²°ì • ê°ì§€")
            
            # ë³€ê²½ëœ íŒŒì¼ ì¸ë±ìŠ¤ ì¶”ì 
            modified_indices = set()
            
            for change in changes:
                idx = change.get("index", 0) - 1  # 1-indexed â†’ 0-indexed
                new_type = change.get("new_type", "").lower()
                reason = change.get("reason", "")
                filename = change.get("filename", "")
                
                if 0 <= idx < len(uncertain_files):
                    file_path = uncertain_files[idx]
                    modified_indices.add(idx)
                    
                    if new_type == "skip":
                        updated[file_path]["classification"] = "unknown"
                        updated[file_path]["human_confirmed"] = True
                        updated[file_path]["needs_review"] = False
                        print(f"      - [{idx+1}] {filename}: ì œì™¸ ({reason})")
                    elif new_type == "metadata":
                        updated[file_path]["classification"] = "metadata"
                        updated[file_path]["human_confirmed"] = True
                        updated[file_path]["needs_review"] = False
                        print(f"      - [{idx+1}] {filename}: ë©”íƒ€ë°ì´í„° ({reason})")
                    elif new_type == "data":
                        updated[file_path]["classification"] = "data"
                        updated[file_path]["human_confirmed"] = True
                        updated[file_path]["needs_review"] = False
                        print(f"      - [{idx+1}] {filename}: ë°ì´í„° ({reason})")
            
            # ì–¸ê¸‰ë˜ì§€ ì•Šì€ íŒŒì¼ ì²˜ë¦¬
            if unmentioned == "approve":
                for idx, file_path in enumerate(uncertain_files):
                    if idx not in modified_indices and file_path in updated:
                        updated[file_path]["human_confirmed"] = True
                        updated[file_path]["needs_review"] = False
                print(f"   âœ… [Parser] ì–¸ê¸‰ë˜ì§€ ì•Šì€ íŒŒì¼ë“¤ì€ AI ì˜ˆì¸¡ ìŠ¹ì¸")
            else:
                remaining = len(uncertain_files) - len(modified_indices)
                if remaining > 0:
                    print(f"   â³ [Parser] {remaining}ê°œ íŒŒì¼ì€ ì—¬ì „íˆ í™•ì¸ í•„ìš”")
            
            return updated
        
        else:
            # ì´í•´ ë¶ˆê°€ - í´ë°±
            print(f"   âš ï¸ [Parser] ì‘ë‹µ í•´ì„ ì–´ë ¤ì›€, ì •ê·œì‹ í´ë°± ì‹œë„")
            return _parse_feedback_regex_fallback(feedback, classifications, uncertain_files)
            
    except Exception as e:
        print(f"   âš ï¸ [Parser] LLM íŒŒì‹± ì‹¤íŒ¨: {e}")
        print("   ğŸ”„ [Parser] ì •ê·œì‹ í´ë°± íŒŒì‹± ì‹œë„...")
        return _parse_feedback_regex_fallback(feedback, classifications, uncertain_files)


def _parse_feedback_regex_fallback(
    feedback: str,
    classifications: Dict[str, FileClassification],
    uncertain_files: List[str]
) -> Dict[str, FileClassification]:
    """LLM ì‹¤íŒ¨ì‹œ ì •ê·œì‹ ê¸°ë°˜ í´ë°± íŒŒì‹±"""
    import re
    
    updated = classifications.copy()
    feedback_lower = feedback.lower().strip()
    
    # ì „ì²´ ìŠ¹ì¸ í‚¤ì›Œë“œ ì²´í¬
    approve_keywords = ["í™•ì¸", "ok", "yes", "y", "approve", "ìŠ¹ì¸", "ëª¨ë‘ ë§ì•„", "ê·¸ëŒ€ë¡œ", "ë§ì•„"]
    if feedback_lower in approve_keywords or any(kw in feedback_lower for kw in approve_keywords):
        # ìˆ˜ì • ì§€ì‹œê°€ ì—†ëŠ” ìˆœìˆ˜ ìŠ¹ì¸ì¸ì§€ í™•ì¸
        if not re.search(r'\d+', feedback_lower):
            for file_path in uncertain_files:
                if file_path in updated:
                    updated[file_path]["human_confirmed"] = True
                    updated[file_path]["needs_review"] = False
            return updated
    
    # ê°œë³„ ìˆ˜ì • íŒ¨í„´ ë§¤ì¹­ (ë” ìœ ì—°í•œ íŒ¨í„´)
    # íŒ¨í„´1: "1:ë°ì´í„°", "1ï¼šë©”íƒ€ë°ì´í„°"
    corrections = re.findall(r'(\d+)\s*[:ï¼š]\s*(ë©”íƒ€ë°ì´í„°|ë°ì´í„°|metadata|data|ì œì™¸|skip)', feedback_lower)
    
    # íŒ¨í„´2: "1ë²ˆ ë°ì´í„°", "1ë²ˆì€ ë©”íƒ€ë°ì´í„°", "ì²«ë²ˆì§¸ data"
    corrections += re.findall(r'(\d+)\s*ë²ˆ?\s*(?:ì€|ëŠ”)?\s*(ë©”íƒ€ë°ì´í„°|ë°ì´í„°|metadata|data|ì œì™¸|skip)', feedback_lower)
    
    for idx_str, new_type in corrections:
        idx = int(idx_str) - 1
        
        if 0 <= idx < len(uncertain_files):
            file_path = uncertain_files[idx]
            
            if new_type in ["ì œì™¸", "skip"]:
                updated[file_path]["classification"] = "unknown"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë©”íƒ€ë°ì´í„°", "metadata"]:
                updated[file_path]["classification"] = "metadata"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë°ì´í„°", "data"]:
                updated[file_path]["classification"] = "data"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
    
    return updated


def process_metadata_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-1] ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ë…¸ë“œ (Hybrid Approach)
    
    Hybrid ì›Œí¬í”Œë¡œìš°:
    1. Rule-based íŒŒì‹±: parse_metadata_content()ë¡œ ê¸°ë³¸ definitions ì¶”ì¶œ
    2. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™”ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
    3. LLM Enrichment: íŒŒì‹±ëœ definitionsë¥¼ ì˜ë£Œ ë„ë©”ì¸ ê´€ì ì—ì„œ í’ë¶€í•˜ê²Œ
    4. ê´€ê³„ ì¶”ë¡ : ê°œë… ê°„ì˜ ê³„ì¸µ/ì˜ë¯¸ ê´€ê³„ ì¶”ë¡ 
    5. Neo4j ì €ì¥: enriched definitions + relationshipsë¥¼ ì˜¨í†¨ë¡œì§€ì— ì €ì¥
    
    ì´ ì ‘ê·¼ë²•ì˜ ì¥ì :
    - ë¹„ìš© íš¨ìœ¨: ê·œì¹™ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ ê¸°ë³¸ ì¶”ì¶œ (LLM ë¹„ìš© 0)
    - í™˜ê° ìµœì†Œí™”: íŒŒì‹±ëœ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ LLMì´ ë¶„ì„
    - ì¬í˜„ì„±: ê·œì¹™ ê¸°ë°˜ íŒŒì‹±ì€ ê²°ì •ì 
    - ëŒ€í™” í™œìš©: ì´ì „ ì‚¬ìš©ì í”¼ë“œë°±ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
    """
    print("\n" + "="*80)
    print("ğŸ“– [METADATA PROCESSOR] Phase 2-1 - ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ (Hybrid)")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    metadata_files = classification_result.get("metadata_files", [])
    progress = state.get("processing_progress", {})
    conversation_history = state.get("conversation_history", {})
    
    # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
    ontology = state.get("ontology_context")
    if not ontology or not ontology.get("definitions"):
        ontology = ontology_manager.load() or {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {},
            "column_hierarchy": []  # NEW: ì»¬ëŸ¼ ê³„ì¸µ ì •ë³´
        }
    
    if not metadata_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "metadata_processing"
        progress["metadata_processed"] = []
        
        return {
            "ontology_context": ontology,
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ - ìŠ¤í‚µ"]
        }
    
    print(f"   ğŸ“‚ ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
    
    # =========================================================================
    # Step 1: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œ ë²ˆë§Œ)
    # =========================================================================
    conversation_context = ""
    if conversation_history:
        conversation_context = extract_relevant_context(conversation_history)
        if conversation_context:
            print(f"\n   ğŸ’¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œë¨ ({len(conversation_context)} chars)")
            print(f"   â”œâ”€ ì´ì „ ëŒ€í™” ì •ë³´ê°€ LLM ë¶„ì„ì— í™œìš©ë©ë‹ˆë‹¤")
    
    # =========================================================================
    # Step 2: íŒŒì¼ë³„ ì²˜ë¦¬ (Processor ê¸°ë°˜ íŒŒì‹±)
    # =========================================================================
    processed_metadata = []
    skipped_metadata = []  # NEW: ìŠ¤í‚µëœ íŒŒì¼ ì¶”ì 
    total_definitions = 0
    all_definitions = {}  # ëª¨ë“  íŒŒì¼ì˜ definitions í•©ì¹¨
    
    print(f"\n   â”€â”€â”€â”€â”€â”€â”€â”€â”€ Processor ê¸°ë°˜ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    for idx, file_path in enumerate(metadata_files):
        filename = os.path.basename(file_path)
        print(f"\n   [{idx+1}/{len(metadata_files)}] {filename}")
        
        try:
            # íŒŒì¼ íƒœê¹…
            ontology["file_tags"][file_path] = {
                "type": "metadata",
                "role": "dictionary",
                "confidence": classification_result["classifications"].get(file_path, {}).get("confidence", 0.8),
                "detected_at": datetime.now().isoformat()
            }
            
            # Processor ê¸°ë°˜ íŒŒì‹± (ìˆ˜ì •ë¨)
            new_definitions = parse_metadata_content(file_path)
            
            if new_definitions:
                ontology["definitions"].update(new_definitions)
                all_definitions.update(new_definitions)
                total_definitions += len(new_definitions)
                processed_metadata.append(file_path)
                print(f"      âœ… Processor: {len(new_definitions)}ê°œ ìš©ì–´ íŒŒì‹±ë¨")
            else:
                print(f"      âš ï¸ íŒŒì‹±ëœ ìš©ì–´ ì—†ìŒ - ìŠ¤í‚µ")
                skipped_metadata.append({
                    "file": file_path,
                    "filename": filename,
                    "reason": "íŒŒì‹±ëœ ìš©ì–´ ì—†ìŒ"
                })
                
        except Exception as e:
            print(f"      âŒ íŒŒì‹± ì‹¤íŒ¨: {e} - ìŠ¤í‚µ")
            skipped_metadata.append({
                "file": file_path,
                "filename": filename,
                "reason": str(e)
            })
    
    # =========================================================================
    # Step 3: LLM Enrichment (ëª¨ë“  definitions í•œë²ˆì—)
    # =========================================================================
    enrichments = []
    relationships_result = {}
    
    if all_definitions:
        print(f"\n   â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM Enrichment â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        try:
            # LLMìœ¼ë¡œ definitions í’ë¶€í•˜ê²Œ ë§Œë“¤ê¸°
            # max_chunks ì„¤ì •: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œë©´ 1ê°œë§Œ, ì•„ë‹ˆë©´ ì „ì²´
            max_chunks = None
            if MetadataEnrichmentConfig.FAST_TEST_MODE:
                max_chunks = MetadataEnrichmentConfig.FAST_TEST_MAX_CHUNKS
                print(f"      âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìµœëŒ€ {max_chunks}ê°œ ì²­í¬ë§Œ ì²˜ë¦¬")
            
            enrichments = enrich_definitions_with_llm(
                definitions=all_definitions,
                conversation_context=conversation_context,
                max_chunks=max_chunks
            )
            
            if enrichments:
                print(f"\n   âœ… LLM Enrichment ì™„ë£Œ: {len(enrichments)}ê°œ ìš©ì–´ ë¶„ì„ë¨")
                
                # Neo4jì— enriched definitions ì €ì¥
                try:
                    ontology_manager.enrich_concepts_batch([
                        {
                            "name": e["name"],
                            "enriched_definition": e.get("enriched_definition", ""),
                            "analysis_context": e.get("analysis_context", "")
                        }
                        for e in enrichments
                    ])
                    print(f"   âœ… Neo4jì— enriched definitions ì €ì¥ë¨")
                except Exception as e:
                    print(f"   âš ï¸ Neo4j enrichment ì €ì¥ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            print(f"   âš ï¸ LLM Enrichment ì‹¤íŒ¨: {e}")
        
        # =========================================================================
        # Step 4: ê´€ê³„ ì¶”ë¡ 
        # =========================================================================
        print(f"\n   â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê´€ê³„ ì¶”ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        try:
            relationships_result = infer_concept_relationships(
                definitions=all_definitions,
                enrichments=enrichments,
                conversation_context=conversation_context
            )
            
            # ê³„ì¸µ íŒíŠ¸ë¥¼ ì˜¨í†¨ë¡œì§€ì— ë°˜ì˜
            hierarchy_hints = relationships_result.get("hierarchy_hints", [])
            if hierarchy_hints:
                for hint in hierarchy_hints:
                    # ê¸°ì¡´ hierarchyì— ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                    existing_entities = {h.get("entity_name") for h in ontology.get("hierarchy", [])}
                    if hint.get("concept") not in existing_entities:
                        ontology["hierarchy"].append({
                            "entity_name": hint.get("concept"),
                            "level": hint.get("level", 99),
                            "identifier_column": hint.get("concept"),
                            "confidence": 0.7,
                            "inferred_from": "metadata_analysis"
                        })
                
                # ë ˆë²¨ ì •ë ¬
                ontology["hierarchy"].sort(key=lambda x: x.get("level", 99))
                print(f"   âœ… {len(hierarchy_hints)}ê°œ ê³„ì¸µ íŒíŠ¸ ë°˜ì˜ë¨")
            
            # ê°œë… ê´€ê³„ë¥¼ ì˜¨í†¨ë¡œì§€ì— ë°˜ì˜
            concept_rels = relationships_result.get("concept_relationships", [])
            if concept_rels:
                for rel in concept_rels:
                    # column_hierarchyì— ì¶”ê°€
                    if "column_hierarchy" not in ontology:
                        ontology["column_hierarchy"] = []
                    
                    ontology["column_hierarchy"].append({
                        "child_column": rel.get("source"),
                        "parent_column": rel.get("target"),
                        "cardinality": rel.get("cardinality", "N:1"),
                        "hierarchy_type": rel.get("relation_type", "PARENT_OF"),
                        "reasoning": rel.get("reasoning", ""),
                        "table_name": "metadata_inferred"
                    })
                
                print(f"   âœ… {len(concept_rels)}ê°œ ê°œë… ê´€ê³„ ë°˜ì˜ë¨")
                
        except Exception as e:
            print(f"   âš ï¸ ê´€ê³„ ì¶”ë¡  ì‹¤íŒ¨: {e}")
    
    # =========================================================================
    # Step 5: ì˜¨í†¨ë¡œì§€ ì €ì¥
    # =========================================================================
    ontology_manager.save(ontology)
    
    progress["phase"] = "metadata_processing"
    progress["metadata_processed"] = processed_metadata
    progress["skipped_metadata_files"] = skipped_metadata  # NEW: ìŠ¤í‚µëœ íŒŒì¼ ê¸°ë¡
    
    # í†µê³„ ìš”ì•½
    enriched_count = len(enrichments)
    rel_count = len(relationships_result.get("concept_relationships", []))
    hierarchy_count = len(relationships_result.get("hierarchy_hints", []))
    skipped_count = len(skipped_metadata)
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (Processor-based Hybrid):")
    print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {len(processed_metadata)}ê°œ")
    if skipped_count > 0:
        print(f"   - ìŠ¤í‚µëœ íŒŒì¼: {skipped_count}ê°œ")
        for skip in skipped_metadata:
            print(f"      â””â”€ {skip['filename']}: {skip['reason']}")
    print(f"   - íŒŒì‹±ëœ ìš©ì–´: {total_definitions}ê°œ (Processor)")
    print(f"   - Enriched ìš©ì–´: {enriched_count}ê°œ (LLM)")
    print(f"   - ì¶”ë¡ ëœ ê´€ê³„: {rel_count}ê°œ")
    print(f"   - ê³„ì¸µ íŒíŠ¸: {hierarchy_count}ê°œ")
    if conversation_context:
        print(f"   - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸: í™œìš©ë¨ âœ“")
    print("="*80)
    
    log_msg = (
        f"ğŸ“– [Metadata] Hybrid ì²˜ë¦¬ ì™„ë£Œ: "
        f"{len(processed_metadata)}ê°œ íŒŒì¼, "
        f"{total_definitions}ê°œ íŒŒì‹±, "
        f"{enriched_count}ê°œ enriched, "
        f"{rel_count}ê°œ ê´€ê³„ ì¶”ë¡ "
    )
    if skipped_count > 0:
        log_msg += f", {skipped_count}ê°œ ìŠ¤í‚µ"
    
    return {
        "ontology_context": ontology,
        "processing_progress": progress,
        "logs": [log_msg]
    }


def process_data_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-2] ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ì¤€ë¹„ ë…¸ë“œ
    """
    print("\n" + "="*80)
    print("ğŸ“Š [DATA PROCESSOR] Phase 2-2 - ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    if not data_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "complete"
        
        return {
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Data] ë°ì´í„° íŒŒì¼ ì—†ìŒ - ì™„ë£Œ"]
        }
    
    print(f"   ğŸ“‚ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    
    first_file = data_files[0]
    
    progress["phase"] = "data_processing"
    progress["current_file"] = first_file
    progress["current_file_index"] = 0
    progress["total_files"] = len(data_files)
    
    print(f"\n   â†’ ì²˜ë¦¬ íŒŒì¼: {os.path.basename(first_file)}")
    print("="*80)
    
    return {
        "file_path": first_file,
        "processing_progress": progress,
        "skip_indexing": False,
        "logs": [f"ğŸ“Š [Data] {len(data_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘"]
    }


def advance_to_next_file_node(state: AgentState) -> Dict[str, Any]:
    """
    [Helper] ë‹¤ìŒ ë°ì´í„° íŒŒì¼ë¡œ ì§„í–‰
    
    ìŠ¤í‚µëœ íŒŒì¼ë„ ì¶”ì í•˜ì—¬ progressì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    print("\n" + "-"*40)
    print("â¡ï¸ [ADVANCE] ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™")
    print("-"*40)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    current_idx = progress.get("current_file_index", 0)
    current_file = progress.get("current_file", "")
    
    # ìŠ¤í‚µ ì—¬ë¶€ í™•ì¸
    was_skipped = state.get("skip_indexing", False)
    skip_reason = state.get("skip_reason", "")
    
    if current_file:
        if was_skipped:
            # ìŠ¤í‚µëœ íŒŒì¼ ê¸°ë¡
            if "skipped_data_files" not in progress:
                progress["skipped_data_files"] = []
            progress["skipped_data_files"].append({
                "file": current_file,
                "filename": os.path.basename(current_file),
                "reason": skip_reason
            })
            print(f"   â­ï¸ ìŠ¤í‚µë¨: {os.path.basename(current_file)} ({skip_reason})")
        elif current_file not in progress.get("data_processed", []):
            # ì •ìƒ ì²˜ë¦¬ëœ íŒŒì¼ ê¸°ë¡
            if "data_processed" not in progress:
                progress["data_processed"] = []
            progress["data_processed"].append(current_file)
    
    next_idx = current_idx + 1
    
    if next_idx >= len(data_files):
        processed_count = len(progress.get("data_processed", []))
        skipped_count = len(progress.get("skipped_data_files", []))
        
        print(f"   âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
        print(f"      - ì²˜ë¦¬ë¨: {processed_count}ê°œ")
        if skipped_count > 0:
            print(f"      - ìŠ¤í‚µë¨: {skipped_count}ê°œ")
            for skip in progress.get("skipped_data_files", []):
                print(f"        â””â”€ {skip['filename']}: {skip['reason']}")
        
        progress["phase"] = "complete"
        progress["current_file"] = None
        progress["all_files_processed"] = True  # ëª…í™•í•œ ì¢…ë£Œ í”Œë˜ê·¸
        
        log_msg = f"âœ… [Complete] ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ (ì²˜ë¦¬: {processed_count}ê°œ"
        if skipped_count > 0:
            log_msg += f", ìŠ¤í‚µ: {skipped_count}ê°œ"
        log_msg += ")"
        
        return {
            "processing_progress": progress,
            "logs": [log_msg]
        }
    
    next_file = data_files[next_idx]
    progress["current_file"] = next_file
    progress["current_file_index"] = next_idx
    
    print(f"   ğŸ“‚ ë‹¤ìŒ íŒŒì¼: [{next_idx + 1}/{len(data_files)}] {os.path.basename(next_file)}")
    
    return {
        "file_path": next_file,
        "processing_progress": progress,
        "raw_metadata": {},
        "entity_identification": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_feedback": None,
        "skip_indexing": False,
        "retry_count": 0,
        "logs": [f"â¡ï¸ [Advance] ë‹¤ìŒ íŒŒì¼: {os.path.basename(next_file)}"]
    }

