# src/agents/nodes/batch.py
"""
Batch Processing Nodes - 2-Phase Workflow
"""

import os
from datetime import datetime
from typing import Dict, Any, List

from src.agents.state import (
    AgentState, FileClassification, ClassificationResult, ProcessingProgress
)
from src.agents.nodes.common import processors, ontology_manager
from src.agents.helpers.llm_helpers import ask_llm_is_metadata
from src.agents.helpers.metadata_helpers import (
    build_metadata_detection_context, 
    parse_metadata_content
)
from src.config import HumanReviewConfig


def batch_classifier_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1] ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ë…¸ë“œ
    """
    print("\n" + "="*80)
    print("ğŸ“‹ [BATCH CLASSIFIER] Phase 1 ì‹œì‘ - ì „ì²´ íŒŒì¼ ë¶„ë¥˜")
    print("="*80)
    
    input_files = state.get("input_files", [])
    
    if not input_files:
        return {
            "logs": ["âŒ Error: ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."],
            "error_message": "No input files provided"
        }
    
    print(f"   ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼: {len(input_files)}ê°œ")
    
    classifications: Dict[str, FileClassification] = {}
    metadata_files: List[str] = []
    data_files: List[str] = []
    uncertain_files: List[str] = []
    
    # Signal file extensions that are always data (not metadata)
    SIGNAL_EXTENSIONS = {'.vital', '.edf', '.bdf'}
    
    for idx, file_path in enumerate(input_files):
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(filename)[1].lower()
        print(f"\n   [{idx+1}/{len(input_files)}] {filename}")
        
        try:
            # Rule-based: Signal files are always data
            if file_ext in SIGNAL_EXTENSIONS:
                classifications[file_path] = {
                    "file_path": file_path,
                    "filename": filename,
                    "classification": "data",
                    "confidence": 1.0,
                    "reasoning": f"Signal file ({file_ext}) - always transactional data",
                    "indicators": {"file_type": "signal"},
                    "needs_review": False,
                    "human_confirmed": False
                }
                data_files.append(file_path)
                print(f"      ğŸ“ˆ Signal ë°ì´í„° (100% - rule-based)")
                continue
            
            selected_processor = next((p for p in processors if p.can_handle(file_path)), None)
            
            if not selected_processor:
                print(f"      âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
                classifications[file_path] = {
                    "file_path": file_path,
                    "filename": filename,
                    "classification": "unknown",
                    "confidence": 0.0,
                    "reasoning": "Unsupported file format",
                    "indicators": {},
                    "needs_review": True,
                    "human_confirmed": False
                }
                uncertain_files.append(file_path)
                continue
            
            raw_metadata = selected_processor.extract_metadata(file_path)
            context = build_metadata_detection_context(file_path, raw_metadata)
            meta_result = ask_llm_is_metadata(context)
            
            confidence = meta_result.get("confidence", 0.0)
            is_metadata = meta_result.get("is_metadata", False)
            reasoning = meta_result.get("reasoning", "")
            indicators = meta_result.get("indicators", {})
            
            classification_type = "metadata" if is_metadata else "data"
            needs_review = confidence < HumanReviewConfig.CLASSIFICATION_CONFIDENCE_THRESHOLD
            
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": classification_type,
                "confidence": confidence,
                "reasoning": reasoning,
                "indicators": indicators,
                "needs_review": needs_review,
                "human_confirmed": False
            }
            
            if needs_review:
                uncertain_files.append(file_path)
                print(f"      âš ï¸ ë¶ˆí™•ì‹¤: {classification_type} ({confidence:.1%})")
            elif is_metadata:
                metadata_files.append(file_path)
                print(f"      ğŸ“– ë©”íƒ€ë°ì´í„° ({confidence:.1%})")
            else:
                data_files.append(file_path)
                print(f"      ğŸ“Š ë°ì´í„° ({confidence:.1%})")
                
        except Exception as e:
            print(f"      âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": "unknown",
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
                "indicators": {},
                "needs_review": True,
                "human_confirmed": False
            }
            uncertain_files.append(file_path)
    
    classification_result: ClassificationResult = {
        "total_files": len(input_files),
        "metadata_files": metadata_files,
        "data_files": data_files,
        "uncertain_files": uncertain_files,
        "classifications": classifications
    }
    
    processing_progress: ProcessingProgress = {
        "phase": "classification",
        "metadata_processed": [],
        "data_processed": [],
        "current_file": None,
        "current_file_index": 0,
        "total_files": len(input_files)
    }
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ:")
    print(f"   - ë©”íƒ€ë°ì´í„°: {len(metadata_files)}ê°œ")
    print(f"   - ë°ì´í„°: {len(data_files)}ê°œ")
    print(f"   - ë¶ˆí™•ì‹¤: {len(uncertain_files)}ê°œ")
    print("="*80)
    
    return {
        "classification_result": classification_result,
        "processing_progress": processing_progress,
        "logs": [
            f"ğŸ“‹ [Phase1] ë¶„ë¥˜ ì™„ë£Œ: ë©”íƒ€ë°ì´í„° {len(metadata_files)}ê°œ, "
            f"ë°ì´í„° {len(data_files)}ê°œ, ë¶ˆí™•ì‹¤ {len(uncertain_files)}ê°œ"
        ]
    }


def classification_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1-2] ë¶„ë¥˜ í™•ì¸ ë…¸ë“œ (Human-in-the-Loop)
    """
    print("\n" + "="*80)
    print("ğŸ§‘ [CLASSIFICATION REVIEW] Human-in-the-Loop")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    uncertain_files = classification_result.get("uncertain_files", [])
    classifications = classification_result.get("classifications", {})
    human_feedback = state.get("human_feedback")
    
    if human_feedback:
        print(f"   ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì‹ : '{human_feedback}'")
        
        updated_classifications = _parse_classification_feedback(
            feedback=human_feedback,
            classifications=classifications,
            uncertain_files=uncertain_files
        )
        
        new_metadata_files = []
        new_data_files = []
        remaining_uncertain = []
        
        for file_path, clf in updated_classifications.items():
            if clf.get("human_confirmed"):
                if clf["classification"] == "metadata":
                    new_metadata_files.append(file_path)
                elif clf["classification"] == "data":
                    new_data_files.append(file_path)
                else:
                    remaining_uncertain.append(file_path)
            elif clf["needs_review"]:
                remaining_uncertain.append(file_path)
            elif clf["classification"] == "metadata":
                new_metadata_files.append(file_path)
            else:
                new_data_files.append(file_path)
        
        all_metadata = classification_result.get("metadata_files", []) + [
            f for f in new_metadata_files if f not in classification_result.get("metadata_files", [])
        ]
        all_data = classification_result.get("data_files", []) + [
            f for f in new_data_files if f not in classification_result.get("data_files", [])
        ]
        
        updated_result: ClassificationResult = {
            "total_files": classification_result["total_files"],
            "metadata_files": all_metadata,
            "data_files": all_data,
            "uncertain_files": remaining_uncertain,
            "classifications": updated_classifications
        }
        
        print(f"   âœ… ë¶„ë¥˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        if remaining_uncertain:
            question = _generate_classification_question(remaining_uncertain, updated_classifications)
            return {
                "classification_result": updated_result,
                "needs_human_review": True,
                "review_type": "classification",
                "human_question": question,
                "human_feedback": None,
                "logs": [f"ğŸ”„ [Review] ì¶”ê°€ í™•ì¸ í•„ìš”: {len(remaining_uncertain)}ê°œ íŒŒì¼"]
            }
        
        progress = state.get("processing_progress", {})
        progress["phase"] = "classification_review"
        
        return {
            "classification_result": updated_result,
            "processing_progress": progress,
            "needs_human_review": False,
            "human_feedback": None,
            "logs": [f"âœ… [Review] ë¶„ë¥˜ í™•ì • ì™„ë£Œ"]
        }
    
    if not uncertain_files:
        print("   âœ… ë¶ˆí™•ì‹¤í•œ íŒŒì¼ ì—†ìŒ - ë¦¬ë·° ìŠ¤í‚µ")
        return {
            "needs_human_review": False,
            "logs": ["âœ… [Review] ëª¨ë“  íŒŒì¼ ë¶„ë¥˜ í™•ì •"]
        }
    
    question = _generate_classification_question(uncertain_files, classifications)
    
    print(f"   â“ {len(uncertain_files)}ê°œ íŒŒì¼ì— ëŒ€í•´ ì‚¬ìš©ì í™•ì¸ ìš”ì²­")
    print("="*80)
    
    return {
        "needs_human_review": True,
        "review_type": "classification",
        "human_question": question,
        "logs": [f"â“ [Review] {len(uncertain_files)}ê°œ íŒŒì¼ ë¶„ë¥˜ í™•ì¸ ìš”ì²­"]
    }


def _generate_classification_question(
    uncertain_files: List[str], 
    classifications: Dict[str, FileClassification]
) -> str:
    """ë¶ˆí™•ì‹¤í•œ íŒŒì¼ë“¤ì— ëŒ€í•œ ì§ˆë¬¸ ìƒì„±"""
    
    question_parts = [
        "ğŸ“‹ **íŒŒì¼ ë¶„ë¥˜ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤**\n",
        "ì•„ë˜ íŒŒì¼ë“¤ì˜ ë¶„ë¥˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\n"
    ]
    
    for idx, file_path in enumerate(uncertain_files[:5], 1):
        clf = classifications.get(file_path, {})
        filename = clf.get("filename", os.path.basename(file_path))
        predicted = clf.get("classification", "unknown")
        confidence = clf.get("confidence", 0.0)
        reasoning = clf.get("reasoning", "")[:100]
        
        pred_emoji = "ğŸ“–" if predicted == "metadata" else "ğŸ“Š" if predicted == "data" else "â“"
        pred_text = "ë©”íƒ€ë°ì´í„°" if predicted == "metadata" else "ë°ì´í„°" if predicted == "data" else "ì•Œ ìˆ˜ ì—†ìŒ"
        
        question_parts.append(
            f"\n**{idx}. {filename}**\n"
            f"   - AI ì˜ˆì¸¡: {pred_emoji} {pred_text} ({confidence:.0%})\n"
            f"   - íŒë‹¨ ê·¼ê±°: {reasoning}...\n"
        )
    
    if len(uncertain_files) > 5:
        question_parts.append(f"\n... ì™¸ {len(uncertain_files) - 5}ê°œ íŒŒì¼\n")
    
    question_parts.append(
        "\n**ì‘ë‹µ ë°©ë²•:**\n"
        "- ëª¨ë‘ ë§ìœ¼ë©´: `í™•ì¸` ë˜ëŠ” `ok`\n"
        "- ìˆ˜ì • í•„ìš”: `1:ë°ì´í„°, 2:ë©”íƒ€ë°ì´í„°` í˜•ì‹\n"
        "- íŒŒì¼ ì œì™¸: `1:skip`\n"
    )
    
    return "".join(question_parts)


def _parse_classification_feedback(
    feedback: str, 
    classifications: Dict[str, FileClassification],
    uncertain_files: List[str]
) -> Dict[str, FileClassification]:
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ íŒŒì‹±í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
    import re
    
    updated = classifications.copy()
    feedback_lower = feedback.lower().strip()
    
    if feedback_lower in ["í™•ì¸", "ok", "yes", "y", "approve", "ìŠ¹ì¸"]:
        for file_path in uncertain_files:
            if file_path in updated:
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
        return updated
    
    corrections = re.findall(r'(\d+)\s*[:ï¼š]\s*(ë©”íƒ€ë°ì´í„°|ë°ì´í„°|metadata|data|ì œì™¸|skip)', feedback_lower)
    
    for idx_str, new_type in corrections:
        idx = int(idx_str) - 1
        
        if 0 <= idx < len(uncertain_files):
            file_path = uncertain_files[idx]
            
            if new_type in ["ì œì™¸", "skip"]:
                updated[file_path]["classification"] = "unknown"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë©”íƒ€ë°ì´í„°", "metadata"]:
                updated[file_path]["classification"] = "metadata"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë°ì´í„°", "data"]:
                updated[file_path]["classification"] = "data"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
    
    return updated


def process_metadata_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-1] ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ë…¸ë“œ
    """
    print("\n" + "="*80)
    print("ğŸ“– [METADATA PROCESSOR] Phase 2-1 - ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    metadata_files = classification_result.get("metadata_files", [])
    progress = state.get("processing_progress", {})
    
    ontology = state.get("ontology_context")
    if not ontology or not ontology.get("definitions"):
        ontology = ontology_manager.load() or {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        }
    
    if not metadata_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "metadata_processing"
        progress["metadata_processed"] = []
        
        return {
            "ontology_context": ontology,
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ - ìŠ¤í‚µ"]
        }
    
    print(f"   ğŸ“‚ ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
    
    processed_metadata = []
    total_definitions = 0
    
    for idx, file_path in enumerate(metadata_files):
        filename = os.path.basename(file_path)
        print(f"\n   [{idx+1}/{len(metadata_files)}] {filename}")
        
        try:
            ontology["file_tags"][file_path] = {
                "type": "metadata",
                "role": "dictionary",
                "confidence": classification_result["classifications"].get(file_path, {}).get("confidence", 0.8),
                "detected_at": datetime.now().isoformat()
            }
            
            new_definitions = parse_metadata_content(file_path)
            ontology["definitions"].update(new_definitions)
            
            total_definitions += len(new_definitions)
            processed_metadata.append(file_path)
            
            print(f"      âœ… ìš©ì–´ {len(new_definitions)}ê°œ ì¶”ê°€")
            
        except Exception as e:
            print(f"      âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    ontology_manager.save(ontology)
    
    progress["phase"] = "metadata_processing"
    progress["metadata_processed"] = processed_metadata
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {len(processed_metadata)}ê°œ")
    print(f"   - ì¶”ê°€ëœ ìš©ì–´: {total_definitions}ê°œ")
    print("="*80)
    
    return {
        "ontology_context": ontology,
        "processing_progress": progress,
        "logs": [f"ğŸ“– [Metadata] {len(processed_metadata)}ê°œ íŒŒì¼ ì²˜ë¦¬, {total_definitions}ê°œ ìš©ì–´ ì¶”ê°€"]
    }


def process_data_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-2] ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ì¤€ë¹„ ë…¸ë“œ
    """
    print("\n" + "="*80)
    print("ğŸ“Š [DATA PROCESSOR] Phase 2-2 - ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    if not data_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "complete"
        
        return {
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Data] ë°ì´í„° íŒŒì¼ ì—†ìŒ - ì™„ë£Œ"]
        }
    
    print(f"   ğŸ“‚ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    
    first_file = data_files[0]
    
    progress["phase"] = "data_processing"
    progress["current_file"] = first_file
    progress["current_file_index"] = 0
    progress["total_files"] = len(data_files)
    
    print(f"\n   â†’ ì²˜ë¦¬ íŒŒì¼: {os.path.basename(first_file)}")
    print("="*80)
    
    return {
        "file_path": first_file,
        "processing_progress": progress,
        "skip_indexing": False,
        "logs": [f"ğŸ“Š [Data] {len(data_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘"]
    }


def advance_to_next_file_node(state: AgentState) -> Dict[str, Any]:
    """
    [Helper] ë‹¤ìŒ ë°ì´í„° íŒŒì¼ë¡œ ì§„í–‰
    """
    print("\n" + "-"*40)
    print("â¡ï¸ [ADVANCE] ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™")
    print("-"*40)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    current_idx = progress.get("current_file_index", 0)
    current_file = progress.get("current_file", "")
    
    if current_file and current_file not in progress.get("data_processed", []):
        if "data_processed" not in progress:
            progress["data_processed"] = []
        progress["data_processed"].append(current_file)
    
    next_idx = current_idx + 1
    
    if next_idx >= len(data_files):
        print(f"   âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ({len(data_files)}ê°œ)")
        progress["phase"] = "complete"
        progress["current_file"] = None
        progress["all_files_processed"] = True  # ëª…í™•í•œ ì¢…ë£Œ í”Œë˜ê·¸
        
        return {
            "processing_progress": progress,
            "logs": [f"âœ… [Complete] ëª¨ë“  ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ({len(data_files)}ê°œ)"]
        }
    
    next_file = data_files[next_idx]
    progress["current_file"] = next_file
    progress["current_file_index"] = next_idx
    
    print(f"   ğŸ“‚ ë‹¤ìŒ íŒŒì¼: [{next_idx + 1}/{len(data_files)}] {os.path.basename(next_file)}")
    
    return {
        "file_path": next_file,
        "processing_progress": progress,
        "raw_metadata": {},
        "finalized_anchor": None,
        "finalized_schema": [],
        "needs_human_review": False,
        "human_feedback": None,
        "skip_indexing": False,
        "retry_count": 0,
        "logs": [f"â¡ï¸ [Advance] ë‹¤ìŒ íŒŒì¼: {os.path.basename(next_file)}"]
    }

