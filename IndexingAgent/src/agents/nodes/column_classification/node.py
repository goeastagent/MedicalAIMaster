# src/agents/nodes/column_classification/node.py
"""
Column Classification Node

ê° ì»¬ëŸ¼ì˜ ì—­í• (column_role)ì„ ë¶„ë¥˜í•˜ê³ , parameter í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.

Workflow:
1. ê° íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘ (name, unique_values, stats)
2. LLM í˜¸ì¶œí•˜ì—¬ column_role ë¶„ë¥˜ (ColumnRole enum ì‚¬ìš©)
3. column_metadata.column_role ì—…ë°ì´íŠ¸
4. parameter í…Œì´ë¸” ìƒì„± (rule-based í›„ì²˜ë¦¬)
   - parameter_name: ì»¬ëŸ¼ëª… â†’ parameter
   - parameter_container: ì»¬ëŸ¼ unique values â†’ parameter(s)

âœ… LLM ì‚¬ìš©: column_role íŒë‹¨
âœ… í›„ì²˜ë¦¬: parameter í…Œì´ë¸” ìƒì„± (rule-based)
"""

from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.database import FileRepository, ColumnRepository
from src.database.repositories import ParameterRepository
from src.config import LLMConfig, ColumnClassificationConfig
from src.agents.models import (
    ColumnRole,
    SourceType,
    ColumnClassificationItem,
    ColumnClassificationResult,
)
from src.utils.llm_client import get_llm_client
from src.agents.prompts import (
    ColumnClassificationPrompt,
    build_column_info_for_prompt,
    build_columns_info_batch,
)

from ...base import BaseNode, LLMMixin, DatabaseMixin
from ...registry import register_node


@register_node
class ColumnClassificationNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Column Classification Node (LLM-based + Rule-based Post-processing)
    
    ê° ì»¬ëŸ¼ì˜ ì—­í• ì„ ë¶„ë¥˜í•˜ê³ , parameter í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - Phase 1: LLMìœ¼ë¡œ column_role ë¶„ë¥˜
    - Phase 2: Rule-basedë¡œ parameter í…Œì´ë¸” ìƒì„±
    """
    
    name = "column_classification"
    description = "ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±"
    order = 420  # file_classification(400) ì´í›„, data_semantic(600) ì´ì „
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = ColumnClassificationPrompt
    
    def __init__(self):
        super().__init__()
        self._file_repo: Optional[FileRepository] = None
        self._column_repo: Optional[ColumnRepository] = None
        self._param_repo: Optional[ParameterRepository] = None
    
    # =========================================================================
    # Main Execution
    # =========================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±
        
        Args:
            state: AgentState (data_files í•„ìš”)
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ:
            - column_classification_result: ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
        """
        self.log("=" * 60)
        self.log("ğŸ” ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±")
        self.log("=" * 60)
        
        started_at = datetime.now()
        
        # data_filesì—ì„œ ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œë“¤
        data_files = state.get("data_files", [])
        
        if not data_files:
            self.log("âš ï¸ No data files to process", indent=1)
            return self._create_empty_result("No data files to process")
        
        self.log(f"ğŸ“‚ Files to process: {len(data_files)}", indent=1)
        
        # ì´ˆê¸°í™”
        total_columns = 0
        columns_by_role: Dict[str, int] = {}
        parameters_created = 0
        parameters_from_column_name = 0
        parameters_from_column_value = 0
        llm_calls = 0
        batches_processed = 0
        
        # Config
        batch_size = ColumnClassificationConfig.COLUMN_BATCH_SIZE
        
        # ê° íŒŒì¼ë³„ ì²˜ë¦¬
        for file_path in data_files:
            file_name = file_path.split('/')[-1]
            self.log(f"ğŸ“„ Processing: {file_name}", indent=1)
            
            # 1. íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
            columns_info = self._get_columns_info_for_file(file_path)
            
            if not columns_info:
                self.log(f"âš ï¸ No columns found for {file_name}", indent=2)
                continue
            
            n_cols = len(columns_info)
            self.log(f"ğŸ“Š Columns: {n_cols}", indent=2)
            total_columns += n_cols
            
            # 2. ë°°ì¹˜ ë¶„í•  (ì»¬ëŸ¼ ìˆ˜ê°€ ë§ìœ¼ë©´)
            batches = [columns_info[i:i+batch_size] for i in range(0, n_cols, batch_size)]
            
            if len(batches) > 1:
                self.log(f"ğŸ“¦ Splitting into {len(batches)} batches (batch_size={batch_size})", indent=2)
            
            # 3. ë°°ì¹˜ë³„ LLM í˜¸ì¶œ
            for batch_idx, batch_cols in enumerate(batches):
                if len(batches) > 1:
                    self.log(f"ğŸ”„ Batch {batch_idx + 1}/{len(batches)} ({len(batch_cols)} columns)", indent=2)
                
                # LLM í˜¸ì¶œ
                classifications = self._call_llm_for_classification(batch_cols, file_name)
                llm_calls += 1
                batches_processed += 1
                
                if not classifications:
                    self.log(f"âŒ LLM classification failed for batch {batch_idx + 1}", indent=3)
                    continue
                
                # 4. ê²°ê³¼ ì²˜ë¦¬ (ë°°ì¹˜ë³„ë¡œ ì¦‰ì‹œ DB ì—…ë°ì´íŠ¸)
                for clf in classifications:
                    role = clf.column_role
                    columns_by_role[role] = columns_by_role.get(role, 0) + 1
                    
                    # 4a. column_metadata.column_role ì—…ë°ì´íŠ¸
                    self._update_column_role(
                        file_path=file_path,
                        column_name=clf.column_name,
                        column_role=clf.column_role,
                        reasoning=clf.reasoning
                    )
                    
                    # 4b. parameter í…Œì´ë¸” ìƒì„± (rule-based í›„ì²˜ë¦¬)
                    if clf.is_parameter_name:
                        # Wide-format: ì»¬ëŸ¼ëª… â†’ parameter
                        self._create_parameter_from_column_name(
                            file_path=file_path,
                            column_name=clf.column_name
                        )
                        parameters_created += 1
                        parameters_from_column_name += 1
                        self.log(f"ğŸ“Œ {clf.column_name} â†’ parameter (column_name)", indent=3)
                    
                    elif clf.is_parameter_container:
                        # Long-format: ì»¬ëŸ¼ì˜ ì „ì²´ unique values â†’ parameter(s)
                        # LLM ì‘ë‹µì˜ parametersê°€ ì•„ë‹Œ, DBì—ì„œ ì¡°íšŒí•œ ì „ì²´ unique_values ì‚¬ìš©
                        col_info = next(
                            (c for c in batch_cols if c['name'] == clf.column_name), 
                            None
                        )
                        if col_info:
                            all_unique_values = col_info.get('unique_values', [])
                            for param_key in all_unique_values:
                                self._create_parameter_from_column_value(
                                    file_path=file_path,
                                    container_column=clf.column_name,
                                    param_key=str(param_key)  # ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                                )
                                parameters_created += 1
                                parameters_from_column_value += 1
                            self.log(f"ğŸ“Œ {clf.column_name} â†’ {len(all_unique_values)} parameters (column_values)", indent=3)
                
                if len(batches) > 1:
                    self.log(f"âœ… Classified {len(classifications)} columns in batch", indent=3)
            
            self.log(f"âœ… Classified {n_cols} columns total", indent=2)
        
        # 4. ê²°ê³¼ ìš”ì•½
        completed_at = datetime.now()
        duration = (completed_at - started_at).total_seconds()
        
        result = ColumnClassificationResult(
            total_files=len(data_files),
            total_columns=total_columns,
            columns_by_role=columns_by_role,
            parameters_created=parameters_created,
            parameters_from_column_name=parameters_from_column_name,
            parameters_from_column_value=parameters_from_column_value,
            llm_calls=llm_calls,
            started_at=started_at.isoformat(),
            completed_at=completed_at.isoformat()
        )
        
        self.log("âœ… Complete!")
        self.log(f"ğŸ“Š Total columns: {total_columns}", indent=1)
        self.log("ğŸ·ï¸  Columns by role:", indent=1)
        for role, count in sorted(columns_by_role.items()):
            self.log(f"- {role}: {count}", indent=2)
        self.log(f"ğŸ“Œ Parameters created: {parameters_created}", indent=1)
        self.log(f"- from column_name: {parameters_from_column_name}", indent=2)
        self.log(f"- from column_value: {parameters_from_column_value}", indent=2)
        self.log(f"ğŸ“¦ Batches processed: {batches_processed}", indent=1)
        self.log(f"â±ï¸  Duration: {duration:.1f}s ({llm_calls} LLM calls)", indent=1)
        self.log("=" * 60)
        
        return {
            "column_classification_result": result.model_dump(),
            "logs": [
                f"ğŸ” [Column Classification] Classified {total_columns} columns, "
                f"created {parameters_created} parameters"
            ]
        }
    
    # =========================================================================
    # Repository Access
    # =========================================================================
    
    def _get_file_repo(self) -> FileRepository:
        """FileRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._file_repo is None:
            self._file_repo = FileRepository()
        return self._file_repo
    
    def _get_column_repo(self) -> ColumnRepository:
        """ColumnRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._column_repo is None:
            self._column_repo = ColumnRepository()
        return self._column_repo
    
    def _get_param_repo(self) -> ParameterRepository:
        """ParameterRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._param_repo is None:
            self._param_repo = ParameterRepository()
        return self._param_repo
    
    # =========================================================================
    # Column Info Collection
    # =========================================================================
    
    def _get_columns_info_for_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        
        Returns:
            [{"name": str, "unique_values": list, "stats": dict}, ...]
        """
        column_repo = self._get_column_repo()
        
        try:
            # DBì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ (file_pathë¡œ ê²€ìƒ‰)
            columns = column_repo.get_columns_by_file_path(file_path)
            
            result = []
            for col in columns:
                result.append({
                    "name": col.get("column_name", ""),
                    "col_id": col.get("col_id"),
                    "unique_values": col.get("unique_values", []),
                    "stats": {
                        "count": col.get("total_count", 0),
                        "null_count": col.get("null_count", 0),
                        "dtype": col.get("data_type", "unknown"),
                        "unique_count": col.get("unique_count", 0),
                    }
                })
            
            return result
            
        except Exception as e:
            self.log(f"âŒ Error getting columns info: {e}", indent=2)
            return []
    
    # =========================================================================
    # LLM Methods
    # =========================================================================
    
    def _call_llm_for_classification(
        self,
        columns_info: List[Dict[str, Any]],
        file_name: str
    ) -> List[ColumnClassificationItem]:
        """LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜"""
        llm = get_llm_client()
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        columns_info_text = build_columns_info_batch(columns_info)
        prompt = self.prompt_class.build(
            columns_info=columns_info_text,
            file_name=file_name
        )
        
        try:
            data = llm.ask_json(prompt, max_tokens=LLMConfig.MAX_TOKENS)
            
            if data.get("error"):
                self.log(f"âŒ LLM returned error: {data.get('error')}", indent=2)
                return []
            
            # PromptTemplateì˜ parse_response ì‚¬ìš©
            classifications = self.prompt_class.parse_response(data)
            
            if classifications is None:
                self.log("âš ï¸ Failed to parse LLM response", indent=2)
                return []
            
            # column_role ê°’ ê²€ì¦ (ColumnRole enum)
            validated = []
            for clf in classifications:
                validated_clf = self._validate_column_role(clf)
                validated.append(validated_clf)
            
            return validated
            
        except Exception as e:
            self.log(f"âŒ LLM call error: {e}", indent=2)
            return []
    
    def _validate_column_role(
        self,
        clf: ColumnClassificationItem
    ) -> ColumnClassificationItem:
        """
        LLM ì‘ë‹µì˜ column_role ê°’ ê²€ì¦
        
        ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì€ 'other'ë¡œ ë³€ê²½
        """
        valid_roles = ColumnRole.values()
        
        if clf.column_role not in valid_roles:
            self.log(f"âš ï¸ Invalid column_role '{clf.column_role}' â†’ 'other'", indent=3)
            clf.column_role = ColumnRole.OTHER.value
        
        return clf
    
    # =========================================================================
    # DB Update Methods
    # =========================================================================
    
    def _update_column_role(
        self,
        file_path: str,
        column_name: str,
        column_role: str,
        reasoning: Optional[str] = None
    ):
        """column_metadata.column_role ì—…ë°ì´íŠ¸"""
        column_repo = self._get_column_repo()
        
        try:
            column_repo.update_column_role(
                file_path=file_path,
                column_name=column_name,
                column_role=column_role,
                column_role_reasoning=reasoning
            )
        except Exception as e:
            print(f"         âŒ Error updating column_role: {e}")
    
    def _create_parameter_from_column_name(
        self,
        file_path: str,
        column_name: str
    ):
        """
        Wide-format: ì»¬ëŸ¼ëª…ì„ parameterë¡œ ìƒì„±
        
        source_type = 'column_name'
        """
        param_repo = self._get_param_repo()
        column_repo = self._get_column_repo()
        
        try:
            # file_id ì¡°íšŒ
            file_info = self._get_file_repo().get_file_by_path(file_path)
            if not file_info:
                self.log(f"âš ï¸ File not found: {file_path}", indent=3)
                return
            
            file_id = file_info.get("file_id")
            
            # col_id ì¡°íšŒ
            col_info = column_repo.get_column_by_name(file_path, column_name)
            col_id = col_info.get("col_id") if col_info else None
            
            # parameter ìƒì„±
            param_repo.create_parameter(
                file_id=file_id,
                param_key=column_name,
                source_type=SourceType.COLUMN_NAME.value,
                source_column_id=col_id
            )
            
        except Exception as e:
            self.log(f"âŒ Error creating parameter: {e}", indent=3)
    
    def _create_parameter_from_column_value(
        self,
        file_path: str,
        container_column: str,
        param_key: str
    ):
        """
        Long-format: ì»¬ëŸ¼ ê°’ì„ parameterë¡œ ìƒì„±
        
        source_type = 'column_value'
        """
        param_repo = self._get_param_repo()
        column_repo = self._get_column_repo()
        
        try:
            # file_id ì¡°íšŒ
            file_info = self._get_file_repo().get_file_by_path(file_path)
            if not file_info:
                self.log(f"âš ï¸ File not found: {file_path}", indent=3)
                return
            
            file_id = file_info.get("file_id")
            
            # col_id ì¡°íšŒ
            col_info = column_repo.get_column_by_name(file_path, container_column)
            col_id = col_info.get("col_id") if col_info else None
            
            # parameter ìƒì„±
            param_repo.create_parameter(
                file_id=file_id,
                param_key=param_key,
                source_type=SourceType.COLUMN_VALUE.value,
                source_column_id=col_id
            )
            
        except Exception as e:
            self.log(f"âŒ Error creating parameter: {e}", indent=3)
    
    # =========================================================================
    # Helper Methods
    # =========================================================================
    
    def _create_empty_result(self, error_msg: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "column_classification_result": {
                "total_files": 0,
                "total_columns": 0,
                "columns_by_role": {},
                "parameters_created": 0,
                "error": error_msg
            },
            "logs": [f"âš ï¸ [Column Classification] {error_msg}"]
        }
    
    # =========================================================================
    # Convenience Methods (Standalone Execution)
    # =========================================================================
    
    @classmethod
    def run_standalone(cls, data_files: List[str] = None) -> Dict[str, Any]:
        """
        ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            data_files: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ëª©ë¡ (Noneì´ë©´ DBì—ì„œ data íŒŒì¼ ì¡°íšŒ)
        
        Returns:
            ë¶„ë¥˜ ê²°ê³¼
        """
        node = cls()
        
        if data_files is None:
            file_repo = node._get_file_repo()
            data_files = file_repo.get_data_file_paths()
        
        state = {"data_files": data_files}
        return node.execute(state)

