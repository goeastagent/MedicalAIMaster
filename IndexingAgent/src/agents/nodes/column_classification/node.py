# src/agents/nodes/column_classification/node.py
"""
Column Classification Node

ê° ì»¬ëŸ¼ì˜ ì—­í• (column_role)ì„ ë¶„ë¥˜í•˜ê³ , parameter í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.

Workflow:
1. ê° íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘ (name, unique_values, stats)
2. LLM í˜¸ì¶œí•˜ì—¬ column_role ë¶„ë¥˜ (ColumnRole enum ì‚¬ìš©)
3. column_metadata.column_role ì—…ë°ì´íŠ¸
4. parameter í…Œì´ë¸” ìƒì„± (rule-based í›„ì²˜ë¦¬)
   - parameter_name: ì»¬ëŸ¼ëª… â†’ parameter
   - parameter_container: ì»¬ëŸ¼ unique values â†’ parameter(s)

âœ… LLM ì‚¬ìš©: column_role íŒë‹¨
âœ… í›„ì²˜ë¦¬: parameter í…Œì´ë¸” ìƒì„± (rule-based)
"""

from typing import Dict, Any, List, Optional
from datetime import datetime

from src.agents.state import AgentState
from src.database import FileRepository, ColumnRepository
from src.database.repositories import ParameterRepository, FileGroupRepository
from src.config import LLMConfig, ColumnClassificationConfig
from src.agents.models import (
    ColumnRole,
    SourceType,
    ColumnClassificationItem,
    ColumnClassificationResult,
)
from src.utils.llm_client import get_llm_client
from src.agents.prompts import (
    ColumnClassificationPrompt,
    build_column_info_for_prompt,
    build_columns_info_batch,
)

from ...base import BaseNode, LLMMixin, DatabaseMixin
from ...registry import register_node


@register_node
class ColumnClassificationNode(BaseNode, LLMMixin, DatabaseMixin):
    """
    Column Classification Node (LLM-based + Rule-based Post-processing)
    
    ê° ì»¬ëŸ¼ì˜ ì—­í• ì„ ë¶„ë¥˜í•˜ê³ , parameter í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - Phase 1: LLMìœ¼ë¡œ column_role ë¶„ë¥˜
    - Phase 2: Rule-basedë¡œ parameter í…Œì´ë¸” ìƒì„±
    """
    
    name = "column_classification"
    description = "ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±"
    order = 420  # file_classification(400) ì´í›„, data_semantic(600) ì´ì „
    requires_llm = True
    
    # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ì—°ê²°
    prompt_class = ColumnClassificationPrompt
    
    def __init__(self):
        super().__init__()
        self._file_repo: Optional[FileRepository] = None
        self._column_repo: Optional[ColumnRepository] = None
        self._param_repo: Optional[ParameterRepository] = None
        self._group_repo: Optional[FileGroupRepository] = None
    
    # =========================================================================
    # Main Execution
    # =========================================================================
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±
        
        ìˆ˜ì •ëœ ë¡œì§:
        1. ê·¸ë£¹ì— ì†í•œ íŒŒì¼ë“¤ â†’ ê·¸ë£¹ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ìƒ˜í”Œ 1ê°œë§Œ ë¶„ì„)
        2. ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ íŒŒì¼ë“¤ â†’ ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ê°œë³„ ì²˜ë¦¬
        
        Args:
            state: AgentState (data_files, file_groups í•„ìš”)
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ:
            - column_classification_result: ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
        """
        self.log("=" * 60)
        self.log("ğŸ” ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜ ë° parameter ìƒì„±")
        self.log("=" * 60)
        
        started_at = datetime.now()
        
        # ì´ˆê¸°í™”
        total_columns = 0
        columns_by_role: Dict[str, int] = {}
        parameters_created = 0
        parameters_from_column_name = 0
        parameters_from_column_value = 0
        parameters_from_group = 0
        llm_calls = 0
        batches_processed = 0
        groups_processed = 0
        ungrouped_files_processed = 0
        
        # Config
        batch_size = ColumnClassificationConfig.COLUMN_BATCH_SIZE
        
        # =====================================================================
        # Phase 1: ê·¸ë£¹ì— ì†í•œ íŒŒì¼ë“¤ ì²˜ë¦¬ (ê·¸ë£¹ ë‹¨ìœ„)
        # =====================================================================
        file_groups = state.get("file_groups", [])
        
        if file_groups:
            self.log(f"ğŸ“¦ Processing {len(file_groups)} file groups...", indent=1)
            
            for group in file_groups:
                group_result = self._process_group(group, batch_size)
                
                if group_result:
                    groups_processed += 1
                    total_columns += group_result['columns']
                    parameters_from_group += group_result['parameters']
                    parameters_created += group_result['parameters']
                    llm_calls += group_result['llm_calls']
                    batches_processed += group_result['batches']
                    
                    # columns_by_role ë³‘í•©
                    for role, count in group_result.get('columns_by_role', {}).items():
                        columns_by_role[role] = columns_by_role.get(role, 0) + count
        
        # =====================================================================
        # Phase 2: ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ íŒŒì¼ë“¤ ì²˜ë¦¬ (ê°œë³„)
        # =====================================================================
        ungrouped_files = self._get_file_repo().get_ungrouped_data_files()
        
        if ungrouped_files:
            self.log(f"ğŸ“„ Processing {len(ungrouped_files)} ungrouped files...", indent=1)
            
            for file_path in ungrouped_files:
                file_result = self._process_single_file(file_path, batch_size)
                
                if file_result:
                    ungrouped_files_processed += 1
                    total_columns += file_result['columns']
                    parameters_from_column_name += file_result.get('params_from_name', 0)
                    parameters_from_column_value += file_result.get('params_from_value', 0)
                    parameters_created += file_result['parameters']
                    llm_calls += file_result['llm_calls']
                    batches_processed += file_result['batches']
                    
                    # columns_by_role ë³‘í•©
                    for role, count in file_result.get('columns_by_role', {}).items():
                        columns_by_role[role] = columns_by_role.get(role, 0) + count
        
        if not file_groups and not ungrouped_files:
            self.log("âš ï¸ No files to process", indent=1)
            return self._create_empty_result("No files to process")
        
        # =====================================================================
        # ê²°ê³¼ ìš”ì•½
        # =====================================================================
        completed_at = datetime.now()
        duration = (completed_at - started_at).total_seconds()
        
        result = ColumnClassificationResult(
            total_files=groups_processed + ungrouped_files_processed,
            total_columns=total_columns,
            columns_by_role=columns_by_role,
            parameters_created=parameters_created,
            parameters_from_column_name=parameters_from_column_name,
            parameters_from_column_value=parameters_from_column_value,
            llm_calls=llm_calls,
            started_at=started_at.isoformat(),
            completed_at=completed_at.isoformat()
        )
        
        self.log("âœ… Complete!")
        self.log(f"ğŸ“¦ Groups processed: {groups_processed}", indent=1)
        self.log(f"ğŸ“„ Ungrouped files processed: {ungrouped_files_processed}", indent=1)
        self.log(f"ğŸ“Š Total columns: {total_columns}", indent=1)
        self.log("ğŸ·ï¸  Columns by role:", indent=1)
        for role, count in sorted(columns_by_role.items()):
            self.log(f"- {role}: {count}", indent=2)
        self.log(f"ğŸ“Œ Parameters created: {parameters_created}", indent=1)
        self.log(f"- from group_common: {parameters_from_group}", indent=2)
        self.log(f"- from column_name: {parameters_from_column_name}", indent=2)
        self.log(f"- from column_value: {parameters_from_column_value}", indent=2)
        self.log(f"ğŸ“¦ Batches processed: {batches_processed}", indent=1)
        self.log(f"â±ï¸  Duration: {duration:.1f}s ({llm_calls} LLM calls)", indent=1)
        self.log("=" * 60)
        
        return {
            "column_classification_result": result.model_dump(),
            "logs": [
                f"ğŸ” [Column Classification] Processed {groups_processed} groups + "
                f"{ungrouped_files_processed} files, created {parameters_created} parameters"
            ]
        }
    
    # =========================================================================
    # Repository Access
    # =========================================================================
    
    def _get_file_repo(self) -> FileRepository:
        """FileRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._file_repo is None:
            self._file_repo = FileRepository()
        return self._file_repo
    
    def _get_column_repo(self) -> ColumnRepository:
        """ColumnRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._column_repo is None:
            self._column_repo = ColumnRepository()
        return self._column_repo
    
    def _get_param_repo(self) -> ParameterRepository:
        """ParameterRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._param_repo is None:
            self._param_repo = ParameterRepository()
        return self._param_repo
    
    def _get_group_repo(self) -> FileGroupRepository:
        """FileGroupRepository ì‹±ê¸€í†¤ ë°˜í™˜"""
        if self._group_repo is None:
            self._group_repo = FileGroupRepository()
        return self._group_repo
    
    # =========================================================================
    # Column Info Collection
    # =========================================================================
    
    def _process_group(self, group: Dict[str, Any], batch_size: int) -> Optional[Dict[str, Any]]:
        """
        íŒŒì¼ ê·¸ë£¹ ë‹¨ìœ„ë¡œ ì»¬ëŸ¼ ë¶„ë¥˜ ë° parameter ìƒì„±
        
        ê·¸ë£¹ì˜ ìƒ˜í”Œ íŒŒì¼ 1ê°œë§Œ ë¶„ì„í•˜ê³ , ê²°ê³¼ëŠ” ê·¸ë£¹(group_id) ë‹¨ìœ„ë¡œ ì €ì¥
        â†’ 6,388ê°œ íŒŒì¼ì„ 1ë²ˆë§Œ ë¶„ì„í•˜ì—¬ ë¹„ìš© ì ˆê°
        
        Args:
            group: file_group ì •ë³´ (group_id, group_name, sample_file_ids ë“±)
            batch_size: LLM ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        group_id = group.get('group_id')
        group_name = group.get('group_name', 'Unknown')
        sample_file_ids = group.get('sample_file_ids', [])
        
        self.log(f"ğŸ“¦ Group: {group_name} (files: {group.get('file_count', '?')})", indent=2)
        
        # ìƒ˜í”Œ íŒŒì¼ ì„ íƒ
        if not sample_file_ids:
            # sample_file_idsê°€ ì—†ìœ¼ë©´ ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
            group_repo = self._get_group_repo()
            files_in_group = group_repo.get_files_in_group(str(group_id))
            if not files_in_group:
                self.log(f"âš ï¸ No files in group {group_name}", indent=3)
                return None
            sample_file_path = files_in_group[0].get('file_path')
        else:
            # sample_file_idsì˜ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
            file_repo = self._get_file_repo()
            file_info = file_repo.get_file_by_id(str(sample_file_ids[0]))
            if not file_info:
                self.log(f"âš ï¸ Sample file not found for group {group_name}", indent=3)
                return None
            sample_file_path = file_info.get('file_path')
        
        self.log(f"ğŸ¯ Sample file: {sample_file_path.split('/')[-1]}", indent=3)
        
        # ìƒ˜í”Œ íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        columns_info = self._get_columns_info_for_file(sample_file_path)
        if not columns_info:
            self.log(f"âš ï¸ No columns found for sample file", indent=3)
            return None
        
        n_cols = len(columns_info)
        self.log(f"ğŸ“Š Columns: {n_cols}", indent=3)
        
        # ê²°ê³¼ ì§‘ê³„
        result = {
            'columns': n_cols,
            'parameters': 0,
            'llm_calls': 0,
            'batches': 0,
            'columns_by_role': {}
        }
        
        # ë°°ì¹˜ ë¶„í• 
        batches = [columns_info[i:i+batch_size] for i in range(0, n_cols, batch_size)]
        
        for batch_idx, batch_cols in enumerate(batches):
            # LLM í˜¸ì¶œ
            classifications = self._call_llm_for_classification(batch_cols, f"[GROUP] {group_name}")
            result['llm_calls'] += 1
            result['batches'] += 1
            
            if not classifications:
                continue
            
            for clf in classifications:
                role = clf.column_role
                result['columns_by_role'][role] = result['columns_by_role'].get(role, 0) + 1
                
                # parameter ìƒì„± (group_id ì‚¬ìš©)
                if clf.is_parameter_name:
                    # Wide-format: ì»¬ëŸ¼ëª… â†’ group parameter
                    self._create_group_parameter(
                        group_id=str(group_id),
                        param_key=clf.column_name,
                        source_type=SourceType.GROUP_COMMON.value,
                        source_column=clf.column_name
                    )
                    result['parameters'] += 1
                    self.log(f"ğŸ“Œ {clf.column_name} â†’ group parameter", indent=4)
                    
                elif clf.is_parameter_container:
                    # Long-format: ì»¬ëŸ¼ ê°’ë“¤ â†’ group parameters
                    col_info = next(
                        (c for c in batch_cols if c['name'] == clf.column_name), 
                        None
                    )
                    if col_info:
                        all_unique_values = col_info.get('unique_values', [])
                        for param_key in all_unique_values:
                            self._create_group_parameter(
                                group_id=str(group_id),
                                param_key=str(param_key),
                                source_type=SourceType.GROUP_COMMON.value,
                                source_column=clf.column_name
                            )
                            result['parameters'] += 1
                        self.log(f"ğŸ“Œ {clf.column_name} â†’ {len(all_unique_values)} group parameters", indent=4)
        
        self.log(f"âœ… Group processed: {result['parameters']} parameters created", indent=3)
        return result
    
    def _process_single_file(self, file_path: str, batch_size: int) -> Optional[Dict[str, Any]]:
        """
        ë‹¨ì¼ íŒŒì¼ì˜ ì»¬ëŸ¼ ë¶„ë¥˜ ë° parameter ìƒì„± (ê¸°ì¡´ ë¡œì§)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            batch_size: LLM ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        file_name = file_path.split('/')[-1]
        self.log(f"ğŸ“„ Processing: {file_name}", indent=2)
        
        # íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        columns_info = self._get_columns_info_for_file(file_path)
        if not columns_info:
            self.log(f"âš ï¸ No columns found for {file_name}", indent=3)
            return None
        
        n_cols = len(columns_info)
        self.log(f"ğŸ“Š Columns: {n_cols}", indent=3)
        
        # ê²°ê³¼ ì§‘ê³„
        result = {
            'columns': n_cols,
            'parameters': 0,
            'params_from_name': 0,
            'params_from_value': 0,
            'llm_calls': 0,
            'batches': 0,
            'columns_by_role': {}
        }
        
        # ë°°ì¹˜ ë¶„í• 
        batches = [columns_info[i:i+batch_size] for i in range(0, n_cols, batch_size)]
        
        for batch_idx, batch_cols in enumerate(batches):
            # LLM í˜¸ì¶œ
            classifications = self._call_llm_for_classification(batch_cols, file_name)
            result['llm_calls'] += 1
            result['batches'] += 1
            
            if not classifications:
                continue
            
            for clf in classifications:
                role = clf.column_role
                result['columns_by_role'][role] = result['columns_by_role'].get(role, 0) + 1
                
                # column_metadata.column_role ì—…ë°ì´íŠ¸
                self._update_column_role(
                    file_path=file_path,
                    column_name=clf.column_name,
                    column_role=clf.column_role,
                    reasoning=clf.reasoning
                )
                
                # parameter ìƒì„± (file_id ì‚¬ìš© - ê¸°ì¡´ ë¡œì§)
                if clf.is_parameter_name:
                    self._create_parameter_from_column_name(
                        file_path=file_path,
                        column_name=clf.column_name
                    )
                    result['parameters'] += 1
                    result['params_from_name'] += 1
                    self.log(f"ğŸ“Œ {clf.column_name} â†’ parameter (column_name)", indent=4)
                    
                elif clf.is_parameter_container:
                    col_info = next(
                        (c for c in batch_cols if c['name'] == clf.column_name), 
                        None
                    )
                    if col_info:
                        all_unique_values = col_info.get('unique_values', [])
                        for param_key in all_unique_values:
                            self._create_parameter_from_column_value(
                                file_path=file_path,
                                container_column=clf.column_name,
                                param_key=str(param_key)
                            )
                            result['parameters'] += 1
                            result['params_from_value'] += 1
                        self.log(f"ğŸ“Œ {clf.column_name} â†’ {len(all_unique_values)} parameters", indent=4)
        
        self.log(f"âœ… File processed: {result['parameters']} parameters", indent=3)
        return result
    
    def _create_group_parameter(
        self, 
        group_id: str, 
        param_key: str, 
        source_type: str,
        source_column: str = None  # ì°¸ê³ ìš© (DBì—ëŠ” ì €ì¥ ì•ˆ í•¨)
    ) -> None:
        """
        ê·¸ë£¹ ë‹¨ìœ„ parameter ìƒì„± (file_id=NULL, group_id=group_id)
        
        Args:
            group_id: íŒŒì¼ ê·¸ë£¹ ID
            param_key: íŒŒë¼ë¯¸í„° í‚¤ (ì˜ˆ: "Solar8000/HR")
            source_type: ì¶œì²˜ íƒ€ì… (group_common)
            source_column: ì¶œì²˜ ì»¬ëŸ¼ëª… (ì°¸ê³ ìš©, DBì—ëŠ” ì €ì¥ ì•ˆ í•¨)
        """
        param_repo = self._get_param_repo()
        group_repo = self._get_group_repo()
        
        # ì¤‘ë³µ ì²´í¬
        existing = param_repo._execute_query("""
            SELECT param_id FROM parameter 
            WHERE group_id = %s::uuid AND param_key = %s
        """, (group_id, param_key), fetch="one")
        
        if existing:
            return  # ì´ë¯¸ ì¡´ì¬
        
        # parameter ìƒì„± (group_id ì‚¬ìš©, source_column_idëŠ” NULL)
        # INSERT ë¬¸ì´ë¯€ë¡œ fetch=Noneìœ¼ë¡œ ëª…ì‹œ (fetchall í˜¸ì¶œ ë°©ì§€)
        param_repo._execute_query("""
            INSERT INTO parameter (file_id, group_id, param_key, source_type, source_column_id)
            VALUES (NULL, %s::uuid, %s, %s, NULL)
        """, (group_id, param_key, source_type), fetch=None)

    def _get_columns_info_for_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        íŒŒì¼ì˜ ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        
        Returns:
            [{"name": str, "unique_values": list, "stats": dict}, ...]
        """
        column_repo = self._get_column_repo()
        
        try:
            # DBì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ (file_pathë¡œ ê²€ìƒ‰)
            columns = column_repo.get_columns_by_file_path(file_path)
            
            result = []
            for col in columns:
                result.append({
                    "name": col.get("column_name", ""),
                    "col_id": col.get("col_id"),
                    "unique_values": col.get("unique_values", []),
                    "stats": {
                        "count": col.get("total_count", 0),
                        "null_count": col.get("null_count", 0),
                        "dtype": col.get("data_type", "unknown"),
                        "unique_count": col.get("unique_count", 0),
                    }
                })
            
            return result
            
        except Exception as e:
            self.log(f"âŒ Error getting columns info: {e}", indent=2)
            return []
    
    # =========================================================================
    # LLM Methods
    # =========================================================================
    
    def _call_llm_for_classification(
        self,
        columns_info: List[Dict[str, Any]],
        file_name: str
    ) -> List[ColumnClassificationItem]:
        """LLMì„ í˜¸ì¶œí•˜ì—¬ ì»¬ëŸ¼ ì—­í•  ë¶„ë¥˜"""
        llm = get_llm_client()
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        columns_info_text = build_columns_info_batch(columns_info)
        prompt = self.prompt_class.build(
            columns_info=columns_info_text,
            file_name=file_name
        )
        
        try:
            data = llm.ask_json(prompt, max_tokens=LLMConfig.MAX_TOKENS)
            
            if data.get("error"):
                self.log(f"âŒ LLM returned error: {data.get('error')}", indent=2)
                return []
            
            # PromptTemplateì˜ parse_response ì‚¬ìš©
            classifications = self.prompt_class.parse_response(data)
            
            if classifications is None:
                self.log("âš ï¸ Failed to parse LLM response", indent=2)
                return []
            
            # column_role ê°’ ê²€ì¦ (ColumnRole enum)
            validated = []
            for clf in classifications:
                validated_clf = self._validate_column_role(clf)
                validated.append(validated_clf)
            
            return validated
            
        except Exception as e:
            self.log(f"âŒ LLM call error: {e}", indent=2)
            return []
    
    def _validate_column_role(
        self,
        clf: ColumnClassificationItem
    ) -> ColumnClassificationItem:
        """
        LLM ì‘ë‹µì˜ column_role ê°’ ê²€ì¦
        
        ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì€ 'other'ë¡œ ë³€ê²½
        """
        valid_roles = ColumnRole.values()
        
        if clf.column_role not in valid_roles:
            self.log(f"âš ï¸ Invalid column_role '{clf.column_role}' â†’ 'other'", indent=3)
            clf.column_role = ColumnRole.OTHER.value
        
        return clf
    
    # =========================================================================
    # DB Update Methods
    # =========================================================================
    
    def _update_column_role(
        self,
        file_path: str,
        column_name: str,
        column_role: str,
        reasoning: Optional[str] = None
    ):
        """column_metadata.column_role ì—…ë°ì´íŠ¸"""
        column_repo = self._get_column_repo()
        
        try:
            column_repo.update_column_role(
                file_path=file_path,
                column_name=column_name,
                column_role=column_role,
                column_role_reasoning=reasoning
            )
        except Exception as e:
            print(f"         âŒ Error updating column_role: {e}")
    
    def _create_parameter_from_column_name(
        self,
        file_path: str,
        column_name: str
    ):
        """
        Wide-format: ì»¬ëŸ¼ëª…ì„ parameterë¡œ ìƒì„±
        
        source_type = 'column_name'
        """
        param_repo = self._get_param_repo()
        column_repo = self._get_column_repo()
        
        try:
            # file_id ì¡°íšŒ
            file_info = self._get_file_repo().get_file_by_path(file_path)
            if not file_info:
                self.log(f"âš ï¸ File not found: {file_path}", indent=3)
                return
            
            file_id = file_info.get("file_id")
            
            # col_id ì¡°íšŒ
            col_info = column_repo.get_column_by_name(file_path, column_name)
            col_id = col_info.get("col_id") if col_info else None
            
            # parameter ìƒì„±
            param_repo.create_parameter(
                file_id=file_id,
                param_key=column_name,
                source_type=SourceType.COLUMN_NAME.value,
                source_column_id=col_id
            )
            
        except Exception as e:
            self.log(f"âŒ Error creating parameter: {e}", indent=3)
    
    def _create_parameter_from_column_value(
        self,
        file_path: str,
        container_column: str,
        param_key: str
    ):
        """
        Long-format: ì»¬ëŸ¼ ê°’ì„ parameterë¡œ ìƒì„±
        
        source_type = 'column_value'
        """
        param_repo = self._get_param_repo()
        column_repo = self._get_column_repo()
        
        try:
            # file_id ì¡°íšŒ
            file_info = self._get_file_repo().get_file_by_path(file_path)
            if not file_info:
                self.log(f"âš ï¸ File not found: {file_path}", indent=3)
                return
            
            file_id = file_info.get("file_id")
            
            # col_id ì¡°íšŒ
            col_info = column_repo.get_column_by_name(file_path, container_column)
            col_id = col_info.get("col_id") if col_info else None
            
            # parameter ìƒì„±
            param_repo.create_parameter(
                file_id=file_id,
                param_key=param_key,
                source_type=SourceType.COLUMN_VALUE.value,
                source_column_id=col_id
            )
            
        except Exception as e:
            self.log(f"âŒ Error creating parameter: {e}", indent=3)
    
    # =========================================================================
    # Helper Methods
    # =========================================================================
    
    def _create_empty_result(self, error_msg: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "column_classification_result": {
                "total_files": 0,
                "total_columns": 0,
                "columns_by_role": {},
                "parameters_created": 0,
                "error": error_msg
            },
            "logs": [f"âš ï¸ [Column Classification] {error_msg}"]
        }
    
    # =========================================================================
    # Convenience Methods (Standalone Execution)
    # =========================================================================
    
    @classmethod
    def run_standalone(cls, data_files: List[str] = None) -> Dict[str, Any]:
        """
        ë…ë¦½ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            data_files: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ëª©ë¡ (Noneì´ë©´ DBì—ì„œ data íŒŒì¼ ì¡°íšŒ)
        
        Returns:
            ë¶„ë¥˜ ê²°ê³¼
        """
        node = cls()
        
        if data_files is None:
            file_repo = node._get_file_repo()
            data_files = file_repo.get_data_file_paths()
        
        state = {"data_files": data_files}
        return node.execute(state)

