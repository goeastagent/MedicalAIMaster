import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

from src.agents.state import AgentState, ColumnSchema, AnchorInfo, ProjectContext, OntologyContext
from src.processors.tabular import TabularProcessor
from src.processors.signal import SignalProcessor
from src.utils.llm_client import get_llm_client
from src.utils.ontology_manager import get_ontology_manager
from src.utils.llm_cache import get_llm_cache
from src.config import HumanReviewConfig

# --- Global resource initialization ---
llm_client = get_llm_client()
ontology_manager = get_ontology_manager()
llm_cache = get_llm_cache()  # Global cache instance
processors = [
    TabularProcessor(llm_client),
    SignalProcessor(llm_client)
]



def load_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 1] Load file and extract basic metadata
    """
    file_path = state["file_path"]
    
    print("\n" + "="*80)
    print(f"üìÇ [LOADER NODE] Starting - {os.path.basename(file_path)}")
    print("="*80)
    
    # 1. Find appropriate Processor
    selected_processor = next((p for p in processors if p.can_handle(file_path)), None)
    
    if not selected_processor:
        return {
            "logs": [f"‚ùå Error: Unsupported file format ({file_path})"],
            "needs_human_review": True,
            "human_question": "Unsupported file format. How would you like to process this file?"
        }

    # 2. Extract metadata (Anchor detection is also performed here)
    try:
        raw_metadata = selected_processor.extract_metadata(file_path)
        processor_type = raw_metadata.get("processor_type", "unknown")
        
        # Check if Processor failed to find or was uncertain about Anchor
        anchor_info = raw_metadata.get("anchor_info", {})
        anchor_status = anchor_info.get("status", "MISSING")
        anchor_msg = anchor_info.get("msg", "")

        log_message = f"‚úÖ [Loader] {processor_type.upper()} analysis complete. Anchor Status: {anchor_status}"

        print(f"\n‚úÖ [LOADER NODE] Complete")
        print(f"   - Processor: {processor_type}")
        print(f"   - Columns: {len(raw_metadata.get('columns', []))}")
        print(f"   - Anchor Status: {anchor_status}")
        print("="*80)

        return {
            "file_type": processor_type,
            "raw_metadata": raw_metadata,
            "logs": [log_message]
        }
    except Exception as e:
        print(f"\n‚ùå [LOADER NODE] Error: {str(e)}")
        print("="*80)
        return {
            "logs": [f"‚ùå [Loader] Critical error: {str(e)}"],
            "error_message": str(e)
        }


def analyze_semantics_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 2] Semantic Analysis (Semantic Reasoning)
    Core brain that finalizes schema based on Processor results
    [NEW] References Global Context (Project Level) to ensure Anchor consistency across files.
    """
    print("\n" + "="*80)
    print("üß† [ANALYZER NODE] Starting - Semantic Analysis")
    print("="*80)
    
    metadata = state["raw_metadata"]
    local_anchor_info = metadata.get("anchor_info", {})
    human_feedback = state.get("human_feedback")
    
    # Get Global Context (initialize if not exists)
    project_context = state.get("project_context", {
        "master_anchor_name": None, 
        "known_aliases": [], 
        "example_id_values": []
    })
    
    finalized_anchor = state.get("finalized_anchor")
    retry_count = state.get("retry_count", 0)
    
    # Prevent infinite loop: force processing after 3+ retries
    if retry_count >= 3:
        log_msg = f"‚ö†Ô∏è [Analyzer] Retry count exceeded ({retry_count}). Forcing local Anchor."
        
        # Use locally found Anchor as-is
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": local_anchor_info.get("target_column", "unknown"),
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": f"Forced confirmation after {retry_count} retries",
            "mapped_to_master": project_context.get("master_anchor_name")
        }
        
        # Skip schema analysis and complete
        return {
            "finalized_anchor": finalized_anchor,
            "finalized_schema": [],
            "project_context": project_context,
            "needs_human_review": False,
            "human_feedback": None,
            "retry_count": retry_count,
            "logs": [log_msg, "‚ö†Ô∏è [Analyzer] Schema analysis skipped (retry exceeded)"]
        }

    # --- Scenario A: Process user feedback (re-entry) ---
    if human_feedback:
        log_msg = f"üó£Ô∏è [Feedback] User feedback received: '{human_feedback}'"
        
        # ‚≠ê [FIX] Parse user input - distinguish column name vs description
        parsed_column = _parse_human_feedback_to_column(
            feedback=human_feedback,
            available_columns=metadata.get("columns", []),
            master_anchor=project_context.get("master_anchor_name"),
            file_path=state.get("file_path", "")
        )
        
        if parsed_column.get("action") == "skip":
            # Skip request
            log_msg += " ‚Üí File skip requested"
            return {
                "finalized_anchor": None,
                "finalized_schema": [],
                "project_context": project_context,
                "needs_human_review": False,
                "human_feedback": None,
                "skip_indexing": True,
                "logs": [log_msg, "‚è≠Ô∏è [Analyzer] File skipped by user request"]
            }
        
        determined_column = parsed_column.get("column_name", human_feedback.strip())
        reasoning = parsed_column.get("reasoning", "User manually confirmed.")
        
        print(f"   ‚Üí Parsing result: '{determined_column}'")
        print(f"   ‚Üí Reasoning: {reasoning}")
        
        # Force Anchor confirmation based on feedback
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": determined_column,
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": reasoning,
            "mapped_to_master": project_context.get("master_anchor_name") 
        }
        
        # ‚≠ê [FIX] Reset needs_human_confirmation after feedback processing
        # Prevents re-entering review_required in check_confidence
        if "anchor_info" in metadata:
            metadata["anchor_info"]["needs_human_confirmation"] = False
            metadata["anchor_info"]["status"] = "CONFIRMED"
        
        # Consider feedback processing complete and proceed (don't return)
    
    # --- Scenario B: When Anchor is not yet finalized -> Check Global Context ---
    if not finalized_anchor:
        
        # [NEW] Case 1: Project already has agreed Anchor (Leader)
        if project_context.get("master_anchor_name"):
            master_name = project_context["master_anchor_name"]
            
            # LLMÏóêÍ≤å ÎπÑÍµê ÏöîÏ≤≠ (Global Context vs Local Data)
            comparison = _compare_with_global_context(
                local_metadata=metadata,
                local_anchor_info=local_anchor_info,
                project_context=project_context
            )
            
            # Debug: comparison result log
            comparison_status = comparison.get("status", "UNKNOWN")
            comparison_msg = comparison.get("message", "")
            print(f"\n[DEBUG] Global Anchor comparison result: {comparison_status}")
            print(f"[DEBUG] Message: {comparison_msg}")
            print(f"[DEBUG] Target Column: {comparison.get('target_column', 'N/A')}")
            
            if comparison["status"] == "MATCH":
                # Match success -> auto confirm
                target_col = comparison["target_column"]
                finalized_anchor = {
                    "status": "CONFIRMED",
                    "column_name": target_col,
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": f"Matched with global master anchor '{master_name}'",
                    "mapped_to_master": master_name
                }
                state["logs"].append(f"üîó [Anchor Link] Matched with Global Anchor '{master_name}' (Local: '{target_col}')")
            
            elif comparison["status"] == "INDIRECT_LINK":
                # ‚≠ê [NEW] Indirect link success -> auto confirm (no human intervention needed!)
                via_col = comparison["target_column"]
                via_table = comparison.get("via_table", "unknown")
                
                finalized_anchor = {
                    "status": "INDIRECT_LINK",
                    "column_name": via_col,  # Link column (e.g., caseid)
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": comparison.get("message"),
                    "mapped_to_master": master_name,
                    "via_table": via_table,
                    "link_type": "indirect"  # Indirect link via FK
                }
                
                print(f"\n‚úÖ [INDIRECT_LINK] Auto-confirmed indirect link!")
                print(f"   - Link column: {via_col}")
                print(f"   - Via table: {via_table}")
                print(f"   - Master Anchor: {master_name}")
                
                state["logs"].append(
                    f"üîó [Indirect Link] Indirectly linked to '{master_name}' in '{via_table}' via '{via_col}'"
                )
                
            else:
                # Conflict or missing -> human intervention
                msg = comparison.get("message", "Anchor mismatch occurred")
                
                # Generate natural question with LLM
                natural_question = _generate_natural_human_question(
                    file_path=state.get("file_path", ""),
                    context={
                        "master_anchor": master_name,
                        "candidates": local_anchor_info.get("target_column"),
                        "reasoning": msg,
                        "columns": metadata.get("columns", [])
                    },
                    issue_type="anchor_conflict"
                )
                
                return {
                    "needs_human_review": True,
                    "human_question": natural_question,
                    "retry_count": retry_count,  # Keep current retry count
                    "logs": [f"‚ö†Ô∏è [Analyzer] Global Anchor mismatch (Status: {comparison_status}). Retry: {retry_count}/3"]
                }

        # [NEW] Case 2: This is the first file (no Global Context)
        else:
            # Flexible judgment: Processor uncertainty + LLM review
            processor_confidence = local_anchor_info.get("confidence", 0.5 if local_anchor_info.get("needs_human_confirmation") else 0.9)
            
            review_decision = _should_request_human_review(
                file_path=state.get("file_path", ""),
                issue_type="anchor_detection",
                context={
                    "processor_msg": local_anchor_info.get("msg"),
                    "candidates": local_anchor_info.get("target_column"),
                    "columns": metadata.get("columns", []),
                    "processor_needs_confirmation": local_anchor_info.get("needs_human_confirmation", False)
                },
                rule_based_confidence=processor_confidence
            )
            
            if review_decision["needs_review"]:
                question = _generate_natural_human_question(
                    file_path=state.get("file_path", ""),
                    context={
                        "reasoning": local_anchor_info.get("msg"),
                        "candidates": local_anchor_info.get("target_column", "None"),
                        "columns": metadata.get("columns", [])
                    },
                    issue_type="anchor_uncertain"
                )
                
                return {
                    "needs_human_review": True,
                    "human_question": question,
                    "logs": [f"‚ö†Ô∏è [Analyzer] Anchor uncertain (first file). {review_decision['reason']}"]
                }
            
            # Confident -> confirm
            finalized_anchor = {
                "status": "CONFIRMED",
                "column_name": local_anchor_info.get("target_column"),
                "is_time_series": local_anchor_info.get("is_time_series"),
                "reasoning": local_anchor_info.get("reasoning"),
                "mapped_to_master": None  # Will become master
            }

    # --- 3. Update Global Context (First-Come Leader Strategy) ---
    # If Anchor is confirmed and no master exists, this file's Anchor becomes master
    if finalized_anchor and not project_context.get("master_anchor_name"):
        project_context["master_anchor_name"] = finalized_anchor["column_name"]
        project_context["known_aliases"].append(finalized_anchor["column_name"])
        state["logs"].append(f"üëë [Project Context] New Master Anchor set: '{finalized_anchor['column_name']}'")

    # --- 4. Detailed schema analysis (common) ---
    schema_analysis = _analyze_columns_with_llm(
        columns=metadata.get("columns", []),
        sample_data=metadata.get("column_details", {}),
        anchor_context=finalized_anchor
    )

    print(f"\n‚úÖ [ANALYZER NODE] Complete")
    print(f"   - Anchor: {finalized_anchor.get('column_name', 'N/A')}")
    print(f"   - Mapped to Master: {finalized_anchor.get('mapped_to_master', 'N/A')}")
    print(f"   - Schema Columns: {len(schema_analysis)}")
    print("="*80)

    result = {
        "finalized_anchor": finalized_anchor,
        "finalized_schema": schema_analysis,
        "project_context": project_context,  # Return updated context
        "raw_metadata": metadata,  # ‚≠ê [FIX] Return updated raw_metadata (needs_human_confirmation reset)
        "needs_human_review": False,
        "human_feedback": None, 
        "logs": ["üß† [Analyzer] Complete schema and ontology analysis."]
    }
    
    print(f"\n[DEBUG ANALYZER] Return value:")
    print(f"   - finalized_schema: {len(result['finalized_schema'])}")
    print(f"   - needs_human_review: {result['needs_human_review']}")
    
    return result


def human_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 3] Human-in-the-loop waiting node
    In actual execution, LangGraph's interrupt mechanism stops here
    In test environment, increase retry count to prevent infinite loop
    """
    print("\n" + "="*80)
    print("üõë [HUMAN REVIEW NODE] Starting - User confirmation required")
    print("="*80)
    
    question = state.get("human_question", "Confirmation required.")
    retry_count = state.get("retry_count", 0)
    
    # Increase retry count
    new_retry_count = retry_count + 1
    
    print(f"\n‚ö†Ô∏è  Question: {question[:150]}...")
    print(f"üîÑ Retry count: {new_retry_count}/3")
    print("="*80)
    
    return {
        "retry_count": new_retry_count,
        "logs": [f"üõë [Human Review] Waiting (retry: {new_retry_count}/3). Question: {question[:100]}..."]
    }


def index_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 4 - Phase 3] Build PostgreSQL DB (ontology-based)
    
    Expert feedback applied:
    - Chunk Processing (safe handling of large files)
    - Auto FK constraint creation (ALTER TABLE)
    - Auto index creation (Level 1-2)
    """
    import pandas as pd
    import os
    
    from database.connection import get_db_manager
    from database.schema_generator import SchemaGenerator
    
    print("\n" + "="*80)
    print("üíæ [INDEXER NODE] Starting - PostgreSQL DB construction")
    print("="*80)
    
    schema = state.get("finalized_schema", [])
    file_path = state["file_path"]
    ontology = state.get("ontology_context", {})
    
    # Generate table name
    table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_").replace("-", "_")
    
    # DB manager
    db_manager = get_db_manager()
    
    try:
        # === 1. Load data (pandas auto-creates table) ===
        print(f"\nüì• [Data] Loading data...")
        
        # Check file size
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        print(f"   - File size: {file_size_mb:.1f}MB")
        
        total_rows = 0
        
        # SQLAlchemy engine for PostgreSQL (for pandas to_sql)
        engine = db_manager.get_sqlalchemy_engine()
        
        # [TEST MODE] Row limit (check environment variable)
        test_limit = os.environ.get("TEST_ROW_LIMIT")
        limit_kwargs = {}
        if test_limit:
            limit_rows = int(test_limit)
            limit_kwargs = {"nrows": limit_rows}
            print(f"‚ö†Ô∏è [TEST MODE] Data load limit applied: processing top {limit_rows} rows only")

        if file_size_mb > 50:  # Chunk processing for files > 50MB
            print(f"   - Large file - Chunk Processing applied (100,000 rows per chunk)")
            
            chunk_size = 100000
            # [TEST MODE] Apply limit even with chunk processing
            # nrows works with chunksize to limit total rows read
            
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, **limit_kwargs)):
                chunk.to_sql(
                    table_name, 
                    engine, 
                    if_exists='append' if i > 0 else 'replace',
                    index=False,
                    method='multi'  # PostgreSQL optimization
                )
                total_rows += len(chunk)
                print(f"      ‚Ä¢ Chunk {i+1}: {len(chunk):,} rows loaded (cumulative: {total_rows:,} rows)")
        else:
            # Load small files at once
            print(f"   - Regular file - loading at once")
            df = pd.read_csv(file_path, **limit_kwargs)
            df.to_sql(
                table_name, 
                engine, 
                if_exists='replace', 
                index=False,
                method='multi'
            )
            total_rows = len(df)
            print(f"   - {total_rows:,} rows loaded")
        
        print(f"‚úÖ Data loading successful")
        
        # === 2. Create indices (performance optimization) ===
        print(f"\nüîç [Index] Creating indices...")
        
        indices = SchemaGenerator.generate_indices(
            table_name=table_name,
            schema=schema,
            ontology_context=ontology
        )
        
        indices_created = []
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        for idx_ddl in indices:
            try:
                cursor.execute(idx_ddl)
                # Extract index name
                idx_name = idx_ddl.split('"')[1] if '"' in idx_ddl else idx_ddl.split()[2]
                indices_created.append(idx_name)
            except Exception as e:
                print(f"‚ö†Ô∏è  Index creation failed: {e}")
        
        conn.commit()
        
        if indices_created:
            print(f"   - {len(indices_created)} indices created: {', '.join(indices_created)}")
        else:
            print(f"   - No indices created")
        
        # === 3. Verification ===
        print(f"\n‚úÖ [Verify] Verifying...")
        
        # Check row count (PostgreSQL)
        cursor.execute(f'SELECT COUNT(*) FROM "{table_name}"')
        actual_rows = cursor.fetchone()[0]
        
        if actual_rows == total_rows:
            print(f"   - Row count matches: {actual_rows:,} rows ‚úÖ")
        else:
            print(f"   ‚ö†Ô∏è Row count mismatch: expected {total_rows:,}, actual {actual_rows:,}")
        
        # === [NEW] Save Column Metadata (Neo4j) ===
        if schema:
            print(f"\nüìã [Column Metadata] Saving column metadata...")
            
            if "column_metadata" not in ontology:
                ontology["column_metadata"] = {}
            
            ontology["column_metadata"][table_name] = {}
            
            for col_schema in schema:
                col_name = col_schema.get("original_name", "unknown")
                ontology["column_metadata"][table_name][col_name] = {
                    "original_name": col_name,
                    "full_name": col_schema.get("full_name"),
                    "inferred_name": col_schema.get("inferred_name"),
                    "description": col_schema.get("description"),
                    "description_kr": col_schema.get("description_kr"),
                    "data_type": col_schema.get("data_type"),
                    "unit": col_schema.get("unit"),
                    "typical_range": col_schema.get("typical_range"),
                    "is_pii": col_schema.get("is_pii", False),
                    "confidence": col_schema.get("confidence", 0)
                }
            
            print(f"   - {len(schema)} column metadata generated")
            
            # Save to Neo4j
            from src.utils.ontology_manager import get_ontology_manager
            ontology_manager = get_ontology_manager()
            ontology_manager.save(ontology)
            print(f"   - Neo4j save complete")
        
        print("="*80)
        
        return {
            "ontology_context": ontology,  # [NEW] Return updated ontology
            "logs": [
                f"üíæ [Indexer] {table_name} created ({total_rows:,} rows)",
                f"üîç [Indexer] Indices: {len(indices_created)}",
                "‚úÖ [Done] Indexing process complete."
            ]
        }
        
    except Exception as e:
        print(f"\n‚ùå [Error] DB save failed: {str(e)}")
        print("="*80)
        
        import traceback
        traceback.print_exc()
        
        return {
            "logs": [f"‚ùå [Indexer] DB save failed: {str(e)}"],
            "error_message": str(e)
        }

# --- Helper Functions (Private) ---

def _analyze_columns_with_llm(columns: List[str], sample_data: Any, anchor_context: Dict) -> List[ColumnSchema]:
    """
    [Helper] Analyze column meaning, data type, PII status, units, etc. using LLM
    
    [Enhancements] Column metadata enrichment:
    - full_name: Abbreviation expansion (e.g., sbp ‚Üí Systolic Blood Pressure)
    - unit: Measurement unit (e.g., mmHg, kg, cm)
    - typical_range: Medical normal range
    - sample_values: Actual sample values
    """
    # Context summary for LLM
    prompt = f"""
    You are a Medical Data Ontologist specializing in clinical database design.
    Analyze the columns of a medical dataset and provide DETAILED metadata.
    
    [Context]
    - Patient Identifier (Anchor): {anchor_context.get('column_name')}
    - Is Time Series: {anchor_context.get('is_time_series')}
    
    [Columns to Analyze]
    """
    
    # If sample_data is a list (from TabularProcessor)
    if isinstance(sample_data, list):
        for col_detail in sample_data:
            col_name = col_detail.get('column_name', 'unknown')
            col_type = col_detail.get('column_type', 'unknown')
            samples = col_detail.get('samples', [])
            
            if col_type == 'categorical':
                unique_vals = col_detail.get('unique_values', [])
                prompt += f"- Column: '{col_name}' | Type: CATEGORICAL | Unique Values: {unique_vals}\n"
            else:
                min_val = col_detail.get('min', 'N/A')
                max_val = col_detail.get('max', 'N/A')
                prompt += f"- Column: '{col_name}' | Type: CONTINUOUS | Range: [{min_val}, {max_val}] | Samples: {samples}\n"
    # If sample_data is a dictionary (backward compatibility)
    elif isinstance(sample_data, dict):
        for col in columns:
            details = sample_data.get(col, {})
            samples = details.get("sample_values", [])
            prompt += f"- Column: '{col}', Samples: {samples}\n"
    else:
        # If neither, provide column names only
        for col in columns:
            prompt += f"- Column: '{col}'\n"

    prompt += """
    [Task]
    For EACH column, provide a JSON object with DETAILED metadata:
    
    1. original_name: The exact column name as provided (REQUIRED)
    2. inferred_name: Human-readable name (e.g., 'sbp' ‚Üí 'Systolic Blood Pressure')
    3. full_name: Full medical term without abbreviation (e.g., 'Systolic Blood Pressure')
    4. description: Brief medical description (what does this column measure?)
    5. description_kr: Korean description for Korean users (ÌïúÍ∏Ä ÏÑ§Î™Ö)
    6. data_type: SQL compatible type (VARCHAR, INT, FLOAT, TIMESTAMP, BOOLEAN)
    7. unit: Measurement unit if applicable (e.g., "mmHg", "kg", "mg/dL", "bpm", "¬∞C", null if N/A)
    8. typical_range: Normal/typical value range in medical context (e.g., "90-140" for systolic BP, null if N/A)
    9. is_pii: Boolean (true if it contains name, phone, address, social security number)
    10. confidence: 0.0 to 1.0 (how confident are you about this analysis?)
    
    [Examples]
    - 'sbp' ‚Üí {"original_name": "sbp", "inferred_name": "Systolic BP", "full_name": "Systolic Blood Pressure", 
               "description": "Peak arterial pressure during heart contraction", "description_kr": "Ïã¨Ïû• ÏàòÏ∂ïÏãú ÏµúÍ≥† ÎèôÎß•Ïïï (ÏàòÏ∂ïÍ∏∞ ÌòàÏïï)",
               "data_type": "FLOAT", "unit": "mmHg", "typical_range": "90-140", "is_pii": false, "confidence": 0.95}
    - 'hr' ‚Üí {"original_name": "hr", "inferred_name": "Heart Rate", "full_name": "Heart Rate",
              "description": "Number of heartbeats per minute", "description_kr": "Î∂ÑÎãπ Ïã¨Î∞ïÏàò",
              "data_type": "INT", "unit": "bpm", "typical_range": "60-100", "is_pii": false, "confidence": 0.95}
    - 'age' ‚Üí {"original_name": "age", "inferred_name": "Patient Age", "full_name": "Patient Age",
               "description": "Age of the patient", "description_kr": "ÌôòÏûê ÎÇòÏù¥",
               "data_type": "INT", "unit": "years", "typical_range": "0-120", "is_pii": false, "confidence": 0.90}

    Respond with a JSON object: {"columns": [list of column objects]}
    """
    
    # LLM call
    response = llm_client.ask_json(prompt)
    
    # Check if response is list or dict (wrapping list) and parse
    if isinstance(response, dict) and "columns" in response:
        result_list = response["columns"]
    elif isinstance(response, list):
        result_list = response
    else:
        result_list = []  # Error handling needed

    # Map results
    final_schema = []
    for idx, item in enumerate(result_list):
        # Use original_name if available, otherwise match by index
        original = item.get("original_name") or (columns[idx] if idx < len(columns) else "unknown")
        
        final_schema.append({
            "original_name": original,
            "inferred_name": item.get("inferred_name", original),
            "full_name": item.get("full_name", item.get("inferred_name", original)),
            "description": item.get("description", ""),
            "description_kr": item.get("description_kr", ""),
            "data_type": item.get("data_type", "VARCHAR"),
            "unit": item.get("unit"),  # None if not applicable
            "typical_range": item.get("typical_range"),  # None if not applicable
            "standard_concept_id": None, 
            "is_pii": item.get("is_pii", False),
            "confidence": item.get("confidence", 0.5)
        })
        
    return final_schema


def _compare_with_global_context(local_metadata: Dict, local_anchor_info: Dict, project_context: Dict) -> Dict[str, Any]:
    """
    [Helper] Compare current file data with project Global Anchor info (using LLM)
    
    ‚≠ê [NEW] Check ontology relationships for indirect connections
    e.g., lab_data without subjectid can link to clinical_data.subjectid via caseid
    """
    master_name = project_context["master_anchor_name"]
    local_cols = local_metadata.get("columns", [])
    local_candidate = local_anchor_info.get("target_column")
    
    # Extract table name from current filename
    file_path = local_metadata.get("file_path", "")
    current_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
    
    # 1. Ïù¥Î¶ÑÏù¥ ÏôÑÏ†ÑÌûà Í∞ôÏùÄ Í≤ΩÏö∞ (Fast Path)
    if master_name in local_cols:
        return {"status": "MATCH", "target_column": master_name, "message": "Exact name match"}

    # ‚≠ê [NEW] 2. Ïò®ÌÜ®Î°úÏßÄ Í∏∞Î∞ò Í∞ÑÏ†ë Ïó∞Í≤∞ ÌôïÏù∏
    indirect_link = _check_indirect_link_via_ontology(
        current_table=current_table,
        local_cols=local_cols,
        master_anchor=master_name
    )
    
    if indirect_link:
        return {
            "status": "INDIRECT_LINK",
            "target_column": indirect_link["via_column"],
            "via_table": indirect_link["via_table"],
            "master_anchor": master_name,
            "message": indirect_link["message"]
        }

    # 3. Î°úÏª¨ ÌõÑÎ≥¥Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ (ProcessorÍ∞Ä Î™ª Ï∞æÏùå)
    if not local_candidate:
        return {
            "status": "MISSING",
            "target_column": None,
            "message": f"No anchor candidate found in local file. Master anchor '{master_name}' not found in columns: {local_cols}"
        }

    # 3. LLMÏùÑ ÌÜµÌïú ÏùòÎØ∏Î°†Ï†Å ÎπÑÍµê
    prompt = f"""
    You are a Medical Data Integration Agent.
    Check if the new file contains the Project's Master Anchor (Patient ID).

    [Project Context / Global Master]
    - Master Anchor Name: '{master_name}'
    - Known Aliases: {project_context.get('known_aliases')}
    
    [New File Info]
    - Candidate Column found by AI: '{local_candidate}'
    - All Columns in file: {local_cols}
    
    [Task]
    Determine if any column in the new file represents the same 'Patient ID' entity as the Global Master.
    - If the candidate '{local_candidate}' is a synonym for '{master_name}' (e.g. 'pid' vs 'subject_id'), return MATCH.
    - If another column in 'All Columns' looks like the ID, return MATCH with that column.
    - If you cannot find a matching column, return MISSING.
    - If you are unsure, return CONFLICT.

    Respond with JSON:
    {{
        "status": "MATCH" or "MISSING" or "CONFLICT",
        "target_column": "name_of_column_in_new_file" (or null if missing),
        "message": "Reasoning for the decision"
    }}
    """
    
    try:
        result = llm_client.ask_json(prompt)
        
        # LLM ÏùëÎãµ Í≤ÄÏ¶ù Î∞è Ï†ïÍ∑úÌôî
        if not isinstance(result, dict):
            return {"status": "CONFLICT", "target_column": None, "message": "LLM returned invalid format"}
        
        status = result.get("status", "CONFLICT").upper()
        if status not in ["MATCH", "MISSING", "CONFLICT"]:
            status = "CONFLICT"
        
        return {
            "status": status,
            "target_column": result.get("target_column"),
            "message": result.get("message", "No explanation provided")
        }
        
    except Exception as e:
        return {"status": "CONFLICT", "target_column": None, "message": f"Error during anchor comparison: {str(e)}"}


# ============================================================================
# Indirect Link Check (Ontology-based)
# ============================================================================

def _check_indirect_link_via_ontology(current_table: str, local_cols: list, master_anchor: str) -> Optional[Dict]:
    """
    ‚≠ê [NEW] Check ontology relationships for indirect connections
    
    Example:
    - lab_data does not have subjectid
    - But ontology has "lab_data.caseid ‚Üí clinical_data.caseid" relationship
    - clinical_data has subjectid
    - Therefore lab_data is indirectly connected to subjectid via caseid
    
    Returns:
        Indirect link info dict or None
    """
    try:
        # Load ontology
        ontology = ontology_manager.load()
        if not ontology:
            return None
        
        relationships = ontology.get("relationships", [])
        file_tags = ontology.get("file_tags", {})
        
        print(f"\nüîó [Indirect Link Check] {current_table}")
        print(f"   - Ontology relationships: {len(relationships)}")
        
        # Find relationships where current table is source
        for rel in relationships:
            source_table = rel.get("source_table", "")
            target_table = rel.get("target_table", "")
            source_column = rel.get("source_column", "")
            target_column = rel.get("target_column", "")
            
            # If current_table is source
            if current_table.lower() in source_table.lower() or source_table.lower() in current_table.lower():
                # Check if link column exists in current file
                if source_column in local_cols:
                    # Check if target_table has master_anchor
                    target_has_master = _check_table_has_column(file_tags, target_table, master_anchor)
                    
                    if target_has_master:
                        message = (
                            f"‚úÖ Indirect link found! "
                            f"'{current_table}.{source_column}' ‚Üí '{target_table}.{target_column}' relation "
                            f"connects to '{master_anchor}'"
                        )
                        print(f"   {message}")
                        
                        return {
                            "via_column": source_column,
                            "via_table": target_table,
                            "via_relation": f"{source_table}.{source_column} ‚Üí {target_table}.{target_column}",
                            "message": message
                        }
        
        print(f"   - No indirect link found")
        return None
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Indirect link check error: {e}")
        return None


def _check_table_has_column(file_tags: Dict, table_name: str, column_name: str) -> bool:
    """
    Check if a specific table has a specific column in file_tags
    """
    for file_path, tag_info in file_tags.items():
        # Extract table name from filename
        file_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
        
        if table_name.lower() in file_table.lower() or file_table.lower() in table_name.lower():
            columns = tag_info.get("columns", [])
            if column_name in columns:
                return True
    
    return False


# ============================================================================
# Ontology Builder Functions (Phase 0-1)
# ============================================================================

def _collect_negative_evidence(col_name: str, samples: list, unique_vals: list) -> dict:
    """
    [Rule] Collect negative evidence (detect data quality issues)
    
    Args:
        col_name: Column name
        samples: Sample values list
        unique_vals: Unique values list
    
    Returns:
        Negative evidence dictionary
    """
    import numpy as np
    
    total = len(samples)
    unique = len(unique_vals)
    
    # Calculate nulls
    null_count = sum(
        1 for s in samples 
        if s is None or s == '' or (isinstance(s, float) and np.isnan(s))
    )
    
    negative_evidence = []
    
    # 1. Near unique but has duplicates (possible data error)
    if total > 0 and unique / total > 0.95 and unique != total:
        dup_rate = (total - unique) / total
        negative_evidence.append({
            "type": "near_unique_with_duplicates",
            "detail": f"{unique/total:.1%} unique BUT {dup_rate:.1%} duplicates - possible data error",
            "severity": "medium"
        })
    
    # 2. ID-like but has nulls (cannot be PK)
    if 'id' in col_name.lower() and null_count > 0:
        null_rate = null_count / total
        negative_evidence.append({
            "type": "identifier_with_nulls",
            "detail": f"Column name suggests ID BUT {null_rate:.1%} null values",
            "severity": "high" if null_rate > 0.1 else "low"
        })
    
    # 3. Cardinality too high (possible free text)
    if unique > 100:
        negative_evidence.append({
            "type": "high_cardinality",
            "detail": f"{unique} unique values - might be free text, not categorical",
            "severity": "low"
        })
    
    return {
        "has_issues": len(negative_evidence) > 0,
        "issues": negative_evidence,
        "null_ratio": null_count / total if total > 0 else 0.0
    }


def _summarize_long_values(values: list, max_length: int = 50) -> list:
    """
    [Rule] Summarize long text (Context Window management)
    
    Args:
        values: Values list
        max_length: Maximum length (summarize if exceeded)
    
    Returns:
        Summarized values list
    """
    summarized = []
    
    for val in values:
        val_str = str(val)
        
        if len(val_str) > max_length:
            # Replace with meta info (save tokens)
            preview = val_str[:20].replace('\n', ' ')
            summarized.append(f"[Text: {len(val_str)} chars, starts='{preview}...']")
        else:
            summarized.append(val_str)
    
    return summarized


def _parse_metadata_content(file_path: str) -> dict:
    """
    [Rule] Parse metadata file (CSV ‚Üí Dictionary)
    
    Args:
        file_path: Metadata file path
    
    Returns:
        definitions dictionary {parameter: description}
    """
    import pandas as pd
    
    definitions = {}
    
    try:
        df = pd.read_csv(file_path)
        
        # Common metadata structure: [Parameter/Name, Description, ...]
        if len(df.columns) >= 2:
            key_col = df.columns[0]
            desc_col = df.columns[1]
            
            for _, row in df.iterrows():
                key = str(row[key_col]).strip()
                desc = str(row[desc_col]).strip()
                
                # Combine additional info (Unit, Type, etc.)
                extra_info = []
                for col in df.columns[2:]:
                    val = row[col]
                    if pd.notna(val) and str(val).strip():
                        extra_info.append(f"{col}={val}")
                
                if extra_info:
                    desc += " | " + " | ".join(extra_info)
                
                definitions[key] = desc
        
        return definitions
        
    except Exception as e:
        print(f"‚ùå [Parse Error] {file_path}: {e}")
        return {}


def _build_metadata_detection_context(file_path: str, metadata: dict) -> dict:
    """
    [Rule] Build context for metadata detection (preprocessing)
    
    Args:
        file_path: File path
        metadata: raw_metadata extracted by Processor
    
    Returns:
        Context to provide to LLM
    """
    basename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # Rule: Parse filename
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    columns = metadata.get("columns", [])
    column_details = metadata.get("column_details", [])
    
    # Rule: Organize sample data
    sample_summary = []
    total_text_length = 0
    
    for col_info in column_details[:5]:  # First 5 columns only
        col_name = col_info.get('column_name', 'unknown')
        samples = col_info.get('samples', [])
        col_type = col_info.get('column_type', 'unknown')
        
        # If categorical, also provide unique values
        if col_type == 'categorical':
            unique_vals = col_info.get('unique_values', [])[:20]
            # Summarize long text (Rule)
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        else:
            unique_vals = samples[:10]
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        
        # Rule: Calculate average text length
        avg_length = 0.0
        if samples:
            text_lengths = [len(str(s)) for s in samples]
            avg_length = sum(text_lengths) / len(text_lengths)
            total_text_length += avg_length
        
        # [NEW] Collect negative evidence (Rule)
        negative_evidence = _collect_negative_evidence(col_name, samples, unique_vals if unique_vals else [])
        
        # Summarize samples too
        samples_summarized = _summarize_long_values(samples[:3], max_length=50)
        
        sample_summary.append({
            "column": col_name,
            "type": col_type,
            "samples": samples_summarized,
            "unique_values": unique_vals_summarized,
            "avg_text_length": round(avg_length, 1),
            "null_ratio": negative_evidence.get("null_ratio", 0.0),  # [NEW]
            "negative_evidence": negative_evidence.get("issues", [])  # [NEW]
        })
    
    # Estimate context size
    context_size = len(json.dumps(sample_summary))
    
    # If too large, reduce samples (Rule)
    if context_size > 3000:
        sample_summary = sample_summary[:3]
        context_size = len(json.dumps(sample_summary))
    
    return {
        "filename": basename,
        "name_parts": parts,
        "base_name": base_name,
        "extension": extension,
        "columns": columns,
        "num_columns": len(columns),
        "sample_data": sample_summary,
        "avg_text_length_overall": round(total_text_length / max(len(sample_summary), 1), 1),
        "context_size_bytes": context_size
    }


def _ask_llm_is_metadata(context: dict) -> dict:
    """
    [LLM] Determine if file is metadata
    
    Args:
        context: Pre-processed context by Rules
    
    Returns:
        Judgment result {is_metadata, confidence, reasoning, indicators}
    """
    # Use global cache
    # Check cache
    cached = llm_cache.get("metadata_detection", context)
    if cached:
        return cached
    
    # LLM prompt
    prompt = f"""
You are a Data Classification Expert.

I have pre-processed file information using rules. Based on these facts, determine if this is METADATA or TRANSACTIONAL DATA.

[PRE-PROCESSED FILE INFORMATION - Extracted by Rules]
Filename: {context['filename']}
Parsed Name Parts: {context['name_parts']}  (parsed by Rule)
Base Name: {context['base_name']}
Extension: {context['extension']}
Number of Columns: {context['num_columns']}
Columns: {context['columns']}

[PRE-PROCESSED SAMPLE DATA - Extracted by Rules]
{json.dumps(context['sample_data'], indent=2)}
(Note: avg_text_length, unique_values, null_ratio, and negative_evidence were calculated by rules)

[IMPORTANT - Check Negative Evidence]
Each column has "negative_evidence" field showing data quality issues if any:
- near_unique_with_duplicates: Almost unique but has some duplicates
- identifier_with_nulls: Column name suggests ID but has null values
- high_cardinality: Too many unique values for categorical

Use this information to improve your judgment.

[DEFINITION]
- METADATA file: Describes OTHER data (e.g., column definitions, parameter lists, codebooks)
  * Contains descriptive text about columns/variables
  * Usually has structure like: [Name/ID, Description, Unit, Type]
  * Content is documentation, not measurements/transactions
  
- TRANSACTIONAL DATA: Actual records/measurements
  * Contains patient records, lab results, events, etc.
  * Values are data points, not descriptions

[YOUR TASK - Interpret Pre-processed Information]
Using the parsed filename and pre-calculated statistics, classify this file:

1. **Filename Analysis**:
   - Look at name_parts: if contains "parameters", "dict", "definition" ‚Üí likely metadata
   - Look at base_name: what domain does it represent?

2. **Column Structure**:
   - Is it Key-Value format? (e.g., [Parameter, Description, Unit])
   - Or wide transactional format? (many columns with diverse types)

3. **Sample Content Analysis**:
   - Check avg_text_length: Long text (>30 chars) ‚Üí likely descriptions
   - Check unique_values: Are they codes/IDs or explanatory text?

IMPORTANT: I already did the heavy lifting (parsing, statistics). 
You interpret the MEANING of these pre-processed facts.

[OUTPUT FORMAT - JSON ONLY]
{{
    "is_metadata": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation based on filename, structure, and content",
    "indicators": {{
        "filename_hint": "strong/weak/none",
        "structure_hint": "dictionary-like/tabular/unclear",
        "content_type": "descriptive/transactional/mixed"
    }}
}}
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # Save to cache
        llm_cache.set("metadata_detection", context, result)
        
        # Validate confidence
        confidence = result.get("confidence", 0.0)
        if confidence < 0.75:
            print(f"‚ö†Ô∏è  [Metadata Detection] Low confidence ({confidence:.2%})")
            print(f"    Reasoning: {result.get('reasoning', 'N/A')[:100]}...")
        
        return result
        
    except Exception as e:
        print(f"‚ùå [Metadata Detection] LLM Error: {e}")
        # Fallback
        return {
            "is_metadata": False,  # Conservative default
            "confidence": 0.0,
            "reasoning": f"LLM error: {str(e)}",
            "indicators": {},
            "needs_human_review": True
        }


def _find_common_columns(current_cols: List[str], existing_tables: dict) -> List[dict]:
    """
    [Rule] Find common columns between current table and existing tables (FK candidate search)
    
    Args:
        current_cols: Column list of current table
        existing_tables: Existing tables info {table_name: {columns: [...], ...}}
    
    Returns:
        FK candidate list
    """
    candidates = []
    
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        # Find exact matching columns (Rule - exact match)
        common_cols = set(current_cols) & set(existing_cols)
        
        for common_col in common_cols:
            candidates.append({
                "column_name": common_col,
                "current_table": "new_table",
                "existing_table": table_name,
                "match_type": "exact_name",
                "confidence_hint": 0.9  # Same name = high probability of FK
            })
    
    # Find similar names (Rule - simple string normalization)
    # e.g., patient_id vs patientid, subjectid vs subject_id
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        for curr_col in current_cols:
            for exist_col in existing_cols:
                # Compare after removing underscores (Rule)
                curr_normalized = curr_col.replace('_', '').lower()
                exist_normalized = exist_col.replace('_', '').lower()
                
                if curr_normalized == exist_normalized and curr_col != exist_col:
                    candidates.append({
                        "current_col": curr_col,
                        "existing_col": exist_col,
                        "existing_table": table_name,
                        "match_type": "similar_name",
                        "confidence_hint": 0.7  # Similar = medium probability
                    })
    
    return candidates


def _extract_filename_hints(filename: str) -> dict:
    """
    [Rule + LLM] Extract semantic hints from filename
    
    Step 1 (Rule): Analyze filename structure
    Step 2 (LLM): Infer meaning (Entity Type, Level)
    
    Args:
        filename: Filename or file path
    
    Returns:
        Filename hints dictionary
    """
    # Use global cache
    
    # === Step 1: Rule-based filename parsing ===
    basename = os.path.basename(filename)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # Split by underscore (Rule)
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    # Extract prefix/suffix (Rule)
    prefix = parts[0] if len(parts) >= 2 else None
    suffix = parts[-1] if len(parts) >= 2 else None
    
    # Structure info extracted by Rule
    parsed_structure = {
        "original_filename": basename,
        "name_without_ext": name_without_ext,
        "extension": extension,
        "parts": parts,
        "base_name": base_name,
        "prefix": prefix,
        "suffix": suffix,
        "has_underscore": '_' in name_without_ext,
        "num_parts": len(parts)
    }
    
    # === Step 2: LLM-based semantic inference ===
    
    # Check cache
    cached = llm_cache.get("filename_hints", parsed_structure)
    if cached:
        return cached
    
    # LLM prompt
    prompt = f"""
You are a Data Architecture Analyst.

I have parsed the filename structure using rules. Based on this, infer the semantic meaning.

[PARSED FILENAME STRUCTURE - Extracted by Rules]
{json.dumps(parsed_structure, indent=2)}

[YOUR TASK - Semantic Interpretation]
Using the PARSED STRUCTURE, infer:

1. **Entity Type**: What domain entity does base_name represent?
   - Examples: "lab" ‚Üí Laboratory, "patient" ‚Üí Patient, "clinical" ‚Üí Case/Clinical
   - Use domain knowledge (medical, financial, etc.)

2. **Scope**: What is the data scope?
   - individual: Patient, Subject
   - event: Case, Admission, Visit, Stay
   - measurement: Lab, Vital, Sensor
   - treatment: Medication, Procedure

3. **Suggested Hierarchy Level**: (1=highest, 5=lowest)
   - Level 1: Patient, Subject
   - Level 2: Case, Admission, Visit
   - Level 3: Sub-event (ICU Stay)
   - Level 4: Measurement (Lab, Vital)
   - Level 5: Detail

4. **Data Type Indicator**: Based on suffix
   - "data", "records", "events" ‚Üí transactional
   - "parameters", "dict", "info" ‚Üí metadata
   - "master", "dim" ‚Üí reference

5. **Related File Patterns**: Predict related files
   - If "lab_data", likely has "lab_parameters" or "lab_dict"

[OUTPUT FORMAT - JSON]
{{
    "entity_type": "Laboratory" or null,
    "scope": "measurement" or null,
    "suggested_level": 4 or null,
    "data_type_indicator": "transactional" or "metadata",
    "related_file_patterns": ["lab_parameters", "lab_dict"],
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation"
}}
"""
    
    try:
        # Use global llm_client
        hints = llm_client.ask_json(prompt)
        
        # Add default fields
        hints["filename"] = basename
        hints["base_name"] = base_name
        hints["parts"] = parts
        
        # Save to cache
        llm_cache.set("filename_hints", parsed_structure, hints)
        
        # Validate confidence
        if hints.get("confidence", 1.0) < 0.7:
            print(f"‚ö†Ô∏è  [Filename Analysis] Low confidence ({hints.get('confidence'):.2%}) for {basename}")
        
        return hints
        
    except Exception as e:
        # On LLM failure, return minimal info
        print(f"‚ùå [Filename Analysis] LLM Error: {e}")
        return {
            "filename": basename,
            "base_name": base_name,
            "parts": parts,
            "entity_type": None,
            "scope": None,
            "suggested_level": None,
            "data_type_indicator": None,
            "related_file_patterns": [],
            "confidence": 0.0,
            "error": str(e)
        }


def _summarize_existing_tables(ontology_context: dict, processed_files_data: dict = None) -> dict:
    """
    [Rule] Summarize existing table info (for LLM)
    
    Args:
        ontology_context: Current ontology context
        processed_files_data: Column info of processed files (optional)
    
    Returns:
        Table summary dictionary
    """
    tables = {}
    
    # file_tagsÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÌååÏùºÎì§Îßå Ï∂îÏ∂ú
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
            
            # Ïª¨Îüº Ï†ïÎ≥¥ (Ï†ÄÏû•Îêú Í≤ÉÏù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©)
            columns = tag_info.get("columns", [])
            
            # ÎòêÎäî processed_files_dataÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
            if not columns and processed_files_data:
                columns = processed_files_data.get(file_path, {}).get("columns", [])
            
            tables[table_name] = {
                "file_path": file_path,
                "type": tag_info.get("type"),
                "columns": columns
            }
    
    return tables


def _infer_relationships_with_llm(
    current_table_name: str,
    current_cols: List[str],
    ontology_context: dict,
    current_metadata: dict
) -> dict:
    """
    [Rule Ï†ÑÏ≤òÎ¶¨ + LLM ÌåêÎã®] ÌÖåÏù¥Î∏î Í∞Ñ Í¥ÄÍ≥Ñ Ï∂îÎ°†
    
    Args:
        current_table_name: ÌòÑÏû¨ ÌÖåÏù¥Î∏î Ïù¥Î¶Ñ
        current_cols: ÌòÑÏû¨ ÌÖåÏù¥Î∏î Ïª¨Îüº Î¶¨Ïä§Ìä∏
        ontology_context: Ïò®ÌÜ®Î°úÏßÄ Ïª®ÌÖçÏä§Ìä∏
        current_metadata: ÌòÑÏû¨ ÌååÏùºÏùò raw_metadata (Ïπ¥ÎîîÎÑêÎ¶¨Ìã∞ Î∂ÑÏÑùÏö©)
    
    Returns:
        {relationships: [...], hierarchy: [...], reasoning: "..."}
    """
    # Ï†ÑÏó≠ Ï∫êÏãú Î∞è llm_client ÏÇ¨Ïö©
    
    # === 1Îã®Í≥Ñ: Rule Prepares ===
    
    # ÌååÏùºÎ™Ö ÌûåÌä∏ (Rule + LLM)
    filename_hints = _extract_filename_hints(current_table_name)
    
    # Í∏∞Ï°¥ ÌÖåÏù¥Î∏î ÏöîÏïΩ
    existing_tables = _summarize_existing_tables(ontology_context)
    
    # FK ÌõÑÎ≥¥ Ï∞æÍ∏∞ (Rule)
    fk_candidates = _find_common_columns(current_cols, existing_tables)
    
    # Ïπ¥ÎîîÎÑêÎ¶¨Ìã∞ Î∂ÑÏÑù (ÌòÑÏû¨Îäî Í∏∞Î≥∏ ÌÜµÍ≥ÑÎßå)
    cardinality_hints = {}
    column_details = current_metadata.get("column_details", [])
    
    for col_info in column_details:
        col_name = col_info.get('column_name')
        samples = col_info.get('samples', [])
        
        if samples:
            unique_count = len(set(samples))
            total_count = len(samples)
            ratio = unique_count / total_count if total_count > 0 else 0
            
            cardinality_hints[col_name] = {
                "uniqueness_ratio": round(ratio, 2),
                "pattern": "UNIQUE" if ratio > 0.95 else "REPEATED"
            }
    
    # === 2Îã®Í≥Ñ: LLM Decides ===
    
    # Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ±
    llm_context = {
        "current_table": current_table_name,
        "current_cols": current_cols,
        "filename_hints": filename_hints,
        "fk_candidates": fk_candidates,
        "cardinality": cardinality_hints,
        "existing_tables": existing_tables,
        "definitions": ontology_context.get("definitions", {})
    }
    
    # Ï∫êÏãú ÌôïÏù∏
    cached = llm_cache.get("relationship_inference", llm_context)
    if cached:
        print(f"‚úÖ [Cache Hit] Í¥ÄÍ≥Ñ Ï∂îÎ°† Ï∫êÏãú ÏÇ¨Ïö©")
        return cached
    
    # LLM ÌîÑÎ°¨ÌîÑÌä∏
    prompt = f"""
You are a Database Schema Architect for Medical Data Integration.

I have pre-processed data using rules. Infer table relationships.

[PRE-PROCESSED INFORMATION]

1. NEW TABLE:
Name: {current_table_name}
Columns: {current_cols}

2. FILENAME HINTS (Parsed by Rule + LLM):
{json.dumps(filename_hints, indent=2)}

3. FK CANDIDATES (Found by Rules - Common Columns):
{json.dumps(fk_candidates, indent=2)}

4. CARDINALITY (Calculated by Rules):
{json.dumps(cardinality_hints, indent=2)}

5. EXISTING TABLES:
{json.dumps(existing_tables, indent=2)}

6. ONTOLOGY DEFINITIONS (Medical Terms):
Available terms: {len(llm_context['definitions'])} definitions
Example: caseid, subjectid, alb, wbc, etc.

[YOUR TASK]

1. **Validate FK Candidates**:
   - Check if common columns are truly Foreign Keys
   - Use CARDINALITY: if REPEATED ‚Üí likely FK
   - Use FILENAME: if base_names related ‚Üí likely FK

2. **Determine Relationship Type**:
   - 1:1, 1:N, N:1, or M:N based on cardinality

3. **Infer Hierarchy**:
   - Which entity is parent? (more abstract)
   - Which is child? (more specific)
   - Use domain knowledge

[OUTPUT FORMAT - JSON]
{{
  "relationships": [
    {{
      "source_table": "{current_table_name}",
      "target_table": "existing_table_name",
      "source_column": "column_name",
      "target_column": "column_name",
      "relation_type": "N:1",
      "confidence": 0.95,
      "description": "Brief explanation",
      "llm_inferred": true
    }}
  ],
  "hierarchy": [
    {{
      "level": 1,
      "entity_name": "Patient",
      "anchor_column": "subjectid",
      "mapping_table": null,
      "confidence": 0.9
    }}
  ],
  "reasoning": "Overall explanation"
}}

If no relationships found, return empty lists.
Be conservative: confidence < 0.8 if unsure.
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # Ï∫êÏãú Ï†ÄÏû•
        llm_cache.set("relationship_inference", llm_context, result)
        
        # Confidence Í≤ÄÏ¶ù
        rels = result.get("relationships", [])
        low_conf_rels = [r for r in rels if r.get("confidence", 0) < 0.8]
        
        if low_conf_rels:
            print(f"‚ö†Ô∏è  [Relationship] Low confidence for {len(low_conf_rels)} relationships")
        
        return result
        
    except Exception as e:
        print(f"‚ùå [Relationship Inference] LLM Error: {e}")
        return {
            "relationships": [],
            "hierarchy": [],
            "reasoning": f"Error: {str(e)}",
            "error": True
        }


def _summarize_existing_tables(ontology_context: dict, processed_files_data: dict = None) -> dict:
    """
    [Rule] Summarize existing table info (for LLM)
    
    Args:
        ontology_context: Current ontology context
        processed_files_data: Column info of processed files (optional)
    
    Returns:
        Table summary dictionary
    """
    tables = {}
    
    # file_tagsÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÌååÏùºÎì§Îßå Ï∂îÏ∂ú
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
            
            # Ïª¨Îüº Ï†ïÎ≥¥ (Ï†ÄÏû•Îêú Í≤É ÏÇ¨Ïö©)
            columns = tag_info.get("columns", [])
            
            tables[table_name] = {
                "file_path": file_path,
                "type": tag_info.get("type"),
                "columns": columns
            }
    
    return tables


# ============================================================================
# LLM Í∏∞Î∞ò Human Review ÌåêÎã® (Ïú†Ïó∞Ìïú Ï°∞Í±¥)
# ============================================================================

def _should_request_human_review(
    file_path: str,
    issue_type: str,
    context: Dict[str, Any],
    rule_based_confidence: float = 1.0
) -> Dict[str, Any]:
    """
    [Helper] Human ReviewÍ∞Ä ÌïÑÏöîÌïúÏßÄ ÌåêÎã® (Rule + LLM Hybrid)
    
    Args:
        file_path: Ï≤òÎ¶¨ Ï§ëÏù∏ ÌååÏùº Í≤ΩÎ°ú
        issue_type: Ïù¥Ïäà Ïú†Ìòï ("metadata_classification", "anchor_detection", "anchor_conflict", etc.)
        context: ÌåêÎã®Ïóê ÌïÑÏöîÌïú Ïª®ÌÖçÏä§Ìä∏ Ï†ïÎ≥¥
        rule_based_confidence: Rule-based Î∂ÑÏÑùÏóêÏÑú ÏñªÏùÄ confidence (0~1)
    
    Returns:
        {
            "needs_review": bool,
            "reason": str,
            "confidence": float,
            "suggested_question": str (optional)
        }
    """
    filename = os.path.basename(file_path)
    
    # === 1Îã®Í≥Ñ: Rule-based ÌåêÎã® (Îπ†Î•¥Í≥† Ï†ÄÎ†¥) ===
    threshold = _get_threshold_for_issue(issue_type)
    
    rule_decision = {
        "needs_review": rule_based_confidence < threshold,
        "reason": f"Confidence {rule_based_confidence:.1%} < Threshold {threshold:.1%}",
        "confidence": rule_based_confidence
    }
    
    # LLM ÌåêÎã®Ïù¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏúºÎ©¥ Rule Í≤∞Í≥ºÎßå Î∞òÌôò
    if not HumanReviewConfig.USE_LLM_FOR_REVIEW_DECISION:
        print(f"   [Rule-only] {issue_type}: needs_review={rule_decision['needs_review']}")
        return rule_decision
    
    # === 2Îã®Í≥Ñ: LLM Í∏∞Î∞ò ÌåêÎã® (Îçî Ïú†Ïó∞) ===
    # RuleÏóêÏÑú Ïù¥ÎØ∏ "ÌôïÏã§Ìûà ÌïÑÏöî"ÌïòÎã§Í≥† ÌåêÎã®Ìïú Í≤ΩÏö∞ LLM Ìò∏Ï∂ú ÏÉùÎûµ (ÎπÑÏö© Ï†àÍ∞ê)
    if rule_based_confidence < 0.5:
        print(f"   [Rule] Low confidence ({rule_based_confidence:.1%}), skipping LLM check")
        return rule_decision
    
    # LLMÏóêÍ≤å ÌåêÎã® ÏöîÏ≤≠
    llm_decision = _ask_llm_for_review_decision(
        filename=filename,
        issue_type=issue_type,
        context=context,
        rule_confidence=rule_based_confidence
    )
    
    # === 3Îã®Í≥Ñ: RuleÍ≥º LLM Í≤∞Í≥º Ï¢ÖÌï© ===
    # Îëò Ï§ë ÌïòÎÇòÎùºÎèÑ "ÌïÑÏöîÌïòÎã§"Í≥† ÌïòÎ©¥ Human Review ÏöîÏ≤≠
    final_needs_review = rule_decision["needs_review"] or llm_decision.get("needs_review", False)
    
    combined_reason = []
    if rule_decision["needs_review"]:
        combined_reason.append(f"Rule: {rule_decision['reason']}")
    if llm_decision.get("needs_review"):
        combined_reason.append(f"LLM: {llm_decision.get('reason', 'LLM recommended review')}")
    
    result = {
        "needs_review": final_needs_review,
        "reason": " | ".join(combined_reason) if combined_reason else "No issues detected",
        "confidence": rule_based_confidence,
        "llm_opinion": llm_decision.get("reason", "N/A")
    }
    
    print(f"   [Hybrid] {issue_type}: needs_review={final_needs_review}")
    print(f"            Rule={rule_decision['needs_review']}, LLM={llm_decision.get('needs_review', 'N/A')}")
    
    return result


def _get_threshold_for_issue(issue_type: str) -> float:
    """Ïù¥Ïäà Ïú†ÌòïÎ≥Ñ Threshold Î∞òÌôò"""
    thresholds = {
        "metadata_classification": HumanReviewConfig.METADATA_CONFIDENCE_THRESHOLD,
        "anchor_detection": HumanReviewConfig.ANCHOR_CONFIDENCE_THRESHOLD,
        "anchor_conflict": HumanReviewConfig.ANCHOR_CONFIDENCE_THRESHOLD,
        "general": 0.7
    }
    return thresholds.get(issue_type, 0.75)


def _ask_llm_for_review_decision(
    filename: str,
    issue_type: str,
    context: Dict[str, Any],
    rule_confidence: float
) -> Dict[str, Any]:
    """LLMÏóêÍ≤å Human Review ÌïÑÏöî Ïó¨Î∂Ä ÌåêÎã® ÏöîÏ≤≠"""
    
    prompt = f"""
    You are an AI assistant helping with medical data processing.
    Based on the following situation, decide if human intervention is needed.

    [Situation]
    - File: {filename}
    - Issue Type: {issue_type}
    - Rule-based Confidence: {rule_confidence:.1%}
    - Context: {json.dumps(context, ensure_ascii=False, default=str)[:500]}...

    [Issue Type Descriptions]
    - metadata_classification: Determining if file is metadata (dictionary) or actual data
    - anchor_detection: Finding the primary identifier column (e.g., patient_id)
    - anchor_conflict: Mismatch between local and global anchor columns

    [Decision Criteria]
    Return "needs_review": true if:
    1. The context shows ambiguous or conflicting information
    2. Critical decisions might affect data integrity
    3. Domain expertise is clearly needed (medical terminology, etc.)
    4. Multiple valid interpretations exist

    Return "needs_review": false if:
    1. The situation is straightforward despite low confidence
    2. Safe defaults can be applied
    3. The issue can be auto-corrected later

    Respond with JSON only:
    {{
        "needs_review": true or false,
        "reason": "Brief explanation in Korean (ÌïúÍµ≠Ïñ¥)"
    }}
    """
    
    try:
        result = llm_client.ask_json(prompt)
        return {
            "needs_review": result.get("needs_review", False),
            "reason": result.get("reason", "LLM did not provide reason")
        }
    except Exception as e:
        print(f"   ‚ö†Ô∏è [LLM Review Decision] Error: {e}")
        # LLM Ïã§Ìå® Ïãú Rule Í≤∞Í≥ºÏóê ÏùòÏ°¥
        return {"needs_review": False, "reason": f"LLM error: {str(e)}"}


def _parse_human_feedback_to_column(
    feedback: str,
    available_columns: List[str],
    master_anchor: Optional[str],
    file_path: str
) -> Dict[str, Any]:
    """
    [Helper] ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±ÏùÑ ÌååÏã±ÌïòÏó¨ Ïã§Ï†ú Ïª¨ÎüºÎ™Ö Ï∂îÏ∂ú
    
    ÏûÖÎ†• Ïú†Ìòï:
    1. Ïã§Ï†ú Ïª¨ÎüºÎ™Ö (Ïòà: "caseid", "subjectid") ‚Üí Í∑∏ÎåÄÎ°ú Î∞òÌôò
    2. "skip" ‚Üí Ïä§ÌÇµ Ïï°ÏÖò Î∞òÌôò
    3. ÏÑ§Î™Ö (Ïòà: "subjectIDÎäî ÌôòÏûêIDÏù¥Í≥† caseIDÎäî ÏàòÏà† IDÏïº") ‚Üí LLMÏúºÎ°ú Ìï¥ÏÑù
    
    Returns:
        {"action": "use_column", "column_name": "caseid", "reasoning": "..."}
        {"action": "skip", "reasoning": "ÏÇ¨Ïö©ÏûêÍ∞Ä Ïä§ÌÇµ ÏöîÏ≤≠"}
    """
    feedback_lower = feedback.strip().lower()
    
    # Case 1: Ïä§ÌÇµ ÏöîÏ≤≠
    if feedback_lower in ["skip", "Ïä§ÌÇµ", "Í±¥ÎÑàÎõ∞Í∏∞", "pass"]:
        return {"action": "skip", "reasoning": "ÏÇ¨Ïö©ÏûêÍ∞Ä Ïä§ÌÇµ ÏöîÏ≤≠"}
    
    # Case 2: Ïã§Ï†ú Ïª¨ÎüºÎ™ÖÍ≥º Ï†ïÌôïÌûà ÏùºÏπò
    columns_lower = [c.lower() for c in available_columns]
    if feedback_lower in columns_lower:
        # ÏõêÎûò ÎåÄÏÜåÎ¨∏Ïûê Ïú†ÏßÄ
        idx = columns_lower.index(feedback_lower)
        return {
            "action": "use_column",
            "column_name": available_columns[idx],
            "reasoning": "User specified column name directly"
        }
    
    # Case 3: Description or complex input ‚Üí Interpret with LLM
    print(f"   ‚Üí User input is not a column name. Interpreting with LLM...")
    
    from src.utils.llm_client import get_llm_client
    
    try:
        llm_client = get_llm_client()
        
        prompt = f"""The user has provided feedback about the identifier (Anchor) column of a data file.
Interpret this feedback and determine which column should be used.

[File Information]
- Filename: {os.path.basename(file_path)}
- Available Columns: {available_columns}
- Project Master Anchor: {master_anchor or 'None'}

[User Feedback]
"{feedback}"

[Analysis Request]
1. Identify which column should be used as the Anchor based on the user's feedback.
2. If the feedback describes relationships (e.g., "A is patient ID and B is surgery ID"),
   select the most appropriate column from the file's columns.
3. Prioritize columns that can link to the Master Anchor.

[Response Format - JSON only]
{{
    "column_name": "Selected column name (from available columns list)",
    "reasoning": "Reason for selection",
    "user_intent": "Summary of user's intent"
}}"""
        
        result = llm_client.ask_json(prompt)
        
        if "error" not in result and result.get("column_name"):
            selected = result["column_name"]
            
            # ÏÑ†ÌÉùÎêú Ïª¨ÎüºÏù¥ Ïã§Ï†úÎ°ú Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏
            if selected.lower() in columns_lower:
                idx = columns_lower.index(selected.lower())
                return {
                    "action": "use_column",
                    "column_name": available_columns[idx],
                    "reasoning": result.get("reasoning", "LLM interpretation result"),
                    "user_intent": result.get("user_intent", feedback)
                }
        
        # LLM failed to return valid column ‚Üí Use first column
        print(f"   ‚ö†Ô∏è LLM failed to return valid column. Using first column: {available_columns[0]}")
        return {
            "action": "use_column",
            "column_name": available_columns[0] if available_columns else "unknown",
            "reasoning": f"LLM interpretation failed. Using default. User input: {feedback}"
        }
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è LLM call failed: {e}")
        # On LLM failure, use first column
        return {
            "action": "use_column",
            "column_name": available_columns[0] if available_columns else feedback.strip(),
            "reasoning": f"LLM failed. Using default. Error: {str(e)}"
        }


def _generate_natural_human_question(
    file_path: str,
    context: Dict[str, Any],
    issue_type: str = "general_uncertainty"
) -> str:
    """
    [Helper] Generate natural questions for users using LLM (Human-in-the-Loop)
    
    Returns:
        Question string to show to the user (English)
    """
    from src.utils.llm_client import get_llm_client
    
    filename = os.path.basename(file_path)
    
    # Extract context
    columns = context.get("columns", [])
    candidates = context.get("candidates", "None")
    reasoning = context.get("reasoning", "No information")
    ai_msg = context.get("message", "")
    global_master = context.get("master_anchor", "None")
    
    # Format column list
    column_list = columns[:10] if len(columns) > 10 else columns
    columns_str = ", ".join(column_list)
    if len(columns) > 10:
        columns_str += f" ... (and {len(columns) - 10} more)"
    
    # === Fallback messages (used when LLM fails) ===
    fallback_messages = {
        "anchor_conflict": f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîó Anchor Column Mismatch - Confirmation Required                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÅ File: {filename}
‚îÇ  
‚îÇ  ‚ùì Issue:
‚îÇ     The project's Master Anchor is '{global_master}'.
‚îÇ     However, this file appears to use '{candidates}' as the identifier.
‚îÇ  
‚îÇ  üí° AI Analysis:
‚îÇ     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
‚îÇ  
‚îÇ  üìã Columns in file:
‚îÇ     {columns_str}
‚îÇ  
‚îÇ  üéØ Action Required:
‚îÇ     1. Is '{candidates}' the same as '{global_master}'? (e.g., both are Patient ID)
‚îÇ     2. If not, which column corresponds to '{global_master}'?
‚îÇ     3. If none exists, type 'skip'.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""",
        "anchor_uncertain": f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîç Anchor Column Identification Required                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÅ File: {filename}
‚îÇ  
‚îÇ  ‚ùì Issue:
‚îÇ     AI could not identify a Patient/Case identifier (Anchor) column.
‚îÇ     Candidate: '{candidates}' (low confidence)
‚îÇ  
‚îÇ  üí° AI Analysis:
‚îÇ     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
‚îÇ  
‚îÇ  üìã Columns in file:
‚îÇ     {columns_str}
‚îÇ  
‚îÇ  üéØ Action Required:
‚îÇ     Please enter the column name that serves as the unique identifier
‚îÇ     (Patient ID, Subject ID, Case ID, etc.).
‚îÇ     Type 'skip' if none exists.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""",
        "metadata_uncertain": f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìñ File Type Confirmation Required                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÅ File: {filename}
‚îÇ  
‚îÇ  ‚ùì Issue:
‚îÇ     AI cannot determine if this file is 'metadata (description/dictionary)'
‚îÇ     or 'actual data'.
‚îÇ  
‚îÇ  üí° AI Analysis:
‚îÇ     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
‚îÇ  
‚îÇ  üìã Columns in file:
‚îÇ     {columns_str}
‚îÇ  
‚îÇ  üéØ Action Required:
‚îÇ     - If metadata (column descriptions, code definitions): type 'metadata'
‚îÇ     - If actual patient/measurement data: type 'data'
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""",
        "general_uncertainty": f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ö†Ô∏è Confirmation Required                                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÅ File: {filename}
‚îÇ  
‚îÇ  ‚ùì Issue:
‚îÇ     {ai_msg or 'Uncertainty occurred during data processing.'}
‚îÇ  
‚îÇ  üìã Columns in file:
‚îÇ     {columns_str}
‚îÇ  
‚îÇ  üéØ User confirmation is required.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
    }
    
    # === LLM prompt ===
    task_descriptions = {
        "anchor_conflict": f"""
In the current file '{filename}', the column '{candidates}' is presumed to be the identifier.
However, the project's Master Anchor is '{global_master}'.
Ask the user if these two columns have the same meaning, or if a different column should be selected.
""",
        "anchor_uncertain": f"""
No clear identifier column was found in the current file '{filename}'.
AI's candidate is '{candidates}' but with low confidence.
Ask the user which column is the patient/case identifier.
""",
        "metadata_uncertain": f"""
It is unclear whether the current file '{filename}' is metadata (description file) or actual data.
Ask the user to confirm the type of file.
""",
        "general_uncertainty": f"Issue during data processing: {ai_msg}"
    }
    
    task_desc = task_descriptions.get(issue_type, task_descriptions["general_uncertainty"])
    
    prompt = f"""You are an AI assistant helping a medical data engineer.
An uncertainty occurred during data processing, and you need to ask the user a question.

[Context]
- Filename: {filename}
- Columns in file: {columns_str}
- AI Analysis: {reasoning}
- Additional info: {ai_msg}

[Issue to Resolve]
{task_desc}

[Question Guidelines]
1. Write in clear, professional English.
2. Be polite and specific in your question.
3. Briefly explain why you're asking this question.
4. Provide options or examples for the user to choose from.
5. Reference specific column names from the column list.
6. Keep it within 3-5 sentences.
7. Do not use code or JSON format.

Question:"""
    
    try:
        llm_client = get_llm_client()
        llm_response = llm_client.ask_text(prompt)
        
        # LLM ÏùëÎãµÏù¥ ÎÑàÎ¨¥ ÏßßÏúºÎ©¥ fallback ÏÇ¨Ïö©
        if len(llm_response.strip()) < 20:
            return fallback_messages.get(issue_type, fallback_messages["general_uncertainty"])
        
        # LLM ÏùëÎãµ Ìè¨Îß∑ÌåÖ
        formatted_response = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìÅ ÌååÏùº: {filename}
‚îÇ  üìã Ïª¨Îüº: {columns_str}
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§

{llm_response.strip()}

‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
        return formatted_response
        
    except Exception as e:
        print(f"‚ö†Ô∏è [Question Gen] LLM Ìò∏Ï∂ú Ïã§Ìå®: {e}")
        return fallback_messages.get(issue_type, fallback_messages["general_uncertainty"])



def ontology_builder_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node] Ïò®ÌÜ®Î°úÏßÄ Íµ¨Ï∂ï - Rule Prepares, LLM Decides
    
    ÌååÏùºÏù¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Ïù∏ÏßÄ ÌåêÎã®ÌïòÍ≥†, Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î©¥ ÌååÏã±ÌïòÏó¨ Ïò®ÌÜ®Î°úÏßÄÏóê Ï∂îÍ∞Ä
    """
    print("\n" + "="*80)
    print("üìö [ONTOLOGY BUILDER NODE] ÏãúÏûë")
    print("="*80)
    
    file_path = state["file_path"]
    metadata = state["raw_metadata"]
    
    # Í∏∞Ï°¥ Ïò®ÌÜ®Î°úÏßÄ Í∞ÄÏ†∏Ïò§Í∏∞ (StateÏóêÏÑú ÎòêÎäî ÎîîÏä§ÌÅ¨ÏóêÏÑú)
    ontology = state.get("ontology_context")
    
    # Ï≤´ ÌååÏùºÏù¥Í±∞ÎÇò ontologyÍ∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ ÎîîÏä§ÌÅ¨ÏóêÏÑú Î°úÎìú
    if not ontology or not ontology.get("definitions"):
        print(f"   - Ïò®ÌÜ®Î°úÏßÄ Î°úÎìú ÏãúÎèÑ...")
        ontology = ontology_manager.load()
    
    # Ïó¨Ï†ÑÌûà ÏóÜÏúºÎ©¥ Îπà Íµ¨Ï°∞
    if not ontology:
        ontology = {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        }
    
    # === Step 1: Rule Prepares (Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨) ===
    print("\nüîß [Rule] Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ï§ë...")
    context = _build_metadata_detection_context(file_path, metadata)
    
    print(f"   - ÌååÏùºÎ™Ö ÌååÏã±: {context.get('name_parts')}")
    print(f"   - Base Name: {context.get('base_name')}")
    print(f"   - Ïª¨Îüº Ïàò: {context.get('num_columns')}Í∞ú")
    print(f"   - Ïª®ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞: {context.get('context_size_bytes', 0)} bytes")
    
    # === Step 2: LLM Decides (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïó¨Î∂Ä ÌåêÎã®) ===
    print("\nüß† [LLM] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïó¨Î∂Ä ÌåêÎã® Ï§ë...")
    
    meta_result = _ask_llm_is_metadata(context)
    
    confidence = meta_result.get("confidence", 0.0)
    is_metadata = meta_result.get("is_metadata", False)
    
    print(f"   - ÌåêÎã®: {'Î©îÌÉÄÎç∞Ïù¥ÌÑ∞' if is_metadata else 'ÏùºÎ∞ò Îç∞Ïù¥ÌÑ∞'}")
    print(f"   - ÌôïÏã†ÎèÑ: {confidence:.2%}")
    print(f"   - Reasoning: {meta_result.get('reasoning', 'N/A')[:80]}...")
    
    # === Step 3: Confidence Check (Ïú†Ïó∞Ìïú ÌåêÎã®) ===
    review_decision = _should_request_human_review(
        file_path=file_path,
        issue_type="metadata_classification",
        context={
            "is_metadata": is_metadata,
            "reasoning": meta_result.get("reasoning"),
            "columns": context.get("columns", []),
            "indicators": meta_result.get("indicators", {})
        },
        rule_based_confidence=confidence
    )
    
    if review_decision["needs_review"]:
        print(f"\n‚ö†Ô∏è  [Low Confidence] Human Review ÏöîÏ≤≠")
        print(f"   Reason: {review_decision['reason']}")
        
        # Íµ¨Ï≤¥Ï†Å ÏßàÎ¨∏ ÏÉùÏÑ± (LLM)
        specific_question = _generate_natural_human_question(
            file_path=file_path,
            context={
                "reasoning": meta_result.get("reasoning"),
                "message": f"Confidence {confidence:.1%}",
                "columns": context.get("columns", [])
            },
            issue_type="metadata_uncertain"
        )
        
        print("="*80)
        
        return {
            "needs_human_review": True,
            "human_question": specific_question,
            "ontology_context": ontology,  # ÌòÑÏû¨ ÏÉÅÌÉú Ïú†ÏßÄ
            "logs": [f"‚ö†Ô∏è [Ontology] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌåêÎã® Î∂àÌôïÏã§ ({confidence:.2%}). {review_decision['reason']}"]
        }
    
    # === Step 4: Branching (ÌôïÏã†ÎèÑ ÎÜíÏùå) ===
    
    # [Branch A] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌååÏùº
    if is_metadata:
        print(f"\nüìñ [Metadata] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌååÏùºÎ°ú ÌôïÏ†ï")
        
        # ÌååÏùº ÌÉúÍ∑∏ Ï†ÄÏû•
        ontology["file_tags"][file_path] = {
            "type": "metadata",
            "role": "dictionary",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat()
        }
        
        # ÎÇ¥Ïö© ÌååÏã± (Rule)
        print(f"   - Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌååÏã± Ï§ë...")
        new_definitions = _parse_metadata_content(file_path)
        ontology["definitions"].update(new_definitions)
        
        print(f"   - Ïö©Ïñ¥ {len(new_definitions)}Í∞ú Ï∂îÍ∞Ä")
        print(f"   - Ï¥ù Ïö©Ïñ¥: {len(ontology['definitions'])}Í∞ú")
        
        # Ïò®ÌÜ®Î°úÏßÄ Ï†ÄÏû• (ÏòÅÍµ¨ Î≥¥Ï°¥)
        print(f"   - Ïò®ÌÜ®Î°úÏßÄ Ï†ÄÏû• Ï§ë...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": True,  # Ï§ëÏöî! Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îäî Ïù∏Îç±Ïã± Ïä§ÌÇµ
            "logs": [f"üìö [Ontology] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Îì±Î°ù: {len(new_definitions)}Í∞ú Ïö©Ïñ¥ Ï∂îÍ∞Ä (Ï†ÄÏû• ÏôÑÎ£å)"]
        }
    
    # [Branch B] ÏùºÎ∞ò Îç∞Ïù¥ÌÑ∞ ÌååÏùº
    else:
        print(f"\nüìä [Data] ÏùºÎ∞ò Îç∞Ïù¥ÌÑ∞ ÌååÏùºÎ°ú ÌôïÏ†ï")
        
        # Ïª¨Îüº Ï†ïÎ≥¥ Ï†ÄÏû• (Í¥ÄÍ≥Ñ Ï∂îÎ°†Ïóê ÌïÑÏöî)
        columns = metadata.get("columns", [])
        
        ontology["file_tags"][file_path] = {
            "type": "transactional_data",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat(),
            "columns": columns  # [NEW] Ïª¨Îüº Ï†ÄÏû•
        }
        
        # Note: Column MetadataÎäî index_data_nodeÏóêÏÑú finalized_schema ÌôïÏ†ï ÌõÑ Ï†ÄÏû•Îê®
        
        # === Phase 2: Í¥ÄÍ≥Ñ Ï∂îÎ°† (Í∏∞Ï°¥ ÌÖåÏù¥Î∏îÏù¥ ÏûàÏùÑ ÎïåÎßå) ===
        existing_data_files = [
            fp for fp, tag in ontology.get("file_tags", {}).items()
            if tag.get("type") == "transactional_data" and fp != file_path
        ]
        
        if existing_data_files:
            print(f"\nüîó [Relationship] Í¥ÄÍ≥Ñ Ï∂îÎ°† ÏãúÏûë...")
            print(f"   - Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÌååÏùº: {len(existing_data_files)}Í∞ú")
            
            # Í¥ÄÍ≥Ñ Ï∂îÎ°† (LLM)
            table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
            
            relationship_result = _infer_relationships_with_llm(
                current_table_name=table_name,
                current_cols=columns,
                ontology_context=ontology,
                current_metadata=metadata
            )
            
            # Í¥ÄÍ≥Ñ Ï∂îÍ∞Ä
            new_relationships = relationship_result.get("relationships", [])
            if new_relationships:
                print(f"   - Í¥ÄÍ≥Ñ {len(new_relationships)}Í∞ú Î∞úÍ≤¨")
                
                # Í∏∞Ï°¥ Í¥ÄÍ≥ÑÏôÄ Î≥ëÌï©
                existing_rels = ontology.get("relationships", [])
                
                # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
                existing_keys = {
                    (r["source_table"], r["target_table"], r["source_column"], r["target_column"])
                    for r in existing_rels
                }
                
                for new_rel in new_relationships:
                    key = (new_rel["source_table"], new_rel["target_table"], 
                           new_rel["source_column"], new_rel["target_column"])
                    if key not in existing_keys:
                        ontology["relationships"].append(new_rel)
                        print(f"      ‚Ä¢ {new_rel['source_table']}.{new_rel['source_column']} "
                              f"‚Üí {new_rel['target_table']}.{new_rel['target_column']} "
                              f"({new_rel['relation_type']}, conf: {new_rel.get('confidence', 0):.2%})")
            
            # Í≥ÑÏ∏µ ÏóÖÎç∞Ïù¥Ìä∏ (Ï§ëÎ≥µ Ï†úÍ±∞ Í∞ïÌôî)
            new_hierarchy = relationship_result.get("hierarchy", [])
            if new_hierarchy:
                print(f"   - Í≥ÑÏ∏µ Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏")
                
                # Í∏∞Ï°¥ Í≥ÑÏ∏µ
                existing_hier = ontology.get("hierarchy", [])
                
                # Ï§ëÎ≥µ Ï†úÍ±∞ Ï†ÑÎûµ: (level, anchor_column) Ï°∞Ìï©ÏúºÎ°ú ÌåêÎã®
                merged_hierarchy = {}  # key: (level, anchor), value: hierarchy_dict
                
                # Í∏∞Ï°¥ Í≥ÑÏ∏µ Î®ºÏ†Ä Ï∂îÍ∞Ä
                for h in existing_hier:
                    key = (h.get("level"), h.get("anchor_column"))
                    merged_hierarchy[key] = h
                
                # ÏÉà Í≥ÑÏ∏µ Î≥ëÌï© (confidence ÎÜíÏùÄ Í≤É Ïö∞ÏÑ†)
                for new_h in new_hierarchy:
                    key = (new_h.get("level"), new_h.get("anchor_column"))
                    
                    if key not in merged_hierarchy:
                        # ÏÉàÎ°úÏö¥ (level, anchor) Ï°∞Ìï©
                        merged_hierarchy[key] = new_h
                        print(f"      ‚Ä¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [NEW]")
                    else:
                        # Ïù¥ÎØ∏ ÏûàÎäî Ï°∞Ìï© - confidence ÎπÑÍµê
                        existing_conf = merged_hierarchy[key].get("confidence", 0)
                        new_conf = new_h.get("confidence", 0)
                        
                        if new_conf > existing_conf:
                            merged_hierarchy[key] = new_h
                            print(f"      ‚Ä¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [UPDATED, conf: {new_conf:.2%}]")
                        else:
                            print(f"      ‚Ä¢ L{new_h['level']}: (Ï§ëÎ≥µ Ïä§ÌÇµ, Í∏∞Ï°¥ confidence {existing_conf:.2%} Ïú†ÏßÄ)")
                
                # Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò ÌõÑ Î†àÎ≤® Ï†ïÎ†¨
                ontology["hierarchy"] = sorted(merged_hierarchy.values(), key=lambda x: x.get("level", 99))
        else:
            print(f"\n   - Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÏóÜÏùå. Í¥ÄÍ≥Ñ Ï∂îÎ°† Ïä§ÌÇµ.")
        
        # Ïò®ÌÜ®Î°úÏßÄ Ï†ÄÏû•
        print(f"   - Ïò®ÌÜ®Î°úÏßÄ Ï†ÄÏû• Ï§ë...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": False,  # ÏùºÎ∞ò Îç∞Ïù¥ÌÑ∞Îäî Ïù∏Îç±Ïã± Í≥ÑÏÜç
            "logs": ["üîç [Ontology] ÏùºÎ∞ò Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏. Í¥ÄÍ≥Ñ Ï∂îÎ°† ÏôÑÎ£å."]
        }