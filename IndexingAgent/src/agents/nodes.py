import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

from src.agents.state import (
    AgentState, ColumnSchema, AnchorInfo, ProjectContext, OntologyContext,
    FileClassification, ClassificationResult, ProcessingProgress,
    ConversationHistory, ConversationTurn
)
from src.processors.tabular import TabularProcessor
from src.processors.signal import SignalProcessor
from src.utils.llm_client import get_llm_client
from src.utils.ontology_manager import get_ontology_manager
from src.utils.llm_cache import get_llm_cache
from src.config import HumanReviewConfig, ProcessingConfig

# Dataset-First Architecture imports
from src.utils.naming import generate_table_name, generate_table_id, generate_schema_hash
from src.utils.dataset_detector import detect_dataset_from_path, get_dataset_source_path

# --- Global resource initialization ---
llm_client = get_llm_client()
ontology_manager = get_ontology_manager()
llm_cache = get_llm_cache()  # Global cache instance
processors = [
    TabularProcessor(llm_client),
    SignalProcessor(llm_client)
]


# =============================================================================
# Conversation History Management (NEW)
# =============================================================================

def create_empty_conversation_history(dataset_id: str = "unknown") -> ConversationHistory:
    """ë¹ˆ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„±"""
    return {
        "session_id": f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "dataset_id": dataset_id,
        "started_at": datetime.now().isoformat(),
        "turns": [],
        "classification_decisions": [],
        "anchor_decisions": [],
        "user_preferences": {}
    }


def add_conversation_turn(
    history: ConversationHistory,
    review_type: str,
    agent_question: str,
    human_response: str,
    agent_action: str,
    file_path: Optional[str] = None,
    context_summary: Optional[str] = None
) -> ConversationHistory:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ìƒˆ í„´ ì¶”ê°€"""
    turn: ConversationTurn = {
        "turn_id": len(history.get("turns", [])) + 1,
        "timestamp": datetime.now().isoformat(),
        "file_path": file_path,
        "review_type": review_type,
        "agent_question": agent_question,
        "human_response": human_response,
        "agent_action": agent_action,
        "context_summary": context_summary
    }
    
    if "turns" not in history:
        history["turns"] = []
    history["turns"].append(turn)
    
    # ë¶„ë¥˜ ê²°ì • ê¸°ë¡
    if review_type == "classification":
        if "classification_decisions" not in history:
            history["classification_decisions"] = []
        history["classification_decisions"].append({
            "file": os.path.basename(file_path) if file_path else "unknown",
            "response": human_response,
            "timestamp": turn["timestamp"]
        })
    
    # ì•µì»¤ ê²°ì • ê¸°ë¡
    elif review_type in ["anchor", "anchor_detection"]:
        if "anchor_decisions" not in history:
            history["anchor_decisions"] = []
        history["anchor_decisions"].append({
            "file": os.path.basename(file_path) if file_path else "unknown",
            "response": human_response,
            "timestamp": turn["timestamp"]
        })
    
    return history


def format_history_for_prompt(history: ConversationHistory, max_turns: int = 5) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬
        max_turns: í¬í•¨í•  ìµœëŒ€ í„´ ìˆ˜ (ìµœê·¼ Nê°œ)
    
    Returns:
        í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ë¬¸ìì—´
    """
    if not history or not history.get("turns"):
        return ""
    
    turns = history.get("turns", [])[-max_turns:]  # ìµœê·¼ Nê°œë§Œ
    
    if not turns:
        return ""
    
    lines = [
        "\n[CONVERSATION HISTORY - Previous User Interactions]",
        "The following shows previous questions and user responses during this indexing session.",
        "Use this context to make better decisions and follow user preferences.",
        ""
    ]
    
    for turn in turns:
        file_info = f" (File: {os.path.basename(turn['file_path'])})" if turn.get('file_path') else ""
        lines.append(f"--- Turn {turn['turn_id']}{file_info} ---")
        lines.append(f"Type: {turn['review_type']}")
        lines.append(f"Agent Asked: {turn['agent_question'][:200]}...")
        lines.append(f"User Response: {turn['human_response']}")
        lines.append(f"Action Taken: {turn['agent_action']}")
        lines.append("")
    
    # í•™ìŠµëœ íŒ¨í„´ ìš”ì•½
    if history.get("user_preferences"):
        lines.append("[LEARNED USER PREFERENCES]")
        for key, value in history["user_preferences"].items():
            lines.append(f"- {key}: {value}")
        lines.append("")
    
    # ë¶„ë¥˜ ê²°ì • ìš”ì•½
    if history.get("classification_decisions"):
        lines.append("[PREVIOUS CLASSIFICATION DECISIONS]")
        for dec in history["classification_decisions"][-3:]:
            lines.append(f"- {dec['file']}: {dec['response']}")
        lines.append("")
    
    # ì•µì»¤ ê²°ì • ìš”ì•½
    if history.get("anchor_decisions"):
        lines.append("[PREVIOUS ANCHOR DECISIONS]")
        for dec in history["anchor_decisions"][-3:]:
            lines.append(f"- {dec['file']}: {dec['response']}")
        lines.append("")
    
    return "\n".join(lines)


def extract_user_preferences(history: ConversationHistory) -> Dict[str, Any]:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ì‚¬ìš©ì ì„ í˜¸ë„ íŒ¨í„´ ì¶”ì¶œ
    
    ì˜ˆ: íŠ¹ì • ìœ í˜•ì˜ íŒŒì¼ì„ í•­ìƒ ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²½í–¥ ë“±
    """
    preferences = {}
    
    turns = history.get("turns", [])
    
    # ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
    classification_responses = [
        t["human_response"].lower() 
        for t in turns 
        if t["review_type"] == "classification"
    ]
    
    if classification_responses:
        # "í™•ì¸" ë˜ëŠ” "ok"ê°€ ìì£¼ ë‚˜ì˜¤ë©´ AI íŒë‹¨ì„ ì‹ ë¢°í•˜ëŠ” ê²½í–¥
        approval_count = sum(1 for r in classification_responses if r in ["í™•ì¸", "ok", "yes", "approve"])
        if approval_count > len(classification_responses) * 0.7:
            preferences["trusts_ai_classification"] = True
    
    # ì•µì»¤ íŒ¨í„´ ë¶„ì„
    anchor_responses = [
        t["human_response"].lower()
        for t in turns
        if t["review_type"] in ["anchor", "anchor_detection"]
    ]
    
    if anchor_responses:
        # íŠ¹ì • ì»¬ëŸ¼ëª…ì„ ìì£¼ ì§€ì •í•˜ë©´ ì„ í˜¸ ì•µì»¤ë¡œ ê¸°ë¡
        from collections import Counter
        common_anchors = Counter(anchor_responses).most_common(2)
        if common_anchors and common_anchors[0][1] > 1:
            preferences["preferred_anchor"] = common_anchors[0][0]
    
    return preferences



def load_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 1] Load file and extract basic metadata
    """
    file_path = state["file_path"]
    
    print("\n" + "="*80)
    print(f"ğŸ“‚ [LOADER NODE] Starting - {os.path.basename(file_path)}")
    print("="*80)
    
    # 1. Find appropriate Processor
    selected_processor = next((p for p in processors if p.can_handle(file_path)), None)
    
    if not selected_processor:
        return {
            "logs": [f"âŒ Error: Unsupported file format ({file_path})"],
            "needs_human_review": True,
            "human_question": "Unsupported file format. How would you like to process this file?"
        }

    # 2. Extract metadata (Anchor detection is also performed here)
    try:
        raw_metadata = selected_processor.extract_metadata(file_path)
        processor_type = raw_metadata.get("processor_type", "unknown")
        
        # Check if Processor failed to find or was uncertain about Anchor
        anchor_info = raw_metadata.get("anchor_info", {})
        anchor_status = anchor_info.get("status", "MISSING")
        anchor_msg = anchor_info.get("msg", "")

        log_message = f"âœ… [Loader] {processor_type.upper()} analysis complete. Anchor Status: {anchor_status}"

        print(f"\nâœ… [LOADER NODE] Complete")
        print(f"   - Processor: {processor_type}")
        print(f"   - Columns: {len(raw_metadata.get('columns', []))}")
        print(f"   - Anchor Status: {anchor_status}")
        print("="*80)

        return {
            "file_type": processor_type,
            "raw_metadata": raw_metadata,
            "logs": [log_message]
        }
    except Exception as e:
        print(f"\nâŒ [LOADER NODE] Error: {str(e)}")
        print("="*80)
        return {
            "logs": [f"âŒ [Loader] Critical error: {str(e)}"],
            "error_message": str(e)
        }


def analyze_semantics_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 2] Semantic Analysis (Semantic Reasoning)
    Core brain that finalizes schema based on Processor results
    [NEW] References Global Context (Project Level) to ensure Anchor consistency across files.
    [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ íŒë‹¨
    """
    print("\n" + "="*80)
    print("ğŸ§  [ANALYZER NODE] Starting - Semantic Analysis")
    print("="*80)
    
    metadata = state["raw_metadata"]
    local_anchor_info = metadata.get("anchor_info", {})
    human_feedback = state.get("human_feedback")
    
    # Get Global Context (initialize if not exists)
    project_context = state.get("project_context", {
        "master_anchor_name": None, 
        "known_aliases": [], 
        "example_id_values": []
    })
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    dataset_id = state.get("current_dataset_id", "unknown")
    conversation_history = state.get("conversation_history")
    if not conversation_history:
        conversation_history = create_empty_conversation_history(dataset_id)
    
    # íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    history_context = format_history_for_prompt(conversation_history, max_turns=5)
    if history_context:
        print(f"   ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œë¨ ({len(conversation_history.get('turns', []))}ê°œ í„´)")
    
    finalized_anchor = state.get("finalized_anchor")
    retry_count = state.get("retry_count", 0)
    
    # Prevent infinite loop: force processing after 3+ retries
    if retry_count >= 3:
        log_msg = f"âš ï¸ [Analyzer] Retry count exceeded ({retry_count}). Forcing local Anchor."
        
        # Use locally found Anchor as-is
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": local_anchor_info.get("target_column", "unknown"),
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": f"Forced confirmation after {retry_count} retries",
            "mapped_to_master": project_context.get("master_anchor_name")
        }
        
        # Skip schema analysis and complete
        return {
            "finalized_anchor": finalized_anchor,
            "finalized_schema": [],
            "project_context": project_context,
            "needs_human_review": False,
            "human_feedback": None,
            "retry_count": retry_count,
            "logs": [log_msg, "âš ï¸ [Analyzer] Schema analysis skipped (retry exceeded)"]
        }

    # --- Scenario A: Process user feedback (re-entry) ---
    if human_feedback:
        log_msg = f"ğŸ—£ï¸ [Feedback] User feedback received: '{human_feedback}'"
        
        # [NEW] ì‚¬ìš©ì í”¼ë“œë°± ì‹œ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
        file_path = state.get("file_path", "")
        if file_path:
            filename = os.path.basename(file_path)
            llm_cache.invalidate_for_file(filename)
        
        # â­ [FIX] Parse user input - distinguish column name vs description
        parsed_column = _parse_human_feedback_to_column(
            feedback=human_feedback,
            available_columns=metadata.get("columns", []),
            master_anchor=project_context.get("master_anchor_name"),
            file_path=state.get("file_path", "")
        )
        
        if parsed_column.get("action") == "skip":
            # Skip request
            log_msg += " â†’ File skip requested"
            return {
                "finalized_anchor": None,
                "finalized_schema": [],
                "project_context": project_context,
                "needs_human_review": False,
                "human_feedback": None,
                "skip_indexing": True,
                "logs": [log_msg, "â­ï¸ [Analyzer] File skipped by user request"]
            }
        
        # [NEW] Handle special case: filename as ID (for .vital files)
        if parsed_column.get("action") == "use_filename_as_id":
            caseid_value = parsed_column.get("caseid_value")
            reasoning = parsed_column.get("reasoning", "Using filename as identifier")
            
            print(f"   â†’ Using filename as ID: caseid={caseid_value}")
            print(f"   â†’ Reasoning: {reasoning}")
            
            # Update metadata with caseid info
            if "anchor_info" not in metadata:
                metadata["anchor_info"] = {}
            
            metadata["anchor_info"]["status"] = "FOUND"
            metadata["anchor_info"]["target_column"] = "caseid"
            metadata["anchor_info"]["caseid_value"] = caseid_value
            metadata["anchor_info"]["is_time_series"] = True
            metadata["anchor_info"]["needs_human_confirmation"] = False
            
            finalized_anchor = {
                "status": "CONFIRMED",
                "column_name": "caseid",
                "caseid_value": caseid_value,
                "is_time_series": metadata.get("is_time_series", True),
                "reasoning": reasoning,
                "mapped_to_master": project_context.get("master_anchor_name")
            }
            
            # Don't return yet - continue to schema analysis
        
        determined_column = parsed_column.get("column_name", human_feedback.strip())
        reasoning = parsed_column.get("reasoning", "User manually confirmed.")
        
        print(f"   â†’ Parsing result: '{determined_column}'")
        print(f"   â†’ Reasoning: {reasoning}")
        
        # Force Anchor confirmation based on feedback
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": determined_column,
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": reasoning,
            "mapped_to_master": project_context.get("master_anchor_name") 
        }
        
        # â­ [FIX] Reset needs_human_confirmation after feedback processing
        # Prevents re-entering review_required in check_confidence
        if "anchor_info" in metadata:
            metadata["anchor_info"]["needs_human_confirmation"] = False
            metadata["anchor_info"]["status"] = "CONFIRMED"
        
        # Consider feedback processing complete and proceed (don't return)
    
    # --- Scenario B: When Anchor is not yet finalized -> Check Global Context ---
    if not finalized_anchor:
        
        # [NEW] Signal íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬: LLMì´ ì¶”ë¡ í•œ ID ì •ë³´ í™•ì¸
        file_type = state.get("file_type", "tabular")
        if file_type == "signal" and local_anchor_info.get("id_value"):
            id_column = local_anchor_info.get("target_column", "file_id")
            id_value = local_anchor_info.get("id_value")
            confidence = local_anchor_info.get("confidence", 0.5)
            needs_confirmation = local_anchor_info.get("needs_human_confirmation", False)
            
            print(f"\nğŸ“¡ [Signal File] LLM-inferred ID: {id_column}={id_value} (confidence: {confidence:.0%})")
            
            # í™•ì‹ ë„ê°€ ë‚®ìœ¼ë©´ ì‚¬ìš©ì í™•ì¸ ìš”ì²­
            if needs_confirmation and confidence < 0.7:
                question = _generate_natural_human_question(
                    file_path=state.get("file_path", ""),
                    context={
                        "reasoning": local_anchor_info.get("reasoning", ""),
                        "candidates": f"{id_column}={id_value}",
                        "columns": [],  # Signal íŒŒì¼ì€ ì»¬ëŸ¼ì´ ì—†ìŒ
                        "message": f"LLM inferred ID with {confidence:.0%} confidence. Please verify."
                    },
                    issue_type="anchor_uncertain",
                    conversation_history=conversation_history  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
                )
                
                return {
                    "needs_human_review": True,
                    "review_type": "anchor",  # [NEW]
                    "human_question": question,
                    "conversation_history": conversation_history,  # [NEW]
                    "logs": [f"âš ï¸ [Analyzer] Signal file ID uncertain ({confidence:.0%}). Needs confirmation."]
                }
            
            # í™•ì‹ ë„ê°€ ë†’ìœ¼ë©´ ìë™ í™•ì •
            finalized_anchor = {
                "status": "CONFIRMED",
                "column_name": id_column,
                "id_value": id_value,
                "is_time_series": True,
                "reasoning": local_anchor_info.get("reasoning", "LLM inferred ID"),
                "confidence": confidence,
                "mapped_to_master": project_context.get("master_anchor_name")
            }
            
            state["logs"].append(f"ğŸ“¡ [Signal] Auto-confirmed: {id_column}={id_value}")
        
        # [NEW] Case 1: Project already has agreed Anchor (Leader)
        elif project_context.get("master_anchor_name"):
            master_name = project_context["master_anchor_name"]
            
            # LLMì—ê²Œ ë¹„êµ ìš”ì²­ (Global Context vs Local Data)
            comparison = _compare_with_global_context(
                local_metadata=metadata,
                local_anchor_info=local_anchor_info,
                project_context=project_context
            )
            
            # Debug: comparison result log
            comparison_status = comparison.get("status", "UNKNOWN")
            comparison_msg = comparison.get("message", "")
            print(f"\n[DEBUG] Global Anchor comparison result: {comparison_status}")
            print(f"[DEBUG] Message: {comparison_msg}")
            print(f"[DEBUG] Target Column: {comparison.get('target_column', 'N/A')}")
            
            if comparison["status"] == "MATCH":
                # Match success -> auto confirm
                target_col = comparison["target_column"]
                finalized_anchor = {
                    "status": "CONFIRMED",
                    "column_name": target_col,
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": f"Matched with global master anchor '{master_name}'",
                    "mapped_to_master": master_name
                }
                state["logs"].append(f"ğŸ”— [Anchor Link] Matched with Global Anchor '{master_name}' (Local: '{target_col}')")
            
            elif comparison["status"] == "INDIRECT_LINK":
                # â­ [NEW] Indirect link success -> auto confirm (no human intervention needed!)
                via_col = comparison["target_column"]
                via_table = comparison.get("via_table", "unknown")
                
                finalized_anchor = {
                    "status": "INDIRECT_LINK",
                    "column_name": via_col,  # Link column (e.g., caseid)
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": comparison.get("message"),
                    "mapped_to_master": master_name,
                    "via_table": via_table,
                    "link_type": "indirect"  # Indirect link via FK
                }
                
                print(f"\nâœ… [INDIRECT_LINK] Auto-confirmed indirect link!")
                print(f"   - Link column: {via_col}")
                print(f"   - Via table: {via_table}")
                print(f"   - Master Anchor: {master_name}")
                
                state["logs"].append(
                    f"ğŸ”— [Indirect Link] Indirectly linked to '{master_name}' in '{via_table}' via '{via_col}'"
                )
                
            else:
                # Conflict or missing -> human intervention
                msg = comparison.get("message", "Anchor mismatch occurred")
                
                # Generate natural question with LLM
                natural_question = _generate_natural_human_question(
                    file_path=state.get("file_path", ""),
                    context={
                        "master_anchor": master_name,
                        "candidates": local_anchor_info.get("target_column"),
                        "reasoning": msg,
                        "columns": metadata.get("columns", [])
                    },
                    issue_type="anchor_conflict",
                    conversation_history=conversation_history  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
                )
                
                return {
                    "needs_human_review": True,
                    "review_type": "anchor",  # [NEW]
                    "human_question": natural_question,
                    "conversation_history": conversation_history,  # [NEW]
                    "retry_count": retry_count,  # Keep current retry count
                    "logs": [f"âš ï¸ [Analyzer] Global Anchor mismatch (Status: {comparison_status}). Retry: {retry_count}/3"]
                }

        # [NEW] Case 2: This is the first file (no Global Context)
        else:
            # Flexible judgment: Processor uncertainty + LLM review
            processor_confidence = local_anchor_info.get("confidence", 0.5 if local_anchor_info.get("needs_human_confirmation") else 0.9)
            
            review_decision = _should_request_human_review(
                file_path=state.get("file_path", ""),
                issue_type="anchor_detection",
                context={
                    "processor_msg": local_anchor_info.get("msg"),
                    "candidates": local_anchor_info.get("target_column"),
                    "columns": metadata.get("columns", []),
                    "processor_needs_confirmation": local_anchor_info.get("needs_human_confirmation", False)
                },
                rule_based_confidence=processor_confidence
            )
            
            if review_decision["needs_review"]:
                question = _generate_natural_human_question(
                    file_path=state.get("file_path", ""),
                    context={
                        "reasoning": local_anchor_info.get("msg"),
                        "candidates": local_anchor_info.get("target_column", "None"),
                        "columns": metadata.get("columns", [])
                    },
                    issue_type="anchor_uncertain",
                    conversation_history=conversation_history  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
                )
                
                return {
                    "needs_human_review": True,
                    "review_type": "anchor",  # [NEW]
                    "human_question": question,
                    "conversation_history": conversation_history,  # [NEW]
                    "logs": [f"âš ï¸ [Analyzer] Anchor uncertain (first file). {review_decision['reason']}"]
                }
            
            # Confident -> confirm
            finalized_anchor = {
                "status": "CONFIRMED",
                "column_name": local_anchor_info.get("target_column"),
                "is_time_series": local_anchor_info.get("is_time_series"),
                "reasoning": local_anchor_info.get("reasoning"),
                "mapped_to_master": None  # Will become master
            }

    # --- 3. Update Global Context (First-Come Leader Strategy) ---
    # If Anchor is confirmed and no master exists, this file's Anchor becomes master
    if finalized_anchor and not project_context.get("master_anchor_name"):
        project_context["master_anchor_name"] = finalized_anchor["column_name"]
        project_context["known_aliases"].append(finalized_anchor["column_name"])
        state["logs"].append(f"ğŸ‘‘ [Project Context] New Master Anchor set: '{finalized_anchor['column_name']}'")

    # --- 4. Detailed schema analysis (common) ---
    schema_analysis = _analyze_columns_with_llm(
        columns=metadata.get("columns", []),
        sample_data=metadata.get("column_details", {}),
        anchor_context=finalized_anchor
    )

    print(f"\nâœ… [ANALYZER NODE] Complete")
    print(f"   - Anchor: {finalized_anchor.get('column_name', 'N/A')}")
    print(f"   - Mapped to Master: {finalized_anchor.get('mapped_to_master', 'N/A')}")
    print(f"   - Schema Columns: {len(schema_analysis)}")
    print("="*80)

    result = {
        "finalized_anchor": finalized_anchor,
        "finalized_schema": schema_analysis,
        "project_context": project_context,  # Return updated context
        "raw_metadata": metadata,  # â­ [FIX] Return updated raw_metadata (needs_human_confirmation reset)
        "needs_human_review": False,
        "human_feedback": None, 
        "logs": ["ğŸ§  [Analyzer] Complete schema and ontology analysis."]
    }
    
    print(f"\n[DEBUG ANALYZER] Return value:")
    print(f"   - finalized_schema: {len(result['finalized_schema'])}")
    print(f"   - needs_human_review: {result['needs_human_review']}")
    
    return result


def human_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 3] Human-in-the-loop waiting node
    In actual execution, LangGraph's interrupt mechanism stops here
    In test environment, increase retry count to prevent infinite loop
    
    [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ì— í„´ ê¸°ë¡
    """
    print("\n" + "="*80)
    print("ğŸ›‘ [HUMAN REVIEW NODE] Starting - User confirmation required")
    print("="*80)
    
    question = state.get("human_question", "Confirmation required.")
    retry_count = state.get("retry_count", 0)
    human_feedback = state.get("human_feedback")
    file_path = state.get("file_path", "")
    review_type = state.get("review_type", "general")
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)
    history = state.get("conversation_history")
    dataset_id = state.get("current_dataset_id", "unknown")
    
    if not history:
        history = create_empty_conversation_history(dataset_id)
    
    # í”¼ë“œë°±ì´ ìˆìœ¼ë©´ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡ (ì¬ì§„ì… ì‹œ)
    if human_feedback:
        # ì‚¬ìš©ì ì‘ë‹µì— ê¸°ë°˜í•œ ì•¡ì…˜ ê²°ì •
        action_taken = _determine_action_from_feedback(human_feedback, review_type)
        
        history = add_conversation_turn(
            history=history,
            review_type=review_type,
            agent_question=question,
            human_response=human_feedback,
            agent_action=action_taken,
            file_path=file_path,
            context_summary=f"Retry #{retry_count+1} for {os.path.basename(file_path)}"
        )
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
        history["user_preferences"] = extract_user_preferences(history)
        
        print(f"   ğŸ“ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ê¸°ë¡ë¨ (í„´ #{len(history['turns'])})")
    
    # Increase retry count
    new_retry_count = retry_count + 1
    
    print(f"\nâš ï¸  Question: {question[:150]}...")
    print(f"ğŸ”„ Retry count: {new_retry_count}/3")
    print(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(history.get('turns', []))}ê°œ í„´")
    print("="*80)
    
    return {
        "retry_count": new_retry_count,
        "conversation_history": history,
        "logs": [f"ğŸ›‘ [Human Review] Waiting (retry: {new_retry_count}/3). Question: {question[:100]}..."]
    }


def _determine_action_from_feedback(feedback: str, review_type: str) -> str:
    """í”¼ë“œë°±ì—ì„œ ì·¨í•œ ì•¡ì…˜ ê²°ì •"""
    feedback_lower = feedback.lower().strip()
    
    if feedback_lower in ["skip", "ì œì™¸", "ìŠ¤í‚µ"]:
        return "Skipped file"
    elif feedback_lower in ["í™•ì¸", "ok", "yes", "approve", "y"]:
        return "Approved AI decision"
    elif review_type == "classification":
        if "ë©”íƒ€ë°ì´í„°" in feedback_lower or "metadata" in feedback_lower:
            return "Reclassified as metadata"
        elif "ë°ì´í„°" in feedback_lower or "data" in feedback_lower:
            return "Reclassified as data"
    elif review_type in ["anchor", "anchor_detection"]:
        return f"Set anchor to: {feedback}"
    
    return f"Applied feedback: {feedback[:50]}"


def index_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 4 - Phase 3] Build PostgreSQL DB (ontology-based)
    
    Expert feedback applied:
    - Chunk Processing (safe handling of large files)
    - Auto FK constraint creation (ALTER TABLE)
    - Auto index creation (Level 1-2)
    
    [NEW] Dataset-First Architecture:
    - í…Œì´ë¸”ëª…ì— ë°ì´í„°ì…‹ prefix ì¶”ê°€
    - ë²„ì „ ê´€ë¦¬ í…Œì´ë¸”ì— ì¸ë±ì‹± ê¸°ë¡
    - ì˜¨í†¨ë¡œì§€ì— dataset_id í¬í•¨
    
    [NEW] Signal file handling:
    - .vital files are registered as metadata only (no raw data import)
    - caseid is extracted from filename and linked to clinical_data
    """
    import pandas as pd
    import os
    
    from src.database.connection import get_db_manager
    from src.database.schema_generator import SchemaGenerator
    from src.database.version_manager import get_version_manager
    
    print("\n" + "="*80)
    print("ğŸ’¾ [INDEXER NODE] Starting - PostgreSQL DB construction")
    print("="*80)
    
    schema = state.get("finalized_schema", [])
    file_path = state["file_path"]
    file_type = state.get("file_type", "tabular")  # [NEW] íŒŒì¼ íƒ€ì… í™•ì¸
    metadata = state.get("raw_metadata", {})  # [NEW] ë©”íƒ€ë°ì´í„° í™•ì¸
    ontology = state.get("ontology_context", {})
    
    # === Dataset-First: ë°ì´í„°ì…‹ ID ë° í…Œì´ë¸”ëª… ìƒì„± ===
    dataset_id = state.get("current_dataset_id")
    if not dataset_id:
        # ê²½ë¡œì—ì„œ ìë™ ê°ì§€
        dataset_id = detect_dataset_from_path(file_path)
        if not dataset_id:
            dataset_id = "default_dataset"
        print(f"ğŸ“ [Dataset] Auto-detected: {dataset_id}")
    
    # [NEW] Signal íŒŒì¼ (.vital) íŠ¹ë³„ ì²˜ë¦¬
    if file_type == "signal" and metadata.get("is_vital_file", False):
        return _handle_vital_file_indexing(state, file_path, metadata, ontology)
    
    # Dataset-First: í…Œì´ë¸”ëª…ì— prefix ì¶”ê°€
    table_name = generate_table_name(file_path, dataset_id)
    table_id = generate_table_id(dataset_id, table_name)
    
    print(f"   ğŸ“‹ Dataset: {dataset_id}")
    print(f"   ğŸ“‹ Table: {table_name}")
    
    # DB manager
    db_manager = get_db_manager()
    
    try:
        # === 1. Load data (pandas auto-creates table) ===
        print(f"\nğŸ“¥ [Data] Loading data...")
        
        # Check file size
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        print(f"   - File size: {file_size_mb:.1f}MB")
        
        total_rows = 0
        
        # SQLAlchemy engine for PostgreSQL (for pandas to_sql)
        engine = db_manager.get_sqlalchemy_engine()
        
        # [TEST MODE] Row limit (check environment variable)
        test_limit = os.environ.get("TEST_ROW_LIMIT")
        limit_kwargs = {}
        if test_limit:
            limit_rows = int(test_limit)
            limit_kwargs = {"nrows": limit_rows}
            print(f"âš ï¸ [TEST MODE] Data load limit applied: processing top {limit_rows} rows only")

        if file_size_mb > 50:  # Chunk processing for files > 50MB
            print(f"   - Large file - Chunk Processing applied (100,000 rows per chunk)")
            
            chunk_size = 100000
            # [TEST MODE] Apply limit even with chunk processing
            # nrows works with chunksize to limit total rows read
            
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, **limit_kwargs)):
                chunk.to_sql(
                    table_name, 
                    engine, 
                    if_exists='append' if i > 0 else 'replace',
                    index=False,
                    method='multi'  # PostgreSQL optimization
                )
                total_rows += len(chunk)
                print(f"      â€¢ Chunk {i+1}: {len(chunk):,} rows loaded (cumulative: {total_rows:,} rows)")
        else:
            # Load small files at once
            print(f"   - Regular file - loading at once")
            df = pd.read_csv(file_path, **limit_kwargs)
            df.to_sql(
                table_name, 
                engine, 
                if_exists='replace', 
                index=False,
                method='multi'
            )
            total_rows = len(df)
            print(f"   - {total_rows:,} rows loaded")
        
        print(f"âœ… Data loading successful")
        
        # === 2. Create indices (performance optimization) ===
        print(f"\nğŸ” [Index] Creating indices...")
        
        indices = SchemaGenerator.generate_indices(
            table_name=table_name,
            schema=schema,
            ontology_context=ontology
        )
        
        indices_created = []
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        for idx_ddl in indices:
            try:
                cursor.execute(idx_ddl)
                # Extract index name
                idx_name = idx_ddl.split('"')[1] if '"' in idx_ddl else idx_ddl.split()[2]
                indices_created.append(idx_name)
            except Exception as e:
                print(f"âš ï¸  Index creation failed: {e}")
        
        conn.commit()
        
        if indices_created:
            print(f"   - {len(indices_created)} indices created: {', '.join(indices_created)}")
        else:
            print(f"   - No indices created")
        
        # === 3. Verification ===
        print(f"\nâœ… [Verify] Verifying...")
        
        # Check row count (PostgreSQL)
        cursor.execute(f'SELECT COUNT(*) FROM "{table_name}"')
        actual_rows = cursor.fetchone()[0]
        
        if actual_rows == total_rows:
            print(f"   - Row count matches: {actual_rows:,} rows âœ…")
        else:
            print(f"   âš ï¸ Row count mismatch: expected {total_rows:,}, actual {actual_rows:,}")
        
        # === [NEW] Save Column Metadata (Neo4j) - Dataset-First ===
        if schema:
            print(f"\nğŸ“‹ [Column Metadata] Saving column metadata...")
            
            if "column_metadata" not in ontology:
                ontology["column_metadata"] = {}
            
            # Dataset-First: ì˜¨í†¨ë¡œì§€ì— dataset_id ì„¤ì •
            ontology["dataset_id"] = dataset_id
            ontology["column_metadata"][table_name] = {}
            
            for col_schema in schema:
                col_name = col_schema.get("original_name", "unknown")
                ontology["column_metadata"][table_name][col_name] = {
                    "original_name": col_name,
                    "full_name": col_schema.get("full_name"),
                    "inferred_name": col_schema.get("inferred_name"),
                    "description": col_schema.get("description"),
                    "description_kr": col_schema.get("description_kr"),
                    "data_type": col_schema.get("data_type"),
                    "unit": col_schema.get("unit"),
                    "typical_range": col_schema.get("typical_range"),
                    "is_pii": col_schema.get("is_pii", False),
                    "confidence": col_schema.get("confidence", 0)
                }
            
            print(f"   - {len(schema)} column metadata generated")
            
            # Save to Neo4j (with dataset_id)
            from src.utils.ontology_manager import get_ontology_manager
            ontology_manager = get_ontology_manager()
            ontology_manager.save(ontology, dataset_id=dataset_id)
            print(f"   - Neo4j save complete (dataset: {dataset_id})")
        
        # === [NEW] Version Management - Dataset-First ===
        print(f"\nğŸ“ [Version] Recording indexing history...")
        try:
            version_manager = get_version_manager(db_manager)
            schema_hash = generate_schema_hash(schema)
            
            version_info = version_manager.record_indexing(
                table_id=table_id,
                dataset_id=dataset_id,
                table_name=table_name,
                original_filename=os.path.basename(file_path),
                original_filepath=file_path,
                row_count=total_rows,
                column_count=len(schema),
                schema_hash=schema_hash
            )
            print(f"   - Version: v{version_info['version']}")
            if version_info.get('is_schema_changed'):
                print(f"   âš ï¸ Schema changed from previous version!")
        except Exception as ve:
            print(f"   âš ï¸ Version recording failed (non-critical): {ve}")
        
        print("="*80)
        
        return {
            "current_dataset_id": dataset_id,        # [NEW] Dataset ID
            "current_table_name": table_name,        # [NEW] Table name with prefix
            "ontology_context": ontology,            # Updated ontology
            "logs": [
                f"ğŸ’¾ [Indexer] {table_name} created ({total_rows:,} rows)",
                f"ğŸ“ [Indexer] Dataset: {dataset_id}",
                f"ğŸ” [Indexer] Indices: {len(indices_created)}",
                "âœ… [Done] Indexing process complete."
            ]
        }
        
    except Exception as e:
        print(f"\nâŒ [Error] DB save failed: {str(e)}")
        print("="*80)
        
        import traceback
        traceback.print_exc()
        
        return {
            "logs": [f"âŒ [Indexer] DB save failed: {str(e)}"],
            "error_message": str(e)
        }

# --- Helper Functions (Private) ---

def _handle_vital_file_indexing(state: AgentState, file_path: str, metadata: Dict, ontology: Dict) -> Dict[str, Any]:
    """
    [ì˜µì…˜ B] Signal íŒŒì¼ ì¸ë±ì‹± - ì •ê·œí™”ëœ í…Œì´ë¸” êµ¬ì¡°
    
    ë‘ ê°œì˜ í…Œì´ë¸”ë¡œ ì •ê·œí™”:
    1. signal_files: íŒŒì¼ ê¸°ë³¸ ì •ë³´ (1 row per file)
    2. signal_tracks: íŠ¸ë™ë³„ ì •ë³´ (N rows per file, LLMì´ ì˜ë¯¸ ë¶„ì„)
    
    Tabular ë°ì´í„°ì™€ ë™ì¼í•œ íŒ¨í„´:
    - Rule: íŠ¸ë™ ì •ë³´ ìˆ˜ì§‘
    - LLM: ê° íŠ¸ë™ì˜ ì˜ë¯¸/ì¹´í…Œê³ ë¦¬ ë¶„ì„
    """
    import pandas as pd
    from src.database.connection import get_db_manager
    from src.utils.ontology_manager import get_ontology_manager
    
    # anchor_infoì—ì„œ ID ì •ë³´ ì¶”ì¶œ (LLMì´ ì¶”ë¡ í•œ ê²°ê³¼)
    anchor_info = metadata.get("anchor_info", {})
    id_column = anchor_info.get("target_column", "file_id")
    id_value = anchor_info.get("id_value") or anchor_info.get("caseid_value")
    confidence = anchor_info.get("confidence", 0.5)
    needs_confirmation = anchor_info.get("needs_human_confirmation", False)
    
    tracks = metadata.get("columns", [])
    column_details = metadata.get("column_details", {})
    
    print(f"\nğŸ“¡ [Signal File] Processing signal file (Normalized Tables)...")
    print(f"   - ID Column: {id_column}")
    print(f"   - ID Value: {id_value}")
    print(f"   - Confidence: {confidence:.0%}")
    print(f"   - Tracks: {len(tracks)}")
    print(f"   - File: {os.path.basename(file_path)}")
    
    # IDê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if id_value is None:
        print(f"   âš ï¸ ID not found. Skipping indexing.")
        return {
            "logs": [f"âš ï¸ [Indexer] Signal file skipped: ID not found"],
            "skip_indexing": True,
            "needs_human_review": True,
            "human_question": f"Cannot determine ID for signal file '{os.path.basename(file_path)}'. Please specify the ID column and value."
        }
    
    # í™•ì‹ ë„ê°€ ë‚®ìœ¼ë©´ ê²½ê³ 
    if needs_confirmation:
        print(f"   âš ï¸ Low confidence ({confidence:.0%}). ID may need verification.")
    
    try:
        db_manager = get_db_manager()
        engine = db_manager.get_sqlalchemy_engine()
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        # === 1. í…Œì´ë¸” ìƒì„± (ì •ê·œí™”ëœ êµ¬ì¡°) ===
        create_tables_sql = """
        -- íŒŒì¼ ê¸°ë³¸ ì •ë³´ í…Œì´ë¸”
        CREATE TABLE IF NOT EXISTS signal_files (
            file_id SERIAL PRIMARY KEY,
            id_column VARCHAR(50) NOT NULL,
            id_value VARCHAR(100) NOT NULL,
            file_path TEXT NOT NULL,
            file_name VARCHAR(255),
            file_format VARCHAR(20),
            file_size_mb FLOAT,
            duration_seconds FLOAT,
            track_count INTEGER,
            confidence FLOAT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_path)
        );
        
        CREATE INDEX IF NOT EXISTS idx_signal_files_id_value ON signal_files(id_value);
        CREATE INDEX IF NOT EXISTS idx_signal_files_id_column ON signal_files(id_column);
        
        -- íŠ¸ë™ë³„ ì •ë³´ í…Œì´ë¸” (ì •ê·œí™”)
        CREATE TABLE IF NOT EXISTS signal_tracks (
            track_id SERIAL PRIMARY KEY,
            file_id INTEGER REFERENCES signal_files(file_id) ON DELETE CASCADE,
            track_name VARCHAR(255) NOT NULL,
            sample_rate FLOAT,
            unit VARCHAR(50),
            min_value FLOAT,
            max_value FLOAT,
            track_type VARCHAR(50),
            inferred_name VARCHAR(255),
            description TEXT,
            clinical_category VARCHAR(100),
            UNIQUE(file_id, track_name)
        );
        
        CREATE INDEX IF NOT EXISTS idx_signal_tracks_file_id ON signal_tracks(file_id);
        CREATE INDEX IF NOT EXISTS idx_signal_tracks_track_name ON signal_tracks(track_name);
        CREATE INDEX IF NOT EXISTS idx_signal_tracks_category ON signal_tracks(clinical_category);
        """
        
        for stmt in create_tables_sql.strip().split(';'):
            if stmt.strip():
                try:
                    cursor.execute(stmt)
                except Exception as e:
                    pass  # Ignore duplicate errors
        
        conn.commit()
        print(f"   âœ… Tables ready (signal_files, signal_tracks)")
        
        # === 2. signal_files ë ˆì½”ë“œ ì‚½ì… ===
        ext = os.path.splitext(file_path)[1].lower()
        format_map = {".vital": "vitaldb", ".edf": "edf", ".bdf": "bdf"}
        file_format = format_map.get(ext, "unknown")
        
        insert_file_sql = """
        INSERT INTO signal_files (id_column, id_value, file_path, file_name, file_format, 
                                  file_size_mb, duration_seconds, track_count, confidence)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (file_path) 
        DO UPDATE SET 
            id_column = EXCLUDED.id_column,
            id_value = EXCLUDED.id_value,
            file_size_mb = EXCLUDED.file_size_mb,
            duration_seconds = EXCLUDED.duration_seconds,
            track_count = EXCLUDED.track_count,
            confidence = EXCLUDED.confidence
        RETURNING file_id;
        """
        
        cursor.execute(insert_file_sql, (
            id_column,
            str(id_value),
            file_path,
            os.path.basename(file_path),
            file_format,
            metadata.get("file_size_mb", 0),
            metadata.get("duration", 0),
            len(tracks),
            confidence
        ))
        
        file_id = cursor.fetchone()[0]
        conn.commit()
        print(f"   âœ… File registered: file_id={file_id}, {id_column}={id_value}")
        
        # === 3. LLMì—ê²Œ íŠ¸ë™ ì˜ë¯¸ ë¶„ì„ ìš”ì²­ ===
        track_analyses = _analyze_tracks_with_llm(tracks, column_details)
        
        # === 4. signal_tracks ë ˆì½”ë“œ ì‚½ì… ===
        insert_track_sql = """
        INSERT INTO signal_tracks (file_id, track_name, sample_rate, unit, min_value, max_value,
                                   track_type, inferred_name, description, clinical_category)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (file_id, track_name) 
        DO UPDATE SET 
            sample_rate = EXCLUDED.sample_rate,
            unit = EXCLUDED.unit,
            min_value = EXCLUDED.min_value,
            max_value = EXCLUDED.max_value,
            track_type = EXCLUDED.track_type,
            inferred_name = EXCLUDED.inferred_name,
            description = EXCLUDED.description,
            clinical_category = EXCLUDED.clinical_category;
        """
        
        tracks_inserted = 0
        for track_name in tracks:
            details = column_details.get(track_name, {})
            analysis = track_analyses.get(track_name, {})
            
            cursor.execute(insert_track_sql, (
                file_id,
                track_name,
                details.get("sample_rate"),
                details.get("unit"),
                details.get("min_val"),
                details.get("max_val"),
                details.get("column_type", "unknown"),
                analysis.get("inferred_name", track_name),
                analysis.get("description", ""),
                analysis.get("clinical_category", "unknown")
            ))
            tracks_inserted += 1
        
        conn.commit()
        print(f"   âœ… Tracks registered: {tracks_inserted} tracks")
        
        # === 5. ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸ (ì •ê·œí™”ëœ êµ¬ì¡° ë°˜ì˜) ===
        if ontology:
            if "file_tags" not in ontology:
                ontology["file_tags"] = {}
            
            # íŠ¸ë™ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìƒì„¸ ì •ë³´ ì €ì¥
            ontology["file_tags"][file_path] = {
                "type": "signal_data",
                "format": file_format,
                "file_id": file_id,
                "id_column": id_column,
                "id_value": id_value,
                "track_count": len(tracks),
                "confidence": confidence,
                "track_analyses": track_analyses  # LLMì´ ë¶„ì„í•œ íŠ¸ë™ ì •ë³´
            }
            
            # ì •ê·œí™”ëœ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
            if "column_metadata" not in ontology:
                ontology["column_metadata"] = {}
            
            # signal_files í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
            ontology["column_metadata"]["signal_files"] = {
                "file_id": {
                    "original_name": "file_id",
                    "description": "Unique file identifier (auto-generated)",
                    "description_kr": "íŒŒì¼ ê³ ìœ  ID (ìë™ ìƒì„±)",
                    "data_type": "INT",
                    "is_pii": False
                },
                "id_column": {
                    "original_name": "id_column",
                    "description": "Type of ID (caseid, patient_id, subject_id, etc.)",
                    "description_kr": "ID íƒ€ì… (caseid, patient_id, subject_id ë“±)",
                    "data_type": "VARCHAR",
                    "is_pii": False
                },
                "id_value": {
                    "original_name": "id_value",
                    "description": "ID value extracted from filename",
                    "description_kr": "íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ID ê°’",
                    "data_type": "VARCHAR",
                    "is_pii": False
                },
                "file_path": {
                    "original_name": "file_path",
                    "description": "Full path to signal file",
                    "description_kr": "ì‹ í˜¸ íŒŒì¼ ì „ì²´ ê²½ë¡œ",
                    "data_type": "TEXT",
                    "is_pii": False
                },
                "file_format": {
                    "original_name": "file_format",
                    "description": "Signal file format (vitaldb, edf, bdf)",
                    "description_kr": "ì‹ í˜¸ íŒŒì¼ í¬ë§·",
                    "data_type": "VARCHAR",
                    "is_pii": False
                }
            }
            
            # signal_tracks í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
            ontology["column_metadata"]["signal_tracks"] = {
                "track_id": {
                    "original_name": "track_id",
                    "description": "Unique track identifier",
                    "description_kr": "íŠ¸ë™ ê³ ìœ  ID",
                    "data_type": "INT",
                    "is_pii": False
                },
                "file_id": {
                    "original_name": "file_id",
                    "description": "Reference to signal_files.file_id",
                    "description_kr": "signal_files.file_id ì°¸ì¡°",
                    "data_type": "INT",
                    "is_pii": False
                },
                "track_name": {
                    "original_name": "track_name",
                    "description": "Original track name from signal file",
                    "description_kr": "ì‹ í˜¸ íŒŒì¼ì˜ ì›ë³¸ íŠ¸ë™ëª…",
                    "data_type": "VARCHAR",
                    "is_pii": False
                },
                "sample_rate": {
                    "original_name": "sample_rate",
                    "description": "Sampling rate in Hz",
                    "description_kr": "ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz)",
                    "data_type": "FLOAT",
                    "unit": "Hz",
                    "is_pii": False
                },
                "unit": {
                    "original_name": "unit",
                    "description": "Measurement unit (mV, mmHg, %, etc.)",
                    "description_kr": "ì¸¡ì • ë‹¨ìœ„",
                    "data_type": "VARCHAR",
                    "is_pii": False
                },
                "inferred_name": {
                    "original_name": "inferred_name",
                    "description": "LLM-inferred human-readable track name",
                    "description_kr": "LLMì´ ì¶”ë¡ í•œ íŠ¸ë™ ì´ë¦„",
                    "data_type": "VARCHAR",
                    "is_pii": False
                },
                "clinical_category": {
                    "original_name": "clinical_category",
                    "description": "Clinical category (cardiac, respiratory, etc.)",
                    "description_kr": "ì„ìƒ ì¹´í…Œê³ ë¦¬",
                    "data_type": "VARCHAR",
                    "is_pii": False
                }
            }
            
            # Neo4jì— ì €ì¥
            ontology_manager = get_ontology_manager()
            ontology_manager.save(ontology)
            print(f"   âœ… Ontology updated")
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "logs": [
                f"ğŸ“¡ [Indexer] Signal file registered: file_id={file_id}, {id_column}={id_value}",
                f"ğŸ’¾ [Indexer] Stored in normalized tables (signal_files + signal_tracks)",
                f"ğŸ” [Indexer] {tracks_inserted} tracks analyzed by LLM",
                "âœ… [Done] Signal file indexing complete."
            ]
        }
        
    except Exception as e:
        import traceback
        print(f"\nâŒ [Error] Vital file indexing failed: {str(e)}")
        traceback.print_exc()
        print("="*80)
        
        return {
            "logs": [f"âŒ [Indexer] Vital file indexing failed: {str(e)}"],
            "error_message": str(e)
        }


def _analyze_tracks_with_llm(tracks: List[str], column_details: Dict) -> Dict[str, Dict]:
    """
    [LLM Decides] Signal íŠ¸ë™ì˜ ì˜ë¯¸ë¥¼ LLMì´ ë¶„ì„
    
    TabularProcessorì˜ _analyze_columns_with_llmê³¼ ë™ì¼í•œ íŒ¨í„´:
    - Ruleì´ ìˆ˜ì§‘í•œ íŠ¸ë™ ì •ë³´ (ì´ë¦„, ë‹¨ìœ„, ìƒ˜í”Œë ˆì´íŠ¸)ë¥¼ LLMì—ê²Œ ì „ë‹¬
    - LLMì´ ê° íŠ¸ë™ì˜ ì˜ë¯¸, ì¹´í…Œê³ ë¦¬, ì„¤ëª…ì„ ì¶”ë¡ 
    
    Args:
        tracks: íŠ¸ë™ëª… ë¦¬ìŠ¤íŠ¸
        column_details: íŠ¸ë™ë³„ ìƒì„¸ ì •ë³´ {track_name: {unit, sample_rate, ...}}
    
    Returns:
        {track_name: {inferred_name, description, clinical_category, ...}}
    """
    if not tracks:
        return {}
    
    # íŠ¸ë™ ì •ë³´ ìš”ì•½ (LLM í”„ë¡¬í”„íŠ¸ìš©)
    tracks_summary = ""
    for track_name in tracks[:20]:  # ìµœëŒ€ 20ê°œë§Œ ë¶„ì„ (í† í° ì ˆì•½)
        details = column_details.get(track_name, {})
        unit = details.get("unit", "N/A")
        sr = details.get("sample_rate", 0)
        col_type = details.get("column_type", "unknown")
        
        tracks_summary += f"- Track: '{track_name}' | Unit: {unit} | Sample Rate: {sr}Hz | Type: {col_type}\n"
    
    if len(tracks) > 20:
        tracks_summary += f"  ... and {len(tracks) - 20} more tracks\n"
    
    prompt = f"""You are a Medical Signal Processing Expert.
Analyze the following signal tracks and provide detailed metadata for each.

[SIGNAL TRACKS - Pre-processed by Rules]
{tracks_summary}

[TASK]
For each track, determine:
1. **inferred_name**: Human-readable name (e.g., 'SNUADC/ECG_II' â†’ 'Lead II ECG')
2. **description**: Brief medical description
3. **clinical_category**: One of the following categories:
   - cardiac_waveform: ECG, ABP waveforms
   - cardiac_vital: HR, BP values
   - respiratory: SpO2, RR, EtCO2
   - neurological: EEG, BIS, EMG
   - temperature: Body temperature
   - anesthesia: MAC, Agent concentration
   - other: Unknown or miscellaneous

[CLINICAL HINTS]
- 'ECG', 'EKG' â†’ cardiac_waveform (Electrocardiogram)
- 'ART', 'ABP', 'IBP' â†’ cardiac_waveform (Arterial Blood Pressure)
- 'NIBP', 'SBP', 'DBP', 'MBP' â†’ cardiac_vital (Non-invasive BP)
- 'SpO2', 'SaO2' â†’ respiratory (Oxygen Saturation)
- 'RR', 'RESP' â†’ respiratory (Respiratory Rate)
- 'EtCO2', 'ETCO2' â†’ respiratory (End-tidal CO2)
- 'BIS', 'SEF' â†’ neurological (Brain monitoring)
- 'MAC', 'FiO2', 'Agent' â†’ anesthesia

[RESPONSE FORMAT - JSON]
{{
    "tracks": {{
        "SNUADC/ECG_II": {{
            "inferred_name": "Lead II ECG",
            "description": "Standard limb lead II electrocardiogram waveform",
            "clinical_category": "cardiac_waveform"
        }},
        "Solar8000/SpO2": {{
            "inferred_name": "Oxygen Saturation",
            "description": "Peripheral oxygen saturation measured by pulse oximetry",
            "clinical_category": "respiratory"
        }}
    }}
}}

Analyze ALL tracks provided. Be concise but accurate.
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # ê²°ê³¼ íŒŒì‹±
        tracks_analysis = result.get("tracks", {})
        
        # ë¶„ì„ë˜ì§€ ì•Šì€ íŠ¸ë™ì— ëŒ€í•´ ê¸°ë³¸ê°’ ì„¤ì •
        for track_name in tracks:
            if track_name not in tracks_analysis:
                tracks_analysis[track_name] = {
                    "inferred_name": track_name,
                    "description": "",
                    "clinical_category": "other"
                }
        
        print(f"   ğŸ§  [LLM] Analyzed {len(tracks_analysis)} tracks")
        return tracks_analysis
        
    except Exception as e:
        print(f"   âš ï¸ [LLM] Track analysis failed: {e}")
        # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {track_name: {
            "inferred_name": track_name,
            "description": "",
            "clinical_category": "other"
        } for track_name in tracks}


def _analyze_columns_with_llm(columns: List[str], sample_data: Any, anchor_context: Dict) -> List[ColumnSchema]:
    """
    [Helper] Analyze column meaning, data type, PII status, units, etc. using LLM
    
    [Enhancements] Column metadata enrichment:
    - full_name: Abbreviation expansion (e.g., sbp â†’ Systolic Blood Pressure)
    - unit: Measurement unit (e.g., mmHg, kg, cm)
    - typical_range: Medical normal range
    - sample_values: Actual sample values
    """
    # Context summary for LLM
    prompt = f"""
    You are a Medical Data Ontologist specializing in clinical database design.
    Analyze the columns of a medical dataset and provide DETAILED metadata.
    
    [Context]
    - Patient Identifier (Anchor): {anchor_context.get('column_name')}
    - Is Time Series: {anchor_context.get('is_time_series')}
    
    [Columns to Analyze]
    """
    
    # If sample_data is a list (from TabularProcessor)
    if isinstance(sample_data, list):
        for col_detail in sample_data:
            col_name = col_detail.get('column_name', 'unknown')
            col_type = col_detail.get('column_type', 'unknown')
            samples = col_detail.get('samples', [])
            
            if col_type == 'categorical':
                unique_vals = col_detail.get('unique_values', [])
                prompt += f"- Column: '{col_name}' | Type: CATEGORICAL | Unique Values: {unique_vals}\n"
            else:
                min_val = col_detail.get('min', 'N/A')
                max_val = col_detail.get('max', 'N/A')
                prompt += f"- Column: '{col_name}' | Type: CONTINUOUS | Range: [{min_val}, {max_val}] | Samples: {samples}\n"
    # If sample_data is a dictionary (backward compatibility)
    elif isinstance(sample_data, dict):
        for col in columns:
            details = sample_data.get(col, {})
            samples = details.get("sample_values", [])
            prompt += f"- Column: '{col}', Samples: {samples}\n"
    else:
        # If neither, provide column names only
        for col in columns:
            prompt += f"- Column: '{col}'\n"

    prompt += """
    [Task]
    For EACH column, provide a JSON object with DETAILED metadata:
    
    1. original_name: The exact column name as provided (REQUIRED)
    2. inferred_name: Human-readable name (e.g., 'sbp' â†’ 'Systolic Blood Pressure')
    3. full_name: Full medical term without abbreviation (e.g., 'Systolic Blood Pressure')
    4. description: Brief medical description (what does this column measure?)
    5. description_kr: Korean description for Korean users (í•œê¸€ ì„¤ëª…)
    6. data_type: SQL compatible type (VARCHAR, INT, FLOAT, TIMESTAMP, BOOLEAN)
    7. unit: Measurement unit if applicable (e.g., "mmHg", "kg", "mg/dL", "bpm", "Â°C", null if N/A)
    8. typical_range: Normal/typical value range in medical context (e.g., "90-140" for systolic BP, null if N/A)
    9. is_pii: Boolean (true if it contains name, phone, address, social security number)
    10. confidence: 0.0 to 1.0 (how confident are you about this analysis?)
    
    [Examples]
    - 'sbp' â†’ {"original_name": "sbp", "inferred_name": "Systolic BP", "full_name": "Systolic Blood Pressure", 
               "description": "Peak arterial pressure during heart contraction", "description_kr": "ì‹¬ì¥ ìˆ˜ì¶•ì‹œ ìµœê³  ë™ë§¥ì•• (ìˆ˜ì¶•ê¸° í˜ˆì••)",
               "data_type": "FLOAT", "unit": "mmHg", "typical_range": "90-140", "is_pii": false, "confidence": 0.95}
    - 'hr' â†’ {"original_name": "hr", "inferred_name": "Heart Rate", "full_name": "Heart Rate",
              "description": "Number of heartbeats per minute", "description_kr": "ë¶„ë‹¹ ì‹¬ë°•ìˆ˜",
              "data_type": "INT", "unit": "bpm", "typical_range": "60-100", "is_pii": false, "confidence": 0.95}
    - 'age' â†’ {"original_name": "age", "inferred_name": "Patient Age", "full_name": "Patient Age",
               "description": "Age of the patient", "description_kr": "í™˜ì ë‚˜ì´",
               "data_type": "INT", "unit": "years", "typical_range": "0-120", "is_pii": false, "confidence": 0.90}

    Respond with a JSON object: {"columns": [list of column objects]}
    """
    
    # LLM call
    response = llm_client.ask_json(prompt)
    
    # Check if response is list or dict (wrapping list) and parse
    if isinstance(response, dict) and "columns" in response:
        result_list = response["columns"]
    elif isinstance(response, list):
        result_list = response
    else:
        result_list = []  # Error handling needed

    # Map results
    final_schema = []
    for idx, item in enumerate(result_list):
        # Use original_name if available, otherwise match by index
        original = item.get("original_name") or (columns[idx] if idx < len(columns) else "unknown")
        
        final_schema.append({
            "original_name": original,
            "inferred_name": item.get("inferred_name", original),
            "full_name": item.get("full_name", item.get("inferred_name", original)),
            "description": item.get("description", ""),
            "description_kr": item.get("description_kr", ""),
            "data_type": item.get("data_type", "VARCHAR"),
            "unit": item.get("unit"),  # None if not applicable
            "typical_range": item.get("typical_range"),  # None if not applicable
            "standard_concept_id": None, 
            "is_pii": item.get("is_pii", False),
            "confidence": item.get("confidence", 0.5)
        })
        
    return final_schema


def _compare_with_global_context(local_metadata: Dict, local_anchor_info: Dict, project_context: Dict) -> Dict[str, Any]:
    """
    [Helper] Compare current file data with project Global Anchor info (using LLM)
    
    â­ [NEW] Check ontology relationships for indirect connections
    e.g., lab_data without subjectid can link to clinical_data.subjectid via caseid
    """
    master_name = project_context["master_anchor_name"]
    local_cols = local_metadata.get("columns", [])
    local_candidate = local_anchor_info.get("target_column")
    
    # Extract table name from current filename
    file_path = local_metadata.get("file_path", "")
    current_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
    
    # 1. ì´ë¦„ì´ ì™„ì „íˆ ê°™ì€ ê²½ìš° (Fast Path)
    if master_name in local_cols:
        return {"status": "MATCH", "target_column": master_name, "message": "Exact name match"}

    # â­ [NEW] 2. ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê°„ì ‘ ì—°ê²° í™•ì¸
    indirect_link = _check_indirect_link_via_ontology(
        current_table=current_table,
        local_cols=local_cols,
        master_anchor=master_name
    )
    
    if indirect_link:
        return {
            "status": "INDIRECT_LINK",
            "target_column": indirect_link["via_column"],
            "via_table": indirect_link["via_table"],
            "master_anchor": master_name,
            "message": indirect_link["message"]
        }

    # 3. ë¡œì»¬ í›„ë³´ê°€ ì—†ëŠ” ê²½ìš° (Processorê°€ ëª» ì°¾ìŒ)
    if not local_candidate:
        return {
            "status": "MISSING",
            "target_column": None,
            "message": f"No anchor candidate found in local file. Master anchor '{master_name}' not found in columns: {local_cols}"
        }

    # 3. LLMì„ í†µí•œ ì˜ë¯¸ë¡ ì  ë¹„êµ
    prompt = f"""
    You are a Medical Data Integration Agent.
    Check if the new file contains the Project's Master Anchor (Patient ID).

    [Project Context / Global Master]
    - Master Anchor Name: '{master_name}'
    - Known Aliases: {project_context.get('known_aliases')}
    
    [New File Info]
    - Candidate Column found by AI: '{local_candidate}'
    - All Columns in file: {local_cols}
    
    [Task]
    Determine if any column in the new file represents the same 'Patient ID' entity as the Global Master.
    - If the candidate '{local_candidate}' is a synonym for '{master_name}' (e.g. 'pid' vs 'subject_id'), return MATCH.
    - If another column in 'All Columns' looks like the ID, return MATCH with that column.
    - If you cannot find a matching column, return MISSING.
    - If you are unsure, return CONFLICT.

    Respond with JSON:
    {{
        "status": "MATCH" or "MISSING" or "CONFLICT",
        "target_column": "name_of_column_in_new_file" (or null if missing),
        "message": "Reasoning for the decision"
    }}
    """
    
    try:
        result = llm_client.ask_json(prompt)
        
        # LLM ì‘ë‹µ ê²€ì¦ ë° ì •ê·œí™”
        if not isinstance(result, dict):
            return {"status": "CONFLICT", "target_column": None, "message": "LLM returned invalid format"}
        
        status = result.get("status", "CONFLICT").upper()
        if status not in ["MATCH", "MISSING", "CONFLICT"]:
            status = "CONFLICT"
        
        return {
            "status": status,
            "target_column": result.get("target_column"),
            "message": result.get("message", "No explanation provided")
        }
        
    except Exception as e:
        return {"status": "CONFLICT", "target_column": None, "message": f"Error during anchor comparison: {str(e)}"}


# ============================================================================
# Indirect Link Check (Ontology-based)
# ============================================================================

def _check_indirect_link_via_ontology(current_table: str, local_cols: list, master_anchor: str) -> Optional[Dict]:
    """
    â­ [NEW] Check ontology relationships for indirect connections
    
    Example:
    - lab_data does not have subjectid
    - But ontology has "lab_data.caseid â†’ clinical_data.caseid" relationship
    - clinical_data has subjectid
    - Therefore lab_data is indirectly connected to subjectid via caseid
    
    Returns:
        Indirect link info dict or None
    """
    try:
        # Load ontology
        ontology = ontology_manager.load()
        if not ontology:
            return None
        
        relationships = ontology.get("relationships", [])
        file_tags = ontology.get("file_tags", {})
        
        print(f"\nğŸ”— [Indirect Link Check] {current_table}")
        print(f"   - Ontology relationships: {len(relationships)}")
        
        # Find relationships where current table is source
        for rel in relationships:
            source_table = rel.get("source_table", "")
            target_table = rel.get("target_table", "")
            source_column = rel.get("source_column", "")
            target_column = rel.get("target_column", "")
            
            # If current_table is source
            if current_table.lower() in source_table.lower() or source_table.lower() in current_table.lower():
                # Check if link column exists in current file
                if source_column in local_cols:
                    # Check if target_table has master_anchor
                    target_has_master = _check_table_has_column(file_tags, target_table, master_anchor)
                    
                    if target_has_master:
                        message = (
                            f"âœ… Indirect link found! "
                            f"'{current_table}.{source_column}' â†’ '{target_table}.{target_column}' relation "
                            f"connects to '{master_anchor}'"
                        )
                        print(f"   {message}")
                        
                        return {
                            "via_column": source_column,
                            "via_table": target_table,
                            "via_relation": f"{source_table}.{source_column} â†’ {target_table}.{target_column}",
                            "message": message
                        }
        
        print(f"   - No indirect link found")
        return None
        
    except Exception as e:
        print(f"   âš ï¸ Indirect link check error: {e}")
        return None


def _check_table_has_column(file_tags: Dict, table_name: str, column_name: str) -> bool:
    """
    Check if a specific table has a specific column in file_tags
    """
    for file_path, tag_info in file_tags.items():
        # Extract table name from filename
        file_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
        
        if table_name.lower() in file_table.lower() or file_table.lower() in table_name.lower():
            columns = tag_info.get("columns", [])
            if column_name in columns:
                return True
    
    return False


# ============================================================================
# Ontology Builder Functions (Phase 0-1)
# ============================================================================

def _collect_negative_evidence(col_name: str, samples: list, unique_vals: list) -> dict:
    """
    [Rule] Collect negative evidence (detect data quality issues)
    
    Args:
        col_name: Column name
        samples: Sample values list
        unique_vals: Unique values list
    
    Returns:
        Negative evidence dictionary
    """
    import numpy as np
    
    total = len(samples)
    unique = len(unique_vals)
    
    # Calculate nulls
    null_count = sum(
        1 for s in samples 
        if s is None or s == '' or (isinstance(s, float) and np.isnan(s))
    )
    
    negative_evidence = []
    
    # 1. Near unique but has duplicates (possible data error)
    if total > 0 and unique / total > 0.95 and unique != total:
        dup_rate = (total - unique) / total
        negative_evidence.append({
            "type": "near_unique_with_duplicates",
            "detail": f"{unique/total:.1%} unique BUT {dup_rate:.1%} duplicates - possible data error",
            "severity": "medium"
        })
    
    # 2. ID-like but has nulls (cannot be PK)
    if 'id' in col_name.lower() and null_count > 0:
        null_rate = null_count / total
        negative_evidence.append({
            "type": "identifier_with_nulls",
            "detail": f"Column name suggests ID BUT {null_rate:.1%} null values",
            "severity": "high" if null_rate > 0.1 else "low"
        })
    
    # 3. Cardinality too high (possible free text)
    if unique > 100:
        negative_evidence.append({
            "type": "high_cardinality",
            "detail": f"{unique} unique values - might be free text, not categorical",
            "severity": "low"
        })
    
    return {
        "has_issues": len(negative_evidence) > 0,
        "issues": negative_evidence,
        "null_ratio": null_count / total if total > 0 else 0.0
    }


def _summarize_long_values(values: list, max_length: int = 50) -> list:
    """
    [Rule] Summarize long text (Context Window management)
    
    Args:
        values: Values list
        max_length: Maximum length (summarize if exceeded)
    
    Returns:
        Summarized values list
    """
    summarized = []
    
    for val in values:
        val_str = str(val)
        
        if len(val_str) > max_length:
            # Replace with meta info (save tokens)
            preview = val_str[:20].replace('\n', ' ')
            summarized.append(f"[Text: {len(val_str)} chars, starts='{preview}...']")
        else:
            summarized.append(val_str)
    
    return summarized


def _parse_metadata_content(file_path: str) -> dict:
    """
    [Rule] Parse metadata file (CSV â†’ Dictionary)
    
    Args:
        file_path: Metadata file path
    
    Returns:
        definitions dictionary {parameter: description}
    """
    import pandas as pd
    
    definitions = {}
    
    try:
        df = pd.read_csv(file_path)
        
        # Common metadata structure: [Parameter/Name, Description, ...]
        if len(df.columns) >= 2:
            key_col = df.columns[0]
            desc_col = df.columns[1]
            
            for _, row in df.iterrows():
                key = str(row[key_col]).strip()
                desc = str(row[desc_col]).strip()
                
                # Combine additional info (Unit, Type, etc.)
                extra_info = []
                for col in df.columns[2:]:
                    val = row[col]
                    if pd.notna(val) and str(val).strip():
                        extra_info.append(f"{col}={val}")
                
                if extra_info:
                    desc += " | " + " | ".join(extra_info)
                
                definitions[key] = desc
        
        return definitions
        
    except Exception as e:
        print(f"âŒ [Parse Error] {file_path}: {e}")
        return {}


def _build_metadata_detection_context(file_path: str, metadata: dict) -> dict:
    """
    [Rule] Build context for metadata detection (preprocessing)
    
    Args:
        file_path: File path
        metadata: raw_metadata extracted by Processor
    
    Returns:
        Context to provide to LLM
    """
    basename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # Rule: Parse filename
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    columns = metadata.get("columns", [])
    column_details = metadata.get("column_details", [])
    
    # Rule: Organize sample data
    sample_summary = []
    total_text_length = 0
    
    # [FIX] column_detailsê°€ dictì¸ ê²½ìš° (Signal íŒŒì¼) vs listì¸ ê²½ìš° (Tabular íŒŒì¼) ì²˜ë¦¬
    if isinstance(column_details, dict):
        # dictì¸ ê²½ìš°: valuesë¥¼ listë¡œ ë³€í™˜
        column_details_list = list(column_details.values())[:5]
    elif isinstance(column_details, list):
        column_details_list = column_details[:5]
    else:
        column_details_list = []
    
    for col_info in column_details_list:  # First 5 columns only
        # col_infoê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
        if not isinstance(col_info, dict):
            continue
        col_name = col_info.get('column_name', 'unknown')
        samples = col_info.get('samples', [])
        col_type = col_info.get('column_type', 'unknown')
        
        # If categorical, also provide unique values
        if col_type == 'categorical':
            unique_vals = col_info.get('unique_values', [])[:20]
            # Summarize long text (Rule)
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        else:
            unique_vals = samples[:10]
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        
        # Rule: Calculate average text length
        avg_length = 0.0
        if samples:
            text_lengths = [len(str(s)) for s in samples]
            avg_length = sum(text_lengths) / len(text_lengths)
            total_text_length += avg_length
        
        # [NEW] Collect negative evidence (Rule)
        negative_evidence = _collect_negative_evidence(col_name, samples, unique_vals if unique_vals else [])
        
        # Summarize samples too
        samples_summarized = _summarize_long_values(samples[:3], max_length=50)
        
        sample_summary.append({
            "column": col_name,
            "type": col_type,
            "samples": samples_summarized,
            "unique_values": unique_vals_summarized,
            "avg_text_length": round(avg_length, 1),
            "null_ratio": negative_evidence.get("null_ratio", 0.0),  # [NEW]
            "negative_evidence": negative_evidence.get("issues", [])  # [NEW]
        })
    
    # Estimate context size
    context_size = len(json.dumps(sample_summary))
    
    # If too large, reduce samples (Rule)
    if context_size > 3000:
        sample_summary = sample_summary[:3]
        context_size = len(json.dumps(sample_summary))
    
    return {
        "filename": basename,
        "name_parts": parts,
        "base_name": base_name,
        "extension": extension,
        "columns": columns,
        "num_columns": len(columns),
        "sample_data": sample_summary,
        "avg_text_length_overall": round(total_text_length / max(len(sample_summary), 1), 1),
        "context_size_bytes": context_size
    }


def _ask_llm_is_metadata(context: dict) -> dict:
    """
    [LLM] Determine if file is metadata
    
    Args:
        context: Pre-processed context by Rules
    
    Returns:
        Judgment result {is_metadata, confidence, reasoning, indicators}
    """
    # Use global cache
    # Check cache
    cached = llm_cache.get("metadata_detection", context)
    if cached:
        return cached
    
    # LLM prompt
    prompt = f"""
You are a Data Classification Expert.

I have pre-processed file information using rules. Based on these facts, determine if this is METADATA or TRANSACTIONAL DATA.

[PRE-PROCESSED FILE INFORMATION - Extracted by Rules]
Filename: {context['filename']}
Parsed Name Parts: {context['name_parts']}  (parsed by Rule)
Base Name: {context['base_name']}
Extension: {context['extension']}
Number of Columns: {context['num_columns']}
Columns: {context['columns']}

[PRE-PROCESSED SAMPLE DATA - Extracted by Rules]
{json.dumps(context['sample_data'], indent=2)}
(Note: avg_text_length, unique_values, null_ratio, and negative_evidence were calculated by rules)

[IMPORTANT - Check Negative Evidence]
Each column has "negative_evidence" field showing data quality issues if any:
- near_unique_with_duplicates: Almost unique but has some duplicates
- identifier_with_nulls: Column name suggests ID but has null values
- high_cardinality: Too many unique values for categorical

Use this information to improve your judgment.

[DEFINITION]
- METADATA file: Describes OTHER data (e.g., column definitions, parameter lists, codebooks)
  * Contains descriptive text about columns/variables
  * Usually has structure like: [Name/ID, Description, Unit, Type]
  * Content is documentation, not measurements/transactions
  
- TRANSACTIONAL DATA: Actual records/measurements
  * Contains patient records, lab results, events, etc.
  * Values are data points, not descriptions

[YOUR TASK - Interpret Pre-processed Information]
Using the parsed filename and pre-calculated statistics, classify this file:

1. **Filename Analysis**:
   - Look at name_parts: if contains "parameters", "dict", "definition" â†’ likely metadata
   - Look at base_name: what domain does it represent?

2. **Column Structure**:
   - Is it Key-Value format? (e.g., [Parameter, Description, Unit])
   - Or wide transactional format? (many columns with diverse types)

3. **Sample Content Analysis**:
   - Check avg_text_length: Long text (>30 chars) â†’ likely descriptions
   - Check unique_values: Are they codes/IDs or explanatory text?

IMPORTANT: I already did the heavy lifting (parsing, statistics). 
You interpret the MEANING of these pre-processed facts.

[OUTPUT FORMAT - JSON ONLY]
{{
    "is_metadata": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation based on filename, structure, and content",
    "indicators": {{
        "filename_hint": "strong/weak/none",
        "structure_hint": "dictionary-like/tabular/unclear",
        "content_type": "descriptive/transactional/mixed"
    }}
}}
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # Save to cache
        llm_cache.set("metadata_detection", context, result)
        
        # Validate confidence
        confidence = result.get("confidence", 0.0)
        if confidence < HumanReviewConfig.METADATA_DETECTION_CONFIDENCE_THRESHOLD:
            print(f"âš ï¸  [Metadata Detection] Low confidence ({confidence:.2%})")
            print(f"    Reasoning: {result.get('reasoning', 'N/A')[:100]}...")
        
        return result
        
    except Exception as e:
        print(f"âŒ [Metadata Detection] LLM Error: {e}")
        # Fallback
        return {
            "is_metadata": False,  # Conservative default
            "confidence": 0.0,
            "reasoning": f"LLM error: {str(e)}",
            "indicators": {},
            "needs_human_review": True
        }


def _find_common_columns(current_cols: List[str], existing_tables: dict) -> List[dict]:
    """
    [Rule] Find common columns between current table and existing tables (FK candidate search)
    
    Args:
        current_cols: Column list of current table
        existing_tables: Existing tables info {table_name: {columns: [...], ...}}
    
    Returns:
        FK candidate list
    """
    candidates = []
    
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        # Find exact matching columns (Rule - exact match)
        common_cols = set(current_cols) & set(existing_cols)
        
        for common_col in common_cols:
            candidates.append({
                "column_name": common_col,
                "current_table": "new_table",
                "existing_table": table_name,
                "match_type": "exact_name",
                "confidence_hint": 0.9  # Same name = high probability of FK
            })
    
    # Find similar names (Rule - simple string normalization)
    # e.g., patient_id vs patientid, subjectid vs subject_id
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        for curr_col in current_cols:
            for exist_col in existing_cols:
                # Compare after removing underscores (Rule)
                curr_normalized = curr_col.replace('_', '').lower()
                exist_normalized = exist_col.replace('_', '').lower()
                
                if curr_normalized == exist_normalized and curr_col != exist_col:
                    candidates.append({
                        "current_col": curr_col,
                        "existing_col": exist_col,
                        "existing_table": table_name,
                        "match_type": "similar_name",
                        "confidence_hint": 0.7  # Similar = medium probability
                    })
    
    return candidates


def _extract_filename_hints(filename: str) -> dict:
    """
    [Rule + LLM] Extract semantic hints from filename
    
    Step 1 (Rule): Analyze filename structure
    Step 2 (LLM): Infer meaning (Entity Type, Level)
    
    Args:
        filename: Filename or file path
    
    Returns:
        Filename hints dictionary
    """
    # Use global cache
    
    # === Step 1: Rule-based filename parsing ===
    basename = os.path.basename(filename)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # Split by underscore (Rule)
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    # Extract prefix/suffix (Rule)
    prefix = parts[0] if len(parts) >= 2 else None
    suffix = parts[-1] if len(parts) >= 2 else None
    
    # Structure info extracted by Rule
    parsed_structure = {
        "original_filename": basename,
        "name_without_ext": name_without_ext,
        "extension": extension,
        "parts": parts,
        "base_name": base_name,
        "prefix": prefix,
        "suffix": suffix,
        "has_underscore": '_' in name_without_ext,
        "num_parts": len(parts)
    }
    
    # === Step 2: LLM-based semantic inference ===
    
    # Check cache
    cached = llm_cache.get("filename_hints", parsed_structure)
    if cached:
        return cached
    
    # LLM prompt
    prompt = f"""
You are a Data Architecture Analyst.

I have parsed the filename structure using rules. Based on this, infer the semantic meaning.

[PARSED FILENAME STRUCTURE - Extracted by Rules]
{json.dumps(parsed_structure, indent=2)}

[YOUR TASK - Semantic Interpretation]
Using the PARSED STRUCTURE, infer:

1. **Entity Type**: What domain entity does base_name represent?
   - Examples: "lab" â†’ Laboratory, "patient" â†’ Patient, "clinical" â†’ Case/Clinical
   - Use domain knowledge (medical, financial, etc.)

2. **Scope**: What is the data scope?
   - individual: Patient, Subject
   - event: Case, Admission, Visit, Stay
   - measurement: Lab, Vital, Sensor
   - treatment: Medication, Procedure

3. **Suggested Hierarchy Level**: (1=highest, 5=lowest)
   - Level 1: Patient, Subject
   - Level 2: Case, Admission, Visit
   - Level 3: Sub-event (ICU Stay)
   - Level 4: Measurement (Lab, Vital)
   - Level 5: Detail

4. **Data Type Indicator**: Based on suffix
   - "data", "records", "events" â†’ transactional
   - "parameters", "dict", "info" â†’ metadata
   - "master", "dim" â†’ reference

5. **Related File Patterns**: Predict related files
   - If "lab_data", likely has "lab_parameters" or "lab_dict"

[OUTPUT FORMAT - JSON]
{{
    "entity_type": "Laboratory" or null,
    "scope": "measurement" or null,
    "suggested_level": 4 or null,
    "data_type_indicator": "transactional" or "metadata",
    "related_file_patterns": ["lab_parameters", "lab_dict"],
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation"
}}
"""
    
    try:
        # Use global llm_client
        hints = llm_client.ask_json(prompt)
        
        # Add default fields
        hints["filename"] = basename
        hints["base_name"] = base_name
        hints["parts"] = parts
        
        # Save to cache
        llm_cache.set("filename_hints", parsed_structure, hints)
        
        # Validate confidence
        if hints.get("confidence", 1.0) < HumanReviewConfig.FILENAME_ANALYSIS_CONFIDENCE_THRESHOLD:
            print(f"âš ï¸  [Filename Analysis] Low confidence ({hints.get('confidence'):.2%}) for {basename}")
        
        return hints
        
    except Exception as e:
        # On LLM failure, return minimal info
        print(f"âŒ [Filename Analysis] LLM Error: {e}")
        return {
            "filename": basename,
            "base_name": base_name,
            "parts": parts,
            "entity_type": None,
            "scope": None,
            "suggested_level": None,
            "data_type_indicator": None,
            "related_file_patterns": [],
            "confidence": 0.0,
            "error": str(e)
        }


def _summarize_existing_tables(ontology_context: dict, processed_files_data: dict = None) -> dict:
    """
    [Rule] Summarize existing table info (for LLM)
    
    Args:
        ontology_context: Current ontology context
        processed_files_data: Column info of processed files (optional)
    
    Returns:
        Table summary dictionary
    """
    tables = {}
    
    # file_tagsì—ì„œ ë°ì´í„° íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
            
            # ì»¬ëŸ¼ ì •ë³´ (ì €ì¥ëœ ê²ƒì´ ìˆìœ¼ë©´ ì‚¬ìš©)
            columns = tag_info.get("columns", [])
            
            # ë˜ëŠ” processed_files_dataì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not columns and processed_files_data:
                columns = processed_files_data.get(file_path, {}).get("columns", [])
            
            tables[table_name] = {
                "file_path": file_path,
                "type": tag_info.get("type"),
                "columns": columns
            }
    
    return tables


def _infer_relationships_with_llm(
    current_table_name: str,
    current_cols: List[str],
    ontology_context: dict,
    current_metadata: dict
) -> dict:
    """
    [Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨] í…Œì´ë¸” ê°„ ê´€ê³„ ì¶”ë¡ 
    
    Args:
        current_table_name: í˜„ì¬ í…Œì´ë¸” ì´ë¦„
        current_cols: í˜„ì¬ í…Œì´ë¸” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        ontology_context: ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸
        current_metadata: í˜„ì¬ íŒŒì¼ì˜ raw_metadata (ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ìš©)
    
    Returns:
        {relationships: [...], hierarchy: [...], reasoning: "..."}
    """
    # ì „ì—­ ìºì‹œ ë° llm_client ì‚¬ìš©
    
    # === 1ë‹¨ê³„: Rule Prepares ===
    
    # íŒŒì¼ëª… íŒíŠ¸ (Rule + LLM)
    filename_hints = _extract_filename_hints(current_table_name)
    
    # ê¸°ì¡´ í…Œì´ë¸” ìš”ì•½
    existing_tables = _summarize_existing_tables(ontology_context)
    
    # FK í›„ë³´ ì°¾ê¸° (Rule)
    fk_candidates = _find_common_columns(current_cols, existing_tables)
    
    # ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ (í˜„ì¬ëŠ” ê¸°ë³¸ í†µê³„ë§Œ)
    cardinality_hints = {}
    column_details = current_metadata.get("column_details", [])
    
    for col_info in column_details:
        col_name = col_info.get('column_name')
        samples = col_info.get('samples', [])
        
        if samples:
            unique_count = len(set(samples))
            total_count = len(samples)
            ratio = unique_count / total_count if total_count > 0 else 0
            
            cardinality_hints[col_name] = {
                "uniqueness_ratio": round(ratio, 2),
                "pattern": "UNIQUE" if ratio > 0.95 else "REPEATED"
            }
    
    # === 2ë‹¨ê³„: LLM Decides ===
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    llm_context = {
        "current_table": current_table_name,
        "current_cols": current_cols,
        "filename_hints": filename_hints,
        "fk_candidates": fk_candidates,
        "cardinality": cardinality_hints,
        "existing_tables": existing_tables,
        "definitions": ontology_context.get("definitions", {})
    }
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("relationship_inference", llm_context)
    if cached:
        print(f"âœ… [Cache Hit] ê´€ê³„ ì¶”ë¡  ìºì‹œ ì‚¬ìš©")
        return cached
    
    # LLM í”„ë¡¬í”„íŠ¸
    prompt = f"""
You are a Database Schema Architect for Medical Data Integration.

I have pre-processed data using rules. Infer table relationships.

[PRE-PROCESSED INFORMATION]

1. NEW TABLE:
Name: {current_table_name}
Columns: {current_cols}

2. FILENAME HINTS (Parsed by Rule + LLM):
{json.dumps(filename_hints, indent=2)}

3. FK CANDIDATES (Found by Rules - Common Columns):
{json.dumps(fk_candidates, indent=2)}

4. CARDINALITY (Calculated by Rules):
{json.dumps(cardinality_hints, indent=2)}

5. EXISTING TABLES:
{json.dumps(existing_tables, indent=2)}

6. ONTOLOGY DEFINITIONS (Medical Terms):
Available terms: {len(llm_context['definitions'])} definitions
Example: caseid, subjectid, alb, wbc, etc.

[YOUR TASK]

1. **Validate FK Candidates**:
   - Check if common columns are truly Foreign Keys
   - Use CARDINALITY: if REPEATED â†’ likely FK
   - Use FILENAME: if base_names related â†’ likely FK

2. **Determine Relationship Type**:
   - 1:1, 1:N, N:1, or M:N based on cardinality

3. **Infer Hierarchy**:
   - Which entity is parent? (more abstract)
   - Which is child? (more specific)
   - Use domain knowledge

[OUTPUT FORMAT - JSON]
{{
  "relationships": [
    {{
      "source_table": "{current_table_name}",
      "target_table": "existing_table_name",
      "source_column": "column_name",
      "target_column": "column_name",
      "relation_type": "N:1",
      "confidence": 0.95,
      "description": "Brief explanation",
      "llm_inferred": true
    }}
  ],
  "hierarchy": [
    {{
      "level": 1,
      "entity_name": "Patient",
      "anchor_column": "subjectid",
      "mapping_table": null,
      "confidence": 0.9
    }}
  ],
  "reasoning": "Overall explanation"
}}

If no relationships found, return empty lists.
Be conservative: confidence < 0.8 if unsure.
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # ìºì‹œ ì €ì¥
        llm_cache.set("relationship_inference", llm_context, result)
        
        # Confidence ê²€ì¦
        rels = result.get("relationships", [])
        low_conf_rels = [r for r in rels if r.get("confidence", 0) < HumanReviewConfig.RELATIONSHIP_CONFIDENCE_THRESHOLD]
        
        if low_conf_rels:
            print(f"âš ï¸  [Relationship] Low confidence for {len(low_conf_rels)} relationships")
        
        return result
        
    except Exception as e:
        print(f"âŒ [Relationship Inference] LLM Error: {e}")
        return {
            "relationships": [],
            "hierarchy": [],
            "reasoning": f"Error: {str(e)}",
            "error": True
        }


# ============================================================================
# LLM ê¸°ë°˜ Human Review íŒë‹¨ (ìœ ì—°í•œ ì¡°ê±´)
# ============================================================================

def _should_request_human_review(
    file_path: str,
    issue_type: str,
    context: Dict[str, Any],
    rule_based_confidence: float = 1.0
) -> Dict[str, Any]:
    """
    [Helper] Human Reviewê°€ í•„ìš”í•œì§€ íŒë‹¨ (Rule + LLM Hybrid)
    
    Args:
        file_path: ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ ê²½ë¡œ
        issue_type: ì´ìŠˆ ìœ í˜• ("metadata_classification", "anchor_detection", "anchor_conflict", etc.)
        context: íŒë‹¨ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        rule_based_confidence: Rule-based ë¶„ì„ì—ì„œ ì–»ì€ confidence (0~1)
    
    Returns:
        {
            "needs_review": bool,
            "reason": str,
            "confidence": float,
            "suggested_question": str (optional)
        }
    """
    filename = os.path.basename(file_path)
    
    # === 1ë‹¨ê³„: Rule-based íŒë‹¨ (ë¹ ë¥´ê³  ì €ë ´) ===
    threshold = _get_threshold_for_issue(issue_type)
    
    rule_decision = {
        "needs_review": rule_based_confidence < threshold,
        "reason": f"Confidence {rule_based_confidence:.1%} < Threshold {threshold:.1%}",
        "confidence": rule_based_confidence
    }
    
    # LLM íŒë‹¨ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ Rule ê²°ê³¼ë§Œ ë°˜í™˜
    if not HumanReviewConfig.USE_LLM_FOR_REVIEW_DECISION:
        print(f"   [Rule-only] {issue_type}: needs_review={rule_decision['needs_review']}")
        return rule_decision
    
    # === 2ë‹¨ê³„: LLM ê¸°ë°˜ íŒë‹¨ (ë” ìœ ì—°) ===
    # Ruleì—ì„œ ì´ë¯¸ "í™•ì‹¤íˆ í•„ìš”"í•˜ë‹¤ê³  íŒë‹¨í•œ ê²½ìš° LLM í˜¸ì¶œ ìƒëµ (ë¹„ìš© ì ˆê°)
    if rule_based_confidence < HumanReviewConfig.LLM_SKIP_CONFIDENCE_THRESHOLD:
        print(f"   [Rule] Low confidence ({rule_based_confidence:.1%}), skipping LLM check")
        return rule_decision
    
    # LLMì—ê²Œ íŒë‹¨ ìš”ì²­
    llm_decision = _ask_llm_for_review_decision(
        filename=filename,
        issue_type=issue_type,
        context=context,
        rule_confidence=rule_based_confidence
    )
    
    # === 3ë‹¨ê³„: Ruleê³¼ LLM ê²°ê³¼ ì¢…í•© ===
    # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ "í•„ìš”í•˜ë‹¤"ê³  í•˜ë©´ Human Review ìš”ì²­
    final_needs_review = rule_decision["needs_review"] or llm_decision.get("needs_review", False)
    
    combined_reason = []
    if rule_decision["needs_review"]:
        combined_reason.append(f"Rule: {rule_decision['reason']}")
    if llm_decision.get("needs_review"):
        combined_reason.append(f"LLM: {llm_decision.get('reason', 'LLM recommended review')}")
    
    result = {
        "needs_review": final_needs_review,
        "reason": " | ".join(combined_reason) if combined_reason else "No issues detected",
        "confidence": rule_based_confidence,
        "llm_opinion": llm_decision.get("reason", "N/A")
    }
    
    print(f"   [Hybrid] {issue_type}: needs_review={final_needs_review}")
    print(f"            Rule={rule_decision['needs_review']}, LLM={llm_decision.get('needs_review', 'N/A')}")
    
    return result


def _get_threshold_for_issue(issue_type: str) -> float:
    """ì´ìŠˆ ìœ í˜•ë³„ Threshold ë°˜í™˜"""
    thresholds = {
        "metadata_classification": HumanReviewConfig.METADATA_CONFIDENCE_THRESHOLD,
        "anchor_detection": HumanReviewConfig.ANCHOR_CONFIDENCE_THRESHOLD,
        "anchor_conflict": HumanReviewConfig.ANCHOR_CONFIDENCE_THRESHOLD,
        "general": HumanReviewConfig.FILENAME_ANALYSIS_CONFIDENCE_THRESHOLD
    }
    return thresholds.get(issue_type, HumanReviewConfig.DEFAULT_CONFIDENCE_THRESHOLD)


def _ask_llm_for_review_decision(
    filename: str,
    issue_type: str,
    context: Dict[str, Any],
    rule_confidence: float
) -> Dict[str, Any]:
    """LLMì—ê²Œ Human Review í•„ìš” ì—¬ë¶€ íŒë‹¨ ìš”ì²­"""
    
    prompt = f"""
    You are an AI assistant helping with medical data processing.
    Based on the following situation, decide if human intervention is needed.

    [Situation]
    - File: {filename}
    - Issue Type: {issue_type}
    - Rule-based Confidence: {rule_confidence:.1%}
    - Context: {json.dumps(context, ensure_ascii=False, default=str)[:500]}...

    [Issue Type Descriptions]
    - metadata_classification: Determining if file is metadata (dictionary) or actual data
    - anchor_detection: Finding the primary identifier column (e.g., patient_id)
    - anchor_conflict: Mismatch between local and global anchor columns

    [Decision Criteria]
    Return "needs_review": true if:
    1. The context shows ambiguous or conflicting information
    2. Critical decisions might affect data integrity
    3. Domain expertise is clearly needed (medical terminology, etc.)
    4. Multiple valid interpretations exist

    Return "needs_review": false if:
    1. The situation is straightforward despite low confidence
    2. Safe defaults can be applied
    3. The issue can be auto-corrected later

    Respond with JSON only:
    {{
        "needs_review": true or false,
        "reason": "Brief explanation in Korean (í•œêµ­ì–´)"
    }}
    """
    
    try:
        result = llm_client.ask_json(prompt)
        return {
            "needs_review": result.get("needs_review", False),
            "reason": result.get("reason", "LLM did not provide reason")
        }
    except Exception as e:
        print(f"   âš ï¸ [LLM Review Decision] Error: {e}")
        # LLM ì‹¤íŒ¨ ì‹œ Rule ê²°ê³¼ì— ì˜ì¡´
        return {"needs_review": False, "reason": f"LLM error: {str(e)}"}


def _parse_human_feedback_to_column(
    feedback: str,
    available_columns: List[str],
    master_anchor: Optional[str],
    file_path: str
) -> Dict[str, Any]:
    """
    [Helper] ì‚¬ìš©ì í”¼ë“œë°±ì„ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ì»¬ëŸ¼ëª… ì¶”ì¶œ
    
    ì…ë ¥ ìœ í˜•:
    1. ì‹¤ì œ ì»¬ëŸ¼ëª… (ì˜ˆ: "caseid", "subjectid") â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜
    2. "skip" â†’ ìŠ¤í‚µ ì•¡ì…˜ ë°˜í™˜
    3. ì„¤ëª… (ì˜ˆ: "subjectIDëŠ” í™˜ìIDì´ê³  caseIDëŠ” ìˆ˜ìˆ  IDì•¼") â†’ LLMìœ¼ë¡œ í•´ì„
    4. [NEW] íŒŒì¼ íƒ€ì… ì„¤ëª… (ì˜ˆ: "it's actual file", "vitaldb íŒ¨í‚¤ì§€ë¡œ ì—´ì–´ì•¼ í•¨") â†’ íŠ¹ìˆ˜ ì²˜ë¦¬
    
    Returns:
        {"action": "use_column", "column_name": "caseid", "reasoning": "..."}
        {"action": "skip", "reasoning": "ì‚¬ìš©ìê°€ ìŠ¤í‚µ ìš”ì²­"}
        {"action": "use_filename_as_id", ...}  [NEW]
    """
    feedback_lower = feedback.strip().lower()
    
    # Case 1: ìŠ¤í‚µ ìš”ì²­
    if feedback_lower in ["skip", "ìŠ¤í‚µ", "ê±´ë„ˆë›°ê¸°", "pass"]:
        return {"action": "skip", "reasoning": "ì‚¬ìš©ìê°€ ìŠ¤í‚µ ìš”ì²­"}
    
    # [NEW] Case 1.5: .vital íŒŒì¼ ê´€ë ¨ í”¼ë“œë°± ê°ì§€
    vital_keywords = ["vital", "vitaldb", "file name is the caseid", "filename is caseid", 
                      "actual file", "actual data", "binary", "signal file"]
    if any(kw in feedback_lower for kw in vital_keywords):
        # íŒŒì¼ëª…ì—ì„œ caseid ì¶”ì¶œ ì‹œë„
        basename = os.path.basename(file_path)
        name_without_ext = os.path.splitext(basename)[0]
        
        import re
        numbers = re.findall(r'\d+', name_without_ext)
        if numbers:
            caseid = int(numbers[-1])
            return {
                "action": "use_filename_as_id",
                "column_name": "caseid",
                "caseid_value": caseid,
                "reasoning": f"User indicated this is a vital file. Caseid={caseid} extracted from filename '{basename}'.",
                "user_intent": "Use filename as caseid for vital file"
            }
        else:
            return {
                "action": "use_filename_as_id",
                "column_name": "caseid",
                "caseid_value": name_without_ext,
                "reasoning": f"User indicated this is a vital file. Using filename '{name_without_ext}' as identifier.",
                "user_intent": "Use filename as identifier for vital file"
            }
    
    # [NEW] Case 2: ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° (signal íŒŒì¼ ë“±)
    if not available_columns:
        print(f"   â†’ No columns available. Processing as special file type...")
        
        # íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ ì‹œë„
        basename = os.path.basename(file_path)
        name_without_ext = os.path.splitext(basename)[0]
        
        import re
        numbers = re.findall(r'\d+', name_without_ext)
        
        if numbers:
            # ìˆ«ìê°€ ìˆìœ¼ë©´ caseidë¡œ ì‚¬ìš©
            caseid = int(numbers[-1])
            return {
                "action": "use_filename_as_id",
                "column_name": "caseid",
                "caseid_value": caseid,
                "reasoning": f"No columns detected. Caseid={caseid} extracted from filename '{basename}'.",
                "user_intent": feedback
            }
        else:
            # ìˆ«ìê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª… ìì²´ë¥¼ IDë¡œ ì‚¬ìš©
            return {
                "action": "use_filename_as_id",
                "column_name": "file_id",
                "caseid_value": name_without_ext,
                "reasoning": f"No columns detected. Using filename '{name_without_ext}' as identifier.",
                "user_intent": feedback
            }
    
    # Case 3: ì‹¤ì œ ì»¬ëŸ¼ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜
    columns_lower = [c.lower() for c in available_columns]
    if feedback_lower in columns_lower:
        # ì›ë˜ ëŒ€ì†Œë¬¸ì ìœ ì§€
        idx = columns_lower.index(feedback_lower)
        return {
            "action": "use_column",
            "column_name": available_columns[idx],
            "reasoning": "User specified column name directly"
        }
    
    # Case 4: Description or complex input â†’ Interpret with LLM
    print(f"   â†’ User input is not a column name. Interpreting with LLM...")
    
    from src.utils.llm_client import get_llm_client
    
    try:
        llm_client = get_llm_client()
        
        prompt = f"""The user has provided feedback about the identifier (Anchor) column of a data file.
Interpret this feedback and determine which column should be used.

[File Information]
- Filename: {os.path.basename(file_path)}
- Available Columns: {available_columns}
- Project Master Anchor: {master_anchor or 'None'}

[User Feedback]
"{feedback}"

[Analysis Request]
1. Identify which column should be used as the Anchor based on the user's feedback.
2. If the feedback describes relationships (e.g., "A is patient ID and B is surgery ID"),
   select the most appropriate column from the file's columns.
3. Prioritize columns that can link to the Master Anchor.

[Response Format - JSON only]
{{
    "column_name": "Selected column name (from available columns list)",
    "reasoning": "Reason for selection",
    "user_intent": "Summary of user's intent"
}}"""
        
        result = llm_client.ask_json(prompt)
        
        if "error" not in result and result.get("column_name"):
            selected = result["column_name"]
            
            # ì„ íƒëœ ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if selected.lower() in columns_lower:
                idx = columns_lower.index(selected.lower())
                return {
                    "action": "use_column",
                    "column_name": available_columns[idx],
                    "reasoning": result.get("reasoning", "LLM interpretation result"),
                    "user_intent": result.get("user_intent", feedback)
                }
        
        # LLM failed to return valid column â†’ Use first column (safely)
        if available_columns:
            print(f"   âš ï¸ LLM failed to return valid column. Using first column: {available_columns[0]}")
            return {
                "action": "use_column",
                "column_name": available_columns[0],
                "reasoning": f"LLM interpretation failed. Using default. User input: {feedback}"
            }
        else:
            # [NEW] ì»¬ëŸ¼ì´ ì—†ì„ ë•Œ ì•ˆì „ ì²˜ë¦¬
            print(f"   âš ï¸ No columns available. Using user feedback as-is.")
            return {
                "action": "use_filename_as_id",
                "column_name": "unknown",
                "reasoning": f"No columns available. User feedback: {feedback}"
            }
        
    except Exception as e:
        print(f"   âš ï¸ LLM call failed: {e}")
        # [NEW] ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬
        if available_columns:
            return {
                "action": "use_column",
                "column_name": available_columns[0],
                "reasoning": f"LLM failed. Using default. Error: {str(e)}"
            }
        else:
            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ
            basename = os.path.basename(file_path)
            name_without_ext = os.path.splitext(basename)[0]
            
            import re
            numbers = re.findall(r'\d+', name_without_ext)
            caseid = int(numbers[-1]) if numbers else name_without_ext
            
            return {
                "action": "use_filename_as_id",
                "column_name": "caseid" if numbers else "file_id",
                "caseid_value": caseid,
                "reasoning": f"LLM failed, no columns. Using filename. Error: {str(e)}"
            }


def _generate_natural_human_question(
    file_path: str,
    context: Dict[str, Any],
    issue_type: str = "general_uncertainty",
    conversation_history: Optional[ConversationHistory] = None
) -> str:
    """
    [Helper] Generate natural questions for users using LLM (Human-in-the-Loop)
    
    [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ì¡°í•˜ì—¬ ë” ë§¥ë½ì— ë§ëŠ” ì§ˆë¬¸ ìƒì„±
    
    Args:
        file_path: í˜„ì¬ íŒŒì¼ ê²½ë¡œ
        context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (columns, candidates, reasoning ë“±)
        issue_type: ì´ìŠˆ ìœ í˜•
        conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì˜µì…˜)
    
    Returns:
        Question string to show to the user (English)
    """
    from src.utils.llm_client import get_llm_client
    
    filename = os.path.basename(file_path)
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸
    history_context = ""
    if conversation_history and conversation_history.get("turns"):
        history_context = format_history_for_prompt(conversation_history, max_turns=3)
    
    # Extract context
    columns = context.get("columns", [])
    candidates = context.get("candidates", "None")
    reasoning = context.get("reasoning", "No information")
    ai_msg = context.get("message", "")
    global_master = context.get("master_anchor", "None")
    
    # Format column list
    column_list = columns[:10] if len(columns) > 10 else columns
    columns_str = ", ".join(column_list)
    if len(columns) > 10:
        columns_str += f" ... (and {len(columns) - 10} more)"
    
    # === Fallback messages (used when LLM fails) ===
    fallback_messages = {
        "anchor_conflict": f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”— Anchor Column Mismatch - Confirmation Required                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ File: {filename}
â”‚  
â”‚  â“ Issue:
â”‚     The project's Master Anchor is '{global_master}'.
â”‚     However, this file appears to use '{candidates}' as the identifier.
â”‚  
â”‚  ğŸ’¡ AI Analysis:
â”‚     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
â”‚  
â”‚  ğŸ“‹ Columns in file:
â”‚     {columns_str}
â”‚  
â”‚  ğŸ¯ Action Required:
â”‚     1. Is '{candidates}' the same as '{global_master}'? (e.g., both are Patient ID)
â”‚     2. If not, which column corresponds to '{global_master}'?
â”‚     3. If none exists, type 'skip'.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""",
        "anchor_uncertain": f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” Anchor Column Identification Required                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ File: {filename}
â”‚  
â”‚  â“ Issue:
â”‚     AI could not identify a Patient/Case identifier (Anchor) column.
â”‚     Candidate: '{candidates}' (low confidence)
â”‚  
â”‚  ğŸ’¡ AI Analysis:
â”‚     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
â”‚  
â”‚  ğŸ“‹ Columns in file:
â”‚     {columns_str}
â”‚  
â”‚  ğŸ¯ Action Required:
â”‚     Please enter the column name that serves as the unique identifier
â”‚     (Patient ID, Subject ID, Case ID, etc.).
â”‚     Type 'skip' if none exists.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""",
        "metadata_uncertain": f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“– File Type Confirmation Required                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ File: {filename}
â”‚  
â”‚  â“ Issue:
â”‚     AI cannot determine if this file is 'metadata (description/dictionary)'
â”‚     or 'actual data'.
â”‚  
â”‚  ğŸ’¡ AI Analysis:
â”‚     {reasoning[:200]}{'...' if len(str(reasoning)) > 200 else ''}
â”‚  
â”‚  ğŸ“‹ Columns in file:
â”‚     {columns_str}
â”‚  
â”‚  ğŸ¯ Action Required:
â”‚     - If metadata (column descriptions, code definitions): type 'metadata'
â”‚     - If actual patient/measurement data: type 'data'
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""",
        "general_uncertainty": f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš ï¸ Confirmation Required                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ File: {filename}
â”‚  
â”‚  â“ Issue:
â”‚     {ai_msg or 'Uncertainty occurred during data processing.'}
â”‚  
â”‚  ğŸ“‹ Columns in file:
â”‚     {columns_str}
â”‚  
â”‚  ğŸ¯ User confirmation is required.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
    }
    
    # === LLM prompt ===
    task_descriptions = {
        "anchor_conflict": f"""
In the current file '{filename}', the column '{candidates}' is presumed to be the identifier.
However, the project's Master Anchor is '{global_master}'.
Ask the user if these two columns have the same meaning, or if a different column should be selected.
""",
        "anchor_uncertain": f"""
No clear identifier column was found in the current file '{filename}'.
AI's candidate is '{candidates}' but with low confidence.
Ask the user which column is the patient/case identifier.
""",
        "metadata_uncertain": f"""
It is unclear whether the current file '{filename}' is metadata (description file) or actual data.
Ask the user to confirm the type of file.
""",
        "general_uncertainty": f"Issue during data processing: {ai_msg}"
    }
    
    task_desc = task_descriptions.get(issue_type, task_descriptions["general_uncertainty"])
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¹ì…˜ ì¶”ê°€
    history_section = ""
    if history_context:
        history_section = f"""
{history_context}

[IMPORTANT - Use Previous Interactions]
- Reference previous user decisions when formulating your question
- If user has shown a pattern (e.g., always approving AI decisions), adjust your question accordingly
- Avoid asking the same question if already answered for similar files
"""
    
    prompt = f"""You are an AI assistant helping a medical data engineer.
An uncertainty occurred during data processing, and you need to ask the user a question.

[Context]
- Filename: {filename}
- Columns in file: {columns_str}
- AI Analysis: {reasoning}
- Additional info: {ai_msg}
{history_section}
[Issue to Resolve]
{task_desc}

[Question Guidelines]
1. Write in clear, professional English.
2. Be polite and specific in your question.
3. Briefly explain why you're asking this question.
4. Provide options or examples for the user to choose from.
5. Reference specific column names from the column list.
6. Keep it within 3-5 sentences.
7. Do not use code or JSON format.
8. If there's conversation history, reference previous user decisions to provide better context.

Question:"""
    
    try:
        llm_client = get_llm_client()
        llm_response = llm_client.ask_text(prompt)
        
        # LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ fallback ì‚¬ìš©
        if len(llm_response.strip()) < 20:
            return fallback_messages.get(issue_type, fallback_messages["general_uncertainty"])
        
        # LLM ì‘ë‹µ í¬ë§·íŒ…
        formatted_response = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ íŒŒì¼: {filename}
â”‚  ğŸ“‹ ì»¬ëŸ¼: {columns_str}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

{llm_response.strip()}

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
        return formatted_response
        
    except Exception as e:
        print(f"âš ï¸ [Question Gen] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return fallback_messages.get(issue_type, fallback_messages["general_uncertainty"])



def ontology_builder_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node] ì˜¨í†¨ë¡œì§€ êµ¬ì¶• - Rule Prepares, LLM Decides
    
    íŒŒì¼ì´ ë©”íƒ€ë°ì´í„°ì¸ì§€ íŒë‹¨í•˜ê³ , ë©”íƒ€ë°ì´í„°ë©´ íŒŒì‹±í•˜ì—¬ ì˜¨í†¨ë¡œì§€ì— ì¶”ê°€
    [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
    """
    print("\n" + "="*80)
    print("ğŸ“š [ONTOLOGY BUILDER NODE] ì‹œì‘")
    print("="*80)
    
    file_path = state["file_path"]
    metadata = state["raw_metadata"]
    
    # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    dataset_id = state.get("current_dataset_id", "unknown")
    conversation_history = state.get("conversation_history")
    if not conversation_history:
        conversation_history = create_empty_conversation_history(dataset_id)
    
    # ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ê°€ì ¸ì˜¤ê¸° (Stateì—ì„œ ë˜ëŠ” ë””ìŠ¤í¬ì—ì„œ)
    ontology = state.get("ontology_context")
    
    # ì²« íŒŒì¼ì´ê±°ë‚˜ ontologyê°€ ë¹„ì–´ìˆìœ¼ë©´ ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
    if not ontology or not ontology.get("definitions"):
        print(f"   - ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹œë„...")
        ontology = ontology_manager.load()
    
    # ì—¬ì „íˆ ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡°
    if not ontology:
        ontology = {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        }
    
    # === Step 1: Rule Prepares (ë°ì´í„° ì „ì²˜ë¦¬) ===
    print("\nğŸ”§ [Rule] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    context = _build_metadata_detection_context(file_path, metadata)
    
    print(f"   - íŒŒì¼ëª… íŒŒì‹±: {context.get('name_parts')}")
    print(f"   - Base Name: {context.get('base_name')}")
    print(f"   - ì»¬ëŸ¼ ìˆ˜: {context.get('num_columns')}ê°œ")
    print(f"   - ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {context.get('context_size_bytes', 0)} bytes")
    
    # === Step 2: LLM Decides (ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨) ===
    print("\nğŸ§  [LLM] ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨ ì¤‘...")
    
    meta_result = _ask_llm_is_metadata(context)
    
    confidence = meta_result.get("confidence", 0.0)
    is_metadata = meta_result.get("is_metadata", False)
    
    print(f"   - íŒë‹¨: {'ë©”íƒ€ë°ì´í„°' if is_metadata else 'ì¼ë°˜ ë°ì´í„°'}")
    print(f"   - í™•ì‹ ë„: {confidence:.2%}")
    print(f"   - Reasoning: {meta_result.get('reasoning', 'N/A')[:80]}...")
    
    # === Step 3: Confidence Check (ìœ ì—°í•œ íŒë‹¨) ===
    review_decision = _should_request_human_review(
        file_path=file_path,
        issue_type="metadata_classification",
        context={
            "is_metadata": is_metadata,
            "reasoning": meta_result.get("reasoning"),
            "columns": context.get("columns", []),
            "indicators": meta_result.get("indicators", {})
        },
        rule_based_confidence=confidence
    )
    
    if review_decision["needs_review"]:
        print(f"\nâš ï¸  [Low Confidence] Human Review ìš”ì²­")
        print(f"   Reason: {review_decision['reason']}")
        
        # êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„± (LLM) - [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
        specific_question = _generate_natural_human_question(
            file_path=file_path,
            context={
                "reasoning": meta_result.get("reasoning"),
                "message": f"Confidence {confidence:.1%}",
                "columns": context.get("columns", [])
            },
            issue_type="metadata_uncertain",
            conversation_history=conversation_history  # [NEW] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
        )
        
        print("="*80)
        
        return {
            "needs_human_review": True,
            "review_type": "classification",  # [NEW] ë¦¬ë·° íƒ€ì… ëª…ì‹œ
            "human_question": specific_question,
            "ontology_context": ontology,  # í˜„ì¬ ìƒíƒœ ìœ ì§€
            "conversation_history": conversation_history,  # [NEW] íˆìŠ¤í† ë¦¬ ì „ë‹¬
            "logs": [f"âš ï¸ [Ontology] ë©”íƒ€ë°ì´í„° íŒë‹¨ ë¶ˆí™•ì‹¤ ({confidence:.2%}). {review_decision['reason']}"]
        }
    
    # === Step 4: Branching (í™•ì‹ ë„ ë†’ìŒ) ===
    
    # [Branch A] ë©”íƒ€ë°ì´í„° íŒŒì¼
    if is_metadata:
        print(f"\nğŸ“– [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        # íŒŒì¼ íƒœê·¸ ì €ì¥
        ontology["file_tags"][file_path] = {
            "type": "metadata",
            "role": "dictionary",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat()
        }
        
        # ë‚´ìš© íŒŒì‹± (Rule)
        print(f"   - ë©”íƒ€ë°ì´í„° íŒŒì‹± ì¤‘...")
        new_definitions = _parse_metadata_content(file_path)
        ontology["definitions"].update(new_definitions)
        
        print(f"   - ìš©ì–´ {len(new_definitions)}ê°œ ì¶”ê°€")
        print(f"   - ì´ ìš©ì–´: {len(ontology['definitions'])}ê°œ")
        
        # ì˜¨í†¨ë¡œì§€ ì €ì¥ (ì˜êµ¬ ë³´ì¡´)
        print(f"   - ì˜¨í†¨ë¡œì§€ ì €ì¥ ì¤‘...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": True,  # ì¤‘ìš”! ë©”íƒ€ë°ì´í„°ëŠ” ì¸ë±ì‹± ìŠ¤í‚µ
            "logs": [f"ğŸ“š [Ontology] ë©”íƒ€ë°ì´í„° ë“±ë¡: {len(new_definitions)}ê°œ ìš©ì–´ ì¶”ê°€ (ì €ì¥ ì™„ë£Œ)"]
        }
    
    # [Branch B] ì¼ë°˜ ë°ì´í„° íŒŒì¼
    else:
        print(f"\nğŸ“Š [Data] ì¼ë°˜ ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        # ì»¬ëŸ¼ ì •ë³´ ì €ì¥ (ê´€ê³„ ì¶”ë¡ ì— í•„ìš”)
        columns = metadata.get("columns", [])
        
        ontology["file_tags"][file_path] = {
            "type": "transactional_data",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat(),
            "columns": columns  # [NEW] ì»¬ëŸ¼ ì €ì¥
        }
        
        # Note: Column MetadataëŠ” index_data_nodeì—ì„œ finalized_schema í™•ì • í›„ ì €ì¥ë¨
        
        # === Phase 2: ê´€ê³„ ì¶”ë¡  (ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆì„ ë•Œë§Œ) ===
        existing_data_files = [
            fp for fp, tag in ontology.get("file_tags", {}).items()
            if tag.get("type") == "transactional_data" and fp != file_path
        ]
        
        if existing_data_files:
            print(f"\nğŸ”— [Relationship] ê´€ê³„ ì¶”ë¡  ì‹œì‘...")
            print(f"   - ê¸°ì¡´ ë°ì´í„° íŒŒì¼: {len(existing_data_files)}ê°œ")
            
            # ê´€ê³„ ì¶”ë¡  (LLM)
            table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_")
            
            relationship_result = _infer_relationships_with_llm(
                current_table_name=table_name,
                current_cols=columns,
                ontology_context=ontology,
                current_metadata=metadata
            )
            
            # ê´€ê³„ ì¶”ê°€
            new_relationships = relationship_result.get("relationships", [])
            if new_relationships:
                print(f"   - ê´€ê³„ {len(new_relationships)}ê°œ ë°œê²¬")
                
                # ê¸°ì¡´ ê´€ê³„ì™€ ë³‘í•©
                existing_rels = ontology.get("relationships", [])
                
                # ì¤‘ë³µ ì²´í¬
                existing_keys = {
                    (r["source_table"], r["target_table"], r["source_column"], r["target_column"])
                    for r in existing_rels
                }
                
                for new_rel in new_relationships:
                    key = (new_rel["source_table"], new_rel["target_table"], 
                           new_rel["source_column"], new_rel["target_column"])
                    if key not in existing_keys:
                        ontology["relationships"].append(new_rel)
                        print(f"      â€¢ {new_rel['source_table']}.{new_rel['source_column']} "
                              f"â†’ {new_rel['target_table']}.{new_rel['target_column']} "
                              f"({new_rel['relation_type']}, conf: {new_rel.get('confidence', 0):.2%})")
            
            # ê³„ì¸µ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì œê±° ê°•í™”)
            new_hierarchy = relationship_result.get("hierarchy", [])
            if new_hierarchy:
                print(f"   - ê³„ì¸µ ì •ë³´ ì—…ë°ì´íŠ¸")
                
                # ê¸°ì¡´ ê³„ì¸µ
                existing_hier = ontology.get("hierarchy", [])
                
                # ì¤‘ë³µ ì œê±° ì „ëµ: (level, anchor_column) ì¡°í•©ìœ¼ë¡œ íŒë‹¨
                merged_hierarchy = {}  # key: (level, anchor), value: hierarchy_dict
                
                # ê¸°ì¡´ ê³„ì¸µ ë¨¼ì € ì¶”ê°€
                for h in existing_hier:
                    key = (h.get("level"), h.get("anchor_column"))
                    merged_hierarchy[key] = h
                
                # ìƒˆ ê³„ì¸µ ë³‘í•© (confidence ë†’ì€ ê²ƒ ìš°ì„ )
                for new_h in new_hierarchy:
                    key = (new_h.get("level"), new_h.get("anchor_column"))
                    
                    if key not in merged_hierarchy:
                        # ìƒˆë¡œìš´ (level, anchor) ì¡°í•©
                        merged_hierarchy[key] = new_h
                        print(f"      â€¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [NEW]")
                    else:
                        # ì´ë¯¸ ìˆëŠ” ì¡°í•© - confidence ë¹„êµ
                        existing_conf = merged_hierarchy[key].get("confidence", 0)
                        new_conf = new_h.get("confidence", 0)
                        
                        if new_conf > existing_conf:
                            merged_hierarchy[key] = new_h
                            print(f"      â€¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [UPDATED, conf: {new_conf:.2%}]")
                        else:
                            print(f"      â€¢ L{new_h['level']}: (ì¤‘ë³µ ìŠ¤í‚µ, ê¸°ì¡´ confidence {existing_conf:.2%} ìœ ì§€)")
                
                # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë ˆë²¨ ì •ë ¬
                ontology["hierarchy"] = sorted(merged_hierarchy.values(), key=lambda x: x.get("level", 99))
        else:
            print(f"\n   - ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì—†ìŒ. ê´€ê³„ ì¶”ë¡  ìŠ¤í‚µ.")
        
        # ì˜¨í†¨ë¡œì§€ ì €ì¥
        print(f"   - ì˜¨í†¨ë¡œì§€ ì €ì¥ ì¤‘...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": False,  # ì¼ë°˜ ë°ì´í„°ëŠ” ì¸ë±ì‹± ê³„ì†
            "logs": ["ğŸ” [Ontology] ì¼ë°˜ ë°ì´í„° í™•ì¸. ê´€ê³„ ì¶”ë¡  ì™„ë£Œ."]
        }


# =============================================================================
# 2-Phase Workflow Nodes (NEW)
# =============================================================================

def batch_classifier_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1] ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ë…¸ë“œ
    
    ëª¨ë“  ì…ë ¥ íŒŒì¼ì„ í•œ ë²ˆì— ë¶„ë¥˜í•˜ì—¬ ë©”íƒ€ë°ì´í„°/ë°ì´í„°ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
    ë¶ˆí™•ì‹¤í•œ íŒŒì¼ì€ classification_review_nodeë¡œ ë³´ëƒ…ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸ“‹ [BATCH CLASSIFIER] Phase 1 ì‹œì‘ - ì „ì²´ íŒŒì¼ ë¶„ë¥˜")
    print("="*80)
    
    input_files = state.get("input_files", [])
    
    if not input_files:
        return {
            "logs": ["âŒ Error: ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."],
            "error_message": "No input files provided"
        }
    
    print(f"   ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼: {len(input_files)}ê°œ")
    
    classifications: Dict[str, FileClassification] = {}
    metadata_files: List[str] = []
    data_files: List[str] = []
    uncertain_files: List[str] = []
    
    # ê° íŒŒì¼ì— ëŒ€í•´ ë¶„ë¥˜ ìˆ˜í–‰
    for idx, file_path in enumerate(input_files):
        filename = os.path.basename(file_path)
        print(f"\n   [{idx+1}/{len(input_files)}] {filename}")
        
        try:
            # 1. Processorë¡œ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            selected_processor = next((p for p in processors if p.can_handle(file_path)), None)
            
            if not selected_processor:
                print(f"      âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
                classifications[file_path] = {
                    "file_path": file_path,
                    "filename": filename,
                    "classification": "unknown",
                    "confidence": 0.0,
                    "reasoning": "Unsupported file format",
                    "indicators": {},
                    "needs_review": True,
                    "human_confirmed": False
                }
                uncertain_files.append(file_path)
                continue
            
            raw_metadata = selected_processor.extract_metadata(file_path)
            
            # 2. ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• (Rule)
            context = _build_metadata_detection_context(file_path, raw_metadata)
            
            # 3. LLMìœ¼ë¡œ ë¶„ë¥˜ íŒë‹¨
            meta_result = _ask_llm_is_metadata(context)
            
            confidence = meta_result.get("confidence", 0.0)
            is_metadata = meta_result.get("is_metadata", False)
            reasoning = meta_result.get("reasoning", "")
            indicators = meta_result.get("indicators", {})
            
            # 4. ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
            classification_type = "metadata" if is_metadata else "data"
            needs_review = confidence < HumanReviewConfig.CLASSIFICATION_CONFIDENCE_THRESHOLD
            
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": classification_type,
                "confidence": confidence,
                "reasoning": reasoning,
                "indicators": indicators,
                "needs_review": needs_review,
                "human_confirmed": False
            }
            
            # 5. ë¶„ë¥˜ë³„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if needs_review:
                uncertain_files.append(file_path)
                print(f"      âš ï¸ ë¶ˆí™•ì‹¤: {classification_type} ({confidence:.1%})")
            elif is_metadata:
                metadata_files.append(file_path)
                print(f"      ğŸ“– ë©”íƒ€ë°ì´í„° ({confidence:.1%})")
            else:
                data_files.append(file_path)
                print(f"      ğŸ“Š ë°ì´í„° ({confidence:.1%})")
                
        except Exception as e:
            print(f"      âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            classifications[file_path] = {
                "file_path": file_path,
                "filename": filename,
                "classification": "unknown",
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
                "indicators": {},
                "needs_review": True,
                "human_confirmed": False
            }
            uncertain_files.append(file_path)
    
    # ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
    classification_result: ClassificationResult = {
        "total_files": len(input_files),
        "metadata_files": metadata_files,
        "data_files": data_files,
        "uncertain_files": uncertain_files,
        "classifications": classifications
    }
    
    # ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
    processing_progress: ProcessingProgress = {
        "phase": "classification",
        "metadata_processed": [],
        "data_processed": [],
        "current_file": None,
        "current_file_index": 0,
        "total_files": len(input_files)
    }
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ:")
    print(f"   - ë©”íƒ€ë°ì´í„°: {len(metadata_files)}ê°œ (í™•ì •)")
    print(f"   - ë°ì´í„°: {len(data_files)}ê°œ (í™•ì •)")
    print(f"   - ë¶ˆí™•ì‹¤: {len(uncertain_files)}ê°œ (ë¦¬ë·° í•„ìš”)")
    print("="*80)
    
    return {
        "classification_result": classification_result,
        "processing_progress": processing_progress,
        "logs": [
            f"ğŸ“‹ [Phase1] ë¶„ë¥˜ ì™„ë£Œ: ë©”íƒ€ë°ì´í„° {len(metadata_files)}ê°œ, "
            f"ë°ì´í„° {len(data_files)}ê°œ, ë¶ˆí™•ì‹¤ {len(uncertain_files)}ê°œ"
        ]
    }


def classification_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 1-2] ë¶„ë¥˜ í™•ì¸ ë…¸ë“œ (Human-in-the-Loop)
    
    ë¶ˆí™•ì‹¤í•œ íŒŒì¼ë“¤ì— ëŒ€í•´ ì‚¬ìš©ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸ§‘ [CLASSIFICATION REVIEW] Human-in-the-Loop")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    uncertain_files = classification_result.get("uncertain_files", [])
    classifications = classification_result.get("classifications", {})
    human_feedback = state.get("human_feedback")
    
    # í”¼ë“œë°± ì²˜ë¦¬ (ì¬ì§„ì…)
    if human_feedback:
        print(f"   ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì‹ : '{human_feedback}'")
        
        # í”¼ë“œë°± íŒŒì‹±
        updated_classifications = _parse_classification_feedback(
            feedback=human_feedback,
            classifications=classifications,
            uncertain_files=uncertain_files
        )
        
        # ë¶„ë¥˜ ê²°ê³¼ ì—…ë°ì´íŠ¸
        new_metadata_files = []
        new_data_files = []
        remaining_uncertain = []
        
        for file_path, clf in updated_classifications.items():
            if clf.get("human_confirmed"):
                if clf["classification"] == "metadata":
                    new_metadata_files.append(file_path)
                elif clf["classification"] == "data":
                    new_data_files.append(file_path)
                else:
                    remaining_uncertain.append(file_path)
            elif clf["needs_review"]:
                remaining_uncertain.append(file_path)
            elif clf["classification"] == "metadata":
                new_metadata_files.append(file_path)
            else:
                new_data_files.append(file_path)
        
        # ê¸°ì¡´ í™•ì • íŒŒì¼ + ìƒˆë¡œ í™•ì •ëœ íŒŒì¼
        all_metadata = classification_result.get("metadata_files", []) + [
            f for f in new_metadata_files if f not in classification_result.get("metadata_files", [])
        ]
        all_data = classification_result.get("data_files", []) + [
            f for f in new_data_files if f not in classification_result.get("data_files", [])
        ]
        
        updated_result: ClassificationResult = {
            "total_files": classification_result["total_files"],
            "metadata_files": all_metadata,
            "data_files": all_data,
            "uncertain_files": remaining_uncertain,
            "classifications": updated_classifications
        }
        
        print(f"   âœ… ë¶„ë¥˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        print(f"      - ë©”íƒ€ë°ì´í„°: {len(all_metadata)}ê°œ")
        print(f"      - ë°ì´í„°: {len(all_data)}ê°œ")
        print(f"      - ë‚¨ì€ ë¶ˆí™•ì‹¤: {len(remaining_uncertain)}ê°œ")
        
        # ì•„ì§ ë¶ˆí™•ì‹¤í•œ íŒŒì¼ì´ ìˆìœ¼ë©´ ê³„ì† ì§ˆë¬¸
        if remaining_uncertain:
            question = _generate_classification_question(remaining_uncertain, updated_classifications)
            return {
                "classification_result": updated_result,
                "needs_human_review": True,
                "review_type": "classification",
                "human_question": question,
                "human_feedback": None,  # ë¦¬ì…‹
                "logs": [f"ğŸ”„ [Review] ì¶”ê°€ í™•ì¸ í•„ìš”: {len(remaining_uncertain)}ê°œ íŒŒì¼"]
            }
        
        # ëª¨ë‘ í™•ì •ë¨
        progress = state.get("processing_progress", {})
        progress["phase"] = "classification_review"
        
        print("="*80)
        
        return {
            "classification_result": updated_result,
            "processing_progress": progress,
            "needs_human_review": False,
            "human_feedback": None,
            "logs": [f"âœ… [Review] ë¶„ë¥˜ í™•ì • ì™„ë£Œ"]
        }
    
    # ì²« ì§„ì…: ì§ˆë¬¸ ìƒì„±
    if not uncertain_files:
        print("   âœ… ë¶ˆí™•ì‹¤í•œ íŒŒì¼ ì—†ìŒ - ë¦¬ë·° ìŠ¤í‚µ")
        return {
            "needs_human_review": False,
            "logs": ["âœ… [Review] ëª¨ë“  íŒŒì¼ ë¶„ë¥˜ í™•ì •"]
        }
    
    # ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ìƒì„±
    question = _generate_classification_question(uncertain_files, classifications)
    
    print(f"   â“ {len(uncertain_files)}ê°œ íŒŒì¼ì— ëŒ€í•´ ì‚¬ìš©ì í™•ì¸ ìš”ì²­")
    print("="*80)
    
    return {
        "needs_human_review": True,
        "review_type": "classification",
        "human_question": question,
        "logs": [f"â“ [Review] {len(uncertain_files)}ê°œ íŒŒì¼ ë¶„ë¥˜ í™•ì¸ ìš”ì²­"]
    }


def _generate_classification_question(uncertain_files: List[str], classifications: Dict[str, FileClassification]) -> str:
    """ë¶ˆí™•ì‹¤í•œ íŒŒì¼ë“¤ì— ëŒ€í•œ ì§ˆë¬¸ ìƒì„±"""
    
    question_parts = [
        "ğŸ“‹ **íŒŒì¼ ë¶„ë¥˜ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤**\n",
        "ì•„ë˜ íŒŒì¼ë“¤ì˜ ë¶„ë¥˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\n"
    ]
    
    for idx, file_path in enumerate(uncertain_files[:5], 1):  # ìµœëŒ€ 5ê°œì”©
        clf = classifications.get(file_path, {})
        filename = clf.get("filename", os.path.basename(file_path))
        predicted = clf.get("classification", "unknown")
        confidence = clf.get("confidence", 0.0)
        reasoning = clf.get("reasoning", "")[:100]
        
        pred_emoji = "ğŸ“–" if predicted == "metadata" else "ğŸ“Š" if predicted == "data" else "â“"
        pred_text = "ë©”íƒ€ë°ì´í„°" if predicted == "metadata" else "ë°ì´í„°" if predicted == "data" else "ì•Œ ìˆ˜ ì—†ìŒ"
        
        question_parts.append(
            f"\n**{idx}. {filename}**\n"
            f"   - AI ì˜ˆì¸¡: {pred_emoji} {pred_text} (í™•ì‹ ë„: {confidence:.0%})\n"
            f"   - íŒë‹¨ ê·¼ê±°: {reasoning}...\n"
        )
    
    if len(uncertain_files) > 5:
        question_parts.append(f"\n... ì™¸ {len(uncertain_files) - 5}ê°œ íŒŒì¼\n")
    
    question_parts.append(
        "\n**ì‘ë‹µ ë°©ë²•:**\n"
        "- ëª¨ë‘ ë§ìœ¼ë©´: `í™•ì¸` ë˜ëŠ” `ok`\n"
        "- ìˆ˜ì •ì´ í•„ìš”í•˜ë©´: `1:ë°ì´í„°, 2:ë©”íƒ€ë°ì´í„°` í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ì™€ ë¶„ë¥˜ë¥¼ ì…ë ¥\n"
        "- íŒŒì¼ ì œì™¸: `1:ì œì™¸` ë˜ëŠ” `1:skip`\n"
    )
    
    return "".join(question_parts)


def _parse_classification_feedback(
    feedback: str, 
    classifications: Dict[str, FileClassification],
    uncertain_files: List[str]
) -> Dict[str, FileClassification]:
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ íŒŒì‹±í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
    
    updated = classifications.copy()
    feedback_lower = feedback.lower().strip()
    
    # "í™•ì¸" ë˜ëŠ” "ok" - ëª¨ë“  ì˜ˆì¸¡ ìŠ¹ì¸
    if feedback_lower in ["í™•ì¸", "ok", "yes", "y", "approve", "ìŠ¹ì¸"]:
        for file_path in uncertain_files:
            if file_path in updated:
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
        return updated
    
    # ê°œë³„ ìˆ˜ì •: "1:ë°ì´í„°, 2:ë©”íƒ€ë°ì´í„°" í˜•ì‹
    import re
    corrections = re.findall(r'(\d+)\s*[:ï¼š]\s*(ë©”íƒ€ë°ì´í„°|ë°ì´í„°|metadata|data|ì œì™¸|skip)', feedback_lower)
    
    for idx_str, new_type in corrections:
        idx = int(idx_str) - 1  # 1-based to 0-based
        
        if 0 <= idx < len(uncertain_files):
            file_path = uncertain_files[idx]
            
            if new_type in ["ì œì™¸", "skip"]:
                # íŒŒì¼ ì œì™¸ (unknownìœ¼ë¡œ ë³€ê²½, ì²˜ë¦¬ ëŒ€ìƒì—ì„œ ì œì™¸ë¨)
                updated[file_path]["classification"] = "unknown"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë©”íƒ€ë°ì´í„°", "metadata"]:
                updated[file_path]["classification"] = "metadata"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
            elif new_type in ["ë°ì´í„°", "data"]:
                updated[file_path]["classification"] = "data"
                updated[file_path]["human_confirmed"] = True
                updated[file_path]["needs_review"] = False
    
    return updated


def process_metadata_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-1] ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ë…¸ë“œ
    
    ë¶„ë¥˜ëœ ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì„ ë¨¼ì € ì²˜ë¦¬í•˜ì—¬ ì˜¨í†¨ë¡œì§€ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸ“– [METADATA PROCESSOR] Phase 2-1 - ë©”íƒ€ë°ì´í„° ì¼ê´„ ì²˜ë¦¬")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    metadata_files = classification_result.get("metadata_files", [])
    progress = state.get("processing_progress", {})
    
    # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
    ontology = state.get("ontology_context")
    if not ontology or not ontology.get("definitions"):
        ontology = ontology_manager.load() or {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        }
    
    if not metadata_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "metadata_processing"
        progress["metadata_processed"] = []
        
        return {
            "ontology_context": ontology,
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ - ìŠ¤í‚µ"]
        }
    
    print(f"   ğŸ“‚ ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
    
    processed_metadata = []
    total_definitions = 0
    
    for idx, file_path in enumerate(metadata_files):
        filename = os.path.basename(file_path)
        print(f"\n   [{idx+1}/{len(metadata_files)}] {filename}")
        
        try:
            # íŒŒì¼ íƒœê·¸ ì €ì¥
            ontology["file_tags"][file_path] = {
                "type": "metadata",
                "role": "dictionary",
                "confidence": classification_result["classifications"].get(file_path, {}).get("confidence", 0.8),
                "detected_at": datetime.now().isoformat()
            }
            
            # ë©”íƒ€ë°ì´í„° íŒŒì‹±
            new_definitions = _parse_metadata_content(file_path)
            ontology["definitions"].update(new_definitions)
            
            total_definitions += len(new_definitions)
            processed_metadata.append(file_path)
            
            print(f"      âœ… ìš©ì–´ {len(new_definitions)}ê°œ ì¶”ê°€")
            
        except Exception as e:
            print(f"      âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ì˜¨í†¨ë¡œì§€ ì €ì¥
    ontology_manager.save(ontology)
    
    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    progress["phase"] = "metadata_processing"
    progress["metadata_processed"] = processed_metadata
    
    print(f"\n" + "-"*40)
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {len(processed_metadata)}ê°œ")
    print(f"   - ì¶”ê°€ëœ ìš©ì–´: {total_definitions}ê°œ")
    print(f"   - ì´ ìš©ì–´ ìˆ˜: {len(ontology.get('definitions', {}))}ê°œ")
    print("="*80)
    
    return {
        "ontology_context": ontology,
        "processing_progress": progress,
        "logs": [f"ğŸ“– [Metadata] {len(processed_metadata)}ê°œ íŒŒì¼ ì²˜ë¦¬, {total_definitions}ê°œ ìš©ì–´ ì¶”ê°€"]
    }


def process_data_batch_node(state: AgentState) -> Dict[str, Any]:
    """
    [Phase 2-2] ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ì¤€ë¹„ ë…¸ë“œ
    
    ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ê³ , ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
    (ê° ë°ì´í„° íŒŒì¼ì€ ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°(analyzer â†’ human_review â†’ indexer)ë¥¼ ë”°ë¦„)
    """
    print("\n" + "="*80)
    print("ğŸ“Š [DATA PROCESSOR] Phase 2-2 - ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("="*80)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    if not data_files:
        print("   â„¹ï¸ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        progress["phase"] = "complete"
        
        return {
            "processing_progress": progress,
            "logs": ["â„¹ï¸ [Data] ë°ì´í„° íŒŒì¼ ì—†ìŒ - ì™„ë£Œ"]
        }
    
    print(f"   ğŸ“‚ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    print(f"   ğŸš€ ì²« ë²ˆì§¸ íŒŒì¼ë¶€í„° ì²˜ë¦¬ ì‹œì‘")
    
    # ì²« ë²ˆì§¸ íŒŒì¼ ì„¤ì •
    first_file = data_files[0]
    
    progress["phase"] = "data_processing"
    progress["current_file"] = first_file
    progress["current_file_index"] = 0
    progress["total_files"] = len(data_files)
    
    print(f"\n   â†’ ì²˜ë¦¬ íŒŒì¼: {os.path.basename(first_file)}")
    print("="*80)
    
    # ì²« íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë‹¤ìŒ ë…¸ë“œì—ì„œ ì‚¬ìš©)
    return {
        "file_path": first_file,
        "processing_progress": progress,
        "skip_indexing": False,
        "logs": [f"ğŸ“Š [Data] {len(data_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘"]
    }


def advance_to_next_file_node(state: AgentState) -> Dict[str, Any]:
    """
    [Helper] ë‹¤ìŒ ë°ì´í„° íŒŒì¼ë¡œ ì§„í–‰
    
    í˜„ì¬ íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    """
    print("\n" + "-"*40)
    print("â¡ï¸ [ADVANCE] ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™")
    print("-"*40)
    
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    current_idx = progress.get("current_file_index", 0)
    current_file = progress.get("current_file", "")
    
    # í˜„ì¬ íŒŒì¼ì„ ì²˜ë¦¬ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€
    if current_file and current_file not in progress.get("data_processed", []):
        if "data_processed" not in progress:
            progress["data_processed"] = []
        progress["data_processed"].append(current_file)
    
    # ë‹¤ìŒ ì¸ë±ìŠ¤
    next_idx = current_idx + 1
    
    if next_idx >= len(data_files):
        # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ
        print(f"   âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ({len(data_files)}ê°œ)")
        progress["phase"] = "complete"
        progress["current_file"] = None
        
        return {
            "processing_progress": progress,
            "logs": [f"âœ… [Complete] ëª¨ë“  ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ({len(data_files)}ê°œ)"]
        }
    
    # ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™
    next_file = data_files[next_idx]
    progress["current_file"] = next_file
    progress["current_file_index"] = next_idx
    
    print(f"   ğŸ“‚ ë‹¤ìŒ íŒŒì¼: [{next_idx + 1}/{len(data_files)}] {os.path.basename(next_file)}")
    
    # ìƒíƒœ ë¦¬ì…‹ (ìƒˆ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•´)
    return {
        "file_path": next_file,
        "processing_progress": progress,
        "raw_metadata": {},  # ë¦¬ì…‹
        "finalized_anchor": None,  # ë¦¬ì…‹
        "finalized_schema": [],  # ë¦¬ì…‹
        "needs_human_review": False,  # ë¦¬ì…‹
        "human_feedback": None,  # ë¦¬ì…‹
        "skip_indexing": False,  # ë¦¬ì…‹
        "retry_count": 0,  # ë¦¬ì…‹
        "logs": [f"â¡ï¸ [Advance] ë‹¤ìŒ íŒŒì¼: {os.path.basename(next_file)}"]
    }


# =============================================================================
# Routing Functions for 2-Phase Workflow
# =============================================================================

def check_classification_needs_review(state: AgentState) -> str:
    """ë¶„ë¥˜ ê²°ê³¼ ì¤‘ ë¶ˆí™•ì‹¤í•œ ê²ƒì´ ìˆëŠ”ì§€ í™•ì¸"""
    classification_result = state.get("classification_result", {})
    uncertain_files = classification_result.get("uncertain_files", [])
    
    if uncertain_files:
        return "needs_review"
    return "all_confident"


def check_has_more_files(state: AgentState) -> str:
    """ë” ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸"""
    classification_result = state.get("classification_result", {})
    data_files = classification_result.get("data_files", [])
    progress = state.get("processing_progress", {})
    
    current_idx = progress.get("current_file_index", 0)
    
    # ì•„ì§ ì²˜ë¦¬í•  íŒŒì¼ì´ ë‚¨ì•„ìˆìœ¼ë©´
    if current_idx + 1 < len(data_files):
        return "has_more"
    return "all_done"


def check_data_needs_review(state: AgentState) -> str:
    """ë°ì´í„° ë¶„ì„ í›„ Human Review í•„ìš” ì—¬ë¶€ í™•ì¸"""
    
    # ê¸°ì¡´ check_confidence ë¡œì§ í™œìš©
    needs_human = state.get("needs_human_review", False)
    finalized_anchor = state.get("finalized_anchor", {})
    anchor_status = finalized_anchor.get("status") if finalized_anchor else None
    
    # Anchorê°€ í™•ì •ëœ ê²½ìš°
    if anchor_status in ["CONFIRMED", "INDIRECT_LINK"]:
        return "approved"
    
    # Processorê°€ í™•ì¸ ìš”ì²­
    if state.get("raw_metadata", {}).get("anchor_info", {}).get("needs_human_confirmation"):
        return "review_required"
    
    # needs_human_review í”Œë˜ê·¸
    if needs_human:
        return "review_required"
    
    return "approved"