import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

from src.agents.state import AgentState, ColumnSchema, AnchorInfo, ProjectContext, OntologyContext
from src.processors.tabular import TabularProcessor
from src.processors.signal import SignalProcessor
from src.utils.llm_client import get_llm_client
from src.utils.ontology_manager import get_ontology_manager
from src.utils.llm_cache import get_llm_cache

# --- ì „ì—­ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ---
llm_client = get_llm_client()
ontology_manager = get_ontology_manager()
llm_cache = get_llm_cache()  # ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
processors = [
    TabularProcessor(llm_client),
    SignalProcessor(llm_client)
]



def load_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 1] íŒŒì¼ ë¡œë“œ ë° ê¸°ì´ˆ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    """
    file_path = state["file_path"]
    
    print("\n" + "="*80)
    print(f"ğŸ“‚ [LOADER NODE] ì‹œì‘ - {os.path.basename(file_path)}")
    print("="*80)
    
    # 1. ì ì ˆí•œ Processor ì°¾ê¸°
    selected_processor = next((p for p in processors if p.can_handle(file_path)), None)
    
    if not selected_processor:
        return {
            "logs": [f"âŒ Error: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤ ({file_path})"],
            "needs_human_review": True,
            "human_question": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ì²˜ë¦¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
        }

    # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì—¬ê¸°ì„œ Anchor íƒì§€ë„ ìˆ˜í–‰ë¨)
    try:
        raw_metadata = selected_processor.extract_metadata(file_path)
        processor_type = raw_metadata.get("processor_type", "unknown")
        
        # Processorê°€ Anchorë¥¼ ëª» ì°¾ì•˜ê±°ë‚˜ ëª¨í˜¸í•˜ë‹¤ê³  íŒë‹¨í–ˆëŠ”ì§€ í™•ì¸
        anchor_info = raw_metadata.get("anchor_info", {})
        anchor_status = anchor_info.get("status", "MISSING")
        anchor_msg = anchor_info.get("msg", "")

        log_message = f"âœ… [Loader] {processor_type.upper()} ë¶„ì„ ì™„ë£Œ. Anchor Status: {anchor_status}"

        print(f"\nâœ… [LOADER NODE] ì™„ë£Œ")
        print(f"   - Processor: {processor_type}")
        print(f"   - Columns: {len(raw_metadata.get('columns', []))}ê°œ")
        print(f"   - Anchor Status: {anchor_status}")
        print("="*80)

        return {
            "file_type": processor_type,
            "raw_metadata": raw_metadata,
            "logs": [log_message]
        }
    except Exception as e:
        print(f"\nâŒ [LOADER NODE] ì—ëŸ¬: {str(e)}")
        print("="*80)
        return {
            "logs": [f"âŒ [Loader] ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
            "error_message": str(e)
        }


def analyze_semantics_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 2] ì˜ë¯¸ë¡ ì  ë¶„ì„ (Semantic Reasoning)
    Processorì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ìŠ¤í‚¤ë§ˆë¥¼ í™•ì •ì§“ëŠ” í•µì‹¬ ë‘ë‡Œ
    [NEW] Global Context(Project Level)ë¥¼ ì°¸ì¡°í•˜ì—¬ íŒŒì¼ ê°„ Anchor í†µì¼ì„±ì„ ë³´ì¥í•¨.
    """
    print("\n" + "="*80)
    print("ğŸ§  [ANALYZER NODE] ì‹œì‘ - ì˜ë¯¸ë¡ ì  ë¶„ì„")
    print("="*80)
    
    metadata = state["raw_metadata"]
    local_anchor_info = metadata.get("anchor_info", {})
    human_feedback = state.get("human_feedback")
    
    # Global Context ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì´ˆê¸°ê°’)
    project_context = state.get("project_context", {
        "master_anchor_name": None, 
        "known_aliases": [], 
        "example_id_values": []
    })
    
    finalized_anchor = state.get("finalized_anchor")
    retry_count = state.get("retry_count", 0)
    
    # ë¬´í•œ ë£¨í”„ ë°©ì§€: ì¬ì‹œë„ê°€ 3ë²ˆ ì´ìƒì´ë©´ ê°•ì œë¡œ ì²˜ë¦¬
    if retry_count >= 3:
        log_msg = f"âš ï¸ [Analyzer] ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({retry_count}íšŒ). ë¡œì»¬ Anchorë¥¼ ê°•ì œ ì‚¬ìš©í•©ë‹ˆë‹¤."
        
        # ë¡œì»¬ì—ì„œ ì°¾ì€ Anchorë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": local_anchor_info.get("target_column", "unknown"),
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": f"Forced confirmation after {retry_count} retries",
            "mapped_to_master": project_context.get("master_anchor_name")
        }
        
        # ìŠ¤í‚¤ë§ˆ ë¶„ì„ ê±´ë„ˆë›°ê³  ì™„ë£Œ
        return {
            "finalized_anchor": finalized_anchor,
            "finalized_schema": [],
            "project_context": project_context,
            "needs_human_review": False,
            "human_feedback": None,
            "retry_count": retry_count,
            "logs": [log_msg, "âš ï¸ [Analyzer] ìŠ¤í‚¤ë§ˆ ë¶„ì„ ê±´ë„ˆëœ€ (ì¬ì‹œë„ ì´ˆê³¼)"]
        }

    # --- Scenario A: ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ (ì¬ì§„ì…) ---
    if human_feedback:
        log_msg = f"ğŸ—£ï¸ [Feedback] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì‹ : '{human_feedback}'"
        
        # í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ Anchor ê°•ì œ í™•ì •
        finalized_anchor = {
            "status": "CONFIRMED",
            "column_name": human_feedback.strip(),
            "is_time_series": local_anchor_info.get("is_time_series", False),
            "reasoning": "User manually confirmed.",
            "mapped_to_master": project_context.get("master_anchor_name") 
        }
        
        # â­ [FIX] í”¼ë“œë°± ì²˜ë¦¬ í›„ needs_human_confirmation ë¦¬ì…‹
        # check_confidenceì—ì„œ ë‹¤ì‹œ review_requiredë¡œ ë¹ ì§€ëŠ” ê²ƒì„ ë°©ì§€
        if "anchor_info" in metadata:
            metadata["anchor_info"]["needs_human_confirmation"] = False
            metadata["anchor_info"]["status"] = "CONFIRMED"
        
        # í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ê°„ì£¼í•˜ê³  ì§„í–‰ (ë¦¬í„´í•˜ì§€ ì•ŠìŒ)
    
    # --- Scenario B: Anchorê°€ ì•„ì§ ë¯¸í™•ì • ìƒíƒœì¼ ë•Œ -> Global Context í™•ì¸ ---
    if not finalized_anchor:
        
        # [NEW] Case 1: í”„ë¡œì íŠ¸ì— ì´ë¯¸ í•©ì˜ëœ Anchor(Leader)ê°€ ìˆëŠ” ê²½ìš°
        if project_context.get("master_anchor_name"):
            master_name = project_context["master_anchor_name"]
            
            # LLMì—ê²Œ ë¹„êµ ìš”ì²­ (Global Context vs Local Data)
            comparison = _compare_with_global_context(
                local_metadata=metadata,
                local_anchor_info=local_anchor_info,
                project_context=project_context
            )
            
            # ë””ë²„ê¹…: ë¹„êµ ê²°ê³¼ ë¡œê·¸
            comparison_status = comparison.get("status", "UNKNOWN")
            comparison_msg = comparison.get("message", "")
            print(f"\n[DEBUG] Global Anchor ë¹„êµ ê²°ê³¼: {comparison_status}")
            print(f"[DEBUG] ë©”ì‹œì§€: {comparison_msg}")
            print(f"[DEBUG] Target Column: {comparison.get('target_column', 'N/A')}")
            
            if comparison["status"] == "MATCH":
                # ë§¤ì¹­ ì„±ê³µ -> ìë™ í™•ì •
                target_col = comparison["target_column"]
                finalized_anchor = {
                    "status": "CONFIRMED",
                    "column_name": target_col,
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": f"Matched with global master anchor '{master_name}'",
                    "mapped_to_master": master_name
                }
                state["logs"].append(f"ğŸ”— [Anchor Link] Global Anchor '{master_name}'ì™€ ë§¤ì¹­ ì„±ê³µ (Local: '{target_col}')")
            
            elif comparison["status"] == "INDIRECT_LINK":
                # â­ [NEW] ê°„ì ‘ ì—°ê²° ì„±ê³µ -> ìë™ í™•ì • (ì‚¬ëŒ ê°œì… ë¶ˆí•„ìš”!)
                via_col = comparison["target_column"]
                via_table = comparison.get("via_table", "unknown")
                
                finalized_anchor = {
                    "status": "INDIRECT_LINK",
                    "column_name": via_col,  # ì—°ê²° ì»¬ëŸ¼ (ì˜ˆ: caseid)
                    "is_time_series": local_anchor_info.get("is_time_series", False),
                    "reasoning": comparison.get("message"),
                    "mapped_to_master": master_name,
                    "via_table": via_table,
                    "link_type": "indirect"  # FKë¥¼ í†µí•œ ê°„ì ‘ ì—°ê²°
                }
                
                print(f"\nâœ… [INDIRECT_LINK] ê°„ì ‘ ì—°ê²° ìë™ í™•ì •!")
                print(f"   - ì—°ê²° ì»¬ëŸ¼: {via_col}")
                print(f"   - ê²½ìœ  í…Œì´ë¸”: {via_table}")
                print(f"   - Master Anchor: {master_name}")
                
                state["logs"].append(
                    f"ğŸ”— [Indirect Link] '{via_col}'ì„ í†µí•´ '{via_table}'ì˜ '{master_name}'ì™€ ê°„ì ‘ ì—°ê²°ë¨"
                )
                
            else:
                # ì¶©ëŒí•˜ê±°ë‚˜(CONFLICT) ëª» ì°¾ìŒ(MISSING) -> ì‚¬ëŒ ê°œì…
                msg = comparison.get("message", "Anchor ë¶ˆì¼ì¹˜ ë°œìƒ")
                return {
                    "needs_human_review": True,
                    "human_question": f"{msg}\n(í”„ë¡œì íŠ¸ í‘œì¤€ Anchor: '{master_name}')\n\në¡œì»¬ í›„ë³´: {local_anchor_info.get('target_column', 'N/A')}",
                    "retry_count": retry_count,  # í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜ ìœ ì§€
                    "logs": [f"âš ï¸ [Analyzer] Global Anchorì™€ ë¶ˆì¼ì¹˜ (Status: {comparison_status}). ì¬ì‹œë„: {retry_count}/3"]
                }

        # [NEW] Case 2: ì´ê²ƒì´ ì²« ë²ˆì§¸ íŒŒì¼ì¸ ê²½ìš° (Global Context ì—†ìŒ)
        else:
            # ê¸°ì¡´ ë¡œì§: Local Anchor ì •ë³´ë§Œìœ¼ë¡œ íŒë‹¨
            if local_anchor_info.get("needs_human_confirmation"):
                question = (
                    f"ë°ì´í„° ë¶„ì„ ê²°ê³¼, í™˜ì ì‹ë³„ì(ID)ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
                    f"AI ì˜ê²¬: {local_anchor_info.get('msg')}\n"
                    f"ì–´ë–¤ ì»¬ëŸ¼ì´ í™˜ì ID ì¸ê°€ìš”? (ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”)"
                )
                return {
                    "needs_human_review": True,
                    "human_question": question,
                    "logs": ["âš ï¸ [Analyzer] Anchor ë¶ˆí™•ì‹¤ (ì²« íŒŒì¼). ì‚¬ìš©ì ì§ˆì˜ ìƒì„±."]
                }
            
            # í™•ì‹ í•˜ëŠ” ê²½ìš° -> í™•ì •
            finalized_anchor = {
                "status": "CONFIRMED",
                "column_name": local_anchor_info.get("target_column"),
                "is_time_series": local_anchor_info.get("is_time_series"),
                "reasoning": local_anchor_info.get("reasoning"),
                "mapped_to_master": None # ìì‹ ì´ ë§ˆìŠ¤í„°ê°€ ë  ì˜ˆì •
            }

    # --- 3. Global Context ì—…ë°ì´íŠ¸ (First-Come Leader Strategy) ---
    # Anchorê°€ í™•ì •ë˜ì—ˆê³ , ì•„ì§ ë§ˆìŠ¤í„°ê°€ ì—†ë‹¤ë©´ ì´ íŒŒì¼ì˜ Anchorê°€ ë§ˆìŠ¤í„°ê°€ ë¨
    if finalized_anchor and not project_context.get("master_anchor_name"):
        project_context["master_anchor_name"] = finalized_anchor["column_name"]
        project_context["known_aliases"].append(finalized_anchor["column_name"])
        state["logs"].append(f"ğŸ‘‘ [Project Context] ìƒˆë¡œìš´ Master Anchor ì„¤ì •: '{finalized_anchor['column_name']}'")

    # --- 4. ìŠ¤í‚¤ë§ˆ ìƒì„¸ ë¶„ì„ (ê³µí†µ) ---
    schema_analysis = _analyze_columns_with_llm(
        columns=metadata.get("columns", []),
        sample_data=metadata.get("column_details", {}),
        anchor_context=finalized_anchor
    )

    print(f"\nâœ… [ANALYZER NODE] ì™„ë£Œ")
    print(f"   - Anchor: {finalized_anchor.get('column_name', 'N/A')}")
    print(f"   - Mapped to Master: {finalized_anchor.get('mapped_to_master', 'N/A')}")
    print(f"   - Schema Columns: {len(schema_analysis)}ê°œ")
    print("="*80)

    result = {
        "finalized_anchor": finalized_anchor,
        "finalized_schema": schema_analysis,
        "project_context": project_context, # ì—…ë°ì´íŠ¸ëœ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
        "raw_metadata": metadata,  # â­ [FIX] ì—…ë°ì´íŠ¸ëœ raw_metadata ë°˜í™˜ (needs_human_confirmation ë¦¬ì…‹ë¨)
        "needs_human_review": False,
        "human_feedback": None, 
        "logs": ["ğŸ§  [Analyzer] ì „ì²´ ìŠ¤í‚¤ë§ˆ ë° ì˜¨í†¨ë¡œì§€ ë¶„ì„ ì™„ë£Œ."]
    }
    
    print(f"\n[DEBUG ANALYZER] ë¦¬í„´ê°’:")
    print(f"   - finalized_schema: {len(result['finalized_schema'])}ê°œ")
    print(f"   - needs_human_review: {result['needs_human_review']}")
    
    return result


def human_review_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 3] Human-in-the-loop ëŒ€ê¸° ë…¸ë“œ
    ì‹¤ì œ ì‹¤í–‰ ì‹œì—ëŠ” LangGraphì˜ interrupt ë©”ì»¤ë‹ˆì¦˜ì— ì˜í•´ ì—¬ê¸°ì„œ ë©ˆì¶”ê²Œ ë¨
    í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œ ë¬´í•œ ë£¨í”„ ë°©ì§€
    """
    print("\n" + "="*80)
    print("ğŸ›‘ [HUMAN REVIEW NODE] ì‹œì‘ - ì‚¬ìš©ì í™•ì¸ í•„ìš”")
    print("="*80)
    
    question = state.get("human_question", "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    retry_count = state.get("retry_count", 0)
    
    # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    new_retry_count = retry_count + 1
    
    print(f"\nâš ï¸  ì§ˆë¬¸: {question[:150]}...")
    print(f"ğŸ”„ ì¬ì‹œë„ íšŸìˆ˜: {new_retry_count}/3")
    print("="*80)
    
    return {
        "retry_count": new_retry_count,
        "logs": [f"ğŸ›‘ [Human Review] ëŒ€ê¸° ì¤‘ (ì¬ì‹œë„: {new_retry_count}/3). ì§ˆë¬¸: {question[:100]}..."]
    }


def index_data_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node 4 - Phase 3] PostgreSQL DB êµ¬ì¶• (ì˜¨í†¨ë¡œì§€ ê¸°ë°˜)
    
    ì „ë¬¸ê°€ í”¼ë“œë°± ë°˜ì˜:
    - Chunk Processing (ëŒ€ìš©ëŸ‰ ì•ˆì „ ì²˜ë¦¬)
    - FK ì œì•½ì¡°ê±´ ìë™ ìƒì„± (ALTER TABLE)
    - ì¸ë±ìŠ¤ ìë™ ìƒì„± (Level 1-2)
    """
    import pandas as pd
    import os
    
    from database.connection import get_db_manager
    from database.schema_generator import SchemaGenerator
    
    print("\n" + "="*80)
    print("ğŸ’¾ [INDEXER NODE] ì‹œì‘ - PostgreSQL DB êµ¬ì¶•")
    print("="*80)
    
    schema = state.get("finalized_schema", [])
    file_path = state["file_path"]
    ontology = state.get("ontology_context", {})
    
    # í…Œì´ë¸”ëª… ìƒì„±
    table_name = os.path.basename(file_path).replace(".csv", "_table").replace(".", "_").replace("-", "_")
    
    # DB ë§¤ë‹ˆì €
    db_manager = get_db_manager()
    
    try:
        # === 1. ë°ì´í„° ì ì¬ (pandasê°€ ìë™ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±) ===
        print(f"\nğŸ“¥ [Data] ë°ì´í„° ì ì¬ ì¤‘...")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        print(f"   - íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")
        
        total_rows = 0
        
        # PostgreSQLìš© SQLAlchemy ì—”ì§„ (pandas to_sqlìš©)
        engine = db_manager.get_sqlalchemy_engine()
        
        if file_size_mb > 50:  # 50MB ì´ìƒì´ë©´ Chunk ì²˜ë¦¬
            print(f"   - ëŒ€ìš©ëŸ‰ íŒŒì¼ - Chunk Processing ì ìš© (100,000í–‰ì”©)")
            
            chunk_size = 100000
            
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                chunk.to_sql(
                    table_name, 
                    engine, 
                    if_exists='append' if i > 0 else 'replace',
                    index=False,
                    method='multi'  # PostgreSQL ìµœì í™”
                )
                total_rows += len(chunk)
                print(f"      â€¢ Chunk {i+1}: {len(chunk):,}í–‰ ì ì¬ (ëˆ„ì : {total_rows:,}í–‰)")
        else:
            # ì‘ì€ íŒŒì¼ì€ í•œ ë²ˆì—
            print(f"   - ì¼ë°˜ íŒŒì¼ - í•œ ë²ˆì— ì ì¬")
            df = pd.read_csv(file_path)
            df.to_sql(
                table_name, 
                engine, 
                if_exists='replace', 
                index=False,
                method='multi'
            )
            total_rows = len(df)
            print(f"   - {total_rows:,}í–‰ ì ì¬ ì™„ë£Œ")
        
        print(f"âœ… ë°ì´í„° ì ì¬ ì„±ê³µ")
        
        # === 2. ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”) ===
        print(f"\nğŸ” [Index] ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        indices = SchemaGenerator.generate_indices(
            table_name=table_name,
            schema=schema,
            ontology_context=ontology
        )
        
        indices_created = []
        conn = db_manager.get_connection()
        cursor = conn.cursor()
        
        for idx_ddl in indices:
            try:
                cursor.execute(idx_ddl)
                # ì¸ë±ìŠ¤ëª… ì¶”ì¶œ
                idx_name = idx_ddl.split('"')[1] if '"' in idx_ddl else idx_ddl.split()[2]
                indices_created.append(idx_name)
            except Exception as e:
                print(f"âš ï¸  ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        conn.commit()
        
        if indices_created:
            print(f"   - ì¸ë±ìŠ¤ {len(indices_created)}ê°œ ìƒì„±: {', '.join(indices_created)}")
        else:
            print(f"   - ìƒì„±ëœ ì¸ë±ìŠ¤ ì—†ìŒ")
        
        # === 3. ê²€ì¦ ===
        print(f"\nâœ… [Verify] ê²€ì¦ ì¤‘...")
        
        # í–‰ ê°œìˆ˜ í™•ì¸ (PostgreSQL)
        cursor.execute(f'SELECT COUNT(*) FROM "{table_name}"')
        actual_rows = cursor.fetchone()[0]
        
        if actual_rows == total_rows:
            print(f"   - í–‰ ê°œìˆ˜ ì¼ì¹˜: {actual_rows:,}í–‰ âœ…")
        else:
            print(f"   âš ï¸ í–‰ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {total_rows:,}, ì‹¤ì œ {actual_rows:,}")
        
        print("="*80)
        
        return {
            "logs": [
                f"ğŸ’¾ [Indexer] {table_name} ìƒì„± ì™„ë£Œ ({total_rows:,}í–‰)",
                f"ğŸ” [Indexer] ì¸ë±ìŠ¤: {len(indices_created)}ê°œ",
                "âœ… [Done] ì¸ë±ì‹± í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ."
            ]
        }
        
    except Exception as e:
        print(f"\nâŒ [Error] DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print("="*80)
        
        import traceback
        traceback.print_exc()
        
        return {
            "logs": [f"âŒ [Indexer] DB ì €ì¥ ì‹¤íŒ¨: {str(e)}"],
            "error_message": str(e)
        }

# --- Helper Functions (Private) ---

def _analyze_columns_with_llm(columns: List[str], sample_data: Any, anchor_context: Dict) -> List[ColumnSchema]:
    """
    [Helper] LLMì„ ì‚¬ìš©í•˜ì—¬ ê° ì»¬ëŸ¼ì˜ ì˜ë¯¸, ë°ì´í„° íƒ€ì…, PII ì—¬ë¶€ë¥¼ ë¶„ì„
    """
    # LLMì—ê²Œ ë³´ë‚¼ ë¬¸ë§¥ ìš”ì•½
    prompt = f"""
    You are a Medical Data Ontologist.
    Analyze the columns of a dataset based on the provided sample data.
    
    [Context]
    - Patient Identifier (Anchor): {anchor_context.get('column_name')}
    - Is Time Series: {anchor_context.get('is_time_series')}
    
    [Columns to Analyze]
    """
    
    # sample_dataê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (TabularProcessorì—ì„œ ì˜¨ ê²½ìš°)
    if isinstance(sample_data, list):
        for col_detail in sample_data:
            col_name = col_detail.get('column_name', 'unknown')
            col_type = col_detail.get('column_type', 'unknown')
            samples = col_detail.get('samples', [])
            
            if col_type == 'categorical':
                unique_vals = col_detail.get('unique_values', [])
                prompt += f"- Column: '{col_name}' | Type: CATEGORICAL | Unique Values: {unique_vals}\n"
            else:
                min_val = col_detail.get('min', 'N/A')
                max_val = col_detail.get('max', 'N/A')
                prompt += f"- Column: '{col_name}' | Type: CONTINUOUS | Range: [{min_val}, {max_val}] | Samples: {samples}\n"
    # sample_dataê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ì´ì „ ë°©ì‹ í˜¸í™˜)
    elif isinstance(sample_data, dict):
        for col in columns:
            details = sample_data.get(col, {})
            samples = details.get("sample_values", [])
            prompt += f"- Column: '{col}', Samples: {samples}\n"
    else:
        # ë‘˜ ë‹¤ ì•„ë‹ˆë©´ ì»¬ëŸ¼ ì´ë¦„ë§Œ ì œê³µ
        for col in columns:
            prompt += f"- Column: '{col}'\n"

    prompt += """
    [Task]
    For EACH column, provide a JSON object with:
    1. inferred_name: Logical name (e.g., 'Systolic BP', 'Admission Date').
    2. description: Brief medical description.
    3. data_type: SQL compatible type (VARCHAR, INT, FLOAT, TIMESTAMP).
    4. is_pii: Boolean (true if it contains name, phone, social security number).
    5. confidence: 0.0 to 1.0.

    Respond with a list of JSON objects.
    """
    
    # LLM í˜¸ì¶œ
    response = llm_client.ask_json(prompt)
    
    # ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë”•ì…”ë„ˆë¦¬(ë¦¬ìŠ¤íŠ¸ë¥¼ ê°ì‹¼)ì¸ì§€ í™•ì¸ í›„ íŒŒì‹±
    if isinstance(response, dict) and "columns" in response:
        result_list = response["columns"]
    elif isinstance(response, list):
        result_list = response
    else:
        result_list = [] # ì—ëŸ¬ ì²˜ë¦¬ í•„ìš”

    # ê²°ê³¼ ë§¤í•‘
    final_schema = []
    for idx, item in enumerate(result_list):
        # ì›ë³¸ ì»¬ëŸ¼ëª… ë§¤ì¹­ (ìˆœì„œê°€ ë³´ì¥ëœë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ LLMì—ê²Œ ì›ë³¸ëª…ì„ ë±‰ê²Œ í•´ì•¼ í•¨)
        # ì•ˆì „í•˜ê²Œ ì›ë³¸ ì»¬ëŸ¼ëª…ì„ LLM ì‘ë‹µì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŒ
        original = columns[idx] if idx < len(columns) else "unknown"
        
        final_schema.append({
            "original_name": original,
            "inferred_name": item.get("inferred_name", original),
            "description": item.get("description", ""),
            "data_type": item.get("data_type", "VARCHAR"),
            "standard_concept_id": None, 
            "is_pii": item.get("is_pii", False),
            "confidence": item.get("confidence", 0.5)
        })
        
    return final_schema


def _compare_with_global_context(local_metadata: Dict, local_anchor_info: Dict, project_context: Dict) -> Dict[str, Any]:
    """
    [Helper] í˜„ì¬ íŒŒì¼ì˜ ë°ì´í„°ì™€ í”„ë¡œì íŠ¸ Global Anchor ì •ë³´ë¥¼ ë¹„êµ (LLM í™œìš©)
    
    â­ [NEW] ì˜¨í†¨ë¡œì§€ì˜ relationshipsë¥¼ í™•ì¸í•˜ì—¬ ê°„ì ‘ ì—°ê²°ë„ ì²˜ë¦¬
    ì˜ˆ: lab_dataì— subjectidê°€ ì—†ì–´ë„ caseidë¥¼ í†µí•´ clinical_data.subjectidì™€ ì—°ê²° ê°€ëŠ¥
    """
    master_name = project_context["master_anchor_name"]
    local_cols = local_metadata.get("columns", [])
    local_candidate = local_anchor_info.get("target_column")
    
    # í˜„ì¬ íŒŒì¼ëª…ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
    file_path = local_metadata.get("file_path", "")
    current_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
    
    # 1. ì´ë¦„ì´ ì™„ì „íˆ ê°™ì€ ê²½ìš° (Fast Path)
    if master_name in local_cols:
        return {"status": "MATCH", "target_column": master_name, "message": "Exact name match"}

    # â­ [NEW] 2. ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê°„ì ‘ ì—°ê²° í™•ì¸
    indirect_link = _check_indirect_link_via_ontology(
        current_table=current_table,
        local_cols=local_cols,
        master_anchor=master_name
    )
    
    if indirect_link:
        return {
            "status": "INDIRECT_LINK",
            "target_column": indirect_link["via_column"],
            "via_table": indirect_link["via_table"],
            "master_anchor": master_name,
            "message": indirect_link["message"]
        }

    # 3. ë¡œì»¬ í›„ë³´ê°€ ì—†ëŠ” ê²½ìš° (Processorê°€ ëª» ì°¾ìŒ)
    if not local_candidate:
        return {
            "status": "MISSING",
            "target_column": None,
            "message": f"No anchor candidate found in local file. Master anchor '{master_name}' not found in columns: {local_cols}"
        }

    # 3. LLMì„ í†µí•œ ì˜ë¯¸ë¡ ì  ë¹„êµ
    prompt = f"""
    You are a Medical Data Integration Agent.
    Check if the new file contains the Project's Master Anchor (Patient ID).

    [Project Context / Global Master]
    - Master Anchor Name: '{master_name}'
    - Known Aliases: {project_context.get('known_aliases')}
    
    [New File Info]
    - Candidate Column found by AI: '{local_candidate}'
    - All Columns in file: {local_cols}
    
    [Task]
    Determine if any column in the new file represents the same 'Patient ID' entity as the Global Master.
    - If the candidate '{local_candidate}' is a synonym for '{master_name}' (e.g. 'pid' vs 'subject_id'), return MATCH.
    - If another column in 'All Columns' looks like the ID, return MATCH with that column.
    - If you cannot find a matching column, return MISSING.
    - If you are unsure, return CONFLICT.

    Respond with JSON:
    {{
        "status": "MATCH" or "MISSING" or "CONFLICT",
        "target_column": "name_of_column_in_new_file" (or null if missing),
        "message": "Reasoning for the decision"
    }}
    """
    
    try:
        result = llm_client.ask_json(prompt)
        
        # LLM ì‘ë‹µ ê²€ì¦ ë° ì •ê·œí™”
        if not isinstance(result, dict):
            return {"status": "CONFLICT", "target_column": None, "message": "LLM returned invalid format"}
        
        status = result.get("status", "CONFLICT").upper()
        if status not in ["MATCH", "MISSING", "CONFLICT"]:
            status = "CONFLICT"
        
        return {
            "status": status,
            "target_column": result.get("target_column"),
            "message": result.get("message", "No explanation provided")
        }
        
    except Exception as e:
        return {"status": "CONFLICT", "target_column": None, "message": f"Error during anchor comparison: {str(e)}"}


# ============================================================================
# ê°„ì ‘ ì—°ê²° í™•ì¸ (Ontology ê¸°ë°˜)
# ============================================================================

def _check_indirect_link_via_ontology(current_table: str, local_cols: list, master_anchor: str) -> Optional[Dict]:
    """
    â­ [NEW] ì˜¨í†¨ë¡œì§€ì˜ relationshipsë¥¼ í™•ì¸í•˜ì—¬ ê°„ì ‘ ì—°ê²° í™•ì¸
    
    ì˜ˆì‹œ:
    - lab_dataì— subjectidê°€ ì—†ìŒ
    - í•˜ì§€ë§Œ ontologyì— "lab_data.caseid â†’ clinical_data.caseid" ê´€ê³„ê°€ ìˆìŒ
    - clinical_dataì— subjectidê°€ ìˆìŒ
    - ë”°ë¼ì„œ lab_dataëŠ” caseidë¥¼ í†µí•´ subjectidì™€ ê°„ì ‘ ì—°ê²°ë¨
    
    Returns:
        ê°„ì ‘ ì—°ê²° ì •ë³´ dict ë˜ëŠ” None
    """
    try:
        # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        ontology = ontology_manager.load()
        if not ontology:
            return None
        
        relationships = ontology.get("relationships", [])
        file_tags = ontology.get("file_tags", {})
        
        print(f"\nğŸ”— [Indirect Link Check] {current_table}")
        print(f"   - ì˜¨í†¨ë¡œì§€ ê´€ê³„ ìˆ˜: {len(relationships)}ê°œ")
        
        # í˜„ì¬ í…Œì´ë¸”ì´ sourceì¸ ê´€ê³„ ì°¾ê¸°
        for rel in relationships:
            source_table = rel.get("source_table", "")
            target_table = rel.get("target_table", "")
            source_column = rel.get("source_column", "")
            target_column = rel.get("target_column", "")
            
            # current_tableì´ sourceì¸ ê²½ìš°
            if current_table.lower() in source_table.lower() or source_table.lower() in current_table.lower():
                # ì—°ê²° ì»¬ëŸ¼ì´ í˜„ì¬ íŒŒì¼ì— ìˆëŠ”ì§€ í™•ì¸
                if source_column in local_cols:
                    # target_tableì— master_anchorê°€ ìˆëŠ”ì§€ í™•ì¸
                    target_has_master = _check_table_has_column(file_tags, target_table, master_anchor)
                    
                    if target_has_master:
                        message = (
                            f"âœ… ê°„ì ‘ ì—°ê²° ë°œê²¬! "
                            f"'{current_table}.{source_column}' â†’ '{target_table}.{target_column}' ê´€ê³„ë¥¼ í†µí•´ "
                            f"'{master_anchor}'ì— ì—°ê²°ë¨"
                        )
                        print(f"   {message}")
                        
                        return {
                            "via_column": source_column,
                            "via_table": target_table,
                            "via_relation": f"{source_table}.{source_column} â†’ {target_table}.{target_column}",
                            "message": message
                        }
        
        print(f"   - ê°„ì ‘ ì—°ê²° ì—†ìŒ")
        return None
        
    except Exception as e:
        print(f"   âš ï¸ ê°„ì ‘ ì—°ê²° í™•ì¸ ì˜¤ë¥˜: {e}")
        return None


def _check_table_has_column(file_tags: Dict, table_name: str, column_name: str) -> bool:
    """
    file_tagsì—ì„œ íŠ¹ì • í…Œì´ë¸”ì— íŠ¹ì • ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    """
    for file_path, tag_info in file_tags.items():
        # íŒŒì¼ëª…ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
        file_table = os.path.basename(file_path).replace(".csv", "").replace(".CSV", "")
        
        if table_name.lower() in file_table.lower() or file_table.lower() in table_name.lower():
            columns = tag_info.get("columns", [])
            if column_name in columns:
                return True
    
    return False


# ============================================================================
# Ontology Builder ê´€ë ¨ í•¨ìˆ˜ë“¤ (Phase 0-1)
# ============================================================================

def _collect_negative_evidence(col_name: str, samples: list, unique_vals: list) -> dict:
    """
    [Rule] ë¶€ì • ì¦ê±° ìˆ˜ì§‘ (ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ê°ì§€)
    
    Args:
        col_name: ì»¬ëŸ¼ëª…
        samples: ìƒ˜í”Œ ê°’ ë¦¬ìŠ¤íŠ¸
        unique_vals: unique ê°’ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ë¶€ì • ì¦ê±° ë”•ì…”ë„ˆë¦¬
    """
    import numpy as np
    
    total = len(samples)
    unique = len(unique_vals)
    
    # null ê³„ì‚°
    null_count = sum(
        1 for s in samples 
        if s is None or s == '' or (isinstance(s, float) and np.isnan(s))
    )
    
    negative_evidence = []
    
    # 1. ê±°ì˜ uniqueì¸ë° ì¤‘ë³µ ìˆìŒ (ë°ì´í„° ì˜¤ë¥˜ ê°€ëŠ¥ì„±)
    if total > 0 and unique / total > 0.95 and unique != total:
        dup_rate = (total - unique) / total
        negative_evidence.append({
            "type": "near_unique_with_duplicates",
            "detail": f"{unique/total:.1%} unique BUT {dup_rate:.1%} duplicates - possible data error",
            "severity": "medium"
        })
    
    # 2. ID ê°™ì€ë° null ìˆìŒ (PK ë¶ˆê°€)
    if 'id' in col_name.lower() and null_count > 0:
        null_rate = null_count / total
        negative_evidence.append({
            "type": "identifier_with_nulls",
            "detail": f"Column name suggests ID BUT {null_rate:.1%} null values",
            "severity": "high" if null_rate > 0.1 else "low"
        })
    
    # 3. Cardinality ë„ˆë¬´ ë†’ìŒ (free text ê°€ëŠ¥ì„±)
    if unique > 100:
        negative_evidence.append({
            "type": "high_cardinality",
            "detail": f"{unique} unique values - might be free text, not categorical",
            "severity": "low"
        })
    
    return {
        "has_issues": len(negative_evidence) > 0,
        "issues": negative_evidence,
        "null_ratio": null_count / total if total > 0 else 0.0
    }


def _summarize_long_values(values: list, max_length: int = 50) -> list:
    """
    [Rule] ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (Context Window ê´€ë¦¬)
    
    Args:
        values: ê°’ ë¦¬ìŠ¤íŠ¸
        max_length: ìµœëŒ€ ê¸¸ì´ (ì´ìƒì´ë©´ ìš”ì•½)
    
    Returns:
        ìš”ì•½ëœ ê°’ ë¦¬ìŠ¤íŠ¸
    """
    summarized = []
    
    for val in values:
        val_str = str(val)
        
        if len(val_str) > max_length:
            # ë©”íƒ€ ì •ë³´ë¡œ ëŒ€ì²´ (í† í° ì ˆì•½)
            preview = val_str[:20].replace('\n', ' ')
            summarized.append(f"[Text: {len(val_str)} chars, starts='{preview}...']")
        else:
            summarized.append(val_str)
    
    return summarized


def _parse_metadata_content(file_path: str) -> dict:
    """
    [Rule] ë©”íƒ€ë°ì´í„° íŒŒì¼ íŒŒì‹± (CSV â†’ Dictionary)
    
    Args:
        file_path: ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    
    Returns:
        definitions ë”•ì…”ë„ˆë¦¬ {parameter: description}
    """
    import pandas as pd
    
    definitions = {}
    
    try:
        df = pd.read_csv(file_path)
        
        # ì¼ë°˜ì ì¸ ë©”íƒ€ë°ì´í„° êµ¬ì¡°: [Parameter/Name, Description, ...]
        if len(df.columns) >= 2:
            key_col = df.columns[0]
            desc_col = df.columns[1]
            
            for _, row in df.iterrows():
                key = str(row[key_col]).strip()
                desc = str(row[desc_col]).strip()
                
                # ì¶”ê°€ ì •ë³´ ê²°í•© (Unit, Type ë“±)
                extra_info = []
                for col in df.columns[2:]:
                    val = row[col]
                    if pd.notna(val) and str(val).strip():
                        extra_info.append(f"{col}={val}")
                
                if extra_info:
                    desc += " | " + " | ".join(extra_info)
                
                definitions[key] = desc
        
        return definitions
        
    except Exception as e:
        print(f"âŒ [Parse Error] {file_path}: {e}")
        return {}


def _build_metadata_detection_context(file_path: str, metadata: dict) -> dict:
    """
    [Rule] ë©”íƒ€ë°ì´í„° ê°ì§€ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì „ì²˜ë¦¬)
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        metadata: Processorê°€ ì¶”ì¶œí•œ raw_metadata
    
    Returns:
        LLMì—ê²Œ ì œê³µí•  ì»¨í…ìŠ¤íŠ¸
    """
    basename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # Rule: íŒŒì¼ëª… íŒŒì‹±
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    columns = metadata.get("columns", [])
    column_details = metadata.get("column_details", [])
    
    # Rule: ìƒ˜í”Œ ë°ì´í„° ì •ë¦¬
    sample_summary = []
    total_text_length = 0
    
    for col_info in column_details[:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
        col_name = col_info.get('column_name', 'unknown')
        samples = col_info.get('samples', [])
        col_type = col_info.get('column_type', 'unknown')
        
        # Categoricalì´ë©´ unique valuesë„ ì œê³µ
        if col_type == 'categorical':
            unique_vals = col_info.get('unique_values', [])[:20]
            # ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (Rule)
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        else:
            unique_vals = samples[:10]
            unique_vals_summarized = _summarize_long_values(unique_vals, max_length=50)
        
        # Rule: í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
        avg_length = 0.0
        if samples:
            text_lengths = [len(str(s)) for s in samples]
            avg_length = sum(text_lengths) / len(text_lengths)
            total_text_length += avg_length
        
        # [NEW] Negative Evidence ìˆ˜ì§‘ (Rule)
        negative_evidence = _collect_negative_evidence(col_name, samples, unique_vals if unique_vals else [])
        
        # ìƒ˜í”Œë„ ìš”ì•½
        samples_summarized = _summarize_long_values(samples[:3], max_length=50)
        
        sample_summary.append({
            "column": col_name,
            "type": col_type,
            "samples": samples_summarized,
            "unique_values": unique_vals_summarized,
            "avg_text_length": round(avg_length, 1),
            "null_ratio": negative_evidence.get("null_ratio", 0.0),  # [NEW]
            "negative_evidence": negative_evidence.get("issues", [])  # [NEW]
        })
    
    # Context í¬ê¸° ì¶”ì •
    context_size = len(json.dumps(sample_summary))
    
    # ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œ ì¶•ì†Œ (Rule)
    if context_size > 3000:
        sample_summary = sample_summary[:3]
        context_size = len(json.dumps(sample_summary))
    
    return {
        "filename": basename,
        "name_parts": parts,
        "base_name": base_name,
        "extension": extension,
        "columns": columns,
        "num_columns": len(columns),
        "sample_data": sample_summary,
        "avg_text_length_overall": round(total_text_length / max(len(sample_summary), 1), 1),
        "context_size_bytes": context_size
    }


def _ask_llm_is_metadata(context: dict) -> dict:
    """
    [LLM] ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨
    
    Args:
        context: Ruleë¡œ ì „ì²˜ë¦¬ëœ ì»¨í…ìŠ¤íŠ¸
    
    Returns:
        íŒë‹¨ ê²°ê³¼ {is_metadata, confidence, reasoning, indicators}
    """
    # ì „ì—­ ìºì‹œ ì‚¬ìš©
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("metadata_detection", context)
    if cached:
        return cached
    
    # LLM í”„ë¡¬í”„íŠ¸
    prompt = f"""
You are a Data Classification Expert.

I have pre-processed file information using rules. Based on these facts, determine if this is METADATA or TRANSACTIONAL DATA.

[PRE-PROCESSED FILE INFORMATION - Extracted by Rules]
Filename: {context['filename']}
Parsed Name Parts: {context['name_parts']}  â† Ruleë¡œ íŒŒì‹±
Base Name: {context['base_name']}
Extension: {context['extension']}
Number of Columns: {context['num_columns']}
Columns: {context['columns']}

[PRE-PROCESSED SAMPLE DATA - Extracted by Rules]
{json.dumps(context['sample_data'], indent=2)}
(Note: avg_text_length, unique_values, null_ratio, and negative_evidence were calculated by rules)

[IMPORTANT - Check Negative Evidence]
Each column has "negative_evidence" field showing data quality issues if any:
- near_unique_with_duplicates: Almost unique but has some duplicates
- identifier_with_nulls: Column name suggests ID but has null values
- high_cardinality: Too many unique values for categorical

Use this information to improve your judgment.

[DEFINITION]
- METADATA file: Describes OTHER data (e.g., column definitions, parameter lists, codebooks)
  * Contains descriptive text about columns/variables
  * Usually has structure like: [Name/ID, Description, Unit, Type]
  * Content is documentation, not measurements/transactions
  
- TRANSACTIONAL DATA: Actual records/measurements
  * Contains patient records, lab results, events, etc.
  * Values are data points, not descriptions

[YOUR TASK - Interpret Pre-processed Information]
Using the parsed filename and pre-calculated statistics, classify this file:

1. **Filename Analysis**:
   - Look at name_parts: if contains "parameters", "dict", "definition" â†’ likely metadata
   - Look at base_name: what domain does it represent?

2. **Column Structure**:
   - Is it Key-Value format? (e.g., [Parameter, Description, Unit])
   - Or wide transactional format? (many columns with diverse types)

3. **Sample Content Analysis**:
   - Check avg_text_length: Long text (>30 chars) â†’ likely descriptions
   - Check unique_values: Are they codes/IDs or explanatory text?

IMPORTANT: I already did the heavy lifting (parsing, statistics). 
You interpret the MEANING of these pre-processed facts.

[OUTPUT FORMAT - JSON ONLY]
{{
    "is_metadata": true or false,
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation based on filename, structure, and content",
    "indicators": {{
        "filename_hint": "strong/weak/none",
        "structure_hint": "dictionary-like/tabular/unclear",
        "content_type": "descriptive/transactional/mixed"
    }}
}}
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # ìºì‹œ ì €ì¥
        llm_cache.set("metadata_detection", context, result)
        
        # í™•ì‹ ë„ ê²€ì¦
        confidence = result.get("confidence", 0.0)
        if confidence < 0.75:
            print(f"âš ï¸  [Metadata Detection] Low confidence ({confidence:.2%})")
            print(f"    Reasoning: {result.get('reasoning', 'N/A')[:100]}...")
        
        return result
        
    except Exception as e:
        print(f"âŒ [Metadata Detection] LLM Error: {e}")
        # Fallback
        return {
            "is_metadata": False,  # ë³´ìˆ˜ì  ê¸°ë³¸ê°’
            "confidence": 0.0,
            "reasoning": f"LLM error: {str(e)}",
            "indicators": {},
            "needs_human_review": True
        }


def _find_common_columns(current_cols: List[str], existing_tables: dict) -> List[dict]:
    """
    [Rule] í˜„ì¬ í…Œì´ë¸”ê³¼ ê¸°ì¡´ í…Œì´ë¸”ë“¤ ì‚¬ì´ì˜ ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸° (FK í›„ë³´ ê²€ìƒ‰)
    
    Args:
        current_cols: í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        existing_tables: ê¸°ì¡´ í…Œì´ë¸”ë“¤ ì •ë³´ {table_name: {columns: [...], ...}}
    
    Returns:
        FK í›„ë³´ ë¦¬ìŠ¤íŠ¸
    """
    candidates = []
    
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        # ì™„ì „ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸° (Rule - ì •í™•í•œ ë§¤ì¹­)
        common_cols = set(current_cols) & set(existing_cols)
        
        for common_col in common_cols:
            candidates.append({
                "column_name": common_col,
                "current_table": "new_table",
                "existing_table": table_name,
                "match_type": "exact_name",
                "confidence_hint": 0.9  # ì´ë¦„ì´ ì™„ì „íˆ ê°™ìœ¼ë©´ ë†’ì€ í™•ë¥ ë¡œ FK
            })
    
    # ìœ ì‚¬í•œ ì´ë¦„ ì°¾ê¸° (Rule - ë‹¨ìˆœ ë¬¸ìì—´ ì •ê·œí™”)
    # ì˜ˆ: patient_id vs patientid, subjectid vs subject_id
    for table_name, table_info in existing_tables.items():
        existing_cols = table_info.get("columns", [])
        
        for curr_col in current_cols:
            for exist_col in existing_cols:
                # ì–¸ë”ìŠ¤ì½”ì–´ ì œê±° í›„ ë¹„êµ (Rule)
                curr_normalized = curr_col.replace('_', '').lower()
                exist_normalized = exist_col.replace('_', '').lower()
                
                if curr_normalized == exist_normalized and curr_col != exist_col:
                    candidates.append({
                        "current_col": curr_col,
                        "existing_col": exist_col,
                        "existing_table": table_name,
                        "match_type": "similar_name",
                        "confidence_hint": 0.7  # ìœ ì‚¬í•˜ë©´ ì¤‘ê°„ í™•ë¥ 
                    })
    
    return candidates


def _extract_filename_hints(filename: str) -> dict:
    """
    [Rule + LLM] íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ë¡ ì  íŒíŠ¸ ì¶”ì¶œ
    
    1ë‹¨ê³„ (Rule): íŒŒì¼ëª… êµ¬ì¡° ë¶„ì„
    2ë‹¨ê³„ (LLM): ì˜ë¯¸ ì¶”ë¡  (Entity Type, Level)
    
    Args:
        filename: íŒŒì¼ëª… ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
    
    Returns:
        íŒŒì¼ëª… íŒíŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    # ì „ì—­ ìºì‹œ ì‚¬ìš©
    
    # === 1ë‹¨ê³„: Rule-based íŒŒì¼ëª… íŒŒì‹± ===
    basename = os.path.basename(filename)
    name_without_ext = os.path.splitext(basename)[0]
    extension = os.path.splitext(basename)[1]
    
    # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë¶„ë¦¬ (Rule)
    parts = name_without_ext.split('_')
    base_name = parts[0] if parts else name_without_ext
    
    # ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì¶”ì¶œ (Rule)
    prefix = parts[0] if len(parts) >= 2 else None
    suffix = parts[-1] if len(parts) >= 2 else None
    
    # Ruleë¡œ ì¶”ì¶œí•œ êµ¬ì¡° ì •ë³´
    parsed_structure = {
        "original_filename": basename,
        "name_without_ext": name_without_ext,
        "extension": extension,
        "parts": parts,
        "base_name": base_name,
        "prefix": prefix,
        "suffix": suffix,
        "has_underscore": '_' in name_without_ext,
        "num_parts": len(parts)
    }
    
    # === 2ë‹¨ê³„: LLM ê¸°ë°˜ ì˜ë¯¸ ì¶”ë¡  ===
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("filename_hints", parsed_structure)
    if cached:
        return cached
    
    # LLM í”„ë¡¬í”„íŠ¸
    prompt = f"""
You are a Data Architecture Analyst.

I have parsed the filename structure using rules. Based on this, infer the semantic meaning.

[PARSED FILENAME STRUCTURE - Extracted by Rules]
{json.dumps(parsed_structure, indent=2)}

[YOUR TASK - Semantic Interpretation]
Using the PARSED STRUCTURE, infer:

1. **Entity Type**: What domain entity does base_name represent?
   - Examples: "lab" â†’ Laboratory, "patient" â†’ Patient, "clinical" â†’ Case/Clinical
   - Use domain knowledge (medical, financial, etc.)

2. **Scope**: What is the data scope?
   - individual: Patient, Subject
   - event: Case, Admission, Visit, Stay
   - measurement: Lab, Vital, Sensor
   - treatment: Medication, Procedure

3. **Suggested Hierarchy Level**: (1=highest, 5=lowest)
   - Level 1: Patient, Subject
   - Level 2: Case, Admission, Visit
   - Level 3: Sub-event (ICU Stay)
   - Level 4: Measurement (Lab, Vital)
   - Level 5: Detail

4. **Data Type Indicator**: Based on suffix
   - "data", "records", "events" â†’ transactional
   - "parameters", "dict", "info" â†’ metadata
   - "master", "dim" â†’ reference

5. **Related File Patterns**: Predict related files
   - If "lab_data", likely has "lab_parameters" or "lab_dict"

[OUTPUT FORMAT - JSON]
{{
    "entity_type": "Laboratory" or null,
    "scope": "measurement" or null,
    "suggested_level": 4 or null,
    "data_type_indicator": "transactional" or "metadata",
    "related_file_patterns": ["lab_parameters", "lab_dict"],
    "confidence": 0.0 to 1.0,
    "reasoning": "Brief explanation"
}}
"""
    
    try:
        # ì „ì—­ llm_client ì‚¬ìš©
        hints = llm_client.ask_json(prompt)
        
        # ê¸°ë³¸ í•„ë“œ ì¶”ê°€
        hints["filename"] = basename
        hints["base_name"] = base_name
        hints["parts"] = parts
        
        # ìºì‹œ ì €ì¥
        llm_cache.set("filename_hints", parsed_structure, hints)
        
        # Confidence ê²€ì¦
        if hints.get("confidence", 1.0) < 0.7:
            print(f"âš ï¸  [Filename Analysis] Low confidence ({hints.get('confidence'):.2%}) for {basename}")
        
        return hints
        
    except Exception as e:
        # LLM ì‹¤íŒ¨ ì‹œ ìµœì†Œ ì •ë³´ë§Œ ë°˜í™˜
        print(f"âŒ [Filename Analysis] LLM Error: {e}")
        return {
            "filename": basename,
            "base_name": base_name,
            "parts": parts,
            "entity_type": None,
            "scope": None,
            "suggested_level": None,
            "data_type_indicator": None,
            "related_file_patterns": [],
            "confidence": 0.0,
            "error": str(e)
        }


def _summarize_existing_tables(ontology_context: dict, processed_files_data: dict = None) -> dict:
    """
    [Rule] ê¸°ì¡´ í…Œì´ë¸” ì •ë³´ ìš”ì•½ (LLMì—ê²Œ ì œê³µìš©)
    
    Args:
        ontology_context: í˜„ì¬ ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸
        processed_files_data: ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ì»¬ëŸ¼ ì •ë³´ (optional)
    
    Returns:
        í…Œì´ë¸” ìš”ì•½ ë”•ì…”ë„ˆë¦¬
    """
    tables = {}
    
    # file_tagsì—ì„œ ë°ì´í„° íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "").replace(".", "_")
            
            # ì»¬ëŸ¼ ì •ë³´ (ì €ì¥ëœ ê²ƒì´ ìˆìœ¼ë©´ ì‚¬ìš©)
            columns = tag_info.get("columns", [])
            
            # ë˜ëŠ” processed_files_dataì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not columns and processed_files_data:
                columns = processed_files_data.get(file_path, {}).get("columns", [])
            
            tables[table_name] = {
                "file_path": file_path,
                "type": tag_info.get("type"),
                "columns": columns
            }
    
    return tables


def _infer_relationships_with_llm(
    current_table_name: str,
    current_cols: List[str],
    ontology_context: dict,
    current_metadata: dict
) -> dict:
    """
    [Rule ì „ì²˜ë¦¬ + LLM íŒë‹¨] í…Œì´ë¸” ê°„ ê´€ê³„ ì¶”ë¡ 
    
    Args:
        current_table_name: í˜„ì¬ í…Œì´ë¸” ì´ë¦„
        current_cols: í˜„ì¬ í…Œì´ë¸” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        ontology_context: ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸
        current_metadata: í˜„ì¬ íŒŒì¼ì˜ raw_metadata (ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ìš©)
    
    Returns:
        {relationships: [...], hierarchy: [...], reasoning: "..."}
    """
    # ì „ì—­ ìºì‹œ ë° llm_client ì‚¬ìš©
    
    # === 1ë‹¨ê³„: Rule Prepares ===
    
    # íŒŒì¼ëª… íŒíŠ¸ (Rule + LLM)
    filename_hints = _extract_filename_hints(current_table_name)
    
    # ê¸°ì¡´ í…Œì´ë¸” ìš”ì•½
    existing_tables = _summarize_existing_tables(ontology_context)
    
    # FK í›„ë³´ ì°¾ê¸° (Rule)
    fk_candidates = _find_common_columns(current_cols, existing_tables)
    
    # ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ (í˜„ì¬ëŠ” ê¸°ë³¸ í†µê³„ë§Œ)
    cardinality_hints = {}
    column_details = current_metadata.get("column_details", [])
    
    for col_info in column_details:
        col_name = col_info.get('column_name')
        samples = col_info.get('samples', [])
        
        if samples:
            unique_count = len(set(samples))
            total_count = len(samples)
            ratio = unique_count / total_count if total_count > 0 else 0
            
            cardinality_hints[col_name] = {
                "uniqueness_ratio": round(ratio, 2),
                "pattern": "UNIQUE" if ratio > 0.95 else "REPEATED"
            }
    
    # === 2ë‹¨ê³„: LLM Decides ===
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    llm_context = {
        "current_table": current_table_name,
        "current_cols": current_cols,
        "filename_hints": filename_hints,
        "fk_candidates": fk_candidates,
        "cardinality": cardinality_hints,
        "existing_tables": existing_tables,
        "definitions": ontology_context.get("definitions", {})
    }
    
    # ìºì‹œ í™•ì¸
    cached = llm_cache.get("relationship_inference", llm_context)
    if cached:
        print(f"âœ… [Cache Hit] ê´€ê³„ ì¶”ë¡  ìºì‹œ ì‚¬ìš©")
        return cached
    
    # LLM í”„ë¡¬í”„íŠ¸
    prompt = f"""
You are a Database Schema Architect for Medical Data Integration.

I have pre-processed data using rules. Infer table relationships.

[PRE-PROCESSED INFORMATION]

1. NEW TABLE:
Name: {current_table_name}
Columns: {current_cols}

2. FILENAME HINTS (Parsed by Rule + LLM):
{json.dumps(filename_hints, indent=2)}

3. FK CANDIDATES (Found by Rules - Common Columns):
{json.dumps(fk_candidates, indent=2)}

4. CARDINALITY (Calculated by Rules):
{json.dumps(cardinality_hints, indent=2)}

5. EXISTING TABLES:
{json.dumps(existing_tables, indent=2)}

6. ONTOLOGY DEFINITIONS (Medical Terms):
Available terms: {len(llm_context['definitions'])} definitions
Example: caseid, subjectid, alb, wbc, etc.

[YOUR TASK]

1. **Validate FK Candidates**:
   - Check if common columns are truly Foreign Keys
   - Use CARDINALITY: if REPEATED â†’ likely FK
   - Use FILENAME: if base_names related â†’ likely FK

2. **Determine Relationship Type**:
   - 1:1, 1:N, N:1, or M:N based on cardinality

3. **Infer Hierarchy**:
   - Which entity is parent? (more abstract)
   - Which is child? (more specific)
   - Use domain knowledge

[OUTPUT FORMAT - JSON]
{{
  "relationships": [
    {{
      "source_table": "{current_table_name}",
      "target_table": "existing_table_name",
      "source_column": "column_name",
      "target_column": "column_name",
      "relation_type": "N:1",
      "confidence": 0.95,
      "description": "Brief explanation",
      "llm_inferred": true
    }}
  ],
  "hierarchy": [
    {{
      "level": 1,
      "entity_name": "Patient",
      "anchor_column": "subjectid",
      "mapping_table": null,
      "confidence": 0.9
    }}
  ],
  "reasoning": "Overall explanation"
}}

If no relationships found, return empty lists.
Be conservative: confidence < 0.8 if unsure.
"""
    
    try:
        result = llm_client.ask_json(prompt)
        
        # ìºì‹œ ì €ì¥
        llm_cache.set("relationship_inference", llm_context, result)
        
        # Confidence ê²€ì¦
        rels = result.get("relationships", [])
        low_conf_rels = [r for r in rels if r.get("confidence", 0) < 0.8]
        
        if low_conf_rels:
            print(f"âš ï¸  [Relationship] Low confidence for {len(low_conf_rels)} relationships")
        
        return result
        
    except Exception as e:
        print(f"âŒ [Relationship Inference] LLM Error: {e}")
        return {
            "relationships": [],
            "hierarchy": [],
            "reasoning": f"Error: {str(e)}",
            "error": True
        }


def _summarize_existing_tables(ontology_context: dict, processed_files_data: dict = None) -> dict:
    """
    [Rule] ê¸°ì¡´ í…Œì´ë¸” ì •ë³´ ìš”ì•½ (LLMì—ê²Œ ì œê³µìš©)
    
    Args:
        ontology_context: í˜„ì¬ ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸
        processed_files_data: ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ì»¬ëŸ¼ ì •ë³´ (optional)
    
    Returns:
        í…Œì´ë¸” ìš”ì•½ ë”•ì…”ë„ˆë¦¬
    """
    tables = {}
    
    # file_tagsì—ì„œ ë°ì´í„° íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ
    for file_path, tag_info in ontology_context.get("file_tags", {}).items():
        if tag_info.get("type") == "transactional_data":
            table_name = os.path.basename(file_path).replace(".csv", "").replace(".", "_")
            
            # ì»¬ëŸ¼ ì •ë³´ (ì €ì¥ëœ ê²ƒ ì‚¬ìš©)
            columns = tag_info.get("columns", [])
            
            tables[table_name] = {
                "file_path": file_path,
                "type": tag_info.get("type"),
                "columns": columns
            }
    
    return tables


def _generate_specific_human_question(
    file_path: str,
    llm_result: dict,
    context: dict
) -> str:
    """
    [Rule] LLM reasoningì„ í™œìš©í•œ êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„±
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        llm_result: LLM íŒë‹¨ ê²°ê³¼
        context: ì „ì²˜ë¦¬ëœ ì»¨í…ìŠ¤íŠ¸
    
    Returns:
        êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë¬¸ìì—´
    """
    filename = os.path.basename(file_path)
    confidence = llm_result.get("confidence", 0.0)
    reasoning = llm_result.get("reasoning", "Unknown")
    indicators = llm_result.get("indicators", {})
    
    # LLMì´ í—·ê°ˆë¦° ì´ìœ  ë¶„ì„
    confusion_points = []
    
    if indicators.get("filename_hint") == "weak" or indicators.get("filename_hint") == "none":
        confusion_points.append("íŒŒì¼ëª…ì´ ì• ë§¤í•¨")
    
    if indicators.get("structure_hint") == "unclear" or indicators.get("structure_hint") == "mixed":
        confusion_points.append("ì»¬ëŸ¼ êµ¬ì¡°ê°€ í˜¼í•©í˜•")
    
    if indicators.get("content_type") == "mixed":
        confusion_points.append("ë‚´ìš©ì´ ì„¤ëª…ë¬¸ê³¼ ë°ì´í„° í˜¼ì¬")
    
    # êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„±
    question = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
íŒŒì¼: {filename}
í™•ì‹ ë„: {confidence:.1%} (ë‚®ìŒ - í™•ì¸ í•„ìš”)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤” AIê°€ í—·ê°ˆë¦° ì´ìœ :
{reasoning}

ë°œê²¬ëœ ì´ìŠˆ:
{chr(10).join('â€¢ ' + p for p in confusion_points) if confusion_points else 'â€¢ (ì´ìŠˆ ì—†ìŒ)'}

ğŸ“‹ ì°¸ê³  ì •ë³´:
- íŒŒì¼ëª… êµ¬ì¡°: {context.get('name_parts', [])}
- ì»¬ëŸ¼ ìˆ˜: {context.get('num_columns', 0)}ê°œ
- ì»¬ëŸ¼ ëª©ë¡: {context.get('columns', [])[:5]}...
- ìƒ˜í”Œ ë°ì´í„° ì¼ë¶€:
"""
    
    # ìƒ˜í”Œ ì¶”ê°€
    samples = context.get('sample_data', [])
    if samples:
        for i, s in enumerate(samples[:2]):
            question += f"\n  ì»¬ëŸ¼ {i+1}: {s.get('column', '?')} = {s.get('samples', [])}"
    
    question += """

â“ ì§ˆë¬¸: ì´ íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°(ì„¤ëª…ì„œ/ì½”ë“œë¶)ì…ë‹ˆê¹Œ, 
        ì•„ë‹ˆë©´ ì‹¤ì œ ì¸¡ì •/íŠ¸ëœì­ì…˜ ë°ì´í„°ì…ë‹ˆê¹Œ?

ë‹µë³€ ì˜µì…˜:
1. "ë©”íƒ€ë°ì´í„°" - ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ëŠ” íŒŒì¼
2. "ë°ì´í„°" - ì‹¤ì œ í™˜ì/ì¸¡ì • ê¸°ë¡
3. "ëª¨ë¥´ê² ìŒ" - ì¶”ê°€ ì¡°ì‚¬ í•„ìš”

>>> ë‹µë³€: """
    
    return question


def ontology_builder_node(state: AgentState) -> Dict[str, Any]:
    """
    [Node] ì˜¨í†¨ë¡œì§€ êµ¬ì¶• - Rule Prepares, LLM Decides
    
    íŒŒì¼ì´ ë©”íƒ€ë°ì´í„°ì¸ì§€ íŒë‹¨í•˜ê³ , ë©”íƒ€ë°ì´í„°ë©´ íŒŒì‹±í•˜ì—¬ ì˜¨í†¨ë¡œì§€ì— ì¶”ê°€
    """
    print("\n" + "="*80)
    print("ğŸ“š [ONTOLOGY BUILDER NODE] ì‹œì‘")
    print("="*80)
    
    file_path = state["file_path"]
    metadata = state["raw_metadata"]
    
    # ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ê°€ì ¸ì˜¤ê¸° (Stateì—ì„œ ë˜ëŠ” ë””ìŠ¤í¬ì—ì„œ)
    ontology = state.get("ontology_context")
    
    # ì²« íŒŒì¼ì´ê±°ë‚˜ ontologyê°€ ë¹„ì–´ìˆìœ¼ë©´ ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
    if not ontology or not ontology.get("definitions"):
        print(f"   - ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹œë„...")
        ontology = ontology_manager.load()
    
    # ì—¬ì „íˆ ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡°
    if not ontology:
        ontology = {
            "definitions": {},
            "relationships": [],
            "hierarchy": [],
            "file_tags": {}
        }
    
    # === Step 1: Rule Prepares (ë°ì´í„° ì „ì²˜ë¦¬) ===
    print("\nğŸ”§ [Rule] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    context = _build_metadata_detection_context(file_path, metadata)
    
    print(f"   - íŒŒì¼ëª… íŒŒì‹±: {context.get('name_parts')}")
    print(f"   - Base Name: {context.get('base_name')}")
    print(f"   - ì»¬ëŸ¼ ìˆ˜: {context.get('num_columns')}ê°œ")
    print(f"   - ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {context.get('context_size_bytes', 0)} bytes")
    
    # === Step 2: LLM Decides (ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨) ===
    print("\nğŸ§  [LLM] ë©”íƒ€ë°ì´í„° ì—¬ë¶€ íŒë‹¨ ì¤‘...")
    
    meta_result = _ask_llm_is_metadata(context)
    
    confidence = meta_result.get("confidence", 0.0)
    is_metadata = meta_result.get("is_metadata", False)
    
    print(f"   - íŒë‹¨: {'ë©”íƒ€ë°ì´í„°' if is_metadata else 'ì¼ë°˜ ë°ì´í„°'}")
    print(f"   - í™•ì‹ ë„: {confidence:.2%}")
    print(f"   - Reasoning: {meta_result.get('reasoning', 'N/A')[:80]}...")
    
    # === Step 3: Confidence Check ===
    if confidence < 0.75:
        print(f"\nâš ï¸  [Low Confidence] Human Review ìš”ì²­")
        
        # êµ¬ì²´ì  ì§ˆë¬¸ ìƒì„±
        specific_question = _generate_specific_human_question(
            file_path, meta_result, context
        )
        
        print("="*80)
        
        return {
            "needs_human_review": True,
            "human_question": specific_question,
            "ontology_context": ontology,  # í˜„ì¬ ìƒíƒœ ìœ ì§€
            "logs": [f"âš ï¸ [Ontology] ë©”íƒ€ë°ì´í„° íŒë‹¨ ë¶ˆí™•ì‹¤ ({confidence:.2%})"]
        }
    
    # === Step 4: Branching (í™•ì‹ ë„ ë†’ìŒ) ===
    
    # [Branch A] ë©”íƒ€ë°ì´í„° íŒŒì¼
    if is_metadata:
        print(f"\nğŸ“– [Metadata] ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        # íŒŒì¼ íƒœê·¸ ì €ì¥
        ontology["file_tags"][file_path] = {
            "type": "metadata",
            "role": "dictionary",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat()
        }
        
        # ë‚´ìš© íŒŒì‹± (Rule)
        print(f"   - ë©”íƒ€ë°ì´í„° íŒŒì‹± ì¤‘...")
        new_definitions = _parse_metadata_content(file_path)
        ontology["definitions"].update(new_definitions)
        
        print(f"   - ìš©ì–´ {len(new_definitions)}ê°œ ì¶”ê°€")
        print(f"   - ì´ ìš©ì–´: {len(ontology['definitions'])}ê°œ")
        
        # ì˜¨í†¨ë¡œì§€ ì €ì¥ (ì˜êµ¬ ë³´ì¡´)
        print(f"   - ì˜¨í†¨ë¡œì§€ ì €ì¥ ì¤‘...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": True,  # ì¤‘ìš”! ë©”íƒ€ë°ì´í„°ëŠ” ì¸ë±ì‹± ìŠ¤í‚µ
            "logs": [f"ğŸ“š [Ontology] ë©”íƒ€ë°ì´í„° ë“±ë¡: {len(new_definitions)}ê°œ ìš©ì–´ ì¶”ê°€ (ì €ì¥ ì™„ë£Œ)"]
        }
    
    # [Branch B] ì¼ë°˜ ë°ì´í„° íŒŒì¼
    else:
        print(f"\nğŸ“Š [Data] ì¼ë°˜ ë°ì´í„° íŒŒì¼ë¡œ í™•ì •")
        
        # ì»¬ëŸ¼ ì •ë³´ ì €ì¥ (ê´€ê³„ ì¶”ë¡ ì— í•„ìš”)
        columns = metadata.get("columns", [])
        
        ontology["file_tags"][file_path] = {
            "type": "transactional_data",
            "confidence": confidence,
            "detected_at": datetime.now().isoformat(),
            "columns": columns  # [NEW] ì»¬ëŸ¼ ì €ì¥
        }
        
        # === Phase 2: ê´€ê³„ ì¶”ë¡  (ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆì„ ë•Œë§Œ) ===
        existing_data_files = [
            fp for fp, tag in ontology.get("file_tags", {}).items()
            if tag.get("type") == "transactional_data" and fp != file_path
        ]
        
        if existing_data_files:
            print(f"\nğŸ”— [Relationship] ê´€ê³„ ì¶”ë¡  ì‹œì‘...")
            print(f"   - ê¸°ì¡´ ë°ì´í„° íŒŒì¼: {len(existing_data_files)}ê°œ")
            
            # ê´€ê³„ ì¶”ë¡  (LLM)
            table_name = os.path.basename(file_path).replace(".csv", "").replace(".", "_")
            
            relationship_result = _infer_relationships_with_llm(
                current_table_name=table_name,
                current_cols=columns,
                ontology_context=ontology,
                current_metadata=metadata
            )
            
            # ê´€ê³„ ì¶”ê°€
            new_relationships = relationship_result.get("relationships", [])
            if new_relationships:
                print(f"   - ê´€ê³„ {len(new_relationships)}ê°œ ë°œê²¬")
                
                # ê¸°ì¡´ ê´€ê³„ì™€ ë³‘í•©
                existing_rels = ontology.get("relationships", [])
                
                # ì¤‘ë³µ ì²´í¬
                existing_keys = {
                    (r["source_table"], r["target_table"], r["source_column"], r["target_column"])
                    for r in existing_rels
                }
                
                for new_rel in new_relationships:
                    key = (new_rel["source_table"], new_rel["target_table"], 
                           new_rel["source_column"], new_rel["target_column"])
                    if key not in existing_keys:
                        ontology["relationships"].append(new_rel)
                        print(f"      â€¢ {new_rel['source_table']}.{new_rel['source_column']} "
                              f"â†’ {new_rel['target_table']}.{new_rel['target_column']} "
                              f"({new_rel['relation_type']}, conf: {new_rel.get('confidence', 0):.2%})")
            
            # ê³„ì¸µ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì œê±° ê°•í™”)
            new_hierarchy = relationship_result.get("hierarchy", [])
            if new_hierarchy:
                print(f"   - ê³„ì¸µ ì •ë³´ ì—…ë°ì´íŠ¸")
                
                # ê¸°ì¡´ ê³„ì¸µ
                existing_hier = ontology.get("hierarchy", [])
                
                # ì¤‘ë³µ ì œê±° ì „ëµ: (level, anchor_column) ì¡°í•©ìœ¼ë¡œ íŒë‹¨
                merged_hierarchy = {}  # key: (level, anchor), value: hierarchy_dict
                
                # ê¸°ì¡´ ê³„ì¸µ ë¨¼ì € ì¶”ê°€
                for h in existing_hier:
                    key = (h.get("level"), h.get("anchor_column"))
                    merged_hierarchy[key] = h
                
                # ìƒˆ ê³„ì¸µ ë³‘í•© (confidence ë†’ì€ ê²ƒ ìš°ì„ )
                for new_h in new_hierarchy:
                    key = (new_h.get("level"), new_h.get("anchor_column"))
                    
                    if key not in merged_hierarchy:
                        # ìƒˆë¡œìš´ (level, anchor) ì¡°í•©
                        merged_hierarchy[key] = new_h
                        print(f"      â€¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [NEW]")
                    else:
                        # ì´ë¯¸ ìˆëŠ” ì¡°í•© - confidence ë¹„êµ
                        existing_conf = merged_hierarchy[key].get("confidence", 0)
                        new_conf = new_h.get("confidence", 0)
                        
                        if new_conf > existing_conf:
                            merged_hierarchy[key] = new_h
                            print(f"      â€¢ L{new_h['level']}: {new_h['entity_name']} ({new_h['anchor_column']}) [UPDATED, conf: {new_conf:.2%}]")
                        else:
                            print(f"      â€¢ L{new_h['level']}: (ì¤‘ë³µ ìŠ¤í‚µ, ê¸°ì¡´ confidence {existing_conf:.2%} ìœ ì§€)")
                
                # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë ˆë²¨ ì •ë ¬
                ontology["hierarchy"] = sorted(merged_hierarchy.values(), key=lambda x: x.get("level", 99))
        else:
            print(f"\n   - ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì—†ìŒ. ê´€ê³„ ì¶”ë¡  ìŠ¤í‚µ.")
        
        # ì˜¨í†¨ë¡œì§€ ì €ì¥
        print(f"   - ì˜¨í†¨ë¡œì§€ ì €ì¥ ì¤‘...")
        ontology_manager.save(ontology)
        
        print("="*80)
        
        return {
            "ontology_context": ontology,
            "skip_indexing": False,  # ì¼ë°˜ ë°ì´í„°ëŠ” ì¸ë±ì‹± ê³„ì†
            "logs": ["ğŸ” [Ontology] ì¼ë°˜ ë°ì´í„° í™•ì¸. ê´€ê³„ ì¶”ë¡  ì™„ë£Œ."]
        }