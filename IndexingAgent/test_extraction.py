#!/usr/bin/env python3
"""
test_extraction.py - ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸

ì›Œí¬í”Œë¡œìš°:
1. ìì—°ì–´ ì¿¼ë¦¬ ì…ë ¥
2. LLMìœ¼ë¡œ í•µì‹¬ ê°œë…/í‚¤ì›Œë“œ ì¶”ì¶œ
3. Neo4j ì˜¨í†¨ë¡œì§€ì—ì„œ ê´€ë ¨ Parameter ê²€ìƒ‰
4. PostgreSQLì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì´ ìˆëŠ” íŒŒì¼ ì •ë³´ ì¡°íšŒ
5. ê²°ê³¼ ì¶œë ¥ (file_id, column list)
"""

import sys
import json
from typing import List, Dict, Any

# ============================================================================
# Imports
# ============================================================================
from src.utils.llm_client import get_llm_client
from src.database.connection import get_db_manager
from src.database.neo4j_connection import get_neo4j_connection


# ============================================================================
# Step 1: LLMìœ¼ë¡œ í•µì‹¬ ê°œë… ì¶”ì¶œ
# ============================================================================
KEYWORD_EXTRACTION_PROMPT = """
ë‹¹ì‹ ì€ ì˜ë£Œ ë°ì´í„° ê²€ìƒ‰ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰í•´ì•¼ í•  í•µì‹¬ ê°œë…/í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ì¿¼ë¦¬
{query}

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
  "concepts": ["ê°œë…1", "ê°œë…2", ...],
  "english_terms": ["term1", "term2", ...],
  "korean_terms": ["ìš©ì–´1", "ìš©ì–´2", ...],
  "data_types": ["numerical", "categorical", "time_series"],
  "intent": "ì¿¼ë¦¬ì˜ ëª©ì  í•œ ë¬¸ì¥ ìš”ì•½"
}}

ì˜ˆì‹œ:
- "í™˜ì ë‚˜ì´ì™€ í˜ˆì••" â†’ concepts: ["age", "blood pressure"], korean_terms: ["ë‚˜ì´", "í˜ˆì••"]
- "ìˆ˜ìˆ  ì¤‘ ì‹¬ë°•ìˆ˜ ë³€í™”" â†’ concepts: ["heart rate", "intraoperative"], data_types: ["time_series"]

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
"""


def extract_keywords_with_llm(query: str) -> Dict[str, Any]:
    """LLMìœ¼ë¡œ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    print("\n" + "="*60)
    print("ğŸ“ Step 1: LLM í‚¤ì›Œë“œ ì¶”ì¶œ")
    print("="*60)
    
    llm = get_llm_client()
    prompt = KEYWORD_EXTRACTION_PROMPT.format(query=query)
    
    result = llm.ask_json(prompt)
    
    print(f"ì¶”ì¶œëœ ê°œë…: {result.get('concepts', [])}")
    print(f"ì˜ì–´ ìš©ì–´: {result.get('english_terms', [])}")
    print(f"í•œêµ­ì–´ ìš©ì–´: {result.get('korean_terms', [])}")
    print(f"ì˜ë„: {result.get('intent', 'N/A')}")
    
    return result


# ============================================================================
# Step 2: Neo4j ì˜¨í†¨ë¡œì§€ ê²€ìƒ‰
# ============================================================================
def search_ontology(keywords: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Neo4jì—ì„œ ê´€ë ¨ Parameter ë…¸ë“œ ê²€ìƒ‰"""
    print("\n" + "="*60)
    print("ğŸ” Step 2: Neo4j ì˜¨í†¨ë¡œì§€ ê²€ìƒ‰")
    print("="*60)
    
    neo4j = get_neo4j_connection()
    
    # ëª¨ë“  ê²€ìƒ‰ì–´ í•©ì¹˜ê¸°
    all_terms = []
    all_terms.extend(keywords.get('concepts', []))
    all_terms.extend(keywords.get('english_terms', []))
    all_terms.extend(keywords.get('korean_terms', []))
    all_terms = list(set([t.lower() for t in all_terms if t]))
    
    print(f"ê²€ìƒ‰ì–´: {all_terms}")
    
    if not all_terms:
        print("âš ï¸  ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # Cypher ì¿¼ë¦¬: Parameter ë…¸ë“œì—ì„œ ìœ ì‚¬í•œ ê²ƒ ê²€ìƒ‰
    # Neo4j ì‹¤ì œ ìŠ¤í‚¤ë§ˆ: name, key, concept, unit, file_id
    # ê¸´ ê²€ìƒ‰ì–´(3ì ì´ìƒ)ëŠ” CONTAINS, ì§§ì€ ê²€ìƒ‰ì–´ëŠ” ì •í™• ë§¤ì¹­
    cypher_query = """
    MATCH (p:Parameter)
    WHERE ANY(term IN $long_terms WHERE 
        toLower(p.name) CONTAINS term OR 
        toLower(p.key) CONTAINS term OR
        toLower(p.concept) CONTAINS term
    )
    OR ANY(term IN $short_terms WHERE 
        toLower(p.key) = term
    )
    OPTIONAL MATCH (p)-[:HAS_CONCEPT]->(c:ConceptCategory)
    OPTIONAL MATCH (p)-[:RELATED_TO|RELATED_CONCEPT]-(related:Parameter)
    RETURN 
        p.name as parameter_name,
        p.key as parameter_key,
        p.concept as concept,
        p.unit as unit,
        p.file_id as file_id,
        c.name as category,
        collect(DISTINCT related.name)[0..3] as related_params
    LIMIT 20
    """
    
    # ê²€ìƒ‰ì–´ë¥¼ ê¸¸ì´ì— ë”°ë¼ ë¶„ë¥˜ (ì§§ì€ ê±´ ì •í™• ë§¤ì¹­, ê¸´ ê±´ CONTAINS)
    short_terms = [t for t in all_terms if len(t) <= 3]
    long_terms = [t for t in all_terms if len(t) > 3]
    
    print(f"  - ê¸´ ê²€ìƒ‰ì–´ (ë¶€ë¶„ë§¤ì¹­): {long_terms}")
    print(f"  - ì§§ì€ ê²€ìƒ‰ì–´ (ì •í™•ë§¤ì¹­): {short_terms}")
    
    results = neo4j.execute_query(cypher_query, {
        "long_terms": long_terms,
        "short_terms": short_terms
    })
    
    found_params = []
    for record in results:
        param_info = {
            "parameter_name": record["parameter_name"],
            "parameter_key": record["parameter_key"],
            "concept": record["concept"],
            "unit": record["unit"],
            "file_id": record["file_id"],
            "category": record["category"],
            "related_params": record["related_params"]
        }
        found_params.append(param_info)
        print(f"  âœ… {record['parameter_name']}")
        print(f"     - Key: {record['parameter_key']}")
        print(f"     - Concept: {record['concept']}")
        if record['unit']:
            print(f"     - Unit: {record['unit']}")
        if record['related_params']:
            print(f"     - Related: {record['related_params']}")
    
    if not found_params:
        print("âš ï¸  Neo4jì—ì„œ ë§¤ì¹­ë˜ëŠ” Parameterë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        # Fallback: ëª¨ë“  Parameter ì¶œë ¥
        print("\nğŸ“‹ ë“±ë¡ëœ Parameter ëª©ë¡ (ìƒìœ„ 20ê°œ):")
        all_params = neo4j.execute_query(
            "MATCH (p:Parameter) RETURN p.name as name, p.concept as concept LIMIT 20"
        )
        for p in all_params:
            print(f"  - {p['name']} ({p['concept']})")
    
    return found_params


# ============================================================================
# Step 3: PostgreSQLì—ì„œ íŒŒì¼/ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
# ============================================================================
def get_file_column_info(parameters: List[Dict[str, Any]]) -> Dict[str, Any]:
    """PostgreSQLì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì´ ìˆëŠ” íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
    print("\n" + "="*60)
    print("ğŸ—„ï¸  Step 3: PostgreSQL íŒŒì¼/ì»¬ëŸ¼ ì¡°íšŒ")
    print("="*60)
    
    if not parameters:
        print("âš ï¸  ê²€ìƒ‰í•  íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"files": [], "columns": []}
    
    conn = get_db_manager().get_connection()
    cur = conn.cursor()
    
    # ì°¾ì€ parameterì˜ keyì™€ name ìˆ˜ì§‘
    param_keys = [p.get("parameter_key") for p in parameters if p.get("parameter_key")]
    param_names = [p.get("parameter_name") for p in parameters if p.get("parameter_name")]
    
    # ëª¨ë“  ê²€ìƒ‰ ëŒ€ìƒ
    all_search_terms = list(set(param_keys + param_names))
    print(f"ê²€ìƒ‰ ëŒ€ìƒ: {all_search_terms}")
    
    # column_metadataì—ì„œ ì¡°íšŒ
    query = """
    SELECT 
        cm.col_id,
        cm.file_id,
        cm.original_name,
        cm.semantic_name,
        cm.description,
        cm.unit,
        cm.concept_category,
        fc.file_name,
        fc.file_path
    FROM column_metadata cm
    JOIN file_catalog fc ON cm.file_id = fc.file_id
    WHERE cm.original_name = ANY(%s)
       OR cm.semantic_name ILIKE ANY(%s)
       OR cm.original_name ILIKE ANY(%s)
    ORDER BY fc.file_name, cm.original_name
    """
    
    # ILIKE íŒ¨í„´ ìƒì„±
    like_patterns = [f"%{name}%" for name in all_search_terms]
    
    try:
        cur.execute(query, (all_search_terms, like_patterns, like_patterns))
        rows = cur.fetchall()
        
        files = {}
        columns = []
        
        for row in rows:
            col_id, file_id, original_name, semantic_name, desc, unit, category, file_name, file_path = row
            
            # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í•‘
            if file_id not in files:
                files[file_id] = {
                    "file_id": str(file_id),
                    "file_name": file_name,
                    "file_path": file_path,
                    "columns": []
                }
            
            col_info = {
                "col_id": str(col_id),
                "original_name": original_name,
                "semantic_name": semantic_name,
                "description": desc,
                "unit": unit,
                "category": category
            }
            files[file_id]["columns"].append(col_info)
            columns.append(col_info)
        
        # ê²°ê³¼ ì¶œë ¥
        if files:
            print(f"\nğŸ“ ê´€ë ¨ íŒŒì¼ {len(files)}ê°œ ë°œê²¬:")
            for file_id, file_info in files.items():
                print(f"\n  ğŸ“„ {file_info['file_name']}")
                print(f"     File ID: {file_info['file_id']}")
                print(f"     Path: {file_info['file_path']}")
                print(f"     ê´€ë ¨ ì»¬ëŸ¼ {len(file_info['columns'])}ê°œ:")
                for col in file_info['columns']:
                    print(f"       - {col['original_name']}: {col['semantic_name'] or 'N/A'}")
                    if col['unit']:
                        print(f"         Unit: {col['unit']}")
        else:
            print("âš ï¸  PostgreSQLì—ì„œ ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "files": list(files.values()),
            "columns": columns,
            "total_files": len(files),
            "total_columns": len(columns)
        }
        
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
        conn.rollback()
        return {"files": [], "columns": [], "error": str(e)}


# ============================================================================
# Step 4: ìµœì¢… ê²°ê³¼ ì •ë¦¬
# ============================================================================
def summarize_results(
    query: str, 
    keywords: Dict[str, Any], 
    ontology_results: List[Dict[str, Any]], 
    db_results: Dict[str, Any]
) -> Dict[str, Any]:
    """ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    # ì˜¨í†¨ë¡œì§€ì—ì„œ ì°¾ì€ íŒŒë¼ë¯¸í„° ì •ë³´
    ontology_params = [
        {
            "name": p.get("parameter_name"),
            "key": p.get("parameter_key"),
            "concept": p.get("concept"),
            "unit": p.get("unit")
        }
        for p in ontology_results
    ]
    
    result = {
        "query": query,
        "intent": keywords.get("intent", ""),
        "extracted_concepts": keywords.get("concepts", []),
        "ontology_matches": len(ontology_results),
        "ontology_params": ontology_params,
        "files_found": db_results.get("total_files", 0),
        "columns_found": db_results.get("total_columns", 0),
        "file_ids": [f["file_id"] for f in db_results.get("files", [])],
        "files": db_results.get("files", []),
        "column_list": [
            {
                "name": c["original_name"],
                "semantic": c["semantic_name"],
                "unit": c["unit"]
            }
            for c in db_results.get("columns", [])
        ]
    }
    
    print(f"\nğŸ” ì¿¼ë¦¬: \"{query}\"")
    print(f"ğŸ“ ì˜ë„: {result['intent']}")
    print(f"ğŸ·ï¸  ì¶”ì¶œëœ ê°œë…: {result['extracted_concepts']}")
    
    print(f"\nğŸ“ˆ ê²€ìƒ‰ ê²°ê³¼:")
    print(f"   - ì˜¨í†¨ë¡œì§€ ë§¤ì¹­: {result['ontology_matches']}ê°œ")
    print(f"   - ê´€ë ¨ íŒŒì¼: {result['files_found']}ê°œ")
    print(f"   - ê´€ë ¨ ì»¬ëŸ¼: {result['columns_found']}ê°œ")
    
    if ontology_params:
        print(f"\nğŸ§¬ ì˜¨í†¨ë¡œì§€ íŒŒë¼ë¯¸í„°:")
        for p in ontology_params:
            unit_str = f" ({p['unit']})" if p.get('unit') else ""
            print(f"   - {p['name']}{unit_str}")
            if p.get('concept'):
                print(f"     Concept: {p['concept']}")
    
    if result['file_ids']:
        print(f"\nğŸ†” í•„ìš”í•œ File IDs:")
        for fid in result['file_ids']:
            print(f"   - {fid}")
    
    if result['column_list']:
        print(f"\nğŸ“‹ í™•ì¸í•  ì»¬ëŸ¼ ëª©ë¡:")
        for col in result['column_list']:
            unit_str = f" ({col['unit']})" if col['unit'] else ""
            print(f"   - {col['name']}: {col['semantic'] or 'N/A'}{unit_str}")
    
    return result


# ============================================================================
# Main
# ============================================================================
def run_extraction(query: str) -> Dict[str, Any]:
    """ì „ì²´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("\n" + "="*60)
    print("ğŸš€ Extraction Pipeline ì‹œì‘")
    print("="*60)
    print(f"ì…ë ¥ ì¿¼ë¦¬: \"{query}\"")
    
    # Step 1: LLM í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords_with_llm(query)
    
    # Step 2: Neo4j ì˜¨í†¨ë¡œì§€ ê²€ìƒ‰
    ontology_results = search_ontology(keywords)
    
    # Step 3: PostgreSQL ì¡°íšŒ
    db_results = get_file_column_info(ontology_results)
    
    # Step 4: ê²°ê³¼ ì •ë¦¬
    final_result = summarize_results(query, keywords, ontology_results, db_results)
    
    print("\n" + "="*60)
    print("âœ… Extraction Pipeline ì™„ë£Œ")
    print("="*60)
    
    return final_result


# ============================================================================
# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
# ============================================================================
if __name__ == "__main__":
    # ì˜ˆì‹œ ì¿¼ë¦¬ë“¤
    EXAMPLE_QUERIES = [
        "í™˜ìì˜ ë‚˜ì´ì™€ ì‹¬ë°•ìˆ˜ ë°ì´í„°ë¥¼ ì°¾ì•„ì¤˜",
        "ìˆ˜ìˆ  ì¤‘ í˜ˆì•• ë³€í™”ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ì–´",
        "Find patient age and heart rate data",
        "ë§ˆì·¨ ì¤‘ í™œë ¥ì§•í›„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°",
    ]
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ì¿¼ë¦¬ ë°›ê¸°
    if len(sys.argv) > 1:
        user_query = " ".join(sys.argv[1:])
    else:
        # ê¸°ë³¸ ì˜ˆì‹œ ì‚¬ìš©
        user_query = EXAMPLE_QUERIES[0]
        print(f"\nğŸ’¡ ê¸°ë³¸ ì˜ˆì‹œ ì¿¼ë¦¬ ì‚¬ìš©: \"{user_query}\"")
        print(f"   ë‹¤ë¥¸ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: python test_extraction.py \"ì¿¼ë¦¬ ë‚´ìš©\"")
    
    # ì‹¤í–‰
    result = run_extraction(user_query)
    
    # JSON ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“„ JSON ê²°ê³¼")
    print("="*60)
    print(json.dumps(result, indent=2, ensure_ascii=False))

