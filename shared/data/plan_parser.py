# shared/data/plan_parser.py
"""
PlanParser - Execution Plan JSON íŒŒì‹± ëª¨ë“ˆ

ì—­í• :
1. ExtractionAgentì˜ execution_plan JSON í•´ì„
2. Cohort/Signal/Join ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
3. DBì—ì„œ íŒŒì¼ ê²½ë¡œ resolve

DataContextì—ì„œ íŒŒì‹± ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜
"""

import logging
from typing import Dict, Any, List, Optional

from shared.database.connection import get_db_manager
from shared.models.plan import (
    CohortMetadata,
    SignalMetadata,
    JoinConfig,
    ParsedPlan,
)
from shared.utils import lazy_property

logger = logging.getLogger(__name__)


class PlanParser:
    """
    Execution Plan JSON íŒŒì„œ
    
    ExtractionAgentê°€ ìƒì„±í•œ planì„ íŒŒì‹±í•˜ê³ ,
    DBì—ì„œ í•„ìš”í•œ ì •ë³´(íŒŒì¼ ê²½ë¡œ ë“±)ë¥¼ resolveí•©ë‹ˆë‹¤.
    
    Usage:
        parser = PlanParser()
        parsed = parser.parse(execution_plan)
        
        print(parsed.cohort.file_path)
        print(parsed.signal.param_keys)
    """
    
    def __init__(self, db_manager=None):
        """
        Args:
            db_manager: DB ë§¤ë‹ˆì € (Noneì´ë©´ lazy loading)
        """
        self._db = db_manager
    
    @lazy_property
    def db(self):
        """Lazy DB connection"""
        return get_db_manager()
    
    def parse(self, execution_plan: Dict[str, Any], resolve_paths: bool = True) -> ParsedPlan:
        """
        Execution Plan íŒŒì‹±
        
        Args:
            execution_plan: ExtractionAgentê°€ ìƒì„±í•œ plan JSON
            resolve_paths: DBì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ resolveí• ì§€ ì—¬ë¶€
        
        Returns:
            ParsedPlan ê°ì²´
        """
        plan = execution_plan.get("execution_plan", {})
        
        # 1. Cohort ë©”íƒ€ë°ì´í„° íŒŒì‹±
        cohort = self._parse_cohort(plan.get("cohort_source", {}), resolve_paths)
        
        # 2. Signal ë©”íƒ€ë°ì´í„° íŒŒì‹±
        signal = self._parse_signal(plan.get("signal_source", {}), resolve_paths)
        
        # 3. Join ì„¤ì • íŒŒì‹±
        join = self._parse_join(
            plan.get("join_specification", {}),
            cohort.entity_identifier,
            signal.entity_identifier_key
        )
        
        # 4. ì›ë³¸ ì¿¼ë¦¬
        original_query = execution_plan.get("original_query")
        
        return ParsedPlan(
            raw_plan=execution_plan,
            cohort=cohort,
            signal=signal,
            join=join,
            original_query=original_query
        )
    
    def _parse_cohort(self, cohort_source: Dict[str, Any], resolve_paths: bool) -> CohortMetadata:
        """Cohort ì†ŒìŠ¤ íŒŒì‹±"""
        if not cohort_source:
            return CohortMetadata()
        
        file_id = cohort_source.get("file_id")
        file_path = None
        
        # DBì—ì„œ íŒŒì¼ ê²½ë¡œ resolve
        if resolve_paths and file_id:
            file_path = self._resolve_file_path(file_id)
        
        return CohortMetadata(
            file_id=file_id,
            file_path=file_path,
            file_name=cohort_source.get("file_name"),
            entity_identifier=cohort_source.get("entity_identifier"),
            row_represents=cohort_source.get("row_represents"),
            filters=cohort_source.get("filters", [])
        )
    
    def _parse_signal(self, signal_source: Dict[str, Any], resolve_paths: bool) -> SignalMetadata:
        """Signal ì†ŒìŠ¤ íŒŒì‹±"""
        if not signal_source:
            return SignalMetadata()
        
        group_id = signal_source.get("group_id")
        files = []
        
        # DBì—ì„œ signal íŒŒì¼ë“¤ resolve
        if resolve_paths and group_id:
            files = self._resolve_signal_files(group_id)
        
        # Parameters íŒŒì‹±
        parameters = signal_source.get("parameters", [])
        param_keys = []
        for p in parameters:
            param_keys.extend(p.get("param_keys", []))
        
        return SignalMetadata(
            group_id=group_id,
            group_name=signal_source.get("group_name"),
            entity_identifier_key=signal_source.get("entity_identifier_key"),
            row_represents=signal_source.get("row_represents"),
            files=files,
            param_keys=param_keys,
            param_info=parameters,
            temporal_config=signal_source.get("temporal_alignment", {})
        )
    
    def _parse_join(
        self, 
        join_spec: Optional[Dict[str, Any]],
        cohort_entity_id: Optional[str],
        signal_entity_id_key: Optional[str]
    ) -> JoinConfig:
        """Join ì„¤ì • íŒŒì‹±"""
        # join_specì´ Noneì¸ ê²½ìš° ë¹ˆ dictë¡œ ëŒ€ì²´
        join_spec = join_spec or {}
        
        # Planì—ì„œ ì œê³µëœ í‚¤ ì‚¬ìš©, ì—†ìœ¼ë©´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
        default_key = signal_entity_id_key or cohort_entity_id or "caseid"
        
        return JoinConfig(
            cohort_key=join_spec.get("cohort_key") or cohort_entity_id or default_key,
            signal_key=join_spec.get("signal_key") or signal_entity_id_key or default_key,
            join_type=join_spec.get("type", "inner")
        )
    
    def _resolve_file_path(self, file_id: str) -> Optional[str]:
        """DBì—ì„œ íŒŒì¼ ê²½ë¡œ ì¡°íšŒ"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            cursor.execute(
                "SELECT file_path FROM file_catalog WHERE file_id = %s",
                (file_id,)
            )
            row = cursor.fetchone()
            conn.commit()
            
            if row:
                return row[0]
            
            logger.warning(f"File not found in DB: {file_id}")
            return None
            
        except Exception as e:
            logger.error(f"Error resolving file path: {e}")
            return None
    
    def _resolve_signal_files(self, group_id: str) -> List[Dict[str, Any]]:
        """DBì—ì„œ Signal ê·¸ë£¹ì˜ íŒŒì¼ë“¤ ì¡°íšŒ"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            
            # file_group í…Œì´ë¸”ì—ì„œ entity_identifier_key ì¡°íšŒ
            cursor.execute("""
                SELECT entity_identifier_key 
                FROM file_group 
                WHERE group_id = %s
            """, (group_id,))
            
            group_row = cursor.fetchone()
            # entity_identifier_key ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: caseid)
            entity_key_db = group_row[0] if group_row else "caseid"
            
            # filename_valuesì—ì„œ ì‚¬ìš©í•  í‚¤ í›„ë³´ë“¤ ìƒì„±
            # DBì—ëŠ” case_id, filename_valuesì—ëŠ” caseidë¡œ ì €ì¥ë  ìˆ˜ ìˆìŒ
            entity_key_candidates = [
                entity_key_db,                      # ì›ë³¸ (case_id ë˜ëŠ” caseid)
                entity_key_db.replace("_", ""),     # underscore ì œê±° (caseid)
                entity_key_db.lower(),              # ì†Œë¬¸ì ë³€í™˜
            ]
            # ì¤‘ë³µ ì œê±°
            entity_key_candidates = list(dict.fromkeys(entity_key_candidates))
            
            # file_catalogì—ì„œ í•´ë‹¹ ê·¸ë£¹ì˜ íŒŒì¼ë“¤ ì¡°íšŒ
            # filename_values JSONBì—ì„œ entity ê°’ì„ ì¶”ì¶œ
            cursor.execute("""
                SELECT file_id, file_path, filename_values
                FROM file_catalog
                WHERE group_id = %s
            """, (group_id,))
            
            rows = cursor.fetchall()
            conn.commit()
            
            files = []
            for row in rows:
                file_id, file_path, filename_values = row
                # filename_values JSONBì—ì„œ entity ê°’ ì¶”ì¶œ
                entity_value = None
                matched_key = None
                if filename_values:
                    # í›„ë³´ í‚¤ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„
                    for key in entity_key_candidates:
                        if key in filename_values:
                            entity_value = filename_values[key]
                            matched_key = key
                            break
                
                if entity_value is not None:
                    files.append({
                        "file_id": str(file_id),
                        "file_path": file_path,
                        "entity_id": str(entity_value),  # DataContext.get_case_ids()ê°€ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ í‚¤
                        matched_key: entity_value  # ì‹¤ì œ ë§¤ì¹­ëœ í‚¤ë„ ìœ ì§€
                    })
            
            logger.info(f"ğŸ“ Resolved {len(files)} signal files for group {group_id}")
            return files
            
        except Exception as e:
            logger.error(f"Error resolving signal files: {e}")
            return []
