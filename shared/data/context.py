# shared/data/context.py
"""
DataContext - Execution Plan ê¸°ë°˜ ë°ì´í„° ë¡œë“œ ë° ê´€ë¦¬

ì—­í• :
1. ExtractionAgentì˜ execution_plan JSON í•´ì„ â†’ PlanParser ìœ„ì„
2. DBì—ì„œ íŒŒì¼ ê²½ë¡œ resolve
3. Processorë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ
4. ìºì‹± (í´ë˜ìŠ¤ ë ˆë²¨, ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
5. AnalysisAgentë¥¼ ìœ„í•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ì œê³µ â†’ AnalysisContextBuilder ìœ„ì„

ì‚¬ìš© ì˜ˆì‹œ:
    ctx = DataContext()
    ctx.load_from_plan(execution_plan)
    
    cohort = ctx.get_cohort()
    signals = ctx.get_signals(caseid="1234")
    merged = ctx.get_merged_data()
    
    # AnalysisAgentìš©
    analysis_ctx = ctx.get_analysis_context()
    stats = ctx.compute_statistics()
"""

import sys
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Iterator, Tuple
from datetime import datetime
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from shared.processors import SignalProcessor, TabularProcessor
from shared.database.connection import get_db_manager
from shared.data.plan_parser import PlanParser
from shared.data.analysis_context import AnalysisContextBuilder
from shared.models.plan import ParsedPlan
from shared.utils import lazy_property

logger = logging.getLogger(__name__)


class DataContext:
    """
    Execution Plan ê¸°ë°˜ ë°ì´í„° ë¡œë“œ ë° ê´€ë¦¬
    
    íŠ¹ì§•:
    - í´ë˜ìŠ¤ ë ˆë²¨ ìºì‹œ: ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ signal/cohort ë°ì´í„° ê³µìœ 
    - Lazy Loading: ìš”ì²­ ì‹œì—ë§Œ ë°ì´í„° ë¡œë“œ
    - Temporal Filter: procedure_window ë“± ìë™ ì ìš©
    - AnalysisAgent ì§€ì›: LLMìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Class-level Cache (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    _signal_cache: Dict[str, pd.DataFrame] = {}   # caseid â†’ signals DataFrame
    _cohort_cache: Dict[str, pd.DataFrame] = {}   # file_id â†’ cohort DataFrame
    
    def __init__(self):
        """DataContext ì´ˆê¸°í™”"""
        # Instance state
        self._plan: Optional[Dict[str, Any]] = None
        self._parsed_plan: Optional[ParsedPlan] = None  # PlanParser ê²°ê³¼
        self._loaded_at: Optional[datetime] = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Metadata from Execution Plan (ExtractionAgentê°€ DBì—ì„œ ì¡°íšŒí•œ ì •ë³´)
        # _parsed_planì—ì„œ ë³µì‚¬ë˜ì–´ ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Cohort metadata
        self._cohort_file_id: Optional[str] = None
        self._cohort_file_path: Optional[str] = None
        self._cohort_file_name: Optional[str] = None
        self._cohort_entity_id: Optional[str] = None  # entity_identifier (ì˜ˆ: "caseid")
        self._cohort_row_represents: Optional[str] = None  # í–‰ì´ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ (ì˜ˆ: "surgical_case")
        self._cohort_filters: List[Dict[str, Any]] = []
        
        # Signal metadata
        self._signal_group_id: Optional[str] = None
        self._signal_group_name: Optional[str] = None  # ê·¸ë£¹ëª… (ì˜ˆ: "vital_signals_by_case")
        self._signal_entity_id_key: Optional[str] = None  # entity_identifier_key (ì˜ˆ: "caseid")
        self._signal_row_represents: Optional[str] = None  # í–‰ì´ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ
        self._signal_files: List[Dict[str, Any]] = []  # [{file_id, file_path, caseid}, ...]
        self._param_keys: List[str] = []
        self._param_info: List[Dict[str, Any]] = []  # [{term, param_key, semantic_name, unit}, ...]
        self._temporal_config: Dict[str, Any] = {}
        
        # Join configuration
        self._join_config: Dict[str, Any] = {}
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Processors & Helpers
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._signal_processor = SignalProcessor()
        self._tabular_processor = TabularProcessor()
        self._plan_parser: Optional[PlanParser] = None
        self._analysis_builder: Optional[AnalysisContextBuilder] = None
        
        # DB
        self._db = None
    
    @lazy_property
    def db(self):
        """Lazy DB connection"""
        return get_db_manager()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Metadata Properties (Execution Planì—ì„œ ì¶”ì¶œí•œ ì •ë³´)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @property
    def entity_id_column(self) -> Optional[str]:
        """
        ì£¼ìš” ì—”í‹°í‹° ì‹ë³„ì ì»¬ëŸ¼ëª… ë°˜í™˜
        
        Signalì˜ entity_identifier_keyë¥¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ cohortì˜ entity_identifier
        
        Returns:
            ì‹ë³„ì ì»¬ëŸ¼ëª… (ì˜ˆ: "caseid", "subject_id") ë˜ëŠ” None
        """
        return self._signal_entity_id_key or self._cohort_entity_id
    
    @property
    def cohort_entity_column(self) -> Optional[str]:
        """Cohort í…Œì´ë¸”ì˜ ì—”í‹°í‹° ì‹ë³„ì ì»¬ëŸ¼ëª…"""
        return self._cohort_entity_id
    
    @property
    def signal_entity_column(self) -> Optional[str]:
        """Signal íŒŒì¼ì˜ ì—”í‹°í‹° ì‹ë³„ì í‚¤"""
        return self._signal_entity_id_key
    
    @property
    def join_keys(self) -> Dict[str, str]:
        """
        Joinì— ì‚¬ìš©í•  í‚¤ ë°˜í™˜
        
        Returns:
            {"cohort_key": "caseid", "signal_key": "caseid"}
        """
        return {
            "cohort_key": self._join_config.get("cohort_key"),
            "signal_key": self._join_config.get("signal_key"),
        }
    
    def get_plan_metadata(self) -> Dict[str, Any]:
        """
        Execution Planì—ì„œ ì¶”ì¶œí•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜
        
        ë™ì  ê°€ì´ë“œ ìƒì„±, íŒíŠ¸ ìƒì„± ë“±ì—ì„œ í™œìš©
        
        Returns:
            {
                "entity_id_column": "caseid",
                "cohort": {"file_name": "clinical_data.csv", "entity_identifier": "caseid", ...},
                "signal": {"group_name": "vital_signals_by_case", "entity_identifier_key": "caseid", ...},
                "join": {"cohort_key": "caseid", "signal_key": "caseid"},
                "parameters": [...]
            }
        """
        return {
            "entity_id_column": self.entity_id_column,
            "cohort": {
                "file_id": self._cohort_file_id,
                "file_name": self._cohort_file_name,
                "file_path": self._cohort_file_path,
                "entity_identifier": self._cohort_entity_id,
                "row_represents": self._cohort_row_represents,
            },
            "signal": {
                "group_id": self._signal_group_id,
                "group_name": self._signal_group_name,
                "entity_identifier_key": self._signal_entity_id_key,
                "row_represents": self._signal_row_represents,
                "file_count": len(self._signal_files),
            },
            "join": self._join_config,
            "parameters": self._param_info,
            "param_keys": self._param_keys,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Main Interface
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def load_from_plan(
        self, 
        execution_plan: Dict[str, Any],
        preload_cohort: bool = True
    ) -> "DataContext":
        """
        Execution Planì„ í•´ì„í•˜ê³  ë°ì´í„° ë¡œë“œ ì¤€ë¹„
        
        PlanParserë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì‹±í•˜ê³ , ê²°ê³¼ë¥¼ ë‚´ë¶€ ì†ì„±ì— ë§¤í•‘í•©ë‹ˆë‹¤.
        
        Args:
            execution_plan: ExtractionAgentê°€ ìƒì„±í•œ plan JSON
            preload_cohort: cohort ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí• ì§€ (ê¸°ë³¸ True)
        
        Returns:
            self (method chaining ì§€ì›)
        """
        self._plan = execution_plan
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PlanParserë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì‹±
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self._plan_parser is None:
            self._plan_parser = PlanParser(db_manager=self._db)
        
        self._parsed_plan = self._plan_parser.parse(execution_plan, resolve_paths=True)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ParsedPlanì—ì„œ ë‚´ë¶€ ì†ì„±ìœ¼ë¡œ ë³µì‚¬ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Cohort metadata
        cohort = self._parsed_plan.cohort
        self._cohort_file_id = cohort.file_id
        self._cohort_file_path = cohort.file_path
        self._cohort_file_name = cohort.file_name
        self._cohort_entity_id = cohort.entity_identifier
        self._cohort_row_represents = cohort.row_represents
        self._cohort_filters = cohort.filters
        
        # Signal metadata
        signal = self._parsed_plan.signal
        self._signal_group_id = signal.group_id
        self._signal_group_name = signal.group_name
        self._signal_entity_id_key = signal.entity_identifier_key
        self._signal_row_represents = signal.row_represents
        self._signal_files = signal.files
        self._param_keys = signal.param_keys
        self._param_info = signal.param_info
        self._temporal_config = signal.temporal_config
        
        # Join configuration
        join = self._parsed_plan.join
        self._join_config = {
            "cohort_key": join.cohort_key,
            "signal_key": join.signal_key,
            "type": join.join_type
        }
        
        self._loaded_at = datetime.now()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AnalysisContextBuilder ì´ˆê¸°í™”
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._analysis_builder = AnalysisContextBuilder(self)
        
        # Cohort ë¯¸ë¦¬ ë¡œë“œ (ì„ íƒì )
        if preload_cohort and self._cohort_file_path:
            self._load_cohort_to_cache()
        
        return self
    
    def get_cohort(self, columns: Optional[List[str]] = None) -> pd.DataFrame:
        """
        í•„í„°ê°€ ì ìš©ëœ Cohort ë°ì´í„° ë°˜í™˜
        
        Args:
            columns: íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            DataFrame
        """
        if not self._cohort_file_id:
            return pd.DataFrame()
        
        # ìºì‹œ í™•ì¸
        if self._cohort_file_id not in DataContext._cohort_cache:
            self._load_cohort_to_cache()
        
        df = DataContext._cohort_cache.get(self._cohort_file_id, pd.DataFrame())
        
        # í•„í„° ì ìš©
        df = self._apply_cohort_filters(df)
        
        # ì»¬ëŸ¼ ì„ íƒ
        if columns:
            available = [c for c in columns if c in df.columns]
            # í•­ìƒ entity_id í¬í•¨
            if self._cohort_entity_id and self._cohort_entity_id not in available:
                available.insert(0, self._cohort_entity_id)
            df = df[available]
        
        return df
    
    def get_signals(
        self, 
        caseid: Optional[str] = None,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True,
        max_cases: Optional[int] = None,
        parallel: bool = True,
        max_workers: int = 4
    ) -> pd.DataFrame:
        """
        Signal ë°ì´í„° ë°˜í™˜
        
        cohort í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, í•„í„°ëœ ì¼€ì´ìŠ¤ ì¤‘ signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            caseid: íŠ¹ì • ì¼€ì´ìŠ¤ë§Œ (Noneì´ë©´ cohort í•„í„° ì ìš©ëœ ìœ íš¨ ì¼€ì´ìŠ¤)
            param_keys: íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ (Noneì´ë©´ planì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°)
            apply_temporal: temporal_alignment ì ìš© ì—¬ë¶€
            max_cases: ìµœëŒ€ ë¡œë“œí•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            parallel: ë³‘ë ¬ ë¡œë”© í™œì„±í™” (ê¸°ë³¸ True)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ 4)
        
        Returns:
            DataFrame with columns: [caseid, Time, param1, param2, ...]
        """
        params = param_keys or self._param_keys
        
        if caseid:
            # ë‹¨ì¼ ì¼€ì´ìŠ¤
            logger.info(f"ğŸ“¡ Loading signal for case: {caseid}")
            return self._get_signal_for_case(caseid, params, apply_temporal)
        else:
            # cohort í•„í„°ê°€ ì ìš©ëœ ì¼€ì´ìŠ¤ ì¤‘ signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ (êµì§‘í•©)
            case_ids = self.get_available_case_ids()
            total_cases = len(case_ids)
            
            # ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ
            if max_cases and total_cases > max_cases:
                logger.warning(f"âš ï¸ Limiting to {max_cases} cases (total: {total_cases})")
                case_ids = case_ids[:max_cases]
            
            n_cases = len(case_ids)
            
            if parallel and n_cases > 1:
                return self._load_signals_parallel(case_ids, params, apply_temporal, max_workers)
            else:
                return self._load_signals_sequential(case_ids, params, apply_temporal)
    
    def _load_signals_parallel(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool,
        max_workers: int
    ) -> pd.DataFrame:
        """ë³‘ë ¬ë¡œ Signal ë¡œë“œ"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals for {n_cases} cases (parallel, {max_workers} workers)...")
        start_time = time.time()
        
        all_signals = []
        completed = 0
        
        def load_case(cid):
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                df[self._join_config["signal_key"]] = cid
            return cid, df
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(load_case, cid): cid for cid in case_ids}
            
            for future in as_completed(futures):
                cid, df = future.result()
                completed += 1
                
                if not df.empty:
                    all_signals.append(df)
                
                # ì§„í–‰ë¥  ë¡œê·¸ (25% ë‹¨ìœ„)
                if completed % max(1, n_cases // 4) == 0 or completed == n_cases:
                    elapsed = time.time() - start_time
                    logger.info(f"   Progress: {completed}/{n_cases} cases ({elapsed:.1f}s)")
        
        if all_signals:
            result = pd.concat(all_signals, ignore_index=True)
            total_time = time.time() - start_time
            logger.info(f"âœ… Signal loading complete: {len(result)} rows from {len(all_signals)} cases ({total_time:.1f}s)")
            return result
        
        logger.warning("âš ï¸ No signal data loaded")
        return pd.DataFrame()
    
    def _load_signals_sequential(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool
    ) -> pd.DataFrame:
        """ìˆœì°¨ì ìœ¼ë¡œ Signal ë¡œë“œ"""
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals for {n_cases} cases (sequential)...")
        start_time = time.time()
        
        all_signals = []
        for i, cid in enumerate(case_ids):
            logger.debug(f"   [{i+1}/{n_cases}] Loading case {cid}...")
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                df[self._join_config["signal_key"]] = cid
                all_signals.append(df)
                
            # ì§„í–‰ë¥  ë¡œê·¸ (ë§¤ 5ê°œë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                elapsed = time.time() - start_time
                logger.info(f"   Progress: {i+1}/{n_cases} cases ({elapsed:.1f}s)")
        
        if all_signals:
            result = pd.concat(all_signals, ignore_index=True)
            total_time = time.time() - start_time
            logger.info(f"âœ… Signal loading complete: {len(result)} rows from {len(all_signals)} cases ({total_time:.1f}s)")
            return result
        
        logger.warning("âš ï¸ No signal data loaded")
        return pd.DataFrame()
    
    def get_signals_dict(
        self,
        case_ids: Optional[List[str]] = None,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True,
        max_cases: Optional[int] = None,
        parallel: bool = True,
        max_workers: int = 4
    ) -> Dict[str, pd.DataFrame]:
        """
        ì¼€ì´ìŠ¤ë³„ DataFrame Dict ë°˜í™˜ (ì¼€ì´ìŠ¤ ë‹¨ìœ„ ë³´ì¡´)
        
        cohort í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, í•„í„°ëœ ì¼€ì´ìŠ¤ ì¤‘ signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            case_ids: ë¡œë“œí•  ì¼€ì´ìŠ¤ ID ëª©ë¡ (Noneì´ë©´ cohort í•„í„° ì ìš©ëœ ìœ íš¨ ì¼€ì´ìŠ¤)
            param_keys: íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ (Noneì´ë©´ planì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°)
            apply_temporal: temporal_alignment ì ìš© ì—¬ë¶€
            max_cases: ìµœëŒ€ ë¡œë“œí•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            parallel: ë³‘ë ¬ ë¡œë”© í™œì„±í™” (ê¸°ë³¸ True)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ 4)
        
        Returns:
            Dict[caseid, DataFrame] - ê° ì¼€ì´ìŠ¤ë³„ ë…ë¦½ ì‹œê³„ì—´ DataFrame
            ì˜ˆ: {"case1": DataFrame([Time, HR, SpO2, ...]), "case2": ...}
        """
        params = param_keys or self._param_keys
        # cohort í•„í„°ê°€ ì ìš©ëœ ì¼€ì´ìŠ¤ ì¤‘ signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ (êµì§‘í•©)
        target_cases = case_ids or self.get_available_case_ids()
        total_cases = len(target_cases)
        
        # ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ
        if max_cases and total_cases > max_cases:
            logger.warning(f"âš ï¸ Limiting to {max_cases} cases (total: {total_cases})")
            target_cases = target_cases[:max_cases]
        
        n_cases = len(target_cases)
        
        if parallel and n_cases > 1:
            return self._load_signals_dict_parallel(target_cases, params, apply_temporal, max_workers)
        else:
            return self._load_signals_dict_sequential(target_cases, params, apply_temporal)
    
    def _load_signals_dict_parallel(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool,
        max_workers: int
    ) -> Dict[str, pd.DataFrame]:
        """ë³‘ë ¬ë¡œ Signal Dict ë¡œë“œ"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals dict for {n_cases} cases (parallel, {max_workers} workers)...")
        start_time = time.time()
        
        result_dict: Dict[str, pd.DataFrame] = {}
        completed = 0
        
        def load_case(cid):
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            return str(cid), df
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(load_case, cid): cid for cid in case_ids}
            
            for future in as_completed(futures):
                cid, df = future.result()
                completed += 1
                
                if not df.empty:
                    result_dict[cid] = df
                
                # ì§„í–‰ë¥  ë¡œê·¸ (25% ë‹¨ìœ„)
                if completed % max(1, n_cases // 4) == 0 or completed == n_cases:
                    elapsed = time.time() - start_time
                    logger.info(f"   Progress: {completed}/{n_cases} cases ({elapsed:.1f}s)")
        
        total_time = time.time() - start_time
        total_rows = sum(len(df) for df in result_dict.values())
        logger.info(f"âœ… Signal dict loading complete: {len(result_dict)} cases, {total_rows} total rows ({total_time:.1f}s)")
        
        return result_dict
    
    def _load_signals_dict_sequential(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool
    ) -> Dict[str, pd.DataFrame]:
        """ìˆœì°¨ì ìœ¼ë¡œ Signal Dict ë¡œë“œ"""
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals dict for {n_cases} cases (sequential)...")
        start_time = time.time()
        
        result_dict: Dict[str, pd.DataFrame] = {}
        
        for i, cid in enumerate(case_ids):
            logger.debug(f"   [{i+1}/{n_cases}] Loading case {cid}...")
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                result_dict[str(cid)] = df
                
            # ì§„í–‰ë¥  ë¡œê·¸ (ë§¤ 5ê°œë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                elapsed = time.time() - start_time
                logger.info(f"   Progress: {i+1}/{n_cases} cases ({elapsed:.1f}s)")
        
        total_time = time.time() - start_time
        total_rows = sum(len(df) for df in result_dict.values())
        logger.info(f"âœ… Signal dict loading complete: {len(result_dict)} cases, {total_rows} total rows ({total_time:.1f}s)")
        
        return result_dict
    
    def get_merged_data(self, how: str = "inner") -> pd.DataFrame:
        """
        Cohort + Signals ì¡°ì¸ëœ ë°ì´í„° ë°˜í™˜
        
        Args:
            how: ì¡°ì¸ ë°©ì‹ ("inner", "left", "outer")
        
        Returns:
            ì¡°ì¸ëœ DataFrame
        """
        cohort_df = self.get_cohort()
        signals_df = self.get_signals()
        
        if cohort_df.empty:
            return signals_df
        if signals_df.empty:
            return cohort_df
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ join í‚¤ (fallback: entity_id_column)
        default_key = self.entity_id_column or "id"
        cohort_key = self._join_config.get("cohort_key") or self._cohort_entity_id or default_key
        signal_key = self._join_config.get("signal_key") or self._signal_entity_id_key or default_key
        
        # í‚¤ íƒ€ì… ë§ì¶”ê¸°
        if cohort_key in cohort_df.columns and signal_key in signals_df.columns:
            cohort_df[cohort_key] = cohort_df[cohort_key].astype(str)
            signals_df[signal_key] = signals_df[signal_key].astype(str)
        
        if cohort_key == signal_key:
            return pd.merge(cohort_df, signals_df, on=cohort_key, how=how)
        else:
            return pd.merge(
                cohort_df, signals_df, 
                left_on=cohort_key, right_on=signal_key, 
                how=how
            )
    
    def iter_cases(
        self,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True
    ) -> Iterator[Dict[str, Any]]:
        """
        ì¼€ì´ìŠ¤ë³„ ë°ì´í„° Iterator (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ìš©)
        
        Yields:
            {
                "entity_id": str,         # ì—”í‹°í‹° ì‹ë³„ì ê°’
                "cohort": pd.Series,      # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ë©”íƒ€ë°ì´í„°
                "signals": pd.DataFrame,  # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ì‹ í˜¸ ë°ì´í„°
                "temporal_range": (start, end) or None
            }
        """
        cohort_df = self.get_cohort()
        case_ids = self.get_case_ids()
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ cohort í‚¤
        default_key = self.entity_id_column or "id"
        cohort_key = self._join_config.get("cohort_key") or self._cohort_entity_id or default_key
        
        for cid in case_ids:
            # Cohort row
            cohort_row = cohort_df[cohort_df[cohort_key].astype(str) == str(cid)] if cohort_key in cohort_df.columns else pd.DataFrame()
            cohort_series = cohort_row.iloc[0] if not cohort_row.empty else pd.Series()
            
            # Signals
            signals = self._get_signal_for_case(str(cid), param_keys, apply_temporal)
            
            # Temporal range
            temporal_range = None
            if apply_temporal and not cohort_series.empty:
                temporal_range = self._get_temporal_range(cohort_series)
            
            yield {
                "entity_id": str(cid),
                "cohort": cohort_series,
                "signals": signals,
                "temporal_range": temporal_range
            }
    
    def iter_cases_batch(
        self,
        batch_size: int = 100,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True,
        max_cases: Optional[int] = None,
        parallel: bool = True,
        max_workers: int = 4,
    ) -> Iterator[Dict[str, Any]]:
        """
        ì¼€ì´ìŠ¤ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìˆœíšŒ (ëŒ€ìš©ëŸ‰ Map-Reduce ì²˜ë¦¬ìš©)
        
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹œê·¸ë„ì„ ë¡œë“œí•˜ê³ ,
        ê° ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ë¥¼ í•´ì œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        
        Args:
            batch_size: ë°°ì¹˜ë‹¹ ì¼€ì´ìŠ¤ ìˆ˜ (ê¸°ë³¸ 100)
            param_keys: ë¡œë“œí•  íŒŒë¼ë¯¸í„° ëª©ë¡ (Noneì´ë©´ planì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°)
            apply_temporal: temporal_alignment ì ìš© ì—¬ë¶€
            max_cases: ìµœëŒ€ ì²˜ë¦¬í•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            parallel: ë°°ì¹˜ ë‚´ ë³‘ë ¬ ë¡œë”© í™œì„±í™”
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        
        Yields:
            {
                "batch_index": int,           # í˜„ì¬ ë°°ì¹˜ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
                "total_batches": int,         # ì „ì²´ ë°°ì¹˜ ìˆ˜
                "batch_size": int,            # í˜„ì¬ ë°°ì¹˜ì˜ ì¼€ì´ìŠ¤ ìˆ˜
                "entity_ids": List[str],      # ë°°ì¹˜ ë‚´ ì—”í‹°í‹° ID ëª©ë¡
                "signals": Dict[str, DataFrame],  # ë°°ì¹˜ ë‚´ ì‹œê·¸ë„ {entity_id: df}
                "metadata_rows": DataFrame,   # ë°°ì¹˜ ë‚´ ë©”íƒ€ë°ì´í„° í–‰ë“¤
            }
        
        Example:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° Map-Reduce ì²˜ë¦¬
            import gc
            
            all_results = []
            for batch in ctx.iter_cases_batch(batch_size=100):
                print(f"Processing batch {batch['batch_index']+1}/{batch['total_batches']}")
                
                for entity_id, signals_df in batch["signals"].items():
                    # map_func í˜¸ì¶œ
                    result = map_func(entity_id, signals_df, ...)
                    all_results.append(result)
                
                # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ í•´ì œ
                del batch
                gc.collect()
            
            # ìµœì¢… ì§‘ê³„
            final = reduce_func(all_results, cohort)
        """
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ ID (cohort + signal êµì§‘í•©)
        all_case_ids = self.get_available_case_ids()
        
        # max_cases ì ìš©
        if max_cases and len(all_case_ids) > max_cases:
            logger.warning(f"âš ï¸ Limiting to {max_cases} cases (total: {len(all_case_ids)})")
            all_case_ids = all_case_ids[:max_cases]
        
        total_cases = len(all_case_ids)
        total_batches = (total_cases + batch_size - 1) // batch_size
        
        if total_cases == 0:
            logger.warning("âš ï¸ No cases available for batch iteration")
            return
        
        logger.info(f"ğŸ“¦ Starting batch iteration: {total_cases} cases in {total_batches} batches (size={batch_size})")
        
        # Cohort ì „ì²´ ë¡œë“œ (ë©”íƒ€ë°ì´í„°ëŠ” ì‘ìœ¼ë¯€ë¡œ ì „ì²´ ë¡œë“œ)
        cohort_df = self.get_cohort()
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ cohort í‚¤
        default_key = self.entity_id_column or "id"
        cohort_key = self._join_config.get("cohort_key") or self._cohort_entity_id or default_key
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_cases)
            batch_case_ids = all_case_ids[start_idx:end_idx]
            
            # ë°°ì¹˜ ë‚´ ì‹œê·¸ë„ ë¡œë“œ
            signals_dict = self.get_signals_dict(
                case_ids=batch_case_ids,
                param_keys=param_keys,
                apply_temporal=apply_temporal,
                parallel=parallel,
                max_workers=max_workers,
            )
            
            # ë°°ì¹˜ ë‚´ ë©”íƒ€ë°ì´í„° í–‰ ì¶”ì¶œ
            metadata_rows = pd.DataFrame()
            if not cohort_df.empty and cohort_key in cohort_df.columns:
                batch_case_ids_str = [str(c) for c in batch_case_ids]
                metadata_rows = cohort_df[
                    cohort_df[cohort_key].astype(str).isin(batch_case_ids_str)
                ].copy()
            
            # ì‹¤ì œ ë¡œë“œëœ entity_ids (ì‹œê·¸ë„ì´ ìˆëŠ” ê²ƒë§Œ)
            loaded_entity_ids = list(signals_dict.keys())
            
            yield {
                "batch_index": batch_idx,
                "total_batches": total_batches,
                "batch_size": len(loaded_entity_ids),
                "entity_ids": loaded_entity_ids,
                "signals": signals_dict,
                "metadata_rows": metadata_rows,
            }
    
    def get_batch_metadata_row(
        self,
        metadata_rows: pd.DataFrame,
        entity_id: str,
    ) -> pd.Series:
        """ë°°ì¹˜ ë©”íƒ€ë°ì´í„°ì—ì„œ íŠ¹ì • ì—”í‹°í‹°ì˜ í–‰ ì¶”ì¶œ
        
        iter_cases_batch()ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ.
        
        Args:
            metadata_rows: iter_cases_batch()ê°€ ë°˜í™˜í•œ metadata_rows
            entity_id: ì—”í‹°í‹° ID
        
        Returns:
            í•´ë‹¹ ì—”í‹°í‹°ì˜ ë©”íƒ€ë°ì´í„° Series (ì—†ìœ¼ë©´ ë¹ˆ Series)
        
        Example:
            for batch in ctx.iter_cases_batch():
                for entity_id, signals in batch["signals"].items():
                    metadata_row = ctx.get_batch_metadata_row(
                        batch["metadata_rows"], 
                        entity_id
                    )
                    result = map_func(entity_id, signals, metadata_row)
        """
        if metadata_rows.empty:
            return pd.Series()
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ cohort í‚¤
        default_key = self.entity_id_column or "id"
        cohort_key = self._join_config.get("cohort_key") or self._cohort_entity_id or default_key
        
        if cohort_key not in metadata_rows.columns:
            return pd.Series()
        
        row = metadata_rows[metadata_rows[cohort_key].astype(str) == str(entity_id)]
        
        if row.empty:
            return pd.Series()
        
        return row.iloc[0]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Query Helpers
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_case_ids(self, signals_only: bool = True) -> List[str]:
        """ì¼€ì´ìŠ¤ ID ëª©ë¡ ë°˜í™˜
        
        Args:
            signals_only: Trueë©´ Signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’)
                         Falseë©´ Cohort ì „ì²´ ì¼€ì´ìŠ¤ ë°˜í™˜
        
        Returns:
            ì¼€ì´ìŠ¤ ID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        if signals_only:
            # Signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ (entity_id í‚¤ ì‚¬ìš©)
            return [f.get("entity_id") for f in self._signal_files if f.get("entity_id")]
        else:
            # Cohort ì „ì²´ ì¼€ì´ìŠ¤
            cohort = self.get_cohort()
            if cohort.empty:
                return []
            
            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ entity ì»¬ëŸ¼
            entity_col = self._cohort_entity_id or self.entity_id_column or "id"
            if entity_col in cohort.columns:
                return cohort[entity_col].astype(str).unique().tolist()
            return []
    
    def get_available_case_ids(self) -> List[str]:
        """ë¶„ì„ ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ ID (Cohortì™€ Signal êµì§‘í•©)"""
        cohort_ids = set(self.get_case_ids(signals_only=False))
        signal_ids = set(self.get_case_ids(signals_only=True))
        return sorted(list(cohort_ids & signal_ids))
    
    def get_available_parameters(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í‚¤ ëª©ë¡"""
        return self._param_keys.copy()
    
    def is_loaded(self) -> bool:
        """Planì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self._plan is not None
    
    def summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ìš”ì•½"""
        cohort_loaded = self._cohort_file_id in DataContext._cohort_cache
        signals_cached = len([
            cid for cid in self.get_case_ids() 
            if cid in DataContext._signal_cache
        ])
        
        return {
            "loaded_at": self._loaded_at.isoformat() if self._loaded_at else None,
            "cohort": {
                "file_id": self._cohort_file_id,
                "file_path": self._cohort_file_path,
                "loaded": cohort_loaded,
                "filters_count": len(self._cohort_filters),
                "total_cases": len(self.get_case_ids()) if cohort_loaded else 0
            },
            "signals": {
                "group_id": self._signal_group_id,
                "total_files": len(self._signal_files),
                "cached_count": signals_cached,
                "param_keys": self._param_keys,
                "temporal_type": self._temporal_config.get("type", "full_record")
            },
            "cache_stats": {
                "cohort_cache_size": len(DataContext._cohort_cache),
                "signal_cache_size": len(DataContext._signal_cache)
            }
        }
    
    @classmethod
    def clear_cache(cls, cache_type: str = "all") -> None:
        """
        ìºì‹œ ì •ë¦¬
        
        Args:
            cache_type: "all", "signals", "cohort"
        """
        if cache_type in ("all", "signals"):
            cls._signal_cache.clear()
        if cache_type in ("all", "cohort"):
            cls._cohort_cache.clear()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AnalysisAgent ì§€ì› ë©”ì„œë“œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_analysis_context(self) -> Dict[str, Any]:
        """
        LLM ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
        
        AnalysisContextBuilderì— ìœ„ì„í•©ë‹ˆë‹¤.
        
        Returns:
            {
                "description": str,
                "cohort": {...},
                "signals": {...},
                "original_query": str
            }
        """
        if self._analysis_builder is None:
            self._analysis_builder = AnalysisContextBuilder(self)
        
        return self._analysis_builder.build_analysis_context().to_dict()
    
    def generate_access_guide(
        self,
        signals_dict: Optional[Dict[str, pd.DataFrame]] = None,
        cohort_df: Optional[pd.DataFrame] = None,
        include_examples: bool = True
    ) -> str:
        """
        í˜„ì¬ ë°ì´í„° êµ¬ì¡°ì— ê¸°ë°˜í•œ ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ìƒì„±
        
        AnalysisContextBuilderì— ìœ„ì„í•©ë‹ˆë‹¤.
        
        Args:
            signals_dict: ì¼€ì´ìŠ¤ë³„ Signal DataFrame Dict
            cohort_df: Cohort DataFrame
            include_examples: ì½”ë“œ ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€
        
        Returns:
            LLM í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ ë¬¸ìì—´
        """
        if self._analysis_builder is None:
            self._analysis_builder = AnalysisContextBuilder(self)
        
        return self._analysis_builder.generate_access_guide(
            signals_dict=signals_dict,
            cohort_df=cohort_df,
            include_examples=include_examples
        )
    
    def compute_statistics(
        self,
        param_keys: Optional[List[str]] = None,
        percentiles: List[float] = [0.25, 0.5, 0.75],
        sample_size: Optional[int] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        íŒŒë¼ë¯¸í„°ë³„ í†µê³„ ê³„ì‚°
        
        AnalysisContextBuilderì— ìœ„ì„í•©ë‹ˆë‹¤.
        
        Args:
            param_keys: ê³„ì‚°í•  íŒŒë¼ë¯¸í„° (Noneì´ë©´ ì „ì²´)
            percentiles: ê³„ì‚°í•  ë°±ë¶„ìœ„ìˆ˜
            sample_size: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            íŒŒë¼ë¯¸í„°ë³„ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if self._analysis_builder is None:
            self._analysis_builder = AnalysisContextBuilder(self)
        
        return self._analysis_builder.compute_statistics(
            param_keys=param_keys,
            percentiles=percentiles,
            sample_size=sample_size
        )
    
    def get_sample_data(
        self,
        n_cases: int = 3,
        n_rows_per_case: int = 5
    ) -> List[Dict[str, Any]]:
        """
        LLMì—ê²Œ ë³´ì—¬ì¤„ ìƒ˜í”Œ ë°ì´í„°
        
        AnalysisContextBuilderì— ìœ„ì„í•©ë‹ˆë‹¤.
        
        Args:
            n_cases: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜
            n_rows_per_case: ì¼€ì´ìŠ¤ë‹¹ ìƒ˜í”Œ í–‰ ìˆ˜
        
        Returns:
            ì¼€ì´ìŠ¤ë³„ ìƒ˜í”Œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if self._analysis_builder is None:
            self._analysis_builder = AnalysisContextBuilder(self)
        
        return self._analysis_builder.get_sample_data(
            n_cases=n_cases,
            n_rows_per_case=n_rows_per_case
        )
    
    def get_parameter_info(self, param_key: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • íŒŒë¼ë¯¸í„°ì˜ ìƒì„¸ ì •ë³´"""
        for p in self._param_info:
            if param_key in p.get("param_keys", []):
                return {
                    "term": p.get("term"),
                    "param_key": param_key,
                    "semantic_name": p.get("semantic_name"),
                    "unit": p.get("unit"),
                    "resolution_mode": p.get("resolution_mode"),
                    "confidence": p.get("confidence")
                }
        return None
    
    def to_execution_context(
        self,
        include_signals: bool = True,
        sample_rows: int = 3,
        max_signal_cases: int = 3
    ) -> Dict[str, Any]:
        """
        Code Generationì„ ìœ„í•œ ExecutionContext ë°ì´í„° ìƒì„±
        
        AnalysisAgentì˜ ExecutionContext ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.
        DataSchema ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ LLMì´ ì •í™•í•œ ì»¬ëŸ¼ëª…ì„ ì•Œ ìˆ˜ ìˆë„ë¡ í•¨.
        
        Args:
            include_signals: Signal ë°ì´í„° ìŠ¤í‚¤ë§ˆ í¬í•¨ ì—¬ë¶€
            sample_rows: ìƒ˜í”Œ ë°ì´í„° í–‰ ìˆ˜
            max_signal_cases: Signal ìƒ˜í”Œë§í•  ìµœëŒ€ ì¼€ì´ìŠ¤ ìˆ˜
        
        Returns:
            ExecutionContext ìƒì„±ì— í•„ìš”í•œ ë”•ì…”ë„ˆë¦¬
            {
                "available_variables": {...},
                "available_imports": [...],
                "data_schemas": {
                    "df": {...},
                    "cohort": {...}
                }
            }
        
        Example:
            ctx_data = data_context.to_execution_context()
            from AnalysisAgent.src.models import ExecutionContext, DataSchema
            exec_ctx = ExecutionContext(
                available_variables=ctx_data["available_variables"],
                data_schemas={k: DataSchema(**v) for k, v in ctx_data["data_schemas"].items()}
            )
        """
        cohort = self.get_cohort()
        case_ids = self.get_case_ids()
        
        # 1. Available Variables
        available_variables = {}
        
        if not cohort.empty:
            available_variables["cohort"] = (
                f"pandas DataFrame - Cohort ë©”íƒ€ë°ì´í„°, "
                f"{len(cohort)} rows Ã— {len(cohort.columns)} columns"
            )
        
        if include_signals and case_ids:
            available_variables["df"] = (
                f"pandas DataFrame - Signal ë°ì´í„°, "
                f"columns: [Time, {', '.join(self._param_keys[:5])}{'...' if len(self._param_keys) > 5 else ''}]"
            )
        
        available_variables["case_ids"] = f"List[str] - {len(case_ids)}ê°œ ì¼€ì´ìŠ¤ ID"
        available_variables["param_keys"] = f"List[str] - {self._param_keys}"
        
        # 2. Data Schemas
        data_schemas = {}
        
        # Cohort ìŠ¤í‚¤ë§ˆ
        if not cohort.empty:
            cohort_schema = self._build_data_schema(
                name="cohort",
                description="Cohort ë©”íƒ€ë°ì´í„° (í™˜ì ì •ë³´)",
                df=cohort,
                sample_rows=sample_rows
            )
            data_schemas["cohort"] = cohort_schema
        
        # Signal ìŠ¤í‚¤ë§ˆ (ìƒ˜í”Œ ì¼€ì´ìŠ¤ì—ì„œ ì¶”ì¶œ)
        if include_signals and case_ids:
            sample_case = case_ids[0] if case_ids else None
            if sample_case:
                signals = self._get_signal_for_case(sample_case, apply_temporal=True)
                if not signals.empty:
                    # ì—¬ëŸ¬ ì¼€ì´ìŠ¤ì˜ shape ì¶”ì •
                    total_rows = len(signals) * len(case_ids)
                    signals_schema = self._build_data_schema(
                        name="df",
                        description=f"Signal ë°ì´í„° (ìƒì²´ì‹ í˜¸, {len(case_ids)} cases)",
                        df=signals,
                        sample_rows=sample_rows,
                        override_shape=(total_rows, len(signals.columns))
                    )
                    data_schemas["df"] = signals_schema
        
        # 3. Available Imports
        available_imports = [
            "pandas as pd",
            "numpy as np", 
            "scipy.stats as stats",
            "datetime",
            "math",
        ]
        
        return {
            "available_variables": available_variables,
            "available_imports": available_imports,
            "data_schemas": data_schemas,
            "case_ids": case_ids,
            "param_keys": self._param_keys,
        }
    
    def _build_data_schema(
        self,
        name: str,
        description: str,
        df: pd.DataFrame,
        sample_rows: int = 3,
        override_shape: Optional[Tuple[int, int]] = None
    ) -> Dict[str, Any]:
        """DataFrameì—ì„œ DataSchema ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        # ì»¬ëŸ¼ ì •ë³´
        columns = list(df.columns)
        dtypes = {col: str(df[col].dtype) for col in columns}
        
        # Shape
        shape = override_shape or (len(df), len(df.columns))
        
        # ìƒ˜í”Œ í–‰
        sample_data = None
        if sample_rows > 0 and not df.empty:
            sample_df = df.head(sample_rows)
            sample_data = sample_df.to_dict(orient="records")
            # ìˆ«ì ë°˜ì˜¬ë¦¼
            for row in sample_data:
                for k, v in row.items():
                    if isinstance(v, float):
                        row[k] = round(v, 4)
        
        # ì»¬ëŸ¼ í†µê³„
        column_stats = {}
        for col in columns[:10]:  # ìµœëŒ€ 10ê°œ ì»¬ëŸ¼ë§Œ
            if pd.api.types.is_numeric_dtype(df[col]):
                column_stats[col] = {
                    "type": "numeric",
                    "mean": round(df[col].mean(), 4) if not df[col].isna().all() else None,
                    "min": round(df[col].min(), 4) if not df[col].isna().all() else None,
                    "max": round(df[col].max(), 4) if not df[col].isna().all() else None,
                }
            else:
                column_stats[col] = {
                    "type": "categorical",
                    "unique_count": df[col].nunique(),
                    "sample_values": df[col].dropna().head(5).tolist(),
                }
        
        return {
            "name": name,
            "description": description,
            "columns": columns,
            "dtypes": dtypes,
            "shape": shape,
            "sample_rows": sample_data,
            "column_stats": column_stats,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - DB ì¡°íšŒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _resolve_file_path(self, file_id: str) -> Optional[str]:
        """file_id â†’ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (DB ì¡°íšŒ)"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT file_path FROM file_catalog
                WHERE file_id = %s
            """, (file_id,))
            
            row = cursor.fetchone()
            conn.commit()
            
            return row[0] if row else None
        except Exception as e:
            print(f"[DataContext] Error resolving file path: {e}")
            return None
    
    def _resolve_signal_files(self, group_id: str) -> List[Dict[str, Any]]:
        """group_id â†’ í•´ë‹¹ ê·¸ë£¹ì˜ ëª¨ë“  signal íŒŒì¼ ì¡°íšŒ"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT file_id, file_path, filename_values
                FROM file_catalog
                WHERE group_id = %s
                ORDER BY file_name
            """, (group_id,))
            
            rows = cursor.fetchall()
            conn.commit()
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ entity_id í‚¤ ê²°ì • (ë™ì )
            # DBì˜ filename_valuesì—ì„œ ì–´ë–¤ í‚¤ë¡œ ì—”í‹°í‹° IDë¥¼ ê°€ì ¸ì˜¬ì§€
            entity_key = self._signal_entity_id_key or "caseid"  # planì—ì„œ ë°›ì€ í‚¤ ì‚¬ìš©
            
            files = []
            for row in rows:
                file_id, file_path, filename_values = row
                entity_id = None
                if filename_values and isinstance(filename_values, dict):
                    # entity_keyë¡œ ë¨¼ì € ì‹œë„, ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ID íŒ¨í„´ë“¤ ì‹œë„
                    entity_id = filename_values.get(entity_key)
                    if entity_id is None:
                        # fallback: ë‹¤ë¥¸ ID íŒ¨í„´ ì‹œë„
                        for key in ["caseid", "case_id", "subject_id", "id"]:
                            if key in filename_values:
                                entity_id = filename_values[key]
                                break
                
                files.append({
                    "file_id": str(file_id),
                    "file_path": file_path,
                    "entity_id": str(entity_id) if entity_id else None
                })
            
            return files
        except Exception as e:
            print(f"[DataContext] Error resolving signal files: {e}")
            return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - ë°ì´í„° ë¡œë“œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _load_cohort_to_cache(self) -> None:
        """Cohort ë°ì´í„°ë¥¼ ìºì‹œì— ë¡œë“œ"""
        if not self._cohort_file_path:
            return
        
        if self._cohort_file_id in DataContext._cohort_cache:
            return  # ì´ë¯¸ ìºì‹œë¨
        
        try:
            df = self._tabular_processor.load_data(self._cohort_file_path)
            DataContext._cohort_cache[self._cohort_file_id] = df
        except Exception as e:
            print(f"[DataContext] Error loading cohort: {e}")
    
    def _get_signal_for_case(
        self, 
        entity_id: str, 
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True
    ) -> pd.DataFrame:
        """íŠ¹ì • ì—”í‹°í‹°ì˜ signal ë°ì´í„° ë¡œë“œ"""
        params = param_keys or self._param_keys
        
        # ìºì‹œ í™•ì¸
        if entity_id in DataContext._signal_cache:
            df = DataContext._signal_cache[entity_id]
        else:
            # íŒŒì¼ ì°¾ê¸°
            file_info = None
            for f in self._signal_files:
                if f.get("entity_id") == entity_id:
                    file_info = f
                    break
            
            if not file_info or not file_info.get("file_path"):
                return pd.DataFrame()
            
            # ë¡œë“œ
            try:
                df = self._signal_processor.load_data(
                    file_info["file_path"],
                    columns=params
                )
                DataContext._signal_cache[entity_id] = df
            except Exception as e:
                print(f"[DataContext] Error loading signal for {entity_id}: {e}")
                return pd.DataFrame()
        
        # íŒŒë¼ë¯¸í„° í•„í„°ë§
        if params:
            # Time ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ í¬í•¨ (ì¼ë¶€ ë°ì´í„°ì…‹ì—ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ)
            time_cols = ["Time"] if "Time" in df.columns else []
            available_cols = time_cols + [p for p in params if p in df.columns]
            if available_cols:
                df = df[available_cols]
        
        # Temporal í•„í„° ì ìš©
        if apply_temporal and self._temporal_config.get("type", "full_record") != "full_record":
            cohort = self.get_cohort()
            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ cohort í‚¤
            default_key = self.entity_id_column or "id"
            cohort_key = self._join_config.get("cohort_key") or self._cohort_entity_id or default_key
            cohort_row = cohort[cohort[cohort_key].astype(str) == str(entity_id)] if cohort_key in cohort.columns else pd.DataFrame()
            
            if not cohort_row.empty:
                df = self._apply_temporal_filter(df, cohort_row.iloc[0])
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - í•„í„°ë§
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _apply_cohort_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cohort í•„í„° ì ìš©"""
        if df.empty or not self._cohort_filters:
            return df
        
        for f in self._cohort_filters:
            col = f.get("column")
            op = f.get("operator", "=")
            val = f.get("value")
            
            if col not in df.columns:
                continue
            
            op_upper = op.upper()
            
            if op_upper == "=" or op == "==":
                df = df[df[col] == val]
            elif op_upper == "!=" or op == "<>":
                df = df[df[col] != val]
            elif op_upper == ">":
                df = df[df[col] > val]
            elif op_upper == ">=":
                df = df[df[col] >= val]
            elif op_upper == "<":
                df = df[df[col] < val]
            elif op_upper == "<=":
                df = df[df[col] <= val]
            elif op_upper == "LIKE":
                pattern = str(val).replace('%', '.*')
                df = df[df[col].astype(str).str.contains(pattern, case=False, na=False, regex=True)]
            elif op_upper == "IN":
                if isinstance(val, list):
                    df = df[df[col].isin(val)]
            elif op_upper == "BETWEEN":
                if isinstance(val, (list, tuple)) and len(val) == 2:
                    df = df[(df[col] >= val[0]) & (df[col] <= val[1])]
        
        return df
    
    def _apply_temporal_filter(
        self, 
        signals_df: pd.DataFrame, 
        cohort_row: pd.Series
    ) -> pd.DataFrame:
        """Temporal alignment ì ìš©"""
        if signals_df.empty:
            return signals_df
        
        temp_type = self._temporal_config.get("type", "full_record")
        if temp_type == "full_record":
            return signals_df
        
        margin = self._temporal_config.get("margin_seconds", 0)
        start_col = self._temporal_config.get("start_column")
        end_col = self._temporal_config.get("end_column")
        
        if not start_col or not end_col:
            return signals_df
        
        start_time = cohort_row.get(start_col)
        end_time = cohort_row.get(end_col)
        
        if pd.isna(start_time) or pd.isna(end_time):
            return signals_df
        
        # Unix timestampë¡œ ë³€í™˜
        start_sec = self._to_seconds(start_time)
        end_sec = self._to_seconds(end_time)
        
        if start_sec is None or end_sec is None:
            return signals_df
        
        # margin ì ìš©
        start_sec = start_sec - margin
        end_sec = end_sec + margin
        
        # ì‹œê°„ ì»¬ëŸ¼ ë™ì  ê°ì§€
        time_column = self._find_time_column(signals_df)
        
        if time_column:
            return signals_df[
                (signals_df[time_column] >= start_sec) & 
                (signals_df[time_column] <= end_sec)
            ].copy()
        
        return signals_df
    
    def _find_time_column(self, df: pd.DataFrame) -> Optional[str]:
        """
        DataFrameì—ì„œ ì‹œê°„/timestamp ì»¬ëŸ¼ì„ ë™ì ìœ¼ë¡œ ê°ì§€
        
        ê°ì§€ ìš°ì„ ìˆœìœ„:
        1. 'Time' (ê¸°ì¡´ í˜¸í™˜ì„±)
        2. datetime64 dtype ì»¬ëŸ¼
        3. ì‹œê°„ ê´€ë ¨ ì´ë¦„ íŒ¨í„´ ë§¤ì¹­
        
        Args:
            df: ë¶„ì„í•  DataFrame
            
        Returns:
            ì‹œê°„ ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ None)
        """
        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ëª… íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ìˆœ)
        time_patterns = [
            'Time', 'time',  # ê¸°ì¡´ í˜¸í™˜ì„± ìš°ì„ 
            'timestamp', 'Timestamp', 'TIMESTAMP',
            'datetime', 'DateTime', 'DATETIME',
            'date', 'Date', 'DATE',
            'dt', 'DT',
        ]
        
        # 1. ìš°ì„ ìˆœìœ„ íŒ¨í„´ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼
        for pattern in time_patterns:
            if pattern in df.columns:
                return pattern
        
        # 2. datetime64 dtype ì»¬ëŸ¼
        for col in df.columns:
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                return col
        
        # 3. ì»¬ëŸ¼ëª…ì— ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ (ì†Œë¬¸ì ë¹„êµ)
        for col in df.columns:
            col_lower = col.lower()
            if any(p in col_lower for p in ['time', 'timestamp', 'datetime']):
                return col
        
        return None
    
    def _to_seconds(self, value: Any) -> Optional[float]:
        """ê°’ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, datetime):
            return value.timestamp()
        if isinstance(value, str):
            try:
                dt = pd.to_datetime(value)
                return dt.timestamp()
            except:
                pass
        return None
    
    def _get_temporal_range(self, cohort_row: pd.Series) -> Optional[Tuple[float, float]]:
        """Temporal range ê³„ì‚°"""
        start_col = self._temporal_config.get("start_column")
        end_col = self._temporal_config.get("end_column")
        margin = self._temporal_config.get("margin_seconds", 0)
        
        if not start_col or not end_col:
            return None
        
        start_time = cohort_row.get(start_col)
        end_time = cohort_row.get(end_col)
        
        start_sec = self._to_seconds(start_time)
        end_sec = self._to_seconds(end_time)
        
        if start_sec is not None and end_sec is not None:
            return (start_sec - margin, end_sec + margin)
        return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - í—¬í¼
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_cohort_column_info(self, cohort: pd.DataFrame) -> List[Dict[str, Any]]:
        """Cohort ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ"""
        columns = []
        for col in cohort.columns:
            col_info = {
                "name": col,
                "dtype": str(cohort[col].dtype),
                "null_count": int(cohort[col].isna().sum()),
                "unique_count": int(cohort[col].nunique())
            }
            
            # ìˆ«ìí˜•ì´ë©´ í†µê³„ ì¶”ê°€
            if pd.api.types.is_numeric_dtype(cohort[col]):
                col_info["type"] = "numeric"
                col_info["stats"] = {
                    "mean": round(cohort[col].mean(), 2) if not cohort[col].isna().all() else None,
                    "min": cohort[col].min() if not cohort[col].isna().all() else None,
                    "max": cohort[col].max() if not cohort[col].isna().all() else None
                }
            else:
                col_info["type"] = "categorical"
                col_info["sample_values"] = cohort[col].dropna().head(5).tolist()
            
            columns.append(col_info)
        
        return columns
    
    def _get_temporal_description(self) -> str:
        """Temporal ì„¤ì • ì„¤ëª… ìƒì„±"""
        temp_type = self._temporal_config.get("type", "full_record")
        margin = self._temporal_config.get("margin_seconds", 0)
        
        descriptions = {
            "full_record": "ì „ì²´ ê¸°ë¡ (ì‹œê°„ ì œí•œ ì—†ìŒ)",
            "procedure_window": f"ì‹œìˆ /ìˆ˜ìˆ  ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)",
            "treatment_window": f"ì¹˜ë£Œ ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)",
            "custom_window": f"ì‚¬ìš©ì ì§€ì • ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)"
        }
        
        return descriptions.get(temp_type, temp_type)
    
    def _generate_description(
        self, 
        cohort_info: Dict[str, Any], 
        signal_info: Dict[str, Any]
    ) -> str:
        """ë°ì´í„° ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""
        parts = []
        
        # ì¼€ì´ìŠ¤ ìˆ˜
        parts.append(f"ì´ {cohort_info['total_cases']}ê°œ ì¼€ì´ìŠ¤ì˜ ë°ì´í„°")
        
        # í•„í„°
        if cohort_info['filters_applied']:
            filter_strs = []
            for f in cohort_info['filters_applied']:
                filter_strs.append(f"{f.get('column')} {f.get('operator')} {f.get('value')}")
            parts.append(f"í•„í„°: {', '.join(filter_strs)}")
        
        # íŒŒë¼ë¯¸í„°
        if signal_info['param_keys']:
            parts.append(f"ì¸¡ì • íŒŒë¼ë¯¸í„°: {', '.join(signal_info['param_keys'][:5])}")
            if len(signal_info['param_keys']) > 5:
                parts.append(f"ì™¸ {len(signal_info['param_keys']) - 5}ê°œ")
        
        # Temporal
        parts.append(f"ì‹œê°„ ë²”ìœ„: {signal_info['temporal_setting']['description']}")
        
        return ". ".join(parts)

