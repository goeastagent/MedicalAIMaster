# shared/data/context.py
"""
DataContext - Execution Plan ê¸°ë°˜ ë°ì´í„° ë¡œë“œ ë° ê´€ë¦¬

ì—­í• :
1. ExtractionAgentì˜ execution_plan JSON í•´ì„
2. DBì—ì„œ íŒŒì¼ ê²½ë¡œ resolve
3. Processorë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ
4. ìºì‹± (í´ë˜ìŠ¤ ë ˆë²¨, ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
5. AnalysisAgentë¥¼ ìœ„í•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ì œê³µ

ì‚¬ìš© ì˜ˆì‹œ:
    ctx = DataContext()
    ctx.load_from_plan(execution_plan)
    
    cohort = ctx.get_cohort()
    signals = ctx.get_signals(caseid="1234")
    merged = ctx.get_merged_data()
    
    # AnalysisAgentìš©
    analysis_ctx = ctx.get_analysis_context()
    stats = ctx.compute_statistics()
"""

import sys
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Iterator, Tuple
from datetime import datetime
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from shared.processors import SignalProcessor, TabularProcessor
from shared.database.connection import get_db_manager

logger = logging.getLogger(__name__)


class DataContext:
    """
    Execution Plan ê¸°ë°˜ ë°ì´í„° ë¡œë“œ ë° ê´€ë¦¬
    
    íŠ¹ì§•:
    - í´ë˜ìŠ¤ ë ˆë²¨ ìºì‹œ: ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ signal/cohort ë°ì´í„° ê³µìœ 
    - Lazy Loading: ìš”ì²­ ì‹œì—ë§Œ ë°ì´í„° ë¡œë“œ
    - Temporal Filter: surgery_window ë“± ìë™ ì ìš©
    - AnalysisAgent ì§€ì›: LLMìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Class-level Cache (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    _signal_cache: Dict[str, pd.DataFrame] = {}   # caseid â†’ signals DataFrame
    _cohort_cache: Dict[str, pd.DataFrame] = {}   # file_id â†’ cohort DataFrame
    
    def __init__(self):
        """DataContext ì´ˆê¸°í™”"""
        # Instance state
        self._plan: Optional[Dict[str, Any]] = None
        self._loaded_at: Optional[datetime] = None
        
        # Parsed plan components
        self._cohort_file_id: Optional[str] = None
        self._cohort_file_path: Optional[str] = None
        self._cohort_entity_id: Optional[str] = None
        self._cohort_filters: List[Dict[str, Any]] = []
        
        self._signal_group_id: Optional[str] = None
        self._signal_files: List[Dict[str, Any]] = []  # [{file_id, file_path, caseid}, ...]
        self._param_keys: List[str] = []
        self._param_info: List[Dict[str, Any]] = []  # [{term, param_key, semantic_name, unit}, ...]
        self._temporal_config: Dict[str, Any] = {}
        
        self._join_config: Dict[str, Any] = {}
        
        # Processors
        self._signal_processor = SignalProcessor()
        self._tabular_processor = TabularProcessor()
        
        # DB
        self._db = None
    
    @property
    def db(self):
        """Lazy DB connection"""
        if self._db is None:
            self._db = get_db_manager()
        return self._db
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Main Interface
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def load_from_plan(
        self, 
        execution_plan: Dict[str, Any],
        preload_cohort: bool = True
    ) -> "DataContext":
        """
        Execution Planì„ í•´ì„í•˜ê³  ë°ì´í„° ë¡œë“œ ì¤€ë¹„
        
        Args:
            execution_plan: ExtractionAgentê°€ ìƒì„±í•œ plan JSON
            preload_cohort: cohort ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí• ì§€ (ê¸°ë³¸ True)
        
        Returns:
            self (method chaining ì§€ì›)
        """
        self._plan = execution_plan
        plan = execution_plan.get("execution_plan", {})
        
        # 1. Cohort source íŒŒì‹±
        cohort_source = plan.get("cohort_source", {})
        if cohort_source:
            self._cohort_file_id = cohort_source.get("file_id")
            self._cohort_entity_id = cohort_source.get("entity_identifier", "caseid")
            self._cohort_filters = cohort_source.get("filters", [])
            
            # DBì—ì„œ íŒŒì¼ ê²½ë¡œ resolve
            if self._cohort_file_id:
                self._cohort_file_path = self._resolve_file_path(self._cohort_file_id)
        
        # 2. Signal source íŒŒì‹±
        signal_source = plan.get("signal_source", {})
        if signal_source:
            self._signal_group_id = signal_source.get("group_id")
            self._temporal_config = signal_source.get("temporal_alignment", {})
            
            # Parameters íŒŒì‹±
            parameters = signal_source.get("parameters", [])
            self._param_info = parameters
            self._param_keys = []
            for p in parameters:
                self._param_keys.extend(p.get("param_keys", []))
            
            # DBì—ì„œ signal íŒŒì¼ë“¤ resolve
            if self._signal_group_id:
                self._signal_files = self._resolve_signal_files(self._signal_group_id)
        
        # 3. Join ì„¤ì • íŒŒì‹±
        join_spec = plan.get("join_specification", {})
        self._join_config = {
            "cohort_key": join_spec.get("cohort_key", self._cohort_entity_id),
            "signal_key": join_spec.get("signal_key", self._cohort_entity_id),
            "type": join_spec.get("type", "inner")
        }
        
        self._loaded_at = datetime.now()
        
        # 4. Cohort ë¯¸ë¦¬ ë¡œë“œ (ì„ íƒì )
        if preload_cohort and self._cohort_file_path:
            self._load_cohort_to_cache()
        
        return self
    
    def get_cohort(self, columns: Optional[List[str]] = None) -> pd.DataFrame:
        """
        í•„í„°ê°€ ì ìš©ëœ Cohort ë°ì´í„° ë°˜í™˜
        
        Args:
            columns: íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            DataFrame
        """
        if not self._cohort_file_id:
            return pd.DataFrame()
        
        # ìºì‹œ í™•ì¸
        if self._cohort_file_id not in DataContext._cohort_cache:
            self._load_cohort_to_cache()
        
        df = DataContext._cohort_cache.get(self._cohort_file_id, pd.DataFrame())
        
        # í•„í„° ì ìš©
        df = self._apply_cohort_filters(df)
        
        # ì»¬ëŸ¼ ì„ íƒ
        if columns:
            available = [c for c in columns if c in df.columns]
            # í•­ìƒ entity_id í¬í•¨
            if self._cohort_entity_id and self._cohort_entity_id not in available:
                available.insert(0, self._cohort_entity_id)
            df = df[available]
        
        return df
    
    def get_signals(
        self, 
        caseid: Optional[str] = None,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True,
        max_cases: Optional[int] = None,
        parallel: bool = True,
        max_workers: int = 4
    ) -> pd.DataFrame:
        """
        Signal ë°ì´í„° ë°˜í™˜
        
        Args:
            caseid: íŠ¹ì • ì¼€ì´ìŠ¤ë§Œ (Noneì´ë©´ ë¡œë“œëœ ì „ì²´)
            param_keys: íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ (Noneì´ë©´ planì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°)
            apply_temporal: temporal_alignment ì ìš© ì—¬ë¶€
            max_cases: ìµœëŒ€ ë¡œë“œí•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            parallel: ë³‘ë ¬ ë¡œë”© í™œì„±í™” (ê¸°ë³¸ True)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ 4)
        
        Returns:
            DataFrame with columns: [caseid, Time, param1, param2, ...]
        """
        params = param_keys or self._param_keys
        
        if caseid:
            # ë‹¨ì¼ ì¼€ì´ìŠ¤
            logger.info(f"ğŸ“¡ Loading signal for case: {caseid}")
            return self._get_signal_for_case(caseid, params, apply_temporal)
        else:
            # ëª¨ë“  ì¼€ì´ìŠ¤
            case_ids = self.get_case_ids()
            total_cases = len(case_ids)
            
            # ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ
            if max_cases and total_cases > max_cases:
                logger.warning(f"âš ï¸ Limiting to {max_cases} cases (total: {total_cases})")
                case_ids = case_ids[:max_cases]
            
            n_cases = len(case_ids)
            
            if parallel and n_cases > 1:
                return self._load_signals_parallel(case_ids, params, apply_temporal, max_workers)
            else:
                return self._load_signals_sequential(case_ids, params, apply_temporal)
    
    def _load_signals_parallel(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool,
        max_workers: int
    ) -> pd.DataFrame:
        """ë³‘ë ¬ë¡œ Signal ë¡œë“œ"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals for {n_cases} cases (parallel, {max_workers} workers)...")
        start_time = time.time()
        
        all_signals = []
        completed = 0
        
        def load_case(cid):
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                df[self._join_config["signal_key"]] = cid
            return cid, df
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(load_case, cid): cid for cid in case_ids}
            
            for future in as_completed(futures):
                cid, df = future.result()
                completed += 1
                
                if not df.empty:
                    all_signals.append(df)
                
                # ì§„í–‰ë¥  ë¡œê·¸ (25% ë‹¨ìœ„)
                if completed % max(1, n_cases // 4) == 0 or completed == n_cases:
                    elapsed = time.time() - start_time
                    logger.info(f"   Progress: {completed}/{n_cases} cases ({elapsed:.1f}s)")
        
        if all_signals:
            result = pd.concat(all_signals, ignore_index=True)
            total_time = time.time() - start_time
            logger.info(f"âœ… Signal loading complete: {len(result)} rows from {len(all_signals)} cases ({total_time:.1f}s)")
            return result
        
        logger.warning("âš ï¸ No signal data loaded")
        return pd.DataFrame()
    
    def _load_signals_sequential(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool
    ) -> pd.DataFrame:
        """ìˆœì°¨ì ìœ¼ë¡œ Signal ë¡œë“œ"""
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals for {n_cases} cases (sequential)...")
        start_time = time.time()
        
        all_signals = []
        for i, cid in enumerate(case_ids):
            logger.debug(f"   [{i+1}/{n_cases}] Loading case {cid}...")
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                df[self._join_config["signal_key"]] = cid
                all_signals.append(df)
                
            # ì§„í–‰ë¥  ë¡œê·¸ (ë§¤ 5ê°œë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                elapsed = time.time() - start_time
                logger.info(f"   Progress: {i+1}/{n_cases} cases ({elapsed:.1f}s)")
        
        if all_signals:
            result = pd.concat(all_signals, ignore_index=True)
            total_time = time.time() - start_time
            logger.info(f"âœ… Signal loading complete: {len(result)} rows from {len(all_signals)} cases ({total_time:.1f}s)")
            return result
        
        logger.warning("âš ï¸ No signal data loaded")
        return pd.DataFrame()
    
    def get_signals_dict(
        self,
        case_ids: Optional[List[str]] = None,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True,
        max_cases: Optional[int] = None,
        parallel: bool = True,
        max_workers: int = 4
    ) -> Dict[str, pd.DataFrame]:
        """
        ì¼€ì´ìŠ¤ë³„ DataFrame Dict ë°˜í™˜ (ì¼€ì´ìŠ¤ ë‹¨ìœ„ ë³´ì¡´)
        
        Args:
            case_ids: ë¡œë“œí•  ì¼€ì´ìŠ¤ ID ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            param_keys: íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ (Noneì´ë©´ planì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°)
            apply_temporal: temporal_alignment ì ìš© ì—¬ë¶€
            max_cases: ìµœëŒ€ ë¡œë“œí•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            parallel: ë³‘ë ¬ ë¡œë”© í™œì„±í™” (ê¸°ë³¸ True)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ 4)
        
        Returns:
            Dict[caseid, DataFrame] - ê° ì¼€ì´ìŠ¤ë³„ ë…ë¦½ ì‹œê³„ì—´ DataFrame
            ì˜ˆ: {"case1": DataFrame([Time, HR, SpO2, ...]), "case2": ...}
        """
        params = param_keys or self._param_keys
        target_cases = case_ids or self.get_case_ids()
        total_cases = len(target_cases)
        
        # ì¼€ì´ìŠ¤ ìˆ˜ ì œí•œ
        if max_cases and total_cases > max_cases:
            logger.warning(f"âš ï¸ Limiting to {max_cases} cases (total: {total_cases})")
            target_cases = target_cases[:max_cases]
        
        n_cases = len(target_cases)
        
        if parallel and n_cases > 1:
            return self._load_signals_dict_parallel(target_cases, params, apply_temporal, max_workers)
        else:
            return self._load_signals_dict_sequential(target_cases, params, apply_temporal)
    
    def _load_signals_dict_parallel(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool,
        max_workers: int
    ) -> Dict[str, pd.DataFrame]:
        """ë³‘ë ¬ë¡œ Signal Dict ë¡œë“œ"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals dict for {n_cases} cases (parallel, {max_workers} workers)...")
        start_time = time.time()
        
        result_dict: Dict[str, pd.DataFrame] = {}
        completed = 0
        
        def load_case(cid):
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            return str(cid), df
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(load_case, cid): cid for cid in case_ids}
            
            for future in as_completed(futures):
                cid, df = future.result()
                completed += 1
                
                if not df.empty:
                    result_dict[cid] = df
                
                # ì§„í–‰ë¥  ë¡œê·¸ (25% ë‹¨ìœ„)
                if completed % max(1, n_cases // 4) == 0 or completed == n_cases:
                    elapsed = time.time() - start_time
                    logger.info(f"   Progress: {completed}/{n_cases} cases ({elapsed:.1f}s)")
        
        total_time = time.time() - start_time
        total_rows = sum(len(df) for df in result_dict.values())
        logger.info(f"âœ… Signal dict loading complete: {len(result_dict)} cases, {total_rows} total rows ({total_time:.1f}s)")
        
        return result_dict
    
    def _load_signals_dict_sequential(
        self,
        case_ids: List[Any],
        params: List[str],
        apply_temporal: bool
    ) -> Dict[str, pd.DataFrame]:
        """ìˆœì°¨ì ìœ¼ë¡œ Signal Dict ë¡œë“œ"""
        import time
        
        n_cases = len(case_ids)
        logger.info(f"ğŸ“¡ Loading signals dict for {n_cases} cases (sequential)...")
        start_time = time.time()
        
        result_dict: Dict[str, pd.DataFrame] = {}
        
        for i, cid in enumerate(case_ids):
            logger.debug(f"   [{i+1}/{n_cases}] Loading case {cid}...")
            df = self._get_signal_for_case(str(cid), params, apply_temporal)
            if not df.empty:
                result_dict[str(cid)] = df
                
            # ì§„í–‰ë¥  ë¡œê·¸ (ë§¤ 5ê°œë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                elapsed = time.time() - start_time
                logger.info(f"   Progress: {i+1}/{n_cases} cases ({elapsed:.1f}s)")
        
        total_time = time.time() - start_time
        total_rows = sum(len(df) for df in result_dict.values())
        logger.info(f"âœ… Signal dict loading complete: {len(result_dict)} cases, {total_rows} total rows ({total_time:.1f}s)")
        
        return result_dict
    
    def get_merged_data(self, how: str = "inner") -> pd.DataFrame:
        """
        Cohort + Signals ì¡°ì¸ëœ ë°ì´í„° ë°˜í™˜
        
        Args:
            how: ì¡°ì¸ ë°©ì‹ ("inner", "left", "outer")
        
        Returns:
            ì¡°ì¸ëœ DataFrame
        """
        cohort_df = self.get_cohort()
        signals_df = self.get_signals()
        
        if cohort_df.empty:
            return signals_df
        if signals_df.empty:
            return cohort_df
        
        cohort_key = self._join_config.get("cohort_key", "caseid")
        signal_key = self._join_config.get("signal_key", "caseid")
        
        # í‚¤ íƒ€ì… ë§ì¶”ê¸°
        if cohort_key in cohort_df.columns and signal_key in signals_df.columns:
            cohort_df[cohort_key] = cohort_df[cohort_key].astype(str)
            signals_df[signal_key] = signals_df[signal_key].astype(str)
        
        if cohort_key == signal_key:
            return pd.merge(cohort_df, signals_df, on=cohort_key, how=how)
        else:
            return pd.merge(
                cohort_df, signals_df, 
                left_on=cohort_key, right_on=signal_key, 
                how=how
            )
    
    def iter_cases(
        self,
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True
    ) -> Iterator[Dict[str, Any]]:
        """
        ì¼€ì´ìŠ¤ë³„ ë°ì´í„° Iterator (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ìš©)
        
        Yields:
            {
                "caseid": str,
                "cohort": pd.Series,      # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ë©”íƒ€ë°ì´í„°
                "signals": pd.DataFrame,  # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ì‹ í˜¸ ë°ì´í„°
                "temporal_range": (start, end) or None
            }
        """
        cohort_df = self.get_cohort()
        case_ids = self.get_case_ids()
        
        for cid in case_ids:
            # Cohort row
            cohort_key = self._join_config.get("cohort_key", "caseid")
            cohort_row = cohort_df[cohort_df[cohort_key].astype(str) == str(cid)]
            cohort_series = cohort_row.iloc[0] if not cohort_row.empty else pd.Series()
            
            # Signals
            signals = self._get_signal_for_case(str(cid), param_keys, apply_temporal)
            
            # Temporal range
            temporal_range = None
            if apply_temporal and not cohort_series.empty:
                temporal_range = self._get_temporal_range(cohort_series)
            
            yield {
                "caseid": str(cid),
                "cohort": cohort_series,
                "signals": signals,
                "temporal_range": temporal_range
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Query Helpers
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_case_ids(self, signals_only: bool = True) -> List[str]:
        """ì¼€ì´ìŠ¤ ID ëª©ë¡ ë°˜í™˜
        
        Args:
            signals_only: Trueë©´ Signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’)
                         Falseë©´ Cohort ì „ì²´ ì¼€ì´ìŠ¤ ë°˜í™˜
        
        Returns:
            ì¼€ì´ìŠ¤ ID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        if signals_only:
            # Signal íŒŒì¼ì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ
            return [f.get("caseid") for f in self._signal_files if f.get("caseid")]
        else:
            # Cohort ì „ì²´ ì¼€ì´ìŠ¤
            cohort = self.get_cohort()
            if cohort.empty:
                return []
            
            entity_col = self._cohort_entity_id or "caseid"
            if entity_col in cohort.columns:
                return cohort[entity_col].astype(str).unique().tolist()
            return []
    
    def get_available_case_ids(self) -> List[str]:
        """ë¶„ì„ ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ ID (Cohortì™€ Signal êµì§‘í•©)"""
        cohort_ids = set(self.get_case_ids(signals_only=False))
        signal_ids = set(self.get_case_ids(signals_only=True))
        return sorted(list(cohort_ids & signal_ids))
    
    def get_available_parameters(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í‚¤ ëª©ë¡"""
        return self._param_keys.copy()
    
    def is_loaded(self) -> bool:
        """Planì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self._plan is not None
    
    def summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ìš”ì•½"""
        cohort_loaded = self._cohort_file_id in DataContext._cohort_cache
        signals_cached = len([
            cid for cid in self.get_case_ids() 
            if cid in DataContext._signal_cache
        ])
        
        return {
            "loaded_at": self._loaded_at.isoformat() if self._loaded_at else None,
            "cohort": {
                "file_id": self._cohort_file_id,
                "file_path": self._cohort_file_path,
                "loaded": cohort_loaded,
                "filters_count": len(self._cohort_filters),
                "total_cases": len(self.get_case_ids()) if cohort_loaded else 0
            },
            "signals": {
                "group_id": self._signal_group_id,
                "total_files": len(self._signal_files),
                "cached_count": signals_cached,
                "param_keys": self._param_keys,
                "temporal_type": self._temporal_config.get("type", "full_record")
            },
            "cache_stats": {
                "cohort_cache_size": len(DataContext._cohort_cache),
                "signal_cache_size": len(DataContext._signal_cache)
            }
        }
    
    @classmethod
    def clear_cache(cls, cache_type: str = "all") -> None:
        """
        ìºì‹œ ì •ë¦¬
        
        Args:
            cache_type: "all", "signals", "cohort"
        """
        if cache_type in ("all", "signals"):
            cls._signal_cache.clear()
        if cache_type in ("all", "cohort"):
            cls._cohort_cache.clear()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AnalysisAgent ì§€ì› ë©”ì„œë“œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_analysis_context(self) -> Dict[str, Any]:
        """
        LLM ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
        
        Returns:
            {
                "description": str,
                "cohort": {...},
                "signals": {...},
                "original_query": str
            }
        """
        cohort = self.get_cohort()
        case_ids = self.get_case_ids()
        
        # Cohort ì •ë³´
        cohort_info = {
            "total_cases": len(case_ids),
            "filters_applied": self._cohort_filters,
            "entity_identifier": self._cohort_entity_id,
            "columns": self._get_cohort_column_info(cohort)
        }
        
        # Signal ì •ë³´
        signal_info = {
            "parameters": self._param_info,
            "param_keys": self._param_keys,
            "temporal_setting": {
                "type": self._temporal_config.get("type", "full_record"),
                "margin_seconds": self._temporal_config.get("margin_seconds", 0),
                "start_column": self._temporal_config.get("start_column"),
                "end_column": self._temporal_config.get("end_column"),
                "description": self._get_temporal_description()
            },
            "available_files": len(self._signal_files)
        }
        
        # Description ìƒì„±
        description = self._generate_description(cohort_info, signal_info)
        
        return {
            "description": description,
            "cohort": cohort_info,
            "signals": signal_info,
            "original_query": self._plan.get("original_query", "") if self._plan else ""
        }
    
    def generate_access_guide(
        self,
        signals_dict: Optional[Dict[str, pd.DataFrame]] = None,
        cohort_df: Optional[pd.DataFrame] = None,
        include_examples: bool = True
    ) -> str:
        """
        í˜„ì¬ ë°ì´í„° êµ¬ì¡°ì— ê¸°ë°˜í•œ ë™ì  ì ‘ê·¼ ê°€ì´ë“œ ìƒì„±
        
        LLMì´ ì½”ë“œë¥¼ ìƒì„±í•  ë•Œ ë°ì´í„° ì ‘ê·¼ ë°©ì‹ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡
        ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì´ë“œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            signals_dict: ì¼€ì´ìŠ¤ë³„ Signal DataFrame Dict
            cohort_df: Cohort DataFrame
            include_examples: ì½”ë“œ ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€
        
        Returns:
            LLM í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ë°ì´í„° ì ‘ê·¼ ê°€ì´ë“œ ë¬¸ìì—´
        """
        guide_parts = ["## Available Data\n"]
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Signals ê°€ì´ë“œ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if signals_dict and len(signals_dict) > 0:
            case_ids = list(signals_dict.keys())
            sample_cid = case_ids[0]
            sample_df = signals_dict[sample_cid]
            columns = list(sample_df.columns)
            
            # ì»¬ëŸ¼ë³„ íƒ€ì… ë¶„ì„
            numeric_cols = [c for c in columns if c != 'Time' and sample_df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
            
            guide_parts.append(f"""### signals: Dict[caseid â†’ DataFrame]
- **Type**: Case-level independent time series data
- **Loaded cases**: {case_ids[:5]}{'...' if len(case_ids) > 5 else ''} (total: {len(case_ids)})
- **Total cases in dataset**: {len(self.get_case_ids())}
- **Each DataFrame**:
  - Columns: {columns}
  - Numeric columns for analysis: {numeric_cols}
  - Sample shape: {sample_df.shape}
""")
            
            if include_examples:
                guide_parts.append("""
**Access Patterns:**
```python
# Single case access
signals['caseid']['ColumnName'].mean()

# Iterate all cases (RECOMMENDED for statistics)
case_stats = {cid: df['ColumnName'].mean() for cid, df in signals.items()}
overall_mean = np.mean(list(case_stats.values()))  # Mean of case means

# Conditional analysis (with cohort)
target_cases = cohort[cohort['column'] == 'value']['caseid'].astype(str).tolist()
filtered_signals = {cid: signals[cid] for cid in target_cases if cid in signals}

# Per-case correlation
case_corrs = {cid: df['Col1'].corr(df['Col2']) for cid, df in signals.items()}
mean_corr = np.nanmean(list(case_corrs.values()))
```

âš ï¸ **WARNING**: Do NOT concat all cases into one DataFrame and compute statistics directly.
   Each case has independent time axis. Use per-case computation then aggregate.
""")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Cohort ê°€ì´ë“œ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if cohort_df is not None and not cohort_df.empty:
            cohort_columns = list(cohort_df.columns)
            
            # ì£¼ìš” ì»¬ëŸ¼ ë¶„ë¥˜
            id_cols = [c for c in cohort_columns if 'id' in c.lower() or 'case' in c.lower()]
            numeric_cols = [c for c in cohort_columns if cohort_df[c].dtype in ['float64', 'int64', 'float32', 'int32']][:10]
            categorical_cols = [c for c in cohort_columns if cohort_df[c].dtype == 'object'][:10]
            
            guide_parts.append(f"""
### cohort: DataFrame
- **Shape**: {cohort_df.shape}
- **ID columns**: {id_cols}
- **Sample numeric columns**: {numeric_cols}{'...' if len(numeric_cols) >= 10 else ''}
- **Sample categorical columns**: {categorical_cols}{'...' if len(categorical_cols) >= 10 else ''}
- **All columns**: {cohort_columns[:20]}{'...' if len(cohort_columns) > 20 else ''}
""")
            
            if include_examples:
                guide_parts.append("""
**Access Patterns:**
```python
# Filter by condition
filtered = cohort[cohort['sex'] == 'M']
case_list = cohort[cohort['age'] > 60]['caseid'].astype(str).tolist()

# Get metadata for specific case
case_info = cohort[cohort['caseid'] == int(caseid)].iloc[0]
```
""")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì¼ë°˜ ê°€ì´ë“œë¼ì¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        guide_parts.append("""
## Analysis Guidelines

1. **Statistics across cases**: Always compute per-case first, then aggregate
2. **Join signals with cohort**: Use caseid to link cohort metadata with signal data
3. **Handle missing data**: Use `dropna()` or check for NaN before calculations
4. **Result variable**: Assign final result to `result` variable
""")
        
        return "\n".join(guide_parts)
    
    def compute_statistics(
        self,
        param_keys: Optional[List[str]] = None,
        percentiles: List[float] = [0.25, 0.5, 0.75],
        sample_size: Optional[int] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        íŒŒë¼ë¯¸í„°ë³„ í†µê³„ ê³„ì‚°
        
        Args:
            param_keys: ê³„ì‚°í•  íŒŒë¼ë¯¸í„° (Noneì´ë©´ ì „ì²´)
            percentiles: ê³„ì‚°í•  ë°±ë¶„ìœ„ìˆ˜
            sample_size: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            {
                "Solar8000/HR": {
                    "count": int,
                    "mean": float,
                    "std": float,
                    "min": float,
                    "max": float,
                    "percentiles": {"25%": ..., "50%": ..., "75%": ...}
                }
            }
        """
        params = param_keys or self._param_keys
        case_ids = self.get_case_ids()
        
        if sample_size and sample_size < len(case_ids):
            import random
            case_ids = random.sample(case_ids, sample_size)
        
        # ëª¨ë“  ì¼€ì´ìŠ¤ì˜ ë°ì´í„° ìˆ˜ì§‘
        all_data = {p: [] for p in params}
        
        for cid in case_ids:
            signals = self._get_signal_for_case(cid, params, apply_temporal=True)
            if signals.empty:
                continue
            
            for p in params:
                if p in signals.columns:
                    values = signals[p].dropna().tolist()
                    all_data[p].extend(values)
        
        # í†µê³„ ê³„ì‚°
        stats = {}
        for p in params:
            values = all_data[p]
            if not values:
                stats[p] = {"count": 0, "error": "No data available"}
                continue
            
            series = pd.Series(values)
            pct_dict = {f"{int(q*100)}%": series.quantile(q) for q in percentiles}
            
            stats[p] = {
                "count": len(values),
                "mean": round(series.mean(), 4),
                "std": round(series.std(), 4),
                "min": round(series.min(), 4),
                "max": round(series.max(), 4),
                "percentiles": {k: round(v, 4) for k, v in pct_dict.items()}
            }
        
        return stats
    
    def get_sample_data(
        self,
        n_cases: int = 3,
        n_rows_per_case: int = 5
    ) -> List[Dict[str, Any]]:
        """
        LLMì—ê²Œ ë³´ì—¬ì¤„ ìƒ˜í”Œ ë°ì´í„°
        
        Args:
            n_cases: ìƒ˜í”Œë§í•  ì¼€ì´ìŠ¤ ìˆ˜
            n_rows_per_case: ì¼€ì´ìŠ¤ë‹¹ ìƒ˜í”Œ í–‰ ìˆ˜
        
        Returns:
            [
                {
                    "caseid": str,
                    "cohort_sample": {...},
                    "signal_sample": [...]
                }
            ]
        """
        case_ids = self.get_case_ids()[:n_cases]
        cohort = self.get_cohort()
        
        samples = []
        for cid in case_ids:
            # Cohort ìƒ˜í”Œ
            cohort_key = self._join_config.get("cohort_key", "caseid")
            cohort_row = cohort[cohort[cohort_key].astype(str) == str(cid)]
            cohort_sample = cohort_row.iloc[0].to_dict() if not cohort_row.empty else {}
            
            # Signal ìƒ˜í”Œ
            signals = self._get_signal_for_case(str(cid), apply_temporal=True)
            signal_sample = []
            if not signals.empty:
                sample_df = signals.head(n_rows_per_case)
                signal_sample = sample_df.to_dict(orient="records")
            
            samples.append({
                "caseid": str(cid),
                "cohort_sample": cohort_sample,
                "signal_sample": signal_sample
            })
        
        return samples
    
    def get_parameter_info(self, param_key: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • íŒŒë¼ë¯¸í„°ì˜ ìƒì„¸ ì •ë³´"""
        for p in self._param_info:
            if param_key in p.get("param_keys", []):
                return {
                    "term": p.get("term"),
                    "param_key": param_key,
                    "semantic_name": p.get("semantic_name"),
                    "unit": p.get("unit"),
                    "resolution_mode": p.get("resolution_mode"),
                    "confidence": p.get("confidence")
                }
        return None
    
    def to_execution_context(
        self,
        include_signals: bool = True,
        sample_rows: int = 3,
        max_signal_cases: int = 3
    ) -> Dict[str, Any]:
        """
        Code Generationì„ ìœ„í•œ ExecutionContext ë°ì´í„° ìƒì„±
        
        AnalysisAgentì˜ ExecutionContext ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.
        DataSchema ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ LLMì´ ì •í™•í•œ ì»¬ëŸ¼ëª…ì„ ì•Œ ìˆ˜ ìˆë„ë¡ í•¨.
        
        Args:
            include_signals: Signal ë°ì´í„° ìŠ¤í‚¤ë§ˆ í¬í•¨ ì—¬ë¶€
            sample_rows: ìƒ˜í”Œ ë°ì´í„° í–‰ ìˆ˜
            max_signal_cases: Signal ìƒ˜í”Œë§í•  ìµœëŒ€ ì¼€ì´ìŠ¤ ìˆ˜
        
        Returns:
            ExecutionContext ìƒì„±ì— í•„ìš”í•œ ë”•ì…”ë„ˆë¦¬
            {
                "available_variables": {...},
                "available_imports": [...],
                "data_schemas": {
                    "df": {...},
                    "cohort": {...}
                }
            }
        
        Example:
            ctx_data = data_context.to_execution_context()
            from AnalysisAgent.src.models import ExecutionContext, DataSchema
            exec_ctx = ExecutionContext(
                available_variables=ctx_data["available_variables"],
                data_schemas={k: DataSchema(**v) for k, v in ctx_data["data_schemas"].items()}
            )
        """
        cohort = self.get_cohort()
        case_ids = self.get_case_ids()
        
        # 1. Available Variables
        available_variables = {}
        
        if not cohort.empty:
            available_variables["cohort"] = (
                f"pandas DataFrame - Cohort ë©”íƒ€ë°ì´í„°, "
                f"{len(cohort)} rows Ã— {len(cohort.columns)} columns"
            )
        
        if include_signals and case_ids:
            available_variables["df"] = (
                f"pandas DataFrame - Signal ë°ì´í„°, "
                f"columns: [Time, {', '.join(self._param_keys[:5])}{'...' if len(self._param_keys) > 5 else ''}]"
            )
        
        available_variables["case_ids"] = f"List[str] - {len(case_ids)}ê°œ ì¼€ì´ìŠ¤ ID"
        available_variables["param_keys"] = f"List[str] - {self._param_keys}"
        
        # 2. Data Schemas
        data_schemas = {}
        
        # Cohort ìŠ¤í‚¤ë§ˆ
        if not cohort.empty:
            cohort_schema = self._build_data_schema(
                name="cohort",
                description="Cohort ë©”íƒ€ë°ì´í„° (í™˜ì ì •ë³´)",
                df=cohort,
                sample_rows=sample_rows
            )
            data_schemas["cohort"] = cohort_schema
        
        # Signal ìŠ¤í‚¤ë§ˆ (ìƒ˜í”Œ ì¼€ì´ìŠ¤ì—ì„œ ì¶”ì¶œ)
        if include_signals and case_ids:
            sample_case = case_ids[0] if case_ids else None
            if sample_case:
                signals = self._get_signal_for_case(sample_case, apply_temporal=True)
                if not signals.empty:
                    # ì—¬ëŸ¬ ì¼€ì´ìŠ¤ì˜ shape ì¶”ì •
                    total_rows = len(signals) * len(case_ids)
                    signals_schema = self._build_data_schema(
                        name="df",
                        description=f"Signal ë°ì´í„° (ìƒì²´ì‹ í˜¸, {len(case_ids)} cases)",
                        df=signals,
                        sample_rows=sample_rows,
                        override_shape=(total_rows, len(signals.columns))
                    )
                    data_schemas["df"] = signals_schema
        
        # 3. Available Imports
        available_imports = [
            "pandas as pd",
            "numpy as np", 
            "scipy.stats as stats",
            "datetime",
            "math",
        ]
        
        return {
            "available_variables": available_variables,
            "available_imports": available_imports,
            "data_schemas": data_schemas,
            "case_ids": case_ids,
            "param_keys": self._param_keys,
        }
    
    def _build_data_schema(
        self,
        name: str,
        description: str,
        df: pd.DataFrame,
        sample_rows: int = 3,
        override_shape: Optional[Tuple[int, int]] = None
    ) -> Dict[str, Any]:
        """DataFrameì—ì„œ DataSchema ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        # ì»¬ëŸ¼ ì •ë³´
        columns = list(df.columns)
        dtypes = {col: str(df[col].dtype) for col in columns}
        
        # Shape
        shape = override_shape or (len(df), len(df.columns))
        
        # ìƒ˜í”Œ í–‰
        sample_data = None
        if sample_rows > 0 and not df.empty:
            sample_df = df.head(sample_rows)
            sample_data = sample_df.to_dict(orient="records")
            # ìˆ«ì ë°˜ì˜¬ë¦¼
            for row in sample_data:
                for k, v in row.items():
                    if isinstance(v, float):
                        row[k] = round(v, 4)
        
        # ì»¬ëŸ¼ í†µê³„
        column_stats = {}
        for col in columns[:10]:  # ìµœëŒ€ 10ê°œ ì»¬ëŸ¼ë§Œ
            if pd.api.types.is_numeric_dtype(df[col]):
                column_stats[col] = {
                    "type": "numeric",
                    "mean": round(df[col].mean(), 4) if not df[col].isna().all() else None,
                    "min": round(df[col].min(), 4) if not df[col].isna().all() else None,
                    "max": round(df[col].max(), 4) if not df[col].isna().all() else None,
                }
            else:
                column_stats[col] = {
                    "type": "categorical",
                    "unique_count": df[col].nunique(),
                    "sample_values": df[col].dropna().head(5).tolist(),
                }
        
        return {
            "name": name,
            "description": description,
            "columns": columns,
            "dtypes": dtypes,
            "shape": shape,
            "sample_rows": sample_data,
            "column_stats": column_stats,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - DB ì¡°íšŒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _resolve_file_path(self, file_id: str) -> Optional[str]:
        """file_id â†’ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (DB ì¡°íšŒ)"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT file_path FROM file_catalog
                WHERE file_id = %s
            """, (file_id,))
            
            row = cursor.fetchone()
            conn.commit()
            
            return row[0] if row else None
        except Exception as e:
            print(f"[DataContext] Error resolving file path: {e}")
            return None
    
    def _resolve_signal_files(self, group_id: str) -> List[Dict[str, Any]]:
        """group_id â†’ í•´ë‹¹ ê·¸ë£¹ì˜ ëª¨ë“  signal íŒŒì¼ ì¡°íšŒ"""
        try:
            conn = self.db.get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT file_id, file_path, filename_values
                FROM file_catalog
                WHERE group_id = %s
                ORDER BY file_name
            """, (group_id,))
            
            rows = cursor.fetchall()
            conn.commit()
            
            files = []
            for row in rows:
                file_id, file_path, filename_values = row
                caseid = None
                if filename_values and isinstance(filename_values, dict):
                    caseid = filename_values.get("caseid")
                
                files.append({
                    "file_id": str(file_id),
                    "file_path": file_path,
                    "caseid": str(caseid) if caseid else None
                })
            
            return files
        except Exception as e:
            print(f"[DataContext] Error resolving signal files: {e}")
            return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - ë°ì´í„° ë¡œë“œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _load_cohort_to_cache(self) -> None:
        """Cohort ë°ì´í„°ë¥¼ ìºì‹œì— ë¡œë“œ"""
        if not self._cohort_file_path:
            return
        
        if self._cohort_file_id in DataContext._cohort_cache:
            return  # ì´ë¯¸ ìºì‹œë¨
        
        try:
            df = self._tabular_processor.load_data(self._cohort_file_path)
            DataContext._cohort_cache[self._cohort_file_id] = df
        except Exception as e:
            print(f"[DataContext] Error loading cohort: {e}")
    
    def _get_signal_for_case(
        self, 
        caseid: str, 
        param_keys: Optional[List[str]] = None,
        apply_temporal: bool = True
    ) -> pd.DataFrame:
        """íŠ¹ì • ì¼€ì´ìŠ¤ì˜ signal ë°ì´í„° ë¡œë“œ"""
        params = param_keys or self._param_keys
        
        # ìºì‹œ í™•ì¸
        if caseid in DataContext._signal_cache:
            df = DataContext._signal_cache[caseid]
        else:
            # íŒŒì¼ ì°¾ê¸°
            file_info = None
            for f in self._signal_files:
                if f.get("caseid") == caseid:
                    file_info = f
                    break
            
            if not file_info or not file_info.get("file_path"):
                return pd.DataFrame()
            
            # ë¡œë“œ
            try:
                df = self._signal_processor.load_data(
                    file_info["file_path"],
                    columns=params
                )
                DataContext._signal_cache[caseid] = df
            except Exception as e:
                print(f"[DataContext] Error loading signal for {caseid}: {e}")
                return pd.DataFrame()
        
        # íŒŒë¼ë¯¸í„° í•„í„°ë§
        if params:
            available_cols = ["Time"] + [p for p in params if p in df.columns]
            df = df[available_cols] if available_cols else df
        
        # Temporal í•„í„° ì ìš©
        if apply_temporal and self._temporal_config.get("type", "full_record") != "full_record":
            cohort = self.get_cohort()
            cohort_key = self._join_config.get("cohort_key", "caseid")
            cohort_row = cohort[cohort[cohort_key].astype(str) == str(caseid)]
            
            if not cohort_row.empty:
                df = self._apply_temporal_filter(df, cohort_row.iloc[0])
        
        return df
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - í•„í„°ë§
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _apply_cohort_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cohort í•„í„° ì ìš©"""
        if df.empty or not self._cohort_filters:
            return df
        
        for f in self._cohort_filters:
            col = f.get("column")
            op = f.get("operator", "=")
            val = f.get("value")
            
            if col not in df.columns:
                continue
            
            op_upper = op.upper()
            
            if op_upper == "=" or op == "==":
                df = df[df[col] == val]
            elif op_upper == "!=" or op == "<>":
                df = df[df[col] != val]
            elif op_upper == ">":
                df = df[df[col] > val]
            elif op_upper == ">=":
                df = df[df[col] >= val]
            elif op_upper == "<":
                df = df[df[col] < val]
            elif op_upper == "<=":
                df = df[df[col] <= val]
            elif op_upper == "LIKE":
                pattern = str(val).replace('%', '.*')
                df = df[df[col].astype(str).str.contains(pattern, case=False, na=False, regex=True)]
            elif op_upper == "IN":
                if isinstance(val, list):
                    df = df[df[col].isin(val)]
            elif op_upper == "BETWEEN":
                if isinstance(val, (list, tuple)) and len(val) == 2:
                    df = df[(df[col] >= val[0]) & (df[col] <= val[1])]
        
        return df
    
    def _apply_temporal_filter(
        self, 
        signals_df: pd.DataFrame, 
        cohort_row: pd.Series
    ) -> pd.DataFrame:
        """Temporal alignment ì ìš©"""
        if signals_df.empty:
            return signals_df
        
        temp_type = self._temporal_config.get("type", "full_record")
        if temp_type == "full_record":
            return signals_df
        
        margin = self._temporal_config.get("margin_seconds", 0)
        start_col = self._temporal_config.get("start_column")
        end_col = self._temporal_config.get("end_column")
        
        if not start_col or not end_col:
            return signals_df
        
        start_time = cohort_row.get(start_col)
        end_time = cohort_row.get(end_col)
        
        if pd.isna(start_time) or pd.isna(end_time):
            return signals_df
        
        # Unix timestampë¡œ ë³€í™˜
        start_sec = self._to_seconds(start_time)
        end_sec = self._to_seconds(end_time)
        
        if start_sec is None or end_sec is None:
            return signals_df
        
        # margin ì ìš©
        start_sec = start_sec - margin
        end_sec = end_sec + margin
        
        # í•„í„°ë§
        if "Time" in signals_df.columns:
            return signals_df[
                (signals_df["Time"] >= start_sec) & 
                (signals_df["Time"] <= end_sec)
            ].copy()
        
        return signals_df
    
    def _to_seconds(self, value: Any) -> Optional[float]:
        """ê°’ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, datetime):
            return value.timestamp()
        if isinstance(value, str):
            try:
                dt = pd.to_datetime(value)
                return dt.timestamp()
            except:
                pass
        return None
    
    def _get_temporal_range(self, cohort_row: pd.Series) -> Optional[Tuple[float, float]]:
        """Temporal range ê³„ì‚°"""
        start_col = self._temporal_config.get("start_column")
        end_col = self._temporal_config.get("end_column")
        margin = self._temporal_config.get("margin_seconds", 0)
        
        if not start_col or not end_col:
            return None
        
        start_time = cohort_row.get(start_col)
        end_time = cohort_row.get(end_col)
        
        start_sec = self._to_seconds(start_time)
        end_sec = self._to_seconds(end_time)
        
        if start_sec is not None and end_sec is not None:
            return (start_sec - margin, end_sec + margin)
        return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Private Methods - í—¬í¼
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _get_cohort_column_info(self, cohort: pd.DataFrame) -> List[Dict[str, Any]]:
        """Cohort ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ"""
        columns = []
        for col in cohort.columns:
            col_info = {
                "name": col,
                "dtype": str(cohort[col].dtype),
                "null_count": int(cohort[col].isna().sum()),
                "unique_count": int(cohort[col].nunique())
            }
            
            # ìˆ«ìí˜•ì´ë©´ í†µê³„ ì¶”ê°€
            if pd.api.types.is_numeric_dtype(cohort[col]):
                col_info["type"] = "numeric"
                col_info["stats"] = {
                    "mean": round(cohort[col].mean(), 2) if not cohort[col].isna().all() else None,
                    "min": cohort[col].min() if not cohort[col].isna().all() else None,
                    "max": cohort[col].max() if not cohort[col].isna().all() else None
                }
            else:
                col_info["type"] = "categorical"
                col_info["sample_values"] = cohort[col].dropna().head(5).tolist()
            
            columns.append(col_info)
        
        return columns
    
    def _get_temporal_description(self) -> str:
        """Temporal ì„¤ì • ì„¤ëª… ìƒì„±"""
        temp_type = self._temporal_config.get("type", "full_record")
        margin = self._temporal_config.get("margin_seconds", 0)
        
        descriptions = {
            "full_record": "ì „ì²´ ê¸°ë¡ (ì‹œê°„ ì œí•œ ì—†ìŒ)",
            "surgery_window": f"ìˆ˜ìˆ  ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)",
            "anesthesia_window": f"ë§ˆì·¨ ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)",
            "custom_window": f"ì‚¬ìš©ì ì§€ì • ì‹œê°„ ë²”ìœ„ (ë§ˆì§„: {margin}ì´ˆ)"
        }
        
        return descriptions.get(temp_type, temp_type)
    
    def _generate_description(
        self, 
        cohort_info: Dict[str, Any], 
        signal_info: Dict[str, Any]
    ) -> str:
        """ë°ì´í„° ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""
        parts = []
        
        # ì¼€ì´ìŠ¤ ìˆ˜
        parts.append(f"ì´ {cohort_info['total_cases']}ê°œ ì¼€ì´ìŠ¤ì˜ ë°ì´í„°")
        
        # í•„í„°
        if cohort_info['filters_applied']:
            filter_strs = []
            for f in cohort_info['filters_applied']:
                filter_strs.append(f"{f.get('column')} {f.get('operator')} {f.get('value')}")
            parts.append(f"í•„í„°: {', '.join(filter_strs)}")
        
        # íŒŒë¼ë¯¸í„°
        if signal_info['param_keys']:
            parts.append(f"ì¸¡ì • íŒŒë¼ë¯¸í„°: {', '.join(signal_info['param_keys'][:5])}")
            if len(signal_info['param_keys']) > 5:
                parts.append(f"ì™¸ {len(signal_info['param_keys']) - 5}ê°œ")
        
        # Temporal
        parts.append(f"ì‹œê°„ ë²”ìœ„: {signal_info['temporal_setting']['description']}")
        
        return ". ".join(parts)

