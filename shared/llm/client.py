# shared/llm/client.py
"""
LLM Client with tenacity retry logic

Supports OpenAI and Anthropic (Claude).
"""

import json
import re
import time
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
import logging

# ê° SDK ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from openai import OpenAI, RateLimitError as OpenAIRateLimitError, APIError as OpenAIAPIError
except ImportError:
    OpenAI = None
    OpenAIRateLimitError = Exception
    OpenAIAPIError = Exception

try:
    from anthropic import Anthropic, RateLimitError as AnthropicRateLimitError, APIError as AnthropicAPIError
except ImportError:
    Anthropic = None
    AnthropicRateLimitError = Exception
    AnthropicAPIError = Exception


# config íŒŒì¼ ì„í¬íŠ¸
from shared.config import LLMConfig

# Logger for retry logging
logger = logging.getLogger(__name__)


class LLMRetryConfig:
    """LLM ì¬ì‹œë„ ì„¤ì •"""
    MAX_ATTEMPTS = 3
    MIN_WAIT_SECONDS = 1
    MAX_WAIT_SECONDS = 10
    EXPONENTIAL_BASE = 2


# Common retryable exceptions
RETRYABLE_EXCEPTIONS = (
    OpenAIRateLimitError,
    OpenAIAPIError,
    AnthropicRateLimitError,
    AnthropicAPIError,
    ConnectionError,
    TimeoutError,
)


def create_retry_decorator():
    """Create a retry decorator with configured settings"""
    return retry(
        stop=stop_after_attempt(LLMRetryConfig.MAX_ATTEMPTS),
        wait=wait_exponential(
            multiplier=LLMRetryConfig.MIN_WAIT_SECONDS,
            min=LLMRetryConfig.MIN_WAIT_SECONDS,
            max=LLMRetryConfig.MAX_WAIT_SECONDS,
            exp_base=LLMRetryConfig.EXPONENTIAL_BASE
        ),
        retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        reraise=True
    )


class AbstractLLMClient(ABC):
    """
    ëª¨ë“  LLM í´ë¼ì´ì–¸íŠ¸ê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê³µí†µ ì¸í„°í˜ì´ìŠ¤
    """
    
    @abstractmethod
    def ask_text(self, prompt: str, max_tokens: int = None) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìš”ì²­
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        pass

    def ask_json(self, prompt: str, max_tokens: int = None) -> Dict[str, Any]:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ JSON ê°ì²´(Dict)ë¡œ ë°˜í™˜.
        JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„í•˜ê±°ë‚˜ ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ëŠ” ë¡œì§ í¬í•¨.
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        system_instruction = (
            "\n\n[SYSTEM IMPORTANT]: You MUST respond with valid JSON only. "
            "Do not add markdown code blocks (```json). Do not add explanations."
        )
        full_prompt = prompt + system_instruction
        
        raw_response = self.ask_text(full_prompt, max_tokens=max_tokens)
        
        return self._clean_and_parse_json(raw_response)

    def _clean_and_parse_json(self, raw_text: str) -> Dict[str, Any]:
        """
        LLMì´ í”íˆ ì €ì§€ë¥´ëŠ” ì‹¤ìˆ˜(Markdown backticks ë“±)ë¥¼ ì œê±°í•˜ê³  JSON íŒŒì‹±
        """
        try:
            # 1. ```json ... ``` íŒ¨í„´ ì œê±°
            text = re.sub(r"```json\s*", "", raw_text, flags=re.IGNORECASE)
            text = re.sub(r"```", "", text)
            
            # 2. ì•ë’¤ ê³µë°± ì œê±°
            text = text.strip()
            
            return json.loads(text)
        except json.JSONDecodeError:
            print(f"[LLM Error] JSON Parsing Failed. Raw text:\n{raw_text[:500]}...")
            return {"error": "JSON_DECODE_ERROR", "raw_text": raw_text}


class OpenAIClient(AbstractLLMClient):
    """OpenAI (ChatGPT) client with retry"""
    
    # Models that require max_completion_tokens instead of max_tokens
    NEW_API_MODELS = {'o1', 'o3', 'gpt-5', 'gpt-4o'}
    
    def __init__(self):
        if not OpenAI:
            raise ImportError("OpenAI library not installed. pip install openai")
        if not LLMConfig.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is not set in config.")
            
        self.client = OpenAI(api_key=LLMConfig.OPENAI_API_KEY)
        self.model = LLMConfig.OPENAI_MODEL
        
        # Check if model uses new API (max_completion_tokens)
        self._use_new_api = any(
            self.model.lower().startswith(prefix) 
            for prefix in self.NEW_API_MODELS
        )
    
    def _get_token_param(self, max_tokens: int = None) -> Dict[str, int]:
        """Get the correct token parameter based on model type"""
        token_value = max_tokens or LLMConfig.MAX_TOKENS
        if self._use_new_api:
            return {"max_completion_tokens": token_value}
        else:
            return {"max_tokens": token_value}

    @create_retry_decorator()
    def ask_text(self, prompt: str, max_tokens: int = None) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=LLMConfig.TEMPERATURE,
            **self._get_token_param(max_tokens)
        )
        return response.choices[0].message.content

    @create_retry_decorator()
    def ask_json(self, prompt: str, max_tokens: int = None) -> Dict[str, Any]:
        """OpenAIëŠ” JSON Modeë¥¼ ì§€ì›í•˜ë¯€ë¡œ ì˜¤ë²„ë¼ì´ë”©í•´ì„œ ìµœì í™”"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=LLMConfig.TEMPERATURE,
                **self._get_token_param(max_tokens)
            )
            return json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            return {"error": "JSON_DECODE_ERROR"}
        except Exception as e:
            return {"error": str(e)}


class ClaudeClient(AbstractLLMClient):
    """Anthropic (Claude) client with retry"""
    
    def __init__(self):
        if not Anthropic:
            raise ImportError("Anthropic library not installed. pip install anthropic")
        if not LLMConfig.ANTHROPIC_API_KEY:
            raise ValueError("ANTHROPIC_API_KEY is not set in config.")

        self.client = Anthropic(api_key=LLMConfig.ANTHROPIC_API_KEY)
        self.model = LLMConfig.ANTHROPIC_MODEL

    @create_retry_decorator()
    def ask_text(self, prompt: str, max_tokens: int = None) -> str:
        message = self.client.messages.create(
            model=self.model,
            max_tokens=max_tokens or LLMConfig.MAX_TOKENS,
            temperature=LLMConfig.TEMPERATURE,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text


# Singleton instance cache
_llm_client_instance = None


def get_llm_client() -> AbstractLLMClient:
    """
    config.pyì˜ ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (Singleton)
    """
    global _llm_client_instance
    
    if _llm_client_instance is not None:
        return _llm_client_instance
    
    provider = LLMConfig.ACTIVE_PROVIDER

    if provider == "openai":
        _llm_client_instance = OpenAIClient()
    elif provider in ("anthropic", "claude"):
        _llm_client_instance = ClaudeClient()
    else:
        raise ValueError(f"Unsupported LLM Provider: {provider}. Supported: openai, anthropic")
    
    return _llm_client_instance


def reset_llm_client():
    """Reset the singleton instance (useful for testing)"""
    global _llm_client_instance
    _llm_client_instance = None


# =============================================================================
# LLM Logging (Input/Output ì €ì¥)
# =============================================================================

class LoggingLLMClient(AbstractLLMClient):
    """
    LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°ì‹¸ì„œ ëª¨ë“  í˜¸ì¶œì˜ ì…ë ¥/ì¶œë ¥ì„ íŒŒì¼ì— ì €ì¥í•˜ëŠ” ë˜í¼
    
    ì‚¬ìš©ë²•:
        from src.utils.llm_client import enable_llm_logging
        enable_llm_logging("./data/llm_logs")
    """
    
    def __init__(self, client: AbstractLLMClient, log_dir: str):
        self._client = client
        self._log_dir = Path(log_dir)
        self._log_dir.mkdir(parents=True, exist_ok=True)
        self._call_counter = 0
        self._session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì„¸ì…˜ í´ë” ìƒì„±
        self._session_dir = self._log_dir / f"session_{self._session_id}"
        self._session_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"LLM Logging enabled. Logs: {self._session_dir}")
    
    def _get_model_name(self) -> str:
        """ë˜í•‘ëœ í´ë¼ì´ì–¸íŠ¸ì˜ ëª¨ë¸ëª… ë°˜í™˜"""
        if hasattr(self._client, 'model'):
            return self._client.model
        return "unknown"
    
    def _save_log(
        self, 
        method: str, 
        prompt: str, 
        response: Any, 
        duration: float,
        max_tokens: Optional[int] = None,
        error: Optional[str] = None
    ):
        """í˜¸ì¶œ ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥"""
        self._call_counter += 1
        timestamp = datetime.now()
        
        # íŒŒì¼ëª…: 001_ask_json_20260101_123456.json
        filename = f"{self._call_counter:03d}_{method}_{timestamp.strftime('%H%M%S')}.json"
        filepath = self._session_dir / filename
        
        log_data = {
            "call_id": self._call_counter,
            "timestamp": timestamp.isoformat(),
            "method": method,
            "model": self._get_model_name(),
            "duration_seconds": round(duration, 3),
            "input": {
                "prompt": prompt,
                "max_tokens": max_tokens,
            },
            "output": {
                "response": response if isinstance(response, (dict, list)) else str(response),
            },
        }
        
        if error:
            log_data["error"] = error
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        # ì½˜ì†”ì—ë„ ê°„ëµíˆ ì¶œë ¥
        print(f"ğŸ“ LLM Call #{self._call_counter} ({method}) - {duration:.2f}s - Saved to {filename}")
    
    def ask_text(self, prompt: str, max_tokens: int = None) -> str:
        """í…ìŠ¤íŠ¸ ì‘ë‹µ ìš”ì²­ (ë¡œê¹… í¬í•¨)"""
        start_time = time.time()
        error = None
        response = ""
        
        try:
            response = self._client.ask_text(prompt, max_tokens=max_tokens)
            return response
        except Exception as e:
            error = str(e)
            raise
        finally:
            duration = time.time() - start_time
            self._save_log("ask_text", prompt, response, duration, max_tokens, error)
    
    def ask_json(self, prompt: str, max_tokens: int = None) -> Dict[str, Any]:
        """JSON ì‘ë‹µ ìš”ì²­ (ë¡œê¹… í¬í•¨)"""
        start_time = time.time()
        error = None
        response = {}
        
        try:
            response = self._client.ask_json(prompt, max_tokens=max_tokens)
            return response
        except Exception as e:
            error = str(e)
            raise
        finally:
            duration = time.time() - start_time
            self._save_log("ask_json", prompt, response, duration, max_tokens, error)
    
    def get_session_dir(self) -> Path:
        """í˜„ì¬ ì„¸ì…˜ì˜ ë¡œê·¸ ë””ë ‰í† ë¦¬ ë°˜í™˜"""
        return self._session_dir
    
    def get_call_count(self) -> int:
        """í˜„ì¬ê¹Œì§€ì˜ í˜¸ì¶œ íšŸìˆ˜ ë°˜í™˜"""
        return self._call_counter


# Logging ìƒíƒœ
_logging_enabled = False
_logging_dir: Optional[str] = None


def enable_llm_logging(log_dir: str = "./data/llm_logs") -> Path:
    """
    LLM ë¡œê¹… í™œì„±í™”. ì´í›„ ëª¨ë“  LLM í˜¸ì¶œì´ íŒŒì¼ì— ì €ì¥ë¨.
    
    Args:
        log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./data/llm_logs)
    
    Returns:
        ì„¸ì…˜ ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    ì‚¬ìš©ë²•:
        from src.utils.llm_client import enable_llm_logging
        session_dir = enable_llm_logging()
        # ... íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ...
        print(f"Logs saved to: {session_dir}")
    """
    global _llm_client_instance, _logging_enabled, _logging_dir
    
    _logging_enabled = True
    _logging_dir = log_dir
    
    # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ ë˜í•‘
    if _llm_client_instance is not None:
        if not isinstance(_llm_client_instance, LoggingLLMClient):
            _llm_client_instance = LoggingLLMClient(_llm_client_instance, log_dir)
            return _llm_client_instance.get_session_dir()
        else:
            return _llm_client_instance.get_session_dir()
    
    # ì•„ì§ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ë˜í•‘
    provider = LLMConfig.ACTIVE_PROVIDER
    
    if provider == "openai":
        base_client = OpenAIClient()
    elif provider in ("anthropic", "claude"):
        base_client = ClaudeClient()
    else:
        raise ValueError(f"Unsupported LLM Provider: {provider}")
    
    _llm_client_instance = LoggingLLMClient(base_client, log_dir)
    return _llm_client_instance.get_session_dir()


def disable_llm_logging():
    """LLM ë¡œê¹… ë¹„í™œì„±í™”"""
    global _logging_enabled
    _logging_enabled = False
    # ê¸°ì¡´ ì‹±ê¸€í†¤ ë¦¬ì…‹ (ë‹¤ìŒ í˜¸ì¶œì‹œ ë¡œê¹… ì—†ì´ ìƒì„±)
    reset_llm_client()


def get_llm_log_session_dir() -> Optional[Path]:
    """í˜„ì¬ ë¡œê¹… ì„¸ì…˜ ë””ë ‰í† ë¦¬ ë°˜í™˜ (ë¡œê¹…ì´ í™œì„±í™”ëœ ê²½ìš°)"""
    global _llm_client_instance
    if isinstance(_llm_client_instance, LoggingLLMClient):
        return _llm_client_instance.get_session_dir()
    return None
