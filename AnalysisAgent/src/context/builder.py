# AnalysisAgent/src/context/builder.py
"""
Context Builder

DataContext ë˜ëŠ” ì§ì ‘ ì œê³µëœ DataFrameì—ì„œ AnalysisContextë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

Usage:
    # DataContextì—ì„œ êµ¬ì¶•
    from shared.data.context import DataContext
    from AnalysisAgent.src.context import ContextBuilder
    
    data_ctx = DataContext()
    data_ctx.load_from_plan(execution_plan)
    
    builder = ContextBuilder()
    analysis_ctx = builder.build_from_data_context(data_ctx)
    
    # ì§ì ‘ DataFrameì—ì„œ êµ¬ì¶•
    analysis_ctx = builder.build_from_dataframes(
        dataframes={"df": signal_df, "cohort": cohort_df},
        descriptions={"df": "Signal data", "cohort": "Cohort metadata"}
    )
"""

import logging
from typing import Dict, List, Any, Optional, TYPE_CHECKING
import pandas as pd
import numpy as np

from .schema import ColumnInfo, DataFrameSchema, AnalysisContext, ToolInfo

if TYPE_CHECKING:
    from shared.data.context import DataContext

logger = logging.getLogger(__name__)


class ContextBuilder:
    """AnalysisContext êµ¬ì¶•ê¸°"""
    
    def __init__(
        self,
        max_sample_values: int = 5,
        max_unique_values: int = 20,
        max_sample_rows: int = 3,
        compute_statistics: bool = True,
    ):
        """
        Args:
            max_sample_values: ì»¬ëŸ¼ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ê°’ ê°œìˆ˜
            max_unique_values: Categorical ì»¬ëŸ¼ì˜ ìµœëŒ€ ê³ ìœ ê°’ ê°œìˆ˜
            max_sample_rows: DataFrameë‹¹ ìµœëŒ€ ìƒ˜í”Œ í–‰ ê°œìˆ˜
            compute_statistics: Numeric ì»¬ëŸ¼ í†µê³„ ê³„ì‚° ì—¬ë¶€
        """
        self.max_sample_values = max_sample_values
        self.max_unique_values = max_unique_values
        self.max_sample_rows = max_sample_rows
        self.compute_statistics = compute_statistics
        
        # Tool Registryê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ì£¼ì… (Phase 4ì—ì„œ êµ¬í˜„)
        self._tool_registry = None
    
    def set_tool_registry(self, registry: Any) -> None:
        """ToolRegistry ì„¤ì • (í–¥í›„ ì‚¬ìš©)"""
        self._tool_registry = registry
    
    def build_from_data_context(
        self,
        data_context: "DataContext",
        additional_hints: Optional[str] = None,
        previous_results: Optional[List[Dict[str, Any]]] = None,
    ) -> AnalysisContext:
        """
        DataContextì—ì„œ AnalysisContext êµ¬ì¶•
        
        Args:
            data_context: shared.data.context.DataContext ì¸ìŠ¤í„´ìŠ¤
            additional_hints: ì¶”ê°€ íŒíŠ¸ (Orchestratorì—ì„œ ì „ë‹¬)
            previous_results: ì´ì „ ë¶„ì„ ê²°ê³¼ ëª©ë¡
        
        Returns:
            AnalysisContext
        """
        logger.info("ğŸ“Š Building AnalysisContext from DataContext...")
        
        dataframes: Dict[str, pd.DataFrame] = {}
        descriptions: Dict[str, str] = {}
        
        # Cohort data
        cohort = data_context.get_cohort()
        if cohort is not None and not cohort.empty:
            dataframes["cohort"] = cohort
            descriptions["cohort"] = "Cohort metadata"
            logger.debug(f"   Cohort: {cohort.shape}")
        
        # Signal data (merged)
        try:
            signals = data_context.get_merged_data()
            if signals is not None and not signals.empty:
                dataframes["df"] = signals
                descriptions["df"] = "Signal data (merged)"
                logger.debug(f"   Signal (merged): {signals.shape}")
        except Exception as e:
            logger.warning(f"   Could not get merged data: {e}")
        
        # Join í‚¤ ì¶”ì¶œ
        join_keys = self._find_join_keys(dataframes)
        
        return self.build_from_dataframes(
            dataframes=dataframes,
            descriptions=descriptions,
            join_keys=join_keys,
            additional_hints=additional_hints,
            previous_results=previous_results,
        )
    
    def build_from_dataframes(
        self,
        dataframes: Dict[str, pd.DataFrame],
        descriptions: Optional[Dict[str, str]] = None,
        join_keys: Optional[List[str]] = None,
        additional_hints: Optional[str] = None,
        previous_results: Optional[List[Dict[str, Any]]] = None,
    ) -> AnalysisContext:
        """
        DataFrame ë”•ì…”ë„ˆë¦¬ì—ì„œ AnalysisContext êµ¬ì¶•
        
        Args:
            dataframes: {"df": DataFrame, "cohort": DataFrame, ...}
            descriptions: {"df": "Signal data", ...}
            join_keys: ê³µí†µ Join í‚¤ ëª©ë¡
            additional_hints: ì¶”ê°€ íŒíŠ¸
            previous_results: ì´ì „ ë¶„ì„ ê²°ê³¼
        
        Returns:
            AnalysisContext
        """
        descriptions = descriptions or {}
        
        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ êµ¬ì¶•
        data_schemas: Dict[str, DataFrameSchema] = {}
        for name, df in dataframes.items():
            if df is not None and not df.empty:
                schema = self._build_dataframe_schema(
                    df=df,
                    name=name,
                    description=descriptions.get(name, "")
                )
                data_schemas[name] = schema
                logger.debug(f"   Schema built for '{name}': {len(schema.columns)} columns")
        
        # Join í‚¤ ìë™ íƒì§€ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if join_keys is None:
            join_keys = self._find_join_keys(dataframes)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ Tools
        available_tools = self._get_available_tools()
        
        ctx = AnalysisContext(
            data_schemas=data_schemas,
            join_keys=join_keys,
            available_tools=available_tools,
            additional_hints=additional_hints,
            previous_results=previous_results or [],
        )
        
        logger.info(f"âœ… AnalysisContext built: {len(data_schemas)} schemas, {len(join_keys)} join keys")
        
        return ctx
    
    def _build_dataframe_schema(
        self,
        df: pd.DataFrame,
        name: str,
        description: str = "",
    ) -> DataFrameSchema:
        """DataFrameì—ì„œ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ"""
        
        # ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
        columns: List[ColumnInfo] = []
        for col_name in df.columns:
            col_info = self._analyze_column(df[col_name], col_name)
            columns.append(col_info)
        
        # ìƒ˜í”Œ í–‰ ì¶”ì¶œ
        sample_rows = self._get_sample_rows(df)
        
        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ íƒì§€
        index_column = None
        if df.index.name and df.index.name in df.columns:
            index_column = df.index.name
        
        return DataFrameSchema(
            name=name,
            description=description,
            shape=(len(df), len(df.columns)),
            columns=columns,
            sample_rows=sample_rows,
            index_column=index_column,
        )
    
    def _analyze_column(self, series: pd.Series, name: str) -> ColumnInfo:
        """ê°œë³„ ì»¬ëŸ¼ ë¶„ì„"""
        
        # ì›ë³¸ dtype
        original_dtype = str(series.dtype)
        
        # íƒ€ì… ì¶”ë¡ 
        dtype = self._infer_column_type(series)
        
        # Nullable ì²´í¬
        nullable = series.isnull().any()
        
        # ìƒ˜í”Œ ê°’
        sample_values = self._get_sample_values(series)
        
        # í†µê³„ (numericë§Œ)
        statistics = None
        if dtype == "numeric" and self.compute_statistics:
            statistics = self._compute_statistics(series)
        
        # ê³ ìœ ê°’ (categoricalë§Œ)
        unique_values = None
        unique_count = None
        if dtype == "categorical":
            unique_count = series.nunique()
            if unique_count <= self.max_unique_values:
                unique_values = series.dropna().unique().tolist()[:self.max_unique_values]
        
        return ColumnInfo(
            name=name,
            dtype=dtype,
            original_dtype=original_dtype,
            nullable=nullable,
            sample_values=sample_values,
            statistics=statistics,
            unique_values=unique_values,
            unique_count=unique_count,
        )
    
    def _infer_column_type(self, series: pd.Series) -> str:
        """ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡ """
        dtype = series.dtype
        
        # Numeric
        if pd.api.types.is_numeric_dtype(dtype):
            return "numeric"
        
        # Boolean
        if pd.api.types.is_bool_dtype(dtype):
            return "boolean"
        
        # Datetime
        if pd.api.types.is_datetime64_any_dtype(dtype):
            return "datetime"
        
        # Object â†’ Categorical or Text
        if dtype == object:
            # ìƒ˜í”Œë§í•´ì„œ íŒë‹¨
            sample = series.dropna().head(100)
            if len(sample) == 0:
                return "unknown"
            
            # ê³ ìœ ê°’ ë¹„ìœ¨ë¡œ íŒë‹¨
            unique_ratio = sample.nunique() / len(sample)
            
            # ëŒ€ë¶€ë¶„ ê³ ìœ í•˜ë©´ Text, ì•„ë‹ˆë©´ Categorical
            if unique_ratio > 0.5:
                # í‰ê·  ë¬¸ìì—´ ê¸¸ì´ë„ ê³ ë ¤
                avg_len = sample.astype(str).str.len().mean()
                if avg_len > 50:
                    return "text"
            
            return "categorical"
        
        # Categorical dtype
        if pd.api.types.is_categorical_dtype(dtype):
            return "categorical"
        
        return "unknown"
    
    def _get_sample_values(self, series: pd.Series) -> List[Any]:
        """ìƒ˜í”Œ ê°’ ì¶”ì¶œ"""
        non_null = series.dropna()
        if len(non_null) == 0:
            return []
        
        # ëœë¤ ìƒ˜í”Œ ëŒ€ì‹  ì²« Nê°œ ì‚¬ìš© (ì¬í˜„ì„±)
        samples = non_null.head(self.max_sample_values).tolist()
        
        # numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        return self._convert_to_python_types(samples)
    
    def _convert_to_python_types(self, values: List[Any]) -> List[Any]:
        """numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        result = []
        for v in values:
            if isinstance(v, (np.integer,)):
                result.append(int(v))
            elif isinstance(v, (np.floating,)):
                result.append(float(v))
            elif isinstance(v, (np.bool_,)):
                result.append(bool(v))
            elif pd.isna(v):
                result.append(None)
            else:
                result.append(v)
        return result
    
    def _compute_statistics(self, series: pd.Series) -> Dict[str, float]:
        """Numeric ì»¬ëŸ¼ í†µê³„ ê³„ì‚°"""
        try:
            non_null = series.dropna()
            total_count = len(series)
            null_count = series.isnull().sum()
            
            if len(non_null) == 0:
                return {
                    "null_count": int(null_count),
                    "null_ratio": 1.0,
                }
            
            return {
                "min": float(non_null.min()),
                "max": float(non_null.max()),
                "mean": float(non_null.mean()),
                "std": float(non_null.std()) if len(non_null) > 1 else 0.0,
                "median": float(non_null.median()),
                "null_count": int(null_count),
                "null_ratio": float(null_count / total_count) if total_count > 0 else 0.0,
            }
        except Exception as e:
            logger.warning(f"Could not compute statistics: {e}")
            return {}
    
    def _get_sample_rows(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ í–‰ ì¶”ì¶œ"""
        sample_df = df.head(self.max_sample_rows)
        
        # numpy íƒ€ì… ë³€í™˜
        records = sample_df.to_dict(orient="records")
        
        return [
            {
                k: (float(v) if isinstance(v, (np.floating,)) else
                    int(v) if isinstance(v, (np.integer,)) else
                    None if pd.isna(v) else v)
                for k, v in record.items()
            }
            for record in records
        ]
    
    def _find_join_keys(self, dataframes: Dict[str, pd.DataFrame]) -> List[str]:
        """ê³µí†µ Join í‚¤ íƒì§€"""
        if len(dataframes) < 2:
            return []
        
        # ëª¨ë“  DataFrameì˜ ì»¬ëŸ¼ ìˆ˜ì§‘
        all_columns = [set(df.columns) for df in dataframes.values() if df is not None]
        
        if not all_columns:
            return []
        
        # êµì§‘í•© (ëª¨ë“  DataFrameì— ìˆëŠ” ì»¬ëŸ¼)
        common_columns = set.intersection(*all_columns)
        
        # ì¼ë°˜ì ì¸ í‚¤ ì»¬ëŸ¼ ì´ë¦„ íŒ¨í„´
        key_patterns = ["id", "caseid", "case_id", "subject_id", "patient_id", "key"]
        
        join_keys = []
        for col in common_columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in key_patterns):
                join_keys.append(col)
        
        return join_keys
    
    def _get_available_tools(self) -> List[ToolInfo]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Tools ëª©ë¡ (ToolRegistryì—ì„œ)"""
        # Phase 4ì—ì„œ êµ¬í˜„
        # í˜„ì¬ëŠ” ë¹ˆ ëª©ë¡ ë°˜í™˜
        if self._tool_registry is None:
            return []
        
        # TODO: ToolRegistry.list_tools() í˜¸ì¶œ
        return []
